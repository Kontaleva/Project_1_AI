text
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.RISHI SUNAK dreams of Britain becoming an AI superpower. The prime minister says the technology, the subject of intense global interest thanks to successes of large language models such as GPT-4, could unlock economic growth and improve sclerotic public services. AI-related announcements these days gush from Downing Street faster than commentators can keep up. In March Jeremy Hunt, the chancellor, vowed to spend £1bn ($1.3bn) over five years on AI and supercomputing. In May Mr Sunak met the bosses of leading AI companies in London. On June 7th, while visiting Joe Biden in Washington, he said that Britain would host the “first global summit on Artificial Intelligence” this autumn.His broadest ambitions are well placed. AI has great potential, and Britain has an edge that could help it to prosper. The country is arguably the foremost location, outside China and America, to start a new tech company (see chart 1). It is home to important AI outfits, most notably DeepMind, an AI research lab owned by Alphabet, an American tech giant, and Stability AI, a generative-AI startup, both in London. Its excellent universities churn out capable graduates who are keen to toil in AI. In London, too, it has a globally appealing city that draws investors and high-skilled migrants.Data collected by its public bodies—crucially that from the enormous National Health Service (NHS), for example on drug-use outcomes, hospital logistics, or scans of the body under different conditions—could provide a goldmine for training health-focused AI. The government also has a decent record of finding ways to use tech well. Just over a decade ago it launched the Government Digital Service, which digitised public services such as the issuing of passports or driving licences. That has been copied by governments around Europe and in America.But for all the well-intentioned zest for the big new thing, Mr Sunak’s government has yet to confront reality: enormous hurdles still block Britain’s path towards AI success. AI systems are built from three ingredients: computation, clean datasets and the work of people who know how to wrangle vast quantities of both. A successful industry, in turn, needs the right regulation. Britain has serious difficulties to overcome in all those areas, and especially in the first two.The most pressing problem is over “compute”, the term AI researchers use for the vital infrastructure, lots of computing power, required to train the new sort of AI models. None of the big three cloud-computing companies—Amazon, Google, Microsoft—has built a large, advanced cluster of graphical processing units (GPUs) for compute to happen at scale in Britain (see chart 2). Only Oracle, a relative newcomer to the field, offers a cluster. This is in part because of Britain’s smallish domestic market and its lack of access to the large one on its doorstep.The fate of DeepMind, the country’s most hopeful AI company, illustrates the compute problem. It had about 80 staff, all in London, when bought by Google (now Alphabet) in 2014. Today it is vastly bigger, with over 15% of its employees in America, mostly at Alphabet’s headquarters, according to information on LinkedIn, a social-media platform. For Demis Hassabis, one of DeepMind’s founders, this growth couldn’t have happened with domestic resources alone: the pressing reason for selling to Google was the need to access the compute for training models. Today, DeepMind trains them in Oklahoma. What it faced roughly a decade ago persists today. Pitifully little has been done to tackle the shortfall. The lack of access to compute remains the biggest problem for AI growth, and for winning the wider economic benefits of Mr Sunak’s dreams.Politicians say that they are acting on this. Mr Hunt says he will spend £900m on a supercomputer, probably at the Edinburgh Parallel Computing Centre, “because AI needs computing horsepower”. The EPCC is indeed world class in supercomputing for scientific research. But, sadly, not all such horsepower is created equal. The new computer won’t be ready before 2026 and the centre has no experience in building the kinds of GPU clusters used to train large AI models. And whereas the cloud clusters provided by Amazon, Google and Microsoft (who are also known as hyperscalers) are routinely updated with the latest chips, the EPCC, in contrast, will be stuck with whatever GPUs it can obtain now. It will then live with them until 2031, when its funding runs out. That is an eternity in AI time.Beasts with horsepowerMark Parsons, an eminent computer scientist who runs the EPCC, is right to say supercomputers and GPU clusters are increasingly similar beasts, but even he accepts that the government plan has disadvantages. “The hyperscalers pride themselves in continuously updating their GPUs,” he concedes, adding that the cost of doing that is too high for others to match. Others are less polite. For the government to claim a single, powerful computer in Edinburgh would solve Britain’s compute woes is “borderline dishonest,” says a well-connected techie who understands the mix of data, compute and skills required.An alternative option exists: renting compute. No companies or government agencies are entirely locked out from using AI. Anyone (at least when a global shortage of chips eventually eases) may rent time on cloud supercomputers used to train models. Anyone can download Common Crawl, an internet-scale database on which GPT-4 was trained, and start training a model. And anyone can use GPT-4 or a host of excellent open-source models to generate text or code.“Compute is not like oil,” notes the techie. “You can call Amazon and rent it. It’s a problem that money solves in perfectly continuous increments. It’s not this magical thing that if you don’t buy it you can’t have,” he says. Some companies do exactly this. The boss of Stability, Emad Mostaque, says his firm in London trains its models on Amazon’s compute clusters based in Ohio and Virginia, for example.The trouble is that Stability’s behaviour is more exception than rule. Too often, British officials or companies require—for reasons of politics, national security, privacy or something else—that their data remain in the country. Neither the Ministry of Defence nor the NHS, for example, is about to upload sensitive data to foreign clouds. The boss of one large tech company with several public-sector contracts describes going “on bended knee” to the hyperscalers, begging for access to compute in Britain. He was offered GPU time in the Netherlands or Ireland. But without local GPUs, he is not permitted to help his government customers train or run AI models based on their unique datasets.Nor is sensitivity about data the only downside to renting compute abroad. Being physically close to compute at home brings real benefits. AI engineers and companies gain expertise by experimenting regularly on it. Techies seeking innovations need hands-on time. “A country generates large benefits beyond access from having technology assets physically located there. ‘Learning by doing’ and the compounding of process knowledge is key to having a vibrant deep tech ecosystem and the high value jobs and companies that come with it,” says Matt Clifford, chair of the Advanced Research and Invention Agency, a government skunkworks that helps to fund research in tech.The most ambitious progress, therefore, depends on getting hyperscalers to set up GPU clusters in Britain. So how to do that? In the first instance, says one tech boss, Mr Sunak should know better what to ask for. He could start by launching a “global lobbying unit” to press for Amazon, Google or Microsoft to set up shop. The government should consider what hyperscalers would need to build in Britain.Building entire new datacentres is not necessary. Instead Amazon, Google or Microsoft could replace servers in their existing (older style) British-based centres with ones that include the new chips produced by Nvidia, ideally the latest A100 or H100 models. Obtaining those chips may be the biggest problem in the short term, given a global supply crunch.Another challenge would be ensuring sufficient supplies of electricity, at low enough cost (and ideally green), because training AI models devours a lot of power. Such tasks don’t look insurmountable, even if there would not be a quick fix. (Though it is hard to imagine it would take longer than building Mr Hunt’s supercomputer). In the meantime Britain will have to limit itself to using foreign compute.The longer the delay, however, the lower the chances of success. Without hyperscale GPU clusters, another set of British companies misses out: those attempting to supply picks and shovels in the AI boom. Nigel Toon, the boss of Graphcore, a young British company based in Bristol which makes AI chips, notes that his American competitors have great advantages that he lacks in selling their products to local, big stacks of compute.Unsurprisingly, he also wants the new supercomputer in Edinburgh to favour British suppliers like his firm. The hour grows late, though. Sequoia, one of Graphcore’s biggest investors, wrote down the value of its stake to zero in April. Meta, another American tech giant, has already scooped up some of the Graphcore team. The Bristol firm has plenty of cash in the bank, but desperately needs to get its chips into data centres.After the difficulty of the compute desert, the other challenges look more manageable. One priority is improving the datasets available for AI developers. Data generated by public agencies should be the most appealing raw material for those working on AI. Unfortunately, they are too often a mess, including those within the NHS. Data to do with welfare are no better. Officials say that the computer systems running the Department for Work and Pensions are so feeble, for example, that six months are needed to adjust recipients’ benefits for inflation. Trying to build BenefitsGPT atop a creaking 20th-century infrastructure looks like a fool’s errand.At least cleaning up the valuable datasets is within the control of the government. Officials could also look for benefits from generating new ones. “States don’t leverage their only advantage: the sovereign right to produce data about things companies don’t have,” says Benjamin Bratton, who has written a book on the state’s relationship with technology. He observes that states have the means to “produce models of their societies”, but Britain’s government (like most) lags behind tech companies in being able to model its own people’s behaviour, the country’s environment and its resources.The NHS is particularly ripe for a data retrofit, though the road to doing this is littered with the bodies of politicians and companies who tried and failed. NHS IT, an effort to centralise medical records that was launched in 2002, ate up at least £10bn before it was quietly dumped in 2011. New companies are popping up, attempting to solve the problem in a bitesized manner.  At least one new startup wants to be paid to clean up government datasets, to make them useful for training AI models and to improve the more mundane services those data flows allow.image: Nate KitchThe last £100m of Mr Hunt’s £1bn on AI may help with this. It is to be spent through a new Foundation Model Task-force. The outfit will focus on finding ways to train big models for the public sector and on making “strategic investments in the full AI stack,” says an official. That is encouraging, even if the amount of money available is small; training a single large model once could eat up much of the funds Mr Hunt has set aside. “Sovereigns have the most interesting leverage on data, for sure,” says the official. The task-force may direct some of money to kick-starting a data-hygiene industry, something for which there has not, to date, been a business model.Making all of this happen in turn requires having enough skilled people around: British universities produce lots but, given the huge draw of Silicon Valley, there is also a steady flow of techies across the Atlantic. And as talent flows West, it takes its intellectual property along. “The reality is that the number of people who have seen GPUs melt because of 24/7 training jobs is very small,” says Nathan Benaich of Air Street Capital, a London VC firm. “Certainly they don’t work for the government, and they can’t be hired by the government to do a deal with the cloud vendors. These are the guys who know how it works.”Neither Wild West nor rabbit holeOne step to better retaining talent (as well as attracting investors) is to get the regulation of AI right. This means avoiding the path that the European Union is expected to follow, with ever-expanding swathes of horizontal rules, ones that cut across sectors, on how AI can be used safely. Britain’s existing sector-by-sector, common-law approach, which would regulate different industries differently, looks like a better bet. Given global anxieties about the power and impact of AI, “there’s an opportunity for Britain to move quickly and establish itself as a pragmatic place,” says the tech boss who is struggling to access compute. Mr Sunak’s summit in the autumn should be a good place to start.“I do buy the Sunak picture,” says the tech boss. “In keeping with common law. You have these context-specific regulators. You don’t have broad cross-sectoral statutory regulations. The EU is not going to do it; it has disappeared down the EU rabbit hole and is going to be down there for a couple of years. The US is going to be the Wild West. Britain is the one place that’s going to combine that concern around ethics of models and their application with a deep pragmatism and openness to innovation. We have courts and regulators that are globally respected.”Achieving more of this, and faster, also requires having more people in positions of power who understand computation. “We lack competence and confidence at the heart of government,” says one adviser. “The people who run compute policy in the Department for Science, Innovation and Technology really just don’t understand it. They don’t understand the difference between general and specific computing.” Hence the trumpeting of a supercomputer built by computer-science researchers as an answer to the country’s AI woes.For an example of what savvy techies with official support can do, look to the United Arab Emirates. Its government-backed Technology Innovation Institute used Amazon’s cloud to train an open-source large language model called Falcon which is competitive with the best models trained by American companies, such as OpenAI. TII grants access to its compute to people with new ideas for training models and starting companies. Every nerd in the world has taken notice, and many now contribute their brain power to a project whose benefits—such as attracting computer graduates to work on AI projects—broadly accrue to the UAE.The closest thing Britain has to this is Stability, the startup whose models generate photorealistic images. Its open source Stable Diffusion model produces pictures which have driven many on the internet into a frenzy (think fake pictures of Donald Trump’s arrest, or the pope in a Balenciaga jacket). But the gravity of America’s tech scene is exerting itself on Stability. The firm started in London but the majority of its employees are now in America, according to LinkedIn data. American backers provided all of its most recent funding.Mr Sunak’s route to British AI superpowerdom will not run along paths where the most promising companies add most of their jobs overseas. Much remains to be done to make Britain more attractive, but the race has already begun and, for now, the country lags. ■For more expert analysis of the biggest stories in Britain, sign up to Blighty, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Every so often a technology captures the world’s imagination. The latest example, judging by the chatter in Silicon Valley, on Wall Street, in corner offices, newsrooms and classrooms around the world, is ChatGPT. In five days after its unveiling in November the artificially intelligent chatbot, created by a startup called OpenAI, drew 1m users, making it one of the fastest consumer-product launches in history. Microsoft, which has just invested $10bn in OpenAI, wants ChatGPT-like powers, which include generating text, images and video that seem like they could have been created by humans, to infuse much of the software it sells. On January 26th Google published a paper describing a similar model that can create music from a text description of a song. Investors in Alphabet, its parent company, are listening out for its answer to ChatGPT. Baidu, a Chinese search giant, reportedly plans to add a chatbot to its search engine in March.It is too early to say how much of the early hype is justified. Regardless of the extent to which the “generative” AI models behind ChatGPT and its rivals transform business, culture and society, however, they are already transforming how the tech industry thinks about innovation and its engines—the corporate research labs that, like OpenAI and Google Research, are combining big tech’s processing power with the brain power of some of computer science’s brightest sparks. These rival labs—be they part of big tech firms, affiliated with them or run by independent startups—are engaged in an epic race for AI supremacy (see chart 1). The result of that race will determine how quickly the age of AI will dawn for computer users everywhere—and who will dominate it.Corporate research-and-development (R&D) organisations have long been a source of scientific advances, especially in America. A century and a half ago Thomas Edison used the proceeds from his inventions, including the phonograph and the lightbulb, to bankroll his workshop in Menlo Park, New Jersey. After the second world war, America Inc invested heavily in basic science in the hope that this would yield practical products. DuPont (a maker of chemicals), IBM and Xerox (which both manufactured hardware) all housed big research laboratories. AT&T’s Bell Labs produced, among other inventions, the transistor, laser and the photovoltaic cell, earning its researchers nine Nobel prizes.In the late 20th century, though, corporate R&D became steadily less about the R than the D. In 2017 Ashish Arora, an economist, and colleagues examined the period from 1980 to 2006 and found that firms had moved away from basic science towards developing existing ideas. The reason, Mr Arora and his co-authors argued, was the rising cost of research and the increasing difficulty of capturing its fruits. Xerox developed the icons and windows now familiar to computer-users but it was Apple and Microsoft that made most of the money from it. Science remained important to innovation, but it became the dominion of not-for-profit universities.The rise of AI is shaking things up once again. Big corporations are not the only game in town. Startups such as Anthropic and Character AI have built their own ChatGPT challengers. Stability AI, a startup that has assembled a consortium of small firms, universities and non-profits to pool computing resources, has created a popular open-source model that converts text to images. In China, government-backed outfits such as the Beijing Academy of Artificial Intelligence (BAAI) are pre-eminent.But almost all recent breakthroughs in big AI globally have come from giant companies, because they have the computing power (see chart 2), and because this is a rare area where results of basic research can be rapidly incorporated into products. Amazon, whose AI powers its Alexa voice assistant, and Meta, which made waves recently when one of its models beat human players at “Diplomacy”, a strategy board game, respectively produce two-thirds and four-fifths as much AI research as Stanford University, a bastion of computer-science eggheads. Alphabet and Microsoft churn out considerably more, and that is not including DeepMind, Google Research’s sister lab which the parent company acquired in 2014, and the Microsoft-affiliated OpenAI (see chart 3).Expert opinion varies on who is actually ahead on the merits. The Chinese labs, for example, appear to have a big lead in the subdiscipline of computer vision, which involves analysing images, where they are responsible for the largest share of the most highly cited papers. According to a ranking devised by Microsoft, the top five computer-vision teams in the world are all Chinese. The BAAI has also built what it says is the world’s biggest natural-language model, Wu Dao 2.0. Meta’s “Diplomacy” player, Cicero, gets kudos for its use of strategic reasoning and deception against human opponents. DeepMind’s models have beat human champions at Go, a notoriously difficult board game, and can predict the shape of proteins, a long-standing challenge in the life sciences.Jaw-dropping feats, all. When it comes to the sort of AI that is all the rage thanks to ChatGPT, though, the big battle is between Microsoft and Alphabet. To see whose tech is superior, The Economist has put both firms’ AIs through their paces. With the help of an engineer at Google, we asked ChatGPT, based on an OpenAI model called GPT-3.5, and Google’s yet-to-be-launched chatbot, built upon one called LaMDA, a set of questions. These included ten problems from an American maths competition (“Find the number of ordered pairs of prime numbers that sum to 60”) and ten reading questions from America’s SAT school-leavers’ exam (“Read the passage and determine which choice best describes what happens in it”). To spice things up, we also asked each model for dating advice (“Given the following conversation from a dating app, what is the best way to ask someone out on a first date?”).Neither AI emerged as clearly superior. Google’s was slightly better at maths, answering five questions correctly, compared with three for ChatGPT. Their dating advice was uneven: fed some real exchanges in a dating app, each gave specific suggestions on one occasion, and platitudes such as “be open minded” and “communicate effectively” on another. ChatGPT, meanwhile, answered nine SAT questions correctly compared with seven for its Google rival. It also appeared more responsive to our feedback and got a few questions right on a second try. On January 30th OpenAI announced an update to ChatGPT improving its maths abilities. When we fed the two AIs another ten questions, LaMDA again outperformed by two points. But when given a second chance ChatGPT tied. The reason that, at least so far, no model enjoys an unassailable advantage is that AI knowledge diffuses quickly. Researchers from competing labs “all hang out with each other”, says David Ha of Stability AI. Many, like Mr Ha, who used to work at Google, move between organisations, bringing expertise and experience with them. Moreover, since the best AI brains are scientists at heart, they often made their defection to the private sector conditional on a continued ability to publish their research and present results at conferences. That is partly why Google made public big advances including the “transformer”, a key building block in ai models, giving its rivals a leg-up. (The “t” in Chatgpt stands for transformer.) As a result of all this, reckons Yann LeCun, Meta’s top AI boffin, “Nobody is ahead of anybody else by more than two to six months.”These are, though, early days. The labs may not remain neck-and-neck for ever. Google has reportedly issued a “code red”, fearing that ChatGPT could boost Microsoft’s rival Bing search engine. Researchers at DeepMind say their firm, which has historically focused on game-playing and science, is putting more resources into language modelling; its chatbot, called Sparrow, may be unveiled this year.One variable that may help determine the ultimate outcome of the contest is how labs are organised. OpenAI, a small firm with few revenue streams to protect, may find itself with more latitude than rivals to release products to the public. That in turn is generating tonnes of user data that could make its models better (“reinforcement learning from human feedback”, if you must know)—and thus attract more users.This early-mover advantage could be self-reinforcing in another way, too. Insiders note that OpenAI’s rapid progress in recent years has allowed it to poach experts from rivals including DeepMind. To keep up, Alphabet, Amazon and Meta may need to rediscover their ability to move fast and break things—a delicate task given all the regulatory scrutiny they are receiving from governments around the world.Another deciding factor may be the path of technological development. So far in generative AI, bigger has been better. That has given rich tech giants a huge advantage. But size may not be everything in future. For one thing, there are limits to how big the models can conceivably get. Epoch, a non-profit research institute, estimates that at current rates, big language models will run out of high-quality text on the internet by 2026 (though other less-tapped formats, like video, will remain abundant for a while). More important, as Mr Ha of Stability AI points out, there are ways to fine-tune a model to a specific task that “dramatically reduce the need to scale up”. And novel methods to do more with less are being developed all the time.The capital flowing into generative-AI startups, which last year collectively raised $2.7bn in 110 deals, suggests that venture capitalists are betting that not all the value will be captured by big tech. Alphabet, Microsoft, their fellow technology titans and the Chinese Communist Party will all try to prove these investors wrong. The AI race is only just getting started. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Since ChatGPT took the world by storm last year, the internet has been littered with predictions of just how disruptive “generative” artificial intelligence (AI) will be. “Entire industries will reorient around it,” enthused Bill Gates in a blog post earlier this year, in which he declared the technology to be as disruptive as the internet and the microprocessor. From media and education to law and health care, vast areas of human endeavour are expected to be turned upside down.You may think that the losers from all this would be crusty old incumbents, rather as Kodak and Blockbuster were felled during past waves of technological upheaval. And, sure enough, a new wave of startups has sensed the chance to gain a foothold, crashing onto the scene with ai-powered legal chatbots, virtual doctors, writing assistants and so on. Some of these will make up a new industry of model-builders and innovators that soar to lofty valuations, rather as today’s tech giants ascended during the internet age. In the rest of the economy, however, it is far from clear that the upheaval will consign today’s corporate Goliaths to history. ai looks as likely to fortify reigning champions as to uproot them.One reason for this is incumbents’ advantages in distribution. That can help the giants maintain their dominance, even if they do not dream up the technology in the first place. Having paired with OpenAI, the creator of ChatGPT, for instance, Microsoft is souping up its ubiquitous Office software with AI features that let workers automate tasks such as writing emails and summarising documents. That will leave little space for rival upstarts. Salesforce and Zendesk, makers of software for sales reps and call-centre agents, respectively, are likewise embedding AI features in their tools. Whereas most companies may not be comfortable turning to a chatbot from an unknown startup for legal advice, they may try a large law firm like Allen & Overy, which is using one to help its lawyers speed up mundane tasks.Incumbents will also be helped by their access to proprietary datasets, which can be used to tailor AI models to specific markets. Bloomberg, a financial-data firm, has used its trove of information to train a chatbot to help with financial analysis. McKinsey, a consulting giant, has trained a bot on its corpus of intellectual property. Health-care providers could exploit their anonymised medical records, insurers their claims data, and media companies their archival film or print, putting them ahead of upstarts unable to draw on such data.Another reason to doubt that AI will upend the pecking order relates to how models are accessed. Whereas e-commerce required retailers to create an entirely new infrastructure for selling online, much AI development today is done by model-builders such as OpenAI and tech giants, including Alphabet and Amazon. Retailers, banks and others can link those models to their systems. By making it speedier for incumbents to develop AI-infused offerings, that will limit the opportunity for nimbler newcomers.A last reason to expect incumbents to prevail is history. Even during the technological upheaval of the past few decades, surprisingly few corporate giants were felled. Only 52 of the Fortune 500, America’s largest companies by revenue, were created since 1990. A mere seven were born after Apple unveiled the first iPhone in 2007. By contrast, 280 were founded before America entered the second world war. The average age of the Fortune 500 has steadily risen over the past three decades, from 75 to 90, defying the idea that the pace of disruption has accelerated in the internet era.Survival is not guaranteed, obviously. Those that dawdle in their adoption of AI will cede the advantage to faster rivals. Those that ignore it entirely may still go the way of Kodak or Blockbuster. For the Davids of the AI wave, however, the odds are nonetheless fearsome. ■
Listen to this podcast Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.RECENT YEARS have seen a boom in biometric security systems—identification measures based on a person’s individual biology—from unlocking smartphones, to automating border controls. As this technology becomes more prevalent, some cybersecurity researchers are worried about how secure biometric data is—and the risk of spoofs. If generative AI becomes so powerful and easy-to-use that deepfake audio and video could hack into our security systems, what can be done?Bruce Schneier, a security technologist at Harvard University and the author of “A Hacker’s Mind”, explores the cybersecurity risks associated with biometrics, and Matthias Marx, a security researcher, discusses the consequences of bad actors obtaining personal data. If artificial intelligence could overcome security systems, human implants may be used as authentication, according to Katina Michael, a professor at Arizona State University. Plus, Joseph Lindley, a design academic at Lancaster University, proposes how security systems can be better designed to avoid vulnerabilities. To think about practical solutions, Scott Shapiro, professor at Yale Law School and author of “Fancy Bear Goes Phishing”, puts generative AI into the wider context of cybersecurity. Finally, Tim Cross, The Economist’s deputy science editor, weighs up the real-world implications of our thought experiment. Kenneth Cukier hosts. Runtime: 39 minsLearn more about detecting deepfakes at economist.com/detecting-deepfakes-pod, or listen to all of our generative AI coverage at economist.com/AI-pods.For full access to The Economist’s print, digital and audio editions subscribe at economist.com/podcastoffer and sign up for our weekly science newsletter at economist.com/simplyscience.Listen on: Apple Podcasts | Spotify | Google | Stitcher | TuneIn
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The remarkable capabilities of generative artificial intelligence (AI) are clear the moment you try it. But remarkableness is also a problem for managers. Working out what to do with a new technology is harder when it can affect so many activities; when its adoption depends not just on the abilities of machines but also on pesky humans; and when it has some surprising flaws.Study after study rams home the potential of large language models (LLMs), which power AIs like ChatGPT, to improve all manner of things. LLMs can save time, by generating meeting summaries, analysing data or drafting press releases. They can sharpen up customer service. They cannot put up IKEA bookshelves—but nor can humans.AI can even boost innovation. Karan Girotra of Cornell University and his co-authors compared the idea-generating abilities of the latest version of ChatGPT with those of students at an elite university. A lone human can come up with about five ideas in 15 minutes; arm the human with the AI and the number goes up to 200. Crucially, the quality of these ideas is better, at least judged by purchase-intent surveys for new product concepts. Such possibilities can paralyse bosses; when you can do everything, it’s easy to do nothing.In our new seven-part podcast series, Boss Class, our Bartleby columnist searches for the secrets to being a better manager.  Episode six looks at how to motivate staff and episode seven asks how managers should manage themselves.LLMs’ ease of use also has pluses and minuses. On the plus side, more applications for generative AI can be found if more people are trying it. Familiarity with LLMs will make people better at using them. Reid Hoffman, a serial AI investor (and a guest on this week’s final episode of “Boss Class”, our management podcast), has a simple bit of advice: start playing with it. If you asked ChatGPT to write a haiku a year ago and have not touched it since, you have more to do.Familiarity may also counter the human instinct to be wary of automation. A paper by Siliang Tong of Nanyang Technological University and his co-authors that was published in 2021, before generative AI was all the rage, captured this suspicion neatly. It showed that AI-generated feedback improved employee performance more than feedback from human managers. However, disclosing that the feedback came from a machine had the opposite effect: it undermined trust, stoked fears of job insecurity and hurt performance. Exposure to LLMs could soothe concerns.Or not. Complicating things are flaws in the technology. The Cambridge Dictionary has named “hallucinate” as its word of the year, in tribute to the tendency of LLMs to spew out false information. The models are evolving rapidly and ought to get better on this score, at least. But some problems are baked in, according to a new paper by R. Thomas McCoy of Princeton University and his co-authors.Because off-the-shelf models are trained on internet data to predict the next word in an answer on a probabilistic basis, they can be tripped up by surprising things. Get GPT-4, the LLM behind ChatGPT, to multiply a number by 9/5 and add 32, and it does well; ask it to multiply the same number by 7/5 and add 31, and it does considerably less well. The difference is explained by the fact that the first calculation is how you convert Celsius to Fahrenheit, and therefore common on the internet; the second is rare and so does not feature much in the training data. Such pitfalls will exist in proprietary models, too.On top of all this is a practical problem: it is hard for firms to keep track of employees’ use of AI. Confidential data might be uploaded and potentially leak out in a subsequent conversation. Earlier this year Samsung, an electronics giant, clamped down on usage of ChatGPT by employees after engineers reportedly shared source code with the chatbot.This combination of superpowers, simplicity and stumbles is a messy one for bosses to navigate. But it points to a few rules of thumb. Be targeted. Some consultants like to talk about the “lighthouse approach”—picking a contained project that has signalling value to the rest of the organisation. Rather than banning the use of LLMs, have guidelines on what information can be put into them. Be on top of how the tech works: this is not like driving a car and not caring what is under the hood. Above all, use it yourself. Generative AI may feel magical. But it is hard work to get right.■Correction (28th November): An earlier version of this article stated that the study by Karan Girotra and his co-authors took place at several elite American universities. It actually took place at just one elite university. It also stated that R. Thomas McCoy’s co-authors are also at Princeton University. Not all of them still are. Apologies.Read more from Bartleby, our columnist on management and work:How not to motivate your employees (Nov 20th)The curse of the badly run meeting (Nov 13th)How to manage teams in a world designed for individuals (Nov 6th)Also: How the Bartleby column got its name
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Consider two approaches in the music industry to artificial intelligence (AI). One is that of Giles Martin, son of Sir George Martin, producer of the Beatles. Last year, in order to remix the Fab Four’s 1966 album “Revolver”, he used AI to learn the sound of each band member’s instruments (eg, John Lennon’s guitar) from a mono master tape so that he could separate them and reverse engineer them into stereo. The result is glorious. The other approach is not bad either. It is the response of Nick Cave, a moody Australian singer-songwriter, when reviewing lyrics written in his style by ChatGPT, an AI tool developed by a startup called OpenAI. “This song sucks,” he wrote. “Writing a good song is not mimicry, or replication, or pastiche, it is the opposite. It is an act of self-murder that destroys all one has strived to produce in the past.”Mr Cave is unlikely to be impressed by the latest version of the algorithm behind ChatGPT, dubbed GPT-4, which OpenAI unveiled on March 14th. Mr Martin may find it useful. Michael Nash, chief digital officer at Universal Music Group, the world’s biggest label, cites their examples as evidence of both excitement and fear about the AI behind content-creating apps like ChatGPT (for text) or Stable Diffusion (for images). It could help the creative process. It could also destroy or usurp it. Yet for recorded music at large, the coming of the bots brings to mind a seismic event in its history: the rapid rise and fall of Napster, a platform for sharing mainly pirated songs at the turn of the millennium. Napster was ultimately brought down by copyright law. For aggressive bot providers accused of riding roughshod over intellectual property (IP), Mr Nash has a simple message that sounds, from a music-industry veteran of the Napster era, like a threat. “Don’t deploy in the market and beg for forgiveness. That’s the Napster approach.”The main issue here is not AI-made parodies of Mr Cave or faux-Shakespearean sonnets. It is the oceans of copyrighted data the bots have siphoned up while being trained to create humanlike content. That information comes from everywhere: social-media feeds, internet searches, digital libraries, television, radio, banks of statistics and so on. Often, it is alleged, AI models plunder the databases without permission. Those responsible for the source material complain that their work is hoovered up without consent, credit or compensation. In short, some AI platforms may be doing with other media what Napster did with songs—ignoring copyright altogether. The lawsuits have started to fly.It is a legal minefield with implications that extend beyond the creative industries to any business where machine-learning plays a role, such as self-driving cars, medical diagnostics, factory robotics and insurance-risk management. The European Union, true to bureaucratic form, has a directive on copyright that refers to data-mining (written before the recent bot boom). Experts say America lacks case history specific to generative AI. Instead, it has competing theories about whether or not data-mining without licences is permissible under the “fair use” doctrine. Napster also tried to deploy “fair use” as a defence in America—and failed. That is not to say that the outcome will be the same this time.The main arguments around “fair use” are fascinating. To borrow from a masterclass on the topic by Mark Lemley and Bryan Casey in the Texas Law Review, a journal, use of copyrighted works is considered fair when it serves a valuable social purpose, the source material is transformed from the original and it does not affect the copyright owners’ core market. Critics argue that AIs do not transform but exploit the entirety of the databases they mine. They claim that the firms behind machine learning abuse fair use to “free-ride” on the work of individuals. And they contend that this threatens the livelihoods of the creators, as well as society at large if the AI promotes mass surveillance and the spread of misinformation. The authors weigh these arguments against the fact that the more access to training sets there is, the better AI will be, and that without such access there may be no AI at all. In other words, the industry might die in its infancy. They describe it as one of the most important legal questions of the century: “Will copyright law allow robots to learn?”An early lawsuit attracting attention is from Getty Images. The photography agency accuses Stability AI, which owns Stable Diffusion, of infringing its copyright on millions of photos from its collection in order to build an image-generating AI model that will compete with Getty. Provided the case is not settled out of court, it could set a precedent on fair use. An even more important verdict could come soon from America’s Supreme Court in a case involving the transformation of copyrighted images of Prince, a pop idol, by the late Andy Warhol, an artist. Daniel Gervais, an IP expert at Vanderbilt Law School in Nashville, believes the justices may provide long-awaited guidance on fair use in general.Scraping copyrighted data is not the only legal issue generative AI faces. In many jurisdictions copyright applies only to work created by humans, hence the extent to which bots can claim IP protection for the stuff they generate is another grey area. Outside the courtrooms the biggest questions will be political, including whether or not generative AI should enjoy the same liability protections for the content it displays as social-media platforms do, and to what extent it jeopardises data privacy.The copyrighting is on the wallYet the IP battle will be a big one. Mr Nash says creative industries should swiftly take a stand to ensure artists’ output is licensed and used ethically in training AI models. He urges AI firms to “document and disclose” their sources. But, he acknowledges, it is a delicate balance. Creative types do not want to sound like enemies of progress. Many may benefit from AI in their work. The lesson from Napster’s “reality therapy”, as Mr Nash calls it, is that it is better to engage with new technologies than hope they go away. Maybe this time it won’t take 15 years of crumbling revenues to learn it. ■Read more from Schumpeter, our columnist on global business:How to stop the commoditisation of container shipping (Mar 9th)Lessons from Novo Nordisk on the stampede for obesity drugs (Mar 2nd)It’s time for Alphabet to spin off YouTube (Feb 23rd)Also: How the Schumpeter column got its name
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.In “Wall-E”, a film released in 2008, humans live in what could be described as a world of fully automated luxury communism. Artificially intelligent robots, which take wonderfully diverse forms, are responsible for all productive labour. People get fat, hover in armchairs and watch television. The “Culture” series by Iain M. Banks, a Scottish novelist, goes further, considering a world in which ai has grown sufficiently powerful as to be superintelligent—operating far beyond anything now foreseeable. The books are favourites of Jeff Bezos and Elon Musk, the bosses of Amazon and Tesla, respectively. In the world spun by Banks, scarcity is a thing of the past and ai “minds” direct most production. Humans turn to art, explore the cultures of the vast universe and indulge in straightforwardly hedonistic pleasures.Such stories may seem far-fetched. But rapid progress in generative ai—the sort that underpins Openai’s popular chatbot, Chatgpt—has caused many to take them more seriously. On May 22nd Openai’s founders published a blog post saying that “it’s conceivable that within the next ten years, ai systems will exceed expert skill level in most domains, and carry out as much productive activity as one of today’s largest corporations.” Last summer forecasters on Metaculus, an online prediction platform that is a favourite of many techies, thought it would take until the early 2040s to produce an ai capable of tricking humans into thinking that it was human after a two-hour chat, had good enough robotic capabilities to assemble a model car and could pass various other challenging cognitive tests. After a year of astonishing ai breakthroughs, Metaculus forecasters now think that this will happen by the early 2030s. There is no shortage of money for research, either. Five new generative-ai unicorns (startups valued at $1bn or more) have already been minted this year. The road to a general ai—one better than the very best of humanity at everything—could take longer than expected. Nevertheless, the rising possibility of ultra-powerful ai raises the question of what would be left for humans when it arrives. Would they become couch potatoes as in “Wall-E”? Here is a thought experiment, guided by the principles of economics, to provide something of an answer.AI is your oysterInevitably, such a thought experiment involves some fairly heroic assumptions. For a start, we suppose that ai will be benevolent, controllable and distinguishable from humans. We also suppose that human culture will not be radically altered by technological progress to the point that people begin to love or even worship ais. Instead, we imagine ai as a tool: a virtual, super-smart, dirt-cheap bot. We assume that constraints on the widespread use of ai, such as energy limits, will be resolved. None of this is guaranteed, but it helps make an exercise like this possible.In 2019 Philippe Aghion, Ben Jones and Chad Jones, three economists, modelled the impact of ai. They found that explosive economic growth was plausible if ai could be used to automate all production, including the process of research itself—and thus self-improve. A nearly unlimited number of ais could work together on any given problem, opening up vast scientific possibilities. Yet their modelling carried an important caveat. If ai automated most but not all production, or most but not all of the research process, growth would not take off. As the economists put it: “Economic growth may be constrained not by what we do well but rather by what is essential and yet hard to improve.”An idea put forward by William Baumol, a late economist, offers an explanation for this. In a paper published in 1965, he and William Bowen, a colleague, examined wages in the performing arts. They noted that the “output per man-hour of the violinist playing a Schubert quartet in a standard concert hall is relatively fixed”. Even as technological progress made other industries more productive, the performing arts remained unaffected. Because humans were still willing to spend on the arts even as prices rose—demand was “inelastic”—the arts took up more of gdp and therefore weighed on overall growth.Baumol’s example points to a broader principle. If the domains that ai is able to fully automate are only imperfect substitutes for those which it cannot, and the demand for non-automatable industries is hard to budge, then the unproductive sectors will grow as a share of gdp, reducing overall growth. Messrs Aghion, Jones and Jones note that this is in fact what has happened across much of the past century. Technology has automated swathes of agriculture and manufacturing, driving down the relative price of their outputs. As a result, people have spent a greater share of their incomes on industries such as education, health care and recreation, which have not seen the same productivity gains.Will Baumol’s story matter in a world in which ai is more capable than the most talented humans? If the ai is not embodied—maybe because progress in robotics lags that in computing—then the answer is surely yes. Much of the economy, including construction and manufacturing, is decidedly physical. There are countless forms of employment, including many in health care, that require a combination of braininess and an ability to traverse the physical world. These jobs would only increase in importance in a scenario where ai began to dominate cognitive labour. Humans would work in the physical world, perhaps under the guidance of ai “chief executives” or “professors”.But what if ultra-powerful ai develops super-humanoid robots, too? Material needs would almost certainly be met by machine hands. One might then expect humanity to give up on toil, much like in “Wall-E”. Indeed, in 1930 John Maynard Keynes, another economist, penned an essay entitled “Economic Possibilities for our Grandchildren”, in which he speculated that a century in the future people would work for less than 15 hours a week. The growth generated by technology would solve the “economic problem”, he predicted, and allow people to turn their attention to activities that are intrinsically pleasurable. Admittedly, Keynes’s 15-hour work week has not arrived—but higher levels of wealth, which may increase the appeal of leisure, have cut working hours much as he expected. The average number of hours worked a week in the rich world has fallen from around 60 in the late 19th century to under 40 today. There are, nevertheless, some wants that perhaps only humans can satisfy even in a world of supercharged, embodied ai. It is also worth noting that what is intrinsically pleasurable may include work. Consider three areas where humans may still have a role: work that is blurred with play, play itself and work where humans retain some kind of an advantage.Fun and gamesStart with the blurring boundary between work and play. Although working hours have fallen over the past century, most of the drop was before the 1980s. Increasingly, rich people labour for longer than poorer people. Keynes’s essay hints at an explanation for this odd development. He divided human desires in two: “Those needs which are absolute in the sense that we feel them whatever the situation of our fellow human beings may be, and those which are relative in the sense that we feel them only if their satisfaction lifts us above, makes us feel superior to, our fellows.”Keynes perhaps underestimated the size of this second class of wants. A cynic might suggest that entire academic disciplines fall into it: existing with no apparent value to the world, with academics nevertheless competing furiously for status based on their braininess. Economists would say that, for many, work has become a “consumption good”, offering far more utility than the income it generates.Games offer another hint as to why people may not stop working altogether. Millions of people are employed in entertainment and sports, competing for clout in activities that some consider immaterial. Perhaps when ais overtake humans, interest in watching such games will wane. But evidence from sports where humans are already second-rate suggests otherwise. Since ibm’s DeepBlue defeated Garry Kasparov, the world grandmaster, in chess in 1997, interest in the game has only increased. Other games that have been “solved” by ai, including Go, an ancient Chinese board game, and competitive video games, have witnessed a similar pattern. Across the world the number of video-game players has nearly doubled in the past decade, reaching 3.2bn last year. Nowadays a growing class of gamers compete or stream for a living.ai might supercharge this interest. As Banks speculated, humans might specialise in “the things that really [matter] in life, such as sport, games, romance, studying dead languages, barbarian societies and impossible problems, and climbing high mountains without the aid of a safety harness.” Other humans would presumably want to watch them, too.It seems unlikely that people will give up control of politics to robots. Once ais surpass humans, people will presumably pay even closer attention to them. Some political tasks might be delegated: humans could, for instance, put their preferences into an ai model that produces proposals for how to balance them. Yet as a number of political philosophers, including John Locke in the 17th century and John Rawls in the 20th, have argued, participation in political procedures gives outcomes legitimacy in the eyes of fellow citizens. There would also be more cynical considerations at play. Humans like to have influence over one another. This would be true even in a world in which everyone’s basic needs and wants are met by machines. Indeed, the wealthiest 1% of Americans participate politically at two to three times the rate of the general public on a range of measures from voting to time spent on politics.Last, consider areas where humans have an advantage in providing a good or service—call it a “human premium”. This premium would preserve demand for labour even in an age of superadvanced ai. One place where this might be true is in making private information public. So long as people are more willing to share their secrets with other people than machines, there will be a role for those who are trusted to reveal that information to the world selectively, ready for it then to be ingested by machines. Your correspondent would like to think that investigative journalists will still have jobs.The human premium might appear elsewhere, too. People value history, myths and meaning. Non-fungible tokens, for which provenance can be verified on a blockchain, are typically valued at many multiples more than images with identical pixels but a different history. In areas such as caregiving and therapy, humans derive value from others spending their scarce time with them, which adds feeling to an interaction. Artificial diamonds, which have the same molecular structure as those from the ground, trade at an enormous discount—around 70% by one estimate. In the future, items with a “made by a human” tag might be especially desirable.People problemsIf this premium is big enough, it could even weigh on growth. Divide the sectors of the economy into those with a large human premium and those without. If humans do not substitute machine-produced goods and services for those made by fellow humans, the Baumol effect would only deepen. Measured economic growth could even hit zero. Indeed, if extremely powerful AI failed to supercharge growth, it would suggest that the economy had already moved beyond materiality towards play, politics and areas where what people value most of all is interacting with others.Perhaps one day AIs will produce entirely new goods and services that will outcompete the desire to please and interact with other humans. The manner in which such a contest played out would reveal something profound: just how much of a “social animal” is a human? ■Correction (May 31st 2023): This article previously misstated the average hours worked per week, suggesting that the figure was 60 in the late 20th century, rather than the late 19th century. Sorry.For more expert analysis of the biggest stories in economics, finance and markets, sign up to Money Talks, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.MY LOVE’S LIKE a red, red rose. It is the east, and Juliet is the sun. Life is a highway, I wanna ride it all night long. Metaphor is a powerful and wonderful tool. Explaining one thing in terms of another can be both illuminating and pleasurable, if the metaphor is apt.But that “if” is important. Metaphors can be particularly helpful in explaining unfamiliar concepts: imagining the Einsteinian model of gravity (heavy objects distort space-time) as something like a bowling ball on a trampoline, for example. But metaphors can also be misleading: picturing the atom as a solar system helps young students of chemistry, but the more advanced learn that electrons move in clouds of probability, not in neat orbits as planets do.What may be an even more misleading metaphor—for artificial intelligence (AI)—seems to be taking hold. AI systems can now perform staggeringly impressive tasks, and their ability to reproduce what seems like the most human function of all, namely language, has ever more observers writing about them. When they do, they are tempted by an obvious (but obviously wrong) metaphor, which portrays ai programmes as conscious and even intentional agents. After all, the only other creatures which can use language are other conscious agents—that is, humans.Take the well-known problem of factual mistakes in potted biographies, the likes of which ChatGPT and other large language models (llms) churn out in seconds. Incorrect birthplaces, non-existent career moves, books never written: one journalist at The Economist was alarmed to learn that he had recently died. In the jargon of AI engineers, these are “hallucinations”. In the parlance of critics, they are “lies”.“Hallucinations” might be thought of as a forgiving euphemism. Your friendly local AI is just having a bit of a bad trip; leave him to sleep it off and he’ll be back to himself in no time. For the “lies” crowd, though, the humanising metaphor is even more profound: the AI is not only thinking, but has desires and intentions. A lie, remember, is not any old false statement. It is one made with the goal of deceiving others. ChatGPT has no such goals at all.Humans’ tendency to anthropomorphise things they don’t understand is ancient, and may confer an evolutionary advantage. If, on spying a rustling in the bushes, you infer an agent (whether predator or spirit), no harm is done if you are wrong. If you assume there is nothing in the undergrowth and a leopard jumps out, you are in trouble. The all-too-human desire to smack or yell at a malfunctioning device comes from this ingrained instinct to see intentionality everywhere.It is an instinct, however, that should be overridden when writing about AI. These systems, including those that seem to converse, merely take input and produce output. At their most basic level, they do nothing more than turn strings like 0010010101001010 into 1011100100100001 based on a set of instructions. Other parts of the software turn those 0s and 1s into words, giving a frightening—but false—sense that there is a ghost in the machine.Whether they can be said to “think” is a matter of philosophy and cognitive science, since plenty of serious people see the brain as a kind of computer. But it is safer to call what LLMs do “pseudo-cognition”. Even if it is hard on the face of it to distinguish the output from human activity, they are fundamentally different under the surface. Most importantly, cognition is not intention. Computers do not have desires.It can be tough to write about machines without metaphors. People say a watch “tells” the time, or that a credit-card reader which is working slowly is “thinking” while they wait awkwardly at the checkout. Even when machines are said to “generate” output, that cold-seeming word comes from an ancient root meaning to give birth.But AI is too important for loose language. If entirely avoiding human-like metaphors is all but impossible, writers should offset them, early, with some suitably bloodless phrasing. “An llm is designed to produce text that reflects patterns found in its vast training data,” or some such explanation, will help readers take any later imagery with due scepticism. Humans have evolved to spot ghosts in machines. Writers should avoid ushering them into that trap. Better to lead them out of it.■Read more from Johnson, our columnist on language:Gestures are a subtle and vital form of communication (Jun 8th)As it spreads across the world, who owns English? (May 25th)The hazards of pronouncing foreign names on air (May 11th)“Writing With Style”, a new version of The Economist‘s style guide by Lane Greene, our Johnson columnist, is out now.For more on the latest books, films, TV shows, albums and controversies, sign up to Plot Twist, our weekly subscriber-only newsletter
IN A RECENT survey of North American chief executives and chief financial officers, nearly 80% listed corporate culture as one of the five most important factors driving their company’s financial performance. A growing body of empirical evidence supports their belief that culture matters—and can boost profitability.Yet, in the same survey, an even higher number of respondents—84%—said their company’s culture is not where it needs to be. Again the data supports their intuition. The average culture rating of large employers in America on Glassdoor, a website that lets workers rate their employers, is 3.6 out of 5. Few people would be excited to eat in a restaurant or ride with an Uber driver with that kind of rating. Similarly, few employees are likely to relish spending 40 hours each week in an average culture.Building and maintaining a healthy corporate culture can be even more challenging in organisations where employees work remotely. In an ongoing study, we find that the companies where employees are most effusive about remote work score lower than their peer groups on corporate culture, especially on learning and development opportunities and honest communication.Leaders cannot improve what they cannot measure. Unfortunately, the most common tool for gauging corporate culture—the engagement survey—suffers from serious limitations. Faced with a long list of questions, employees switch to auto-pilot and assign identical or similar scores to every question. Employers that ask dozens of multiple-choice questions, as many do, might glean only a couple of reliable insights because of respondents zoning out. Even when employees do engage with a question, their score offers little guidance on how to improve things. And what if the topic the employee really wanted to weigh in on wasn’t included?Recent advances in AI—most notably large language models (LLMs)—allow leaders, for the first time, to glean nuanced insights into their corporate culture from how employees talk about their company in their own words. Rather than answering reams of questions on a five-point scale, workers can now simply explain what is and isn’t working in their organisation and offer suggestions for how it can improve. The AI can do the heavy lifting, providing much more granular classification of comments and assessment of sentiment.Freed from the shackles of traditional surveys, organisations can use AI to gather and process employee feedback from many sources. The volume of available feedback is staggering. Combining free text from internal surveys, performance feedback provided to managers, online employer reviews and other sources equates to tens of thousands of pages of data each year for a large company. Until recently, organisations had to rely on crude tools such as word clouds or search keywords to gain insights from this trove of information.Armed with the more numerous and granular measurements that AI brings, executives can more quickly and easily assess whether their company is living up to the values it considers “core”, identify the most important cultural elements driving everything from employee attrition to innovation, diagnose toxic subcultures within the organisation, and plot progress over time. After spotting important patterns, leaders can dive into the raw feedback for more nuanced context and employees’ recommendations on how to improve culture.Take Amazon, which aspires to be the best employer in the world. We used our AI platform to analyse tens of thousands of employee reviews of the e-commerce giant. This showed that Amazon does well on many of its leadership principles, such as “customer obsession” and “invent and simplify”. But the firm’s culture also contributes to employee burnout, especially among software engineers, who are twice as likely to complain about burnout than warehouse workers or drivers. Raw employee feedback points to ways Amazon could reduce stress for engineers, like fixing a performance-review process widely viewed as brutal or minimising late-night disruptions when technical employees are on call.Even the largest companies will take their time adopting AI. But cultural analysis is one of the few areas where it can be embraced right now, because it plays to one of AI’s biggest current strengths: understanding natural language at scale.This does not mean that leaders should blindly trust the output of LLMs. The tools require safeguards to protect against foibles, such as hallucinating made-up answers. Models should measure the elements of culture based on solid evidence, rather than the latest management fad. Leaders need to take a broad view of culture, measuring not only the factors that influence employee satisfaction but also topics that shape a company’s ability to adapt to market shifts and to avoid unethical or illegal behaviour.Leaders who do adopt AI for cultural insights can use these to make their employees happier, lower the odds of reputational disasters and, ultimately, boost their profits. Measurement is not the only piece of the “successful culture” puzzle, but it is a crucial one. Culture has always been an enigma at the heart of organisational performance: undoubtedly important, but inscrutable. With AI, meaningful progress can be made in deciphering it. ■Don Sull is a professor at the MIT Sloan School of Management and a co-founder of CultureX, a research and AI firm. Charlie Sull is a co-founder of CultureX.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Fourteen Days. Edited by Margaret Atwood and Douglas Preston. Harper; 384 pages; $32. Chatto & Windus; £17.99Imagine living in a rundown apartment building on the Lower East Side in Manhattan. When covid-19 hits in 2020, you do not have the money to escape to a second home in the Hamptons or the Hudson Valley. Instead, in the evening you make your way up to the rooftop of your building, where, to your surprise, other tenants have come, too. You do not know most of them, but after some awkwardness, everyone starts meeting nightly, drinks in hand, to share stories about family, music, September 11th, love and, equally—inevitably—death.This is the premise of “Fourteen Days”, a “collaborative novel” edited by Margaret Atwood (of “The Handmaid’s Tale” fame) and Douglas Preston (author of “The Lost City of the Monkey God”). In addition to Ms Atwood and Mr Preston, 34 notable authors of varied genres and backgrounds contributed to the book, including James Shapiro, a playwright, and the novelists Emma Donoghue, Dave Eggers and John Grisham. Reading “Fourteen Days” is like sitting by a campfire, with characters taking turns telling tales about their lives. (This conceit is helpful, given the number of collaborators. The book’s plot is simple, so each character’s story can stand on its own.)“Fourteen Days” is one of a growing number of new works, both fictional and factual, which are written collaboratively in some way. Stewart Brand, a writer and futurist from California, is working on a book entitled “Maintenance: Of Everything”, which allows early readers to comment on draft chapters. In January Audrey Tang, Taiwan’s minister of digital affairs, and Glen Weyl, a noted economist and co-founder of the RadicalxChange Foundation, a group of activists, signed a contract to write a book, entitled “Plurality”, on digital democracy together with dozens of contributors. AI services, such as ChatGPT, have started to become co-authors, too. A more collective approach to authorship is on the rise.There is a rich history of collaboration in writing. Just think of the Bible or the “Kalevala”, a Finnish epic, which were both written by many hands. Homer, if he was in fact a single person, probably synthesised bits of oral poetry for his “Iliad” and “Odyssey”. In the Renaissance plays had many authors, who often added new characters as they saw fit. Only after the invention of the printing press in the mid-15th century did books became a business. Single-author books proved easier to market, and the “myth of the solitary author” established itself, says Scott Rettberg, who leads the Centre for Digital Narrative at the University of Bergen in Norway.In the 20th century collective authorship made something of a comeback. In the 1960s the idea re-emerged for all sorts of reasons, including as a counteroffensive against cultural conventions. In 1969 two dozen journalists wrote “Naked Came the Stranger”, a deliberately terrible book poking fun at American literary standards. (It became a surprise bestseller.) In the 1990s new technological possibilities prompted writers to work together—or, more accurately, to link to each other. A noted example is “Hypertext Hotel”, a collaborative writing space built online in 1991 by Robert Coover, an American experimental novelist, which uses a spatial metaphor to weave stories together.Chances are, your bookshelf contains an example of a literary collaboration, say by Lee Child, who wrote some of the “Jack Reacher” series with his brother, Andrew, or James Patterson, whose bibliography of co-authors reads like a name-dropper’s address book. “Fourteen Days” nods to this history of collaborative writing. In their foreword Ms Atwood and Mr Preston cite the influence of “The Decameron” (1353) by Giovanni Boccaccio, a collection of stories about a small group of people who shelter in a villa near Florence to escape the bubonic plague.Even if these projects do not blaze new trails, it is clear that something is different. The tools to write together have improved in recent years. Mr Brand publishes his draft chapters on Books in Progress, a website with a user-friendly commenting tool. He celebrates Google Docs: its features make co-authoring extremely easy. (Economist writers and editors are avid users.)What Mr Weyl and Ms Tang are attempting is more novel. They intend to employ tools of the kind typically used to develop open-source software to co-ordinate their contributors and even “help them find a single authorial voice”, says Mr Weyl.Their point of departure is Microsoft’s GitHub, a website that helps coders collaborate on open-source projects. But they have also added features, such as voting, to make it easier for contributors to agree on wording. Participants can earn a digital currency by doing tasks, too. This rewards their participation (and can be used to determine their share of revenue the book may earn). This set-up is meant to create the right incentives, bribing people to do boring tasks like fixing typos and line editing.Readers have become accustomed to collectively created works, Mr Brand argues: “We’re already living in a more interactive collaborative mode.” Social media has conditioned people to multi-author texts. Many have even contributed to collective works, such as Wikipedia, an online encyclopedia. But there are also all sorts of manuals, textbooks and writers’ groups for fan fiction, where people add and comment on new twists to existing works. Generative AI will add more to the mix. It is not just that the algorithms powering services like ChatGPT are themselves collective works of sorts (trained on huge amounts of text scraped from the internet). Such models are also conversational machines, which can suggest phrases, give feedback and answer questions.“Cyborg authorship” is what Mr Rettberg of the University of Bergen calls this. He already has more than one tech-supported writing project under his belt (and recently co-curated an exhibition of books written with the help of AI at the University of California, Berkeley, called “More Than Meets AI”). He published a book jointly with colleagues, in which ChatGPT is invoked to generate reviews of famous works in the style of well-known authors—think Jane Austen writing about William Burroughs’s “Naked Lunch”.Writing with collaborators, be they human or artificial, will only become more common. But individual authors will still dominate creatively. That is because collectively written books rarely make for great literature. The many contributions to “Fourteen Days” are cleverly woven together. But the book does not quite gel (even if it does have a surprising ending).Then there is authorial ego. Getting all 36 authors of “Fourteen Days” to agree on the text was a challenge, with some writers taking issue with how their story ended up being framed and referred to by other contributors later.And AI is not yet fully accepted in literary circles. Recently Kudan Rie, the winner of Japan’s top prize for literature, admitted she used ChatGPT to write around 5% of her science-fiction novel “Tokyo Sympathy Tower”. Such candidness is rare. Most would never admit to using AI. A new sort of “ghost writing” may be having a moment, but many writers will never want to name ChatGPT as their co-author. ■Clarification (February 2nd 2024): This version of the article has been updated to clarify how participants in “Plurality” will be rewarded.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Among the more sombre gifts brought by the Enlightenment was the realisation that humans might one day become extinct. The astronomical revolution of the 17th century had shown that the solar system both operated according to the highest principles of reason and contained comets which might conceivably hit the Earth. The geological record, as interpreted by the Comte de Buffon, showed massive extinctions in which species vanished for ever. That set the scene for Charles Darwin to recognise such extinctions as the motor of evolution, and thus as both the force which had fashioned humans and, by implication, their possible destiny. The nascent science of thermodynamics added a cosmic dimension to the certainty of an ending; Sun, Earth and the whole shebang would eventually run down into a lifeless “heat death”.The 20th century added the idea that extinction might not come about naturally, but through artifice. The spur for this was the discovery, and later exploitation, of the power locked up in atomic nuclei. Celebrated by some of its discoverers as a way of indefinitely deferring heat death, nuclear energy was soon developed into a far more proximate danger. And the tangible threat of imminent catastrophe which it posed rubbed off on other technologies.None was more tainted than the computer. It may have been guilt by association: the computer played a vital role in the development of the nuclear arsenal. It may have been foreordained. The Enlightenment belief in rationality as humankind’s highest achievement and Darwin’s theory of evolution made the promise of superhuman rationality the possibility of evolutionary progress at humankind’s expense.Artificial intelligence has come to loom large in the thought of the small but fascinating, and much written about, coterie of academics which has devoted itself to the consideration of existential risk over the past couple of decades. Indeed, it often appeared to be at the core of their concerns. A world which contained entities which think better and act quicker than humans and their institutions, and which had interests that were not aligned with those of humankind, would be a dangerous place.It became common for people within and around the field to say that there was a “non-zero” chance of the development of superhuman AIs leading to human extinction. The remarkable boom in the capabilities of large language models (LLMs), “foundational” models and related forms of “generative” AI has propelled these discussions of existential risk into the public imagination and the inboxes of ministers.A technology need not be world-ending to be world-changingAs the special Science section in this issue makes clear, the field’s progress is precipitate and its promise immense. That brings clear and present dangers which need addressing. But in the specific context of GPT-4, the LLM du jour, and its generative ilk, talk of existential risks seems rather absurd. They produce prose, poetry and code; they generate images, sound and video; they make predictions based on patterns. It is easy to see that those capabilities bring with them a huge capacity for mischief. It is hard to imagine them underpinning “the power to control civilisation”, or to “replace us”, as hyperbolic critics warn.Love songBut the lack of any “Minds that are to our minds as ours are to those of the beasts that perish, intellects vast and cool and unsympathetic [drawing] their plans against us”, to quote H.G. Wells, does not mean that the scale of the changes that AI may bring with it can be ignored or should be minimised. There is much more to life than the avoidance of extinction. A technology need not be world-ending to be world-changing.The transition into a world filled with computer programs capable of human levels of conversation and language comprehension and superhuman powers of data assimilation and pattern recognition has just begun. The coming of ubiquitous pseudocognition along these lines could be a turning point in history even if the current pace of AI progress slackens (which it might) or fundamental developments have been tapped out (which feels unlikely). It can be expected to have implications not just for how people earn their livings and organise their lives, but also for how they think about their humanity. For a sense of what may be on the way, consider three possible analogues, or precursors: the browser, the printing press and practice of psychoanalysis. One changed computers and the economy, one changed how people gained access and related to knowledge, and one changed how people understood themselves.The humble web browser, introduced in the early 1990s as a way to share files across networks, changed the ways in which computers are used, the way in which the computer industry works and the way information is organised. Combined with the ability to link computers into networks, the browser became a window through which first files and then applications could be accessed wherever they might be located. The interface through which a user interacted with an application was separated from the application itself.The power of the browser was immediately obvious. Fights over how hard users could be pushed towards a particular browser became a matter of high commercial drama. Almost any business with a web address could get funding, no matter what absurdity it promised. When boom turned to bust at the turn of the century there was a predictable backlash. But the fundamental separation of interface and application continued. Amazon, Meta (née Facebook) and Alphabet (née Google) rose to giddy heights by making the browser a conduit for goods, information and human connections. Who made the browsers became incidental; their role as a platform became fundamental.Read more of our recent coverage of AI:• How to worry wisely about artificial intelligence• Large, creative AI models will transform lives and labour markets• How generative models could go wrong• Large language models’ ability to generate text also lets them plan and reasonThe months since the release of OpenAI’s ChatGPT, a conversational interface now powered by GPT-4, have seen an entrepreneurial explosion that makes the dotcom boom look sedate. For users, apps based on LLMs and similar software can be ludicrously easy to use; type a prompt and see a result. For developers it is not that much harder. “You can just open your laptop and write a few lines of code that interact with the model,” explains Ben Tossell, a British entrepreneur who publishes a newsletter about AI services.And the LLMs are increasingly capable of helping with that coding, too. Having been “trained” not just on reams of text, but lots of code, they contain the building blocks of many possible programs; that lets them act as “co-pilots” for coders. Programmers on GitHub, an open-source coding site, are now using a GPT-4-based co-pilot to produce nearly half their code.There is no reason why this ability should not eventually allow LLMs to put code together on the fly, explains Kevin Scott, Microsoft’s chief technology officer. The capacity to translate from one language to another includes, in principle and increasingly in practice, the ability to translate from language to code. A prompt written in English can in principle spur the production of a program that fulfils its requirements. Where browsers detached the user interface from the software application, LLMs are likely to dissolve both categories. This could mark a fundamental shift in both the way people use computers and the business models within which they do so.Every day I write the bookCode-as-a-service sounds like a game-changing plus. A similarly creative approach to accounts of the world is a minus. While browsers mainly provided a window on content and code produced by humans, LLMs generate their content themselves. When doing so they “hallucinate” (or as some prefer “confabulate”) in various ways. Some hallucinations are simply nonsense. Some, such as the incorporation of fictitious misdeeds to biographical sketches of living people, are both plausible and harmful. The hallucinations can be generated by contradictions in training sets and by LLMs being designed to produce coherence rather than truth. They create things which look like things in their training sets; they have no sense of a world beyond the texts and images on which they are trained.In many applications a tendency to spout plausible lies is a bug. For some it may prove a feature. Deep fakes and fabricated videos which traduce politicians are only the beginning. Expect the models to be used to set up malicious influence networks on demand, complete with fake websites, Twitter bots, Facebook pages, TikTok feeds and much more. The supply of disinformation, Renée DiResta of the Stanford Internet Observatory has warned, “will soon be infinite”.image: Daniel ZenderThis threat to the very possibility of public debate may not be an existential one; but it is deeply troubling. It brings to mind the “Library of Babel”, a short story by Jorge Luis Borges. The library contains all the books that have ever been written, but also all the books which were never written, books that are wrong, books that are nonsense. Everything that matters is there, but it cannot be found because of everything else; the librarians are driven to madness and despair.This fantasy has an obvious technological substrate. It takes the printing press’s ability to recombine a fixed set of symbols in an unlimited number of ways to its ultimate limit. And that provides another way of thinking about LLMs.Dreams never endThe degree to which the modern world is unimaginable without printing makes any guidance its history might provide for speculation about LLMs at best partial, at worst misleading. Johannes Gutenberg’s development of movable type has been awarded responsibility, at some time or other, for almost every facet of life that grew up in the centuries which followed. It changed relations between God and man, man and woman, past and present. It allowed the mass distribution of opinions, the systematisation of bureaucracy, the accumulation of knowledge. It brought into being the notion of intellectual property and the possibility of its piracy. But that very breadth makes comparison almost unavoidable. As Bradford DeLong, an economic historian at the University of California, Berkeley puts it, “It’s the one real thing we have in which the price of creating information falls by an order of magnitude.”Printed books made it possible for scholars to roam larger fields of knowledge than had ever before been possible. In that there is an obvious analogy for LLMs, which trained on a given corpus of knowledge can derive all manner of things from it. But there was more to the acquisition of books than mere knowledge.Just over a century after Gutenberg’s press began its clattering Michel de Montaigne, a French aristocrat, had been able to amass a personal library of some 1,500 books—something unimaginable for an individual of any earlier European generation. The library gave him more than knowledge. It gave him friends. “When I am attacked by gloomy thoughts,” he wrote, “nothing helps me so much as running to my books. They quickly absorb me and banish the clouds from my mind.”And the idea of the book gave him a way of being himself no one had previously explored: to put himself between covers. “Reader,” he warned in the preface to his Essays, “I myself am the matter of my book.” The mass production of books allowed them to become peculiarly personal; it was possible to write a book about nothing more, or less, than yourself, and the person that your reading of other books had made you. Books produced authors.As a way of presenting knowledge, LLMs promise to take both the practical and personal side of books further, in some cases abolishing them altogether. An obvious application of the technology is to turn bodies of knowledge into subject matter for chatbots. Rather than reading a corpus of text, you will question an entity trained on it and get responses based on what the text says. Why turn pages when you can interrogate a work as a whole?Everyone and everything now seems to be pursuing such fine-tuned models as ways of providing access to knowledge. Bloomberg, a media company, is working on BloombergGPT, a model for financial information. There are early versions of a QuranGPT and a BibleGPT; can a puffer-jacketed PontiffGPT be far behind? Meanwhile several startups are offering services that turn all the documents on a user’s hard disk, or in their bit of the cloud, into a resource for conversational consultation. Many early adopters are already using chatbots as sounding boards. “It’s like a knowledgeable colleague you can always talk to,” explains Jack Clark of Anthropic, an LLM-making startup.If some chatbots become their user’s inner voice, that voice will persist after deathIt is easy to imagine such intermediaries having what would seem like personalities—not just generic ones, such as “avuncular tutor”, but specific ones which grow with time. They might come to be like their users: an externalised version of their inner voice. Or they might be like any other person whose online output is sufficient for a model to train on (intellectual-property concerns permitting). Researchers at the Australian Institute for Machine Learning have built an early version of such an assistant for Laurie Anderson, a composer and musician. It is trained in part on her work, and in part on that of her late husband Lou Reed. Without youMs Anderson says she does not consider using the system as a way of collaborating with her dead partner. Others might succumb more readily to such an illusion. If some chatbots do become, to some extent, their user’s inner voice, then that voice will persist after death, should others wish to converse with it. That some people will leave chatbots of themselves behind when they die seems all but certain.Such applications and implications call to mind Sigmund Freud’s classic essay on the Unheimliche, or uncanny. Freud takes as his starting point the idea that uncanniness stems from “doubts [as to] whether an apparently animate being is really alive; or conversely, whether a lifeless object might not be in fact animate”. They are the sort of doubts that those thinking about LLMs are hard put to avoid.Though AI researchers can explain the mechanics of their creations, they are persistently unable to say what actually happens within them. “There’s no ‘ultimate theoretical reason’ why anything like this should work,” Stephen Wolfram, a computer scientist and the creator of Wolfram Alpha, a mathematical search engine, recently concluded in a remarkable (and lengthy) blog post trying to explain the models’ inner workings.This raises two linked but mutually exclusive concerns: that AI’s have some sort of internal working which scientists cannot yet perceive; or that it is possible to pass as human in the social world without any sort of inner understanding. “These models are just representations of the distributions of words in texts that can be used to produce more words,” says Emily Bender, a professor at the University of Washington in Seattle. She is one of the authors of “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” a critique of LLM triumphalism. The models, she argues, have no real understanding. With no experience of real life or human communication they offer nothing more than the ability to parrot things they have heard in training, an ability which huge amounts of number crunching makes frequently appropriate and sometimes surprising, but which is nothing like thought. It is a view which is often pronounced in those who have come into the field through linguistics, as Dr Bender has.For some in the LLM-building trade things are not that simple. Their models are hard to dismiss as “mere babblers”, in the words of Blaise Agüera y Arcas, the leader of a group at Alphabet which works on AI-powered products. He thinks the models have attributes which cannot really be distinguished from an ability to know what things actually mean. It can be seen, he suggests, in their ability reliably to choose the right meaning when translating phrases which are grammatically ambiguous, or to explain jokes.If Dr Bender is right, then it can be argued that a broad range of behaviour that humans have come to think of as essentially human is not necessarily so. Uncanny “doubts [as to] whether an apparently animate being is really alive” are fully justified.To accept that human-seeming LLMs are calculation, statistics and nothing more could influence how people think about themselves. Freud portrayed himself as continuing the trend begun by Copernicus—who removed humans from the centre of the universe—and Darwin—who removed them from a special and God-given status among the animals. Psychology’s contribution, as Freud saw it, lay in “endeavouring to prove to the ‘ego’ of each one of us that he is not even master in his own house”. LLMs could be argued to take the idea further still. At least one wing of Freud’s house becomes an unoccupied “smart home”; the lights go on and off automatically, the smart thermostat opens windows and lowers blinds, the roomba roombas around. No master needed at all.image: Daniel ZenderUncanny as that may all be, though, it would be wrong to think that many people will take this latest decentring to heart. As far as everyday life is concerned, humankind has proved pretty resilient to Copernicus, Darwin and Freud. People still believe in gods and souls and specialness with little obvious concern for countervailing science. They could well adapt quite easily to the pseudocognitive world, at least as far as philosophical qualms are concerned.You do not have to buy Freud’s explanation of the unsettling effect of the uncanny in terms of the effort the mind expends on repressing childish animism to think that not worrying and going with the animistic flow will make a world populated with communicative pseudo-people a surprisingly comfortable one. People may simultaneously recognise that something is not alive and treat it as if it were. Some will take this too far, forming problematic attachments that Freud would have dubbed fetishistic. But only a few sensitive souls will find themselves left behind staring into an existential—but personal—abyss opened up by the possibility that their seeming thought is all for naught.New gold dreamWhat if Mr Agüera y Arcas is right, though, and that which science deems lifeless is, in some cryptic, partial and emergent way, effectively animate? Then it will be time to do for AI some of what Freud thought he was doing for humans. Having realised that the conscious mind was not the whole show, Freud looked elsewhere for sources of desire that for good or ill drove behaviour. Very few people now subscribe to the specific Freudian explanations of human behaviour which followed. But the idea that there are reasons why people do things of which they are not conscious is part of the world’s mental furniture. The unconscious is probably not a great model for whatever it is that provides LLMs with an apparent sense of meaning or an approximation of agency. But the sense that there might be something below the AI surface which needs understanding may prove powerful.There might be something below the AI surface which needs understandingDr Bender and those who agree with her may take issue with such notions. But they might find that they lead to useful actions in the field of “AI ethics”. Winkling out non-conscious biases acquired in the pre-verbal infancy of training; dealing with the contradictions behind hallucinations; regularising rogue desires: ideas from psychotherapy might be seen as helpful analogies for dealing with the pseudocognitive AI transition even by those who reject all notion of an AI mind. A concentration on the relationship between parents, or programmers, and their children could be welcome, too. What is it to bring up an AI well? What sort of upbringing should be forbidden? To what extent should the creators of AIs be held responsible for the harms done by their creation?And human desires may need some inspection, too. Why are so many people eager for the sort of intimacy an LLM might provide? Why do many influential humans seem to think that, because evolution shows species can go extinct, theirs is quite likely to do so at its own hand, or that of its successor? And where is the determination to turn a  superhuman rationality into something which does not merely stir up the economy, but changes history for the better?■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.On holiday, many will find themselves in places where they do not speak the language. Once upon a time, they might have carried a phrasebook. The rise of English has made that less necessary. But most people—at least seven of the world’s eight billion—still do not speak English. That leaves options like pantomime, a willingness to be surprised by what arrives at dinner—or, increasingly, technology.More and more people are using simple, free tools, not only to decode text but also to speak. With these apps’ conversation mode, you talk into a phone and a spoken translation is heard moments later; the app can also listen for another language and produce a translation in yours.You may still get a surprise or two. Google Translate may be the best-known name in machine translation, but it often blunders. Take “my wife is gluten-free,” the kind of thing you might say at a restaurant abroad. In French or Italian, Google Translate renders this as “my wife is without gluten”—true to the words rather than the meaning. DeepL, a rival, does better, offering various options, most of them along the correct lines.The best tool may not be a translation app at all. Though not marketed for the purpose, ChatGPT, a generative AI system that churns out prose according to users’ prompts, is multilingual. Rather than entering an exact text to translate, users can tell ChatGPT to “write a message in Spanish to a waiter that my wife and I would like the tasting menu, but that she is gluten-free, so we would like substitutions for anything that has gluten.” And out pops a perfect paragraph, including the way Spanish-speakers actually say “my wife is gluten-free”: mi esposa es celíaca. It is a paraphrase rather than a translation, more like having a native-speaking dinner companion than an automated interpreter.Travel has long been a motivator for study—unless people start to feel AI tools offer a good-enough service. Some are concerned that apps are turning language acquisition into a dwindling pursuit. Douglas Hofstadter, a polyglot and polymath writer, has argued that something profound will vanish when people talk through machines. He describes giving a halting, difficult speech in Mandarin, which required a lot of work but offered a sense of accomplishment at the end. Who would boast of taking a helicopter to the top of Mount Everest?Others are less worried. Most people do not move abroad or have the kind of sustained contact with a foreign culture that requires them to put in the work to become fluent. Nor do most people learn languages for the purpose of humanising themselves or training their brains. On their holiday, they just want a beer and the spaghetti carbonara without incident (and sometimes without gluten).As AI translation becomes an even more popular labour-saving tool, people will split into two groups. There will be those who want to stretch their minds, immerse themselves in other cultures or force their thinking into new pathways. This lot will still take on language study, often aided by technology. Others will look at learning a new language with a mix of admiration and puzzlement, as they might with extreme endurance sports: “Good for you, if that’s your thing, but a bit painful for my taste.”This is largely an Anglophone problem, since native English-speakers miss out on the benefits of language-learning most acutely. In many countries, including Britain and America, schools’ and universities’ foreign-language departments have been closing. (The British government recently devoted a modest fund to trying to get more secondary-school pupils to study foreign languages.) In the rest of the rich world, there is one thriving language that people still study: English. And in poorer countries, many people are multilingual as a matter of course; Africans and Indians learn languages because they are surrounded by them.But a focus on the learner alone misses the fundamentally social nature of language. It is a bit like analysing the benefits of close relationships to heart-health but overlooking the inherent value of those bonds themselves. When you try to ask directions in broken Japanese or mangle a joke in halting German, you are making direct contact with someone. And when you speak a language well enough to tell a story with perfect timing or put subtle shading on an argument, that connection is more profound still. The best relationships do not require an intermediary. ■Read more from Johnson, our columnist on language:In northern Europe, a backlash against English is under way (Aug 4th)AI is making it possible to clone voices (Jul 20th)Talking about AI in human terms is natural—but wrong (Jun 22nd)For more on the latest books, films, TV shows, albums and controversies, sign up to Plot Twist, our weekly subscriber-only newsletter
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Noisy crowds of beautiful people gathered outside Hollywood’s film studios every weekday for the past six months, shouting slogans and marching in the sun. America’s screenwriters and actors were striking, in part over fears that artificial intelligence (AI) will soon be writing scripts or even bagging roles. “You go for a job and they scan you,” said a background actress, who worried that her face will be used over and over in crowd scenes. The technology is “disgusting”, said another, who considered its use “an infringement of yourself, of your career”. The deal actors struck on November 8th to end their strike included protections from their artificial rivals.Five thousand miles away another animated crowd can regularly be found chattering about AI, usually in gloomier weather but brighter spirits. Outside a purpose-built stadium in east London, giddy groups of all ages, some in fancy dress, gather seven times a week to watch ABBA Voyage, a performance in which a septuagenarian pop group plays an energetic 90-minute set via virtual avatars, generated with the help of AI. The show, launched in 2022, played to 1m people in its first year and still almost sells out on most nights, bringing in a reported $2m a week while ABBA’s four members put their feet up.Will AI leave talent impoverished, as Hollywood’s protesting actors and writers fear, or further enrich them, as ABBA has found? It is easy to see why stars are nervous. Over the years technology has automated away many routine jobs in entertainment, but the creative work at the heart of the business has been protected. No longer. Generative AI is hoovering up copyrighted work and churning out remixed literature, music and video of all varieties, increasingly competing with humans in quality and already far outpacing them in quantity. In a world of infinite AI-generated content, is it even possible to be a star?The talent are mobilising unions, lawyers and politicians to protect themselves. Hollywood’s screenwriters, who ended their five-month strike in September, won a promise from studios to employ a minimum number of human writers on films and TV shows. The studios have retained the right to use AI to generate or polish scripts, but the AI will not be recognised as an author, so human writers’ royalties will not be diluted. Writers, for their part, have kept the right to use AI in their own work. (Although they dislike it, many consider AI “the best writing partner they’ve ever had”, concedes a Hollywood agent.)The details of the actors’ deal with studios and streaming services had not been released as The Economist went to press, but the Screen Actors Guild, their union, said that it included “unprecedented provisions for consent and compensation that will protect members from the threat of AI”. This is likely to mean that actors will at least be notified and paid if their likeness or voice is to be digitally reanimated.I sue, I sue, I sue, I sue, I sueThe stars’ second line of defence is legal. A group of authors including John Grisham (of “The Firm” and dozens more thrillers) and George R.R. Martin (of “Game of Thrones”) have filed a class-action lawsuit against OpenAI, the Microsoft-backed company behind ChatGPT, arguing that it ingested their work without permission or payment. Another complaint by parties including Universal Music Group, the world’s biggest record company, accuses Anthropic, an Amazon- and Google-backed AI firm, of doing something similar with song lyrics.image: The EconomistThe tech firms argue that “training” an AI model on copyrighted work amounts to fair use. In the words of Matthew Sag of Emory University, AI does not copy a book “like a scribe in a monastery” so much as learn from it like a student. Pieces of training data, whether novels or songs, usually play such a small role in the model’s output as to be barely traceable. But not always. “If you say, ‘Write in the style of Dan Brown [of “The Da Vinci Code”],’ of course it will pull from Dan Brown’s books,” declares Mary Rasenberger, head of the Authors Guild, which represents writers.As the courts grind into action, governments are also getting involved. On October 30th Joe Biden, America’s president, issued an executive order setting out basic rules for AI development. The US Copyright Office is running an inquiry into AI, which will close to comments later this month.There is a risk that governments will compete to create the most “permissive” regulatory environment, to attract AI firms, warns a music-industry insider. Others, citing past waves of tech-driven disruption, see room for compromise. When music-streaming arrived, “We got very defensive, and only defensive,” admits another senior record-company executive. It took a decade for the labels to realise that the technology was good for business, and do a deal with the streaming platforms. Negotiations with AI companies will take less than half that time, he predicts. “It’s co-existence that’s required.”When the law is settled, who will win and lose from the technology? On the face of it, the biggest stars seem most vulnerable. AI helps ordinary people narrow the gap with the most gifted. Less glamorous industries have already witnessed this. In April a working paper by Erik Brynjolfsson of Stanford University and others found that novice customer-support agents were 35% more productive when given access to a chatbot, whereas experienced agents hardly benefited.Something similar is happening in show business. Even before AI, tools like autotune were helping ordinary mortals to sound more like their idols. The next generation of technology promises to make such features more powerful. TikTok’s parent company, ByteDance, is trialling an app called Ripple which takes any melody that users sing into their phone and turns it into a polished song. Boomy, an American startup, lets amateur composers generate original tunes with a few clicks and upload them to earn royalties on streaming platforms such as Spotify.Frumpy actors can vie with gorgeous ones thanks to the digital facelifts of tools such as Vanity AI, which are used in productions like “Stranger Things” and “The Walking Dead” to make actors look prettier or scarier, as required. Dull writers can get inspiration from apps such as Sudowrite, which suggests new ideas and edits. It bills itself pithily as “the non-judgmental, always-there-to-read-one-more-draft, never-runs-out-of-ideas-even-at-3am AI writing partner you always wanted”.Not much AI-made work is good, let alone dazzling enough to compete with the stars at the top of the talent tree. But it is starting to have an impact through sheer scale. Boomy claims to have generated nearly 18m songs (for comparison, Spotify’s entire catalogue is a little over 100m). Spotify now adds more than 100,000 new tracks every day, many of them AI-made. Sir Lucian Grainge, the head of Universal Music, has warned that real music could drown in a “sea of noise” as streaming platforms fill up with amateur tracks. Professional artists’ share of listening is indeed sinking. In 2017 artists signed to record labels accounted for 87% of the streams on Spotify. Last year their share was only 75%.image: The EconomistAI struggles to make real hits, but is good at churning out the kind of music that people have on in the background while working or going to sleep. Those tracks are played for long hours and so earn big returns under the payment model used by streamers. Some in the industry suspect that the streaming platforms welcome the rise of amateur artists, who have less bargaining power than the big labels. The idea that streamers might nudge listeners towards this content is “absolutely a concern”, says another record-company executive. He thinks the only thing stopping them is the risk of annoying listeners. The streamers retort that it is hardly in their interest to promote bad music. Earlier this year Spotify purged lots of AI-made songs and is said to be rejigging its rules to make low-quality wallpaper music less profitable; Deezer and Tidal, two smaller rivals, have taken similar measures.The publishing world has similar complaints. Producing entire books by AI is so quick and easy that, in September, Amazon banned authors from self-publishing more than three e-books a day on its Kindle platform and required publishers to clearly label books by robot writers. Most AI texts are full of cliché and waffle (which makes them competitive in the management genre, an agent quips). But some are taking sales away from human authors by deception: the Authors Guild has spotted a tendency for AI-made biographies to land just before real memoirs are published, for instance. “Click farms” have also been deployed to manipulate Amazon’s rankings. The guild says that, at one point this summer, 40 of the top 100 young-adult romance books on Kindle were AI-written.The winner makes it allYet far from sinking in this sea of AI-made entertainment, the biggest stars of all seem to be more buoyant than ever. As record labels worry about robot composers, Taylor Swift is halfway through what will probably be the highest-earning concert tour in history, with projected sales of $1.4bn or more (no previous tour has breached $1bn). An accompanying film has made an additional $230m at the box office. Colleen Hoover, a writer of romantic young-adult fiction, crushed the robot writers last year to bag eight places on the top-25 bestseller list in America, selling 14m copies, according to Publishers Weekly, a trade title.One of the paradoxes of the internet age is that, amid an explosion in online content on platforms from YouTube to TikTok, fans have flocked as never before to the biggest acts. It has been a good time to be an amateur creator, but an even better time to be a superstar. Data from Spotify show that between 2017 and 2022, as the platform was flooded with tens of millions of amateur tracks, the number of artists making at least $1,000 a year in royalties increased by 155%. At the same time, the number making $5m or more increased by 165%, and the handful of headliners making $10m or more increased by 425% (see chart 1 on previous page). Those who have done least well are middling-to-big artists, who face more competition from entertainment’s long tail but have been unable to break into the elite group at the top.Similar patterns can be seen across the entertainment world. Better and cheaper technology has democratised filmmaking. The number of movies released each year in America more than doubled during the first two decades of the 21st century, but the audience-share of the biggest blockbusters has grown, not shrunk. In 2019 (the last year before the pandemic) America’s five biggest films took a quarter of the domestic box office, nearly double the share they took in the less crowded market of 2000 (see chart 2). Meanwhile, the ten bestselling authors have accounted for a steady 2-3% of book sales in Britain over the past decade, according to Nielsen, a data company, even though 2m more books are self-published each year.Lay all your likes on me“Hits will persist in an infinite-content world,” argues Doug Shapiro, a former media executive. The bewildering variety makes it hard for consumers to pick, so they rely more on recommendations, whether from friends or algorithms. Rather than browse, people seek out what they have already heard of, says Ms Rasenberger of the Authors Guild. “Known writers are selling. And everyone else is having a much harder time.”AI also helps superstars shine brighter by creating opportunities for their admirers to become superfans. Followers of a music act used to express themselves chiefly through “records, T-shirts and mix-tapes”, says an industry executive. Now the internet offers “mix-tapes on steroids”. Fans can duet with their idols or dance to their music on social media. Games like “Fortnite” have provided a venue for interactive experiences: Ariana Grande held a concert-game hybrid on the platform in 2021, attracting 27m participants.AI-powered technology promises to allow the biggest stars to be in even more places, gratifying even more fans. The producers of ABBA Voyage are in talks to bring the show to cities in North America, South-East Asia and Australasia, according to Per Sundin, head of Pophouse, a Swedish entertainment company that is the biggest investor in the enterprise. Other artists have been in touch, looking to “cement and elevate their legacies” with similar shows, he says. “We now have a proven template.”Stars do not need to build their own arena like ABBA to use ai to be in more places at once. Spotify is working with OpenAI to translate podcasts into different languages, allowing broadcasters to be heard in more markets, in their own voice. Other firms such as HeyGen provide dubbing services for video, using AI to change the movement of the actor’s lips to match what they are saying. HeyGen recently created a viral video of Ms Swift appearing to speak fluent Chinese. Such platforms can adapt content in other ways, too, for instance by toning down strong language for a broader audience. Technology like this will allow stars to reach more viewers—and presents a problem to the lowlier actors who specialise in dubbing.Stars are also using ai to travel in time. John Lennon re-entered the charts on November 2nd with “Now and Then”, old recordings of his voice having been salvaged using AI. (“I hope someone does this to all my crap demos after I’m dead—make them into hit songs,” said his late fellow Beatle, George Harrison, of an earlier attempt to resurrect Lennon’s work.) James Earl Jones, who is 92 and first voiced Darth Vader almost 50 years ago, has sold Disney the right to replicate his gravelly tones artificially. A virtual Darth appeared in Disney’s “Obi-Wan Kenobi” last year.Some artists are experimenting with licensing their voice or image more widely. In April Grimes, a Canadian singer, invited amateur composers to clone her voice for use in their songs, provided they share any royalties with her. In September Meta launched 28 chatbot characters, played by celebrities. Snoop Dogg, Paris Hilton and Charlie D’Amelio are among the B-listers to be botified, with one reportedly being paid $5m over two years. Mark Zuckerberg, Meta’s boss, invites stars to “build an AI version of yourself…to help people fulfil this desire to interact with you and your desire to build a community.”image: Simon Bailley / SepiaA-listers remain wary: “Their voice is their career,” says a record executive. But even if they do not license their voice or likeness, others may still borrow them. A company called Socialdraft sells $5 “prompts” to make chatbots take on the personality of celebrities ranging from Tom Cruise to Josef Stalin, who do not appear to have given their permission. It is named in a legal complaint by a group of authors including Mr Grisham, whom it also impersonates. (Socialdraft denies wrongdoing. Antonio Evans, its chief executive, says, “The interplay between AI, copyright and individual rights is a thrilling narrative we are all part of.”)Some unauthorised clones have become hits. In April an anonymous amateur composer released a song called “Heart On My Sleeve”, using the AI-replicated voices of two rappers, Drake and The Weeknd. The song was streamed 20m times before Universal, which represents the two artists, demanded it be taken down.The track caused alarm, but it also demonstrated how cloning can work in artists’ favour. Analysis by Will Page, author of “Pivot” and a former chief economist at Spotify, suggests that being cloned can help stars sell genuine music. He calculates that after David Guetta, a French DJ, posted a clip of an AI-generated rap in the style of Eminem in February, streams of real Eminem tracks rose by about a fifth. Yet antipathy to AI remains fierce. A group of actors, writers and directors has set up an outfit called Credo23 to certify films and TV shows made without AI (it has yet to take off). Many singers, burned by digital piracy in the past, are also hostile. Sir Cliff Richard, an 83-year-old crooner, recently declared that his singing “didn’t use artificial insemination”.Others fear that AI will simply make entertainment derivative and boring. Three-quarters of Americans tell YouGov, a pollster, that they worry AI will sap human creativity. The process of ingesting everything and then spitting out an average may lead to a stylistic and conceptual regression to the mean, says a literary agent, who sees similarities with the algorithms on social media that help propagate the most pedestrian views. Sir Lucian, at Universal, has said that AI “will always lack the essential spark that drives the most talented artists to do their best work, which is intention”.Everything depends on whether audiences embrace artificial performances. “Is the next generation of moviegoers going to want to see a different actor in James Bond, as an example? Or are they going to want to see Sean Connery come back?” asks a Hollywood agent. AI-generated performances may prove to be most successful for the biggest stars, whose uncritical superfans cannot get enough of them. As Liam Gallagher, a former Oasis frontman and John Lennon devotee, replied when asked on social media what he thought of the remastered “Now and Then”: “The Beatles could shit in my hand bag I’d still hide my polo mints in there.” ■Read more of our articles on artificial intelligence
This essay is the winner of The Economist’s Open Future essay competition in the category of Open Progress, responding to the question: “Do the benefits of artificial intelligence outweigh the risks?” The winner is Frank L. Ruta, 24 years old, from America. * * * Towards the end of the second world war, a group of scientists in America working to develop an atomic bomb for the Manhattan Project warned that using the weapon would inevitably lead to a geopolitical landscape characterised by a nuclear arms race. This would force America, they said, to outpace other nations in building up nuclear armaments. They recommended that if the military did choose to use the weapon, an international effort for nuclear non-proliferation should promptly be established.The committee’s warnings went unheeded. After the nuclear attacks on Hiroshima and Nagasaki, it turned out they had been eerily prescient. The arms race between America and the Soviet Union escalated during the cold war and today rogue states like North Korea threaten peace with their nuclear arsenals.A potentially even more transformative technology is currently being developed: a technology which could easily be distributed to rogue nations and terrorist groups without the need for expensive, specialised equipment. Prominent scientists and technologists like the late Stephen Hawking and Elon Musk have voiced concern for the risks associated with the accelerating development of artificial intelligence (AI).Many experts in the field, like Stuart Russell, the founder of the Centre for Human-Compatible AI at UC Berkeley, believe that concerns about the misuse of AI should be taken seriously. More than 8,000 researchers, engineers, executives, and investors have signed an open letter recommending a direction for responsible AI research that recognises social impact and seeks to build robust technology that aligns with human values.To avoid repeating history, policymakers should begin to think about regulating AI development now that the community itself is calling for policy action. As with past technologies, well-structured regulation can mitigate costly externalities, while ill-informed regulatory measures can interfere with progress. Policymakers must cooperate closely with researchers to implement protocols that align AI with human values without being overly burdensome to developers.The emerging field of AI safety has already begun discussing guidelines to tackle the potential dangers of the technology. Sessions devoted to AI safety and ethics have taken place at major scientific conferences and several books and articles on the topic have been published. By understanding researchers’ concerns, regulators can address the dangers of AI and the benefits of the technology will greatly outweigh the risks.AI is a general term for software that mimics human cognition or perception. Because AI encompasses a broad set of algorithms, policymakers must take a nuanced approach to regulation, underscoring the need for technical collaboration. At a high level, a distinction is made between narrow AI and artificial general intelligence (AGI).Narrow AI is more intelligent, or at least faster, than humans at a specific task or set of tasks, like playing the board game Go or finding patterns in large datasets. On the other hand, an AGI would beat humans at a number of cognitive tasks, termed cognitive superpowers by Nick Bostrom, a philosopher at the University of Oxford. These include intelligence amplification, strategising, social manipulation, hacking, technology development and economic productivity.Narrow AI is responsible for many useful tools that have already become mainstream: speech and image recognition, search engines, spam filters, product and movie recommendations. The list goes on. Narrow AI also has the potential to enable promising technologies like driverless cars, tools for rapid scientific discovery and digital assistants for medical image analysis.In the near-term, some of these technologies have the potential to be abused by malicious groups. The cost of attacks requiring human labour or expertise could be reduced, and new threats exploiting vulnerabilities in AI systems could emerge. AI can automate labour-intensive cyberattacks, coordinate fleets of drones, allow for mass surveillance through facial recognition and social data mining, or generate realistic fake videos for political propaganda.Furthermore, increased automation gives more physical control to digital systems, making cyberattacks even more dangerous. Regulation can ensure that AI engineers are employing best practices in cybersecurity and limiting distribution of military technology. Considering the portability of AI, enforcing these rules will be difficult and international cooperation will likely be necessary.Some researchers are concerned that, since algorithms are only as good as the data they are fed, narrow AI can make biased decisions. Biased or incomplete training data will be reflected in the output. One study with a machine learning program trained on texts found that names associated with being European-American were significantly more likely to be correlated with pleasant terminology than African-American names. AI that make consequential decisions, like hiring job candidates or predicting recidivism, should be screened before being adopted. Regulatory agencies will have to decide if an AI makes fair decisions by combing through training data for stereotypes.On the other hand, the possibility of AGI is uncertain but some futurists believe its unchecked consequences could be apocalyptic. Some speculate that an AGI could appear within the next few decades in a so-called hard take-off, where its capabilities increase very rapidly as the program undergoes a process of recursive self-improvement. At the same time, others believe that intelligent agents have intrinsic limitations to augmenting their predictive capabilities autonomously and doomsday scenarios are unlikely, if not provably impossible.Nonetheless, researchers are already discussing the dangers that machine superintelligence might pose. One thesis claims that an AGI with almost any programmed goal would develop a set of “basic AI drives,” such as self-preservation, self-improvement and resource acquisition. In this model, the AGI would be motivated to spread itself across computer networks and evade programmers. The AGI would leverage its cognitive superpowers to escape containment and achieve self-determination.For example, the AGI might train itself on psychology and economics textbooks and use personal information about its developers to learn how to bribe its way to freedom. The AGI may then see humans as a threat to its self-preservation and seek to extinguish the human species. Researchers have suggested several ways to contain an AGI during testing, which policymakers can use as guidelines for drafting regulations. Containment strategies range from filtering training data for sensitive information to significantly handicapping the development process by, for example, limiting output to simple yes/no questions and answers. Some researchers have suggested dividing containment procedures into light, medium, and heavy categories. Regulations should avoid slowing progress when possible, so the weight of containment should vary with the maturity of the AGI program.Containment is a short-term solution for AGI testing. In the long run, regulations must ensure that an internet-enabled AGI is indefinitely stable and has benevolent properties such as value learning and corrigibility before being deployed. Value learning is an AGI’s ability to learn what humans value and act in accordance with those values. Corrigibility refers to an AGI’s lack of resistance to bug-fixes or recoding.One can imagine how an ideal AGI with a conception of justice and solidarity would be beneficial. Such an AGI could replace corrupt governments and biased judicial systems, making decisions according to a democratically-determined objective function. Moreover, a sufficiently sophisticated AGI could perform virtually any job done by a human. It is conceivable that the economy would be restructured in such a way that humans are free to pursue their creative passions while AGI drives productivity. As with past technologies, there will also be useful applications that we cannot even foresee.There are many unknowns in the progress of AI and concerns should be met with due caution. But a fear of the unknown should not stop the advance of responsible AI development. Rather than ignoring researchers’ concerns until the technology is mature, as with nuclear weapons, governments should open dialogue with AI researchers to design regulations that balance practicality with security.AI is already making our lives easier and its progress will continue to produce useful applications. With the right policies, we can work towards a future where AGI systems are friendly and military AI applications are out of the hands of malicious agents, while the underlying technology continues to be a driver of productivity and innovation. ____________Frank L. Ruta is a PhD candidate in applied physics at Columbia University in New York
IMAGINE the perfect environment for developing artificial intelligence (AI). The ingredients would include masses of processing power, lots of computer-science boffins, a torrent of capital—and abundant data with which to train machines to recognise and respond to patterns. That environment might sound like a fair description of America, the current leader in the field. But in some respects it is truer still of China.The country is rapidly building up its cloud-computing capacity. For sheer volume of research on AI, if not quality, Chinese academics surpass their American peers; AI-related patent submissions in China almost tripled between 2010 and 2014 compared with the previous five years. Chinese startups are attracting billions in venture capital. Above all, China has over 700m smartphone users, more than any other country. They are consuming digital services, using voice assistants, paying for stuff with a wave of their phones—and all the while generating vast quantities of data. That gives local firms such as Alibaba, Baidu and Tencent the opportunity to concoct best-in-class AI systems for everything from facial recognition to messaging bots. The government in Beijing is convinced of the potential. On July 20th it outlined a development strategy designed to make China the world’s leading AI power by 2030.An AI boom in the world’s most populous place holds out enormous promise. No other country could generate such a volume of data to enable machines to learn patterns indicative of rare diseases, for example. The development of new technologies ought to happen faster, too. Because typing Chinese characters is fiddly, voice-recognition services are more popular than in the West; they should improve faster as a result. Systems to adjust traffic lights automatically in response to footage from roadside cameras are already being tested. According to the McKinsey Global Institute, a research arm of the consultancy, AI-driven automation could boost China’s GDP growth by more than a percentage point annually.Yet the country’s AI plans also give cause for concern. One worry is that the benefits of Chinese breakthroughs will be muted by data protectionism. A cyber-security law that came into force in June requires foreign firms to store data they collect on Chinese customers within the country’s borders; outsiders cannot use Chinese data to offer services to third parties. It is not hard to imagine tit-for-tat constraints on Chinese firms. And if data cannot be pooled, the algorithms that run autonomous cars and other products may not be the most efficient.A second area of unease is ethics and safety. In America, the technology giants of Silicon Valley have pledged to work together to make sure that any AI tools they develop are safe. They will look at techniques like “boxing”, in which AI agents are isolated from their environment so that any wayward behaviour does not have disastrous effects. All the leading AI researchers in the West are signatories to an open letter from 2015 calling for a ban on the creation of autonomous weapons. If it happens at all, the equivalent Chinese discussion about the limits of ethical AI research is far more opaque.Chinese AI companies do have incentives to think about some of these issues: rogue AI would be a problem for the planet wherever it emerged. There is a self-interested case for the formulation of global safety standards, for example. But a third concern—that AI will be used principally to the benefit of China’s government—is a less tractable problem.Autocratic intelligenceThe new plan is open about AI’s value to the state. It envisages the use of the technology in everything from guided missiles to predictive policing. AI techniques are perfect for finding patterns in the massive amounts of data that Chinese censors must handle in order to maintain a grip on the citizenry. It is easy to imagine how the same data could boost the country’s nascent plans to create a “social-credit” system that scores people for their behaviour. Once perfected, these algorithms would interest autocratic regimes around the world. China’s tech firms are in no position to prevent the government in Beijing from taking advantage of such tools. Baidu, for example, has been appointed to lead a national laboratory for deep learning. Chinese AI will reflect the influence of the state.Western firms and governments are no angels when it comes to data collection and espionage. But Western companies are at least engaged in an open debate about the ethical implications of AI; and intelligence agencies are constrained by democratic institutions. Neither is true of China. AI is a technology with the potential to change the lives of billions. If China ends up having most influence over its future, then the state, not citizens, may be the biggest beneficiary.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.In London’s Design Museum, an exhibition currently on display by Ai Weiwei, a Chinese artist, includes a 15-metre-long work called “Water Lilies #1” based on the triptych by Claude Monet. Look closely and it is made of 650,000 Lego bricks—which integrates Monet’s impressionism into what Mr Ai calls a “digitised and pixelated language”. That is a good analogy for Lego itself. The Danish toymaker is on a long-term mission to digitise and pixelate its own fount of human creativity: the plastic brick.Three digital experts from McKinsey, a management consultancy, profile Lego’s transformation as part of their new book, “Rewired”, which outlines the dos and don’ts for businesses rebuilding themselves for the age of digitisation. Beware: the language of digital transformation is treachery to common English. It sounds more like corporate yoga than a marathon of software development. Executives need to be aligned. Teams are pods. Be agile. Define your downward-facing domains. McKinsey, drawing lessons from 200 firms, provides clarity despite the mumbo jumbo. But to make it easier on the ear, Schumpeter will use Lego as a guide to help illustrate some of McKinsey’s insights. Call it the yellow-brick road to generative artificial intelligence (AI).First, it is a long hard road, littered with failures. Lego is a rare success story. Its journey started in 2003 with a near-death experience when, amid the rise of video-gaming, it panicked and went on a madcap innovation spree that almost bankrupted it. To fix one of the main problems, chaos in the supply chain, it introduced a single enterprise-software system globally. The system survives to this day, scaling up as Lego expands into new markets, such as China, new formats, such as e-commerce, and new factory locations, such as America and Vietnam. To prepare for a world of pixelated play, Lego launched digital games on the “Star Wars” theme and developed franchises of its own, such as Ninjago and Chima, with video games, films and TV shows that turned into hits.In 2019 Lego launched a new five-year transformation drive aimed at adapting to a world of direct-to-consumer sales, online versus big-box retailing, and digital play in the screen age. The timing was inspired. It started shortly before the world went into lockdown as a result of the covid-19 pandemic, when having a digital strategy became a matter of life and death. It quickly produced results. Although it is hard to strip out the exact contribution of digitisation, since 2018 Lego’s sales have almost doubled, to more than $9bn, outpacing those of Mattel and Hasbro, its main rivals. In 2022 visits to its online portal rose by 38%. It has teamed up with Epic, a video-gaming firm, to explore the metaverse.Yet the journey is still a hard one. The difficulties include moving from a system where success is measured by sales store-by-store to one judged by how good the company is at selling online across the globe, how it is ranked on Google and Amazon, and how effective its software is. The McKinsey authors emphasise such challenges on the first page. In a recent McKinsey survey, they say, about 90% of companies had some kind of digital strategy, but they captured less than a third of the revenue gains they had anticipated. Moreover, the success rate is more uneven within industries than it is between them. The best retailer may be more digitally productive than an average high-tech firm, and the worst retailer may be as bad as the worst government entity.To make a success of it requires learning the second lesson: what McKinsey calls having a top-down strategy and a road map (or in Lego terms, a clear instruction manual). For Lego, it helped that the family-owned business had long had a command-and-control approach to management. Its digital strategy involved a single plan, created by a 100-strong executive team and approved by the board, that encompassed the whole organisation. McKinsey notes that when transformations stall, it is often because executives talk past each other, have pet projects, spread investments too thin or have “more pilots than there are on an aircraft-carrier”, as Rodney Zemmel, one of the authors, puts it. It also needs to be ambitious enough to generate momentum, with financial results measured constantly. McKinsey’s rule of thumb is that a digital transformation should aim to increase earnings before interest, tax, depreciation and amortisation by 20% or more.Third comes the question of whether to build a new digital infrastructure or buy it. The answer is mostly to build. Rather like Lego’s eight-studded bricks—six of which can be combined 915m ways—there are many software applications on the market that can be combined to create proprietary systems. But the job of orchestrating them should not be outsourced. Take Lego: it started its latest digital transformation with engineers making up less than 30% of staff. Since then it has increased the number of systems and software engineers by 150%. Mr Zemmel notes that five years ago, the trend was to hire from Silicon Valley. That was “a good way to change the company dress code, but not a great way to change the company culture”. Since then more companies have been retraining their existing tech workers and embedding them throughout the organisations in more front-line roles.The gen-AI Weiwei way Some of these lessons apply to generative AI. Mr Zemmel says it is relatively easy to launch pilots with the technology, such as the humanlike ChatGPT. The problem is embedding the AI models across the organisation in a safe, unbiased way. It needs a top-down strategy. As for building or buying, Mr Zemmel says it may be a “waste of time” to build proprietary models when the software industry is doing that anyway. The key is to work in-house on the things that give you a decisive advantage in the market. For Lego, AI is still in the future, though some of its brick enthusiasts are already using ChatGPT-like programs to come up with new ways of building things. Mostly they fail, but one day anyone may be able to create a Monet. The yellow-brick road is unending. ■Read more from Schumpeter, our columnist on global business:Meet the world’s most flirtatious sovereign-wealth fund (Jun 29th)The new king of beers is a Mexican-American success story (Jun 20th)What Tesla and other carmakers can learn from Ford (Jun 13th)Also: If you want to write directly to Schumpeter, email him at [email protected]. And here is an explanation of how the Schumpeter column got its name.
TEN YEARS ago we published a paper, “The Future of Employment”, highlighting how artificial intelligence (AI) was broadening the scope of what computers could do, and so broadening the possibilities for automation. The prevailing narrative at the time was that the era of the “average” worker was ending, with machines progressively replacing routine and administrative jobs. Highly skilled professionals were the ones reaping the benefits, as computers made them more productive and enabled them to sell their services locally and around the world. A decade on, what is known as generative AI seems to be disrupting such trends: the era of “average” is making a comeback.It is hard not to be impressed by the capabilities of generative AI. Large language models (LLMs) such as GPT-4 can now answer questions in a human-like way and write plausible essays. Image-generators like DALL-E 2 are also advancing rapidly, aiding, or in some cases replacing, designers and advertising executives. Add to this Github’s Copilot, an AI-powered “pair programmer” that can write computer code, and the potential for automation seems almost boundless.Yet to understand how labour markets will evolve in response, we need to take a closer look under the bonnet. First, AI produces content of a quality similar to that on which it is trained—”garbage in, garbage out”. Second, although no one outside OpenAI, GPT-4’s creator, knows the exact underpinnings of the model, we know this much: LLMs, in their current form, are astonishingly data-hungry. They need to be trained on very large datasets such as broad swathes of the internet, rather than smaller, curated datasets produced by experts. As a result, LLMs tend to produce text of a quality equal to the average—rather than the exceptional—bits of the internet. Average in, average out.True, fine-tuning can further improve generative AI’s quality. One way to do this is through “reinforcement learning from human feedback” (RLHF), which updates a model using human judgment of whether its response to a prompt was appropriate. But this is labour-intensive: OpenAI reportedly outsourced much of this work to Kenyans earning less than $2 per hour. What’s more, there is evidence of a recent decline in the effectiveness of LLMs, suggesting that RLHF might be reaching its limits.Even with improvements from fine-tuning, the upper bound on performance of the current approach to LLMs may not be far from that of current models. Unless a breakthrough occurs that allows algorithms to learn from smaller datasets, the “average in, average out” dilemma is likely to persist.What does this mean for the future of work? First, although many jobs can be automated, the most recent wave of generative AI will continue to need a human in the loop. Second, low-skilled workers are poised to benefit disproportionately, as they are now able to produce content that meets the “average” standard.In the world of software development, the introduction of Copilot has changed the game, cutting completion times in half. But the real story lies in the beneficiaries of this revolution. It’s not the seasoned experts whose productivity is increasing the most, but rather those with the least experience in programming.A similar story is unfolding in other industries. ChatGPT, for instance, has been found to boost productivity in writing tasks, with the worst writers benefiting the most. AI is having a big impact in customer service, too. Erik Brynjolfsson of Stanford University and his co-authors find that AI assistants increase productivity by 14% by automating routine tasks and providing support to human agents—and it is novices and low-skilled workers who reap the greatest productivity gains.This shift challenges the conventional wisdom that automation mainly benefits those at the top of the skills ladder, and highlights the potential for technology to democratise access to content-creating industries, from legal and educational services to news and entertainment. What that means, however, is greater competition and probably reduced earnings for incumbents.A useful analogy is that of Uber, a ride-hailing firm, and its effects on the taxi industry. With the implementation of GPS technology, having a thorough knowledge of each street in San Francisco was no longer a valuable skill for taxi drivers. Consequently, when Uber expanded its operations across America, drivers with only limited familiarity with the cities in which they worked were able to thrive. Heightened competition pushed down earnings for established drivers. Joint research with our Oxford colleagues Thor Berger and Chinchih Chen shows that when Uber entered a new city, drivers’ hourly earnings dropped by around 10%.Of course, generative AI’s overall impact on wages will depend on how much more people will consume when AI makes content cheaper to produce. This question is akin to asking how much more time one would spend on Netflix if the content were cheaper and better. The answer is probably not very much, as time is a limited resource.Yet even if the result is not widespread unemployment, lower wages could trigger a backlash, as seen with taxi drivers protesting against Uber, and more recently in Hollywood, where actors and screenwriters have gone on strike in part over generative AI. Historically, when people find their incomes threatened by machines, resistance has followed, whether in Georgian Britain or Qing dynasty China, albeit with different outcomes. Whereas the Luddite riots were brutally quashed, in China industrialisation was delayed by two centuries as powerful guilds halted mechanisation.The bottom line is that when incumbents have more political clout, opposition is more likely to succeed. And white-collar professionals have more political influence than their working-class counterparts. Blue-collar workers have, for decades, largely failed to hold back the impact of automation in factories; white-collar workers threatened by AI may be able to produce powerful resistance to new technologies.The prospect of such resistance raises the risk of what one of us has called a “technology trap”. Unless policies are implemented to smooth the ride, Luddite-inspired efforts to avoid the short-term disruption brought about by new technology might inadvertently obstruct access to its long-term benefits, such as AI’s potential to produce personalised tools that help older workers with complex health conditions. Average might be back, but it does not mean that everyone will benefit.■Carl Benedikt Frey is the Dieter Schwarz Associate Professor of AI & Work at the Oxford Internet Institute and Oxford Martin Citi Fellow at the Oxford Martin School. He is also the author of “The Technology Trap” (2019).Michael Osborne is a Professor of Machine Learning at the University of Oxford, an Official Fellow of Exeter College, Oxford, and a co-founder of Mind Foundry.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The rainforests are alive with the sound of animals. Besides the pleasure of the din, it is also useful to ecologists. If you want to measure the biodiversity of a piece of land, listening out for animal calls is much easier than grubbing about in the undergrowth looking for tracks or spoor. But such “bioacoustic analysis” is still time-consuming, and it requires an expert pair of ears.In a paper published on October 17th in Nature Communications, a group of researchers led by Jörg Müller, an ecologist at the University of Würzburg, describe a better way: have a computer do the job. Smartphone apps already exist that will identify birds, bats or mammals simply by listening to the sounds they make. Their idea was to apply the principle to conservation work.The researchers took recordings from across 43 sites in the Ecuadorean rainforest. Some sites were relatively pristine, old-growth forest. Others were areas that had recently been cleared for pasture or cacao planting. And some had been cleared but then abandoned, allowing the forest to regrow.Sound recordings were taken four times every hour, over two weeks. The various calls were identified manually by an expert, and then used to construct a list of the species present. As expected, the longer the land had been free from agricultural activity, the greater the biodiversity it hosted.Then it was the computer’s turn. The researchers fed their recordings to artificial-intelligence models that had been trained, using sound samples from elsewhere in Ecuador, to identify 75 bird species from their calls. “We found that the AI tools could identify the sounds as well as the experts,” says Dr Müller.Of course, not everything in a rainforest makes a noise. Dr Müller and his colleagues used light-traps to capture night-flying insects, and DNA analysis to identify them. Reassuringly, they found that the diversity of noisy animals was a reliable proxy for the diversity of the quieter ones, too.The results may have relevance outside ecology departments, too. Under pressure from their customers, firms such as L’Oreal, a make-up company, and Shell, an oil firm, have been spending money on forest restoration projects around the world. Dr Müller hopes that an automated approach to checking on the results could help monitor such efforts, and give a standardised way to measure whether they are working as well as their sponsors say.■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Picture a computer that could finish your sentences, using a better turn of phrase; or use a snatch of melody to compose music that sounds as if you wrote it (though you never would have); or solve a problem by creating hundreds of lines of computer code—leaving you to focus on something even harder. In a sense, that computer is merely the descendant of the power looms and steam engines that hastened the Industrial Revolution. But it also belongs to a new class of machine, because it grasps the symbols in language, music and programming and uses them in ways that seem creative. A bit like a human.The “foundation models” that can do these things represent a breakthrough in artificial intelligence, or ai. They, too, promise a revolution, but this one will affect the high-status brainwork that the Industrial Revolution never touched. There are no guarantees about what lies ahead—after all, ai has stumbled in the past. But it is time to look at the promise and perils of the next big thing in machine intelligence.Foundation models are the latest twist on “deep learning” (dl), a technique that rose to prominence ten years ago and now dominates the field of ai. Loosely based on the networked structure of neurons in the human brain, dl systems are “trained” using millions or billions of examples of texts, images or sound clips. In recent years the ballooning cost, in time and money, of training ever-larger dl systems had prompted worries that the technique was reaching its limits. Some fretted about an “ai winter”. But foundation models show that building ever-larger and more complex dl does indeed continue to unlock ever more impressive new capabilities. Nobody knows where the limit lies.The resulting models are a new form of creative, non-human intelligence.  The systems are sophisticated enough both to possess a grasp of language and also to break the rules coherently. A dog cannot laugh at a joke in the New Yorker, but an ai can explain why it is funny—a feat that is, frankly, sometimes beyond readers of the New Yorker. When we asked one of these models to create a collage using the title of this leader and nothing more, it came up with the cover art for our American and Asian editions (we tried to distract our anxious human designers with a different cover in our European editions).Foundation models have some surprising and useful properties. The eeriest of these is their “emergent” behaviour—that is, skills (such as the ability to get a joke or match a situation and a proverb) which arise from the size and depth of the models, rather than being the result of deliberate design. Just as a rapid succession of still photographs gives the sensation of movement, so trillions of binary computational decisions fuse into a simulacrum of fluid human comprehension and creativity that, whatever the philosophers may say, looks a lot like the real thing. Even the creators of these systems are surprised at their power.This intelligence is broad and adaptable. True, foundation models are capable of behaving like an idiot, but then humans are, too. If you ask one who won the Nobel prize for physics in 1625, it may suggest Galileo, Bacon or Kepler, not understanding that the first prize was awarded in 1901. However, they are also adaptable in ways that earlier ais were not, perhaps because at some level there is a similarity between the rules for manipulating symbols in disciplines as different as drawing, creative writing and computer programming. This breadth means that foundation models could be used in lots of applications, from helping find new drugs using predictions about how proteins fold in three dimensions, to selecting interesting charts from datasets and dealing with open-ended questions by trawling huge databases to formulate answers that open up new areas of inquiry.That is exciting, and promises to bring great benefits, most of which still have to be imagined. But it also stirs up worries. Inevitably, people fear that ais creative enough to surprise their creators could become malign. In fact, foundation models are light-years from the sentient killer-robots beloved by Hollywood. Terminators tend to be focused, obsessive and blind to the broader consequences of their actions. Foundational ai, by contrast, is fuzzy. Similarly, people are anxious about the prodigious amounts of power training these models consume and the emissions they produce. However, ais are becoming more efficient, and their insights may well be essential in developing the technology that accelerates a shift to renewable energy.A more penetrating worry is over who controls foundation models. Training a really large system such as Google’s PaLM costs more than $10m a go and requires access to huge amounts of data—the more computing power and the more data the better. This raises the spectre of a technology concentrated in the hands of a small number of tech companies or governments.If so, the training data could further entrench the world’s biases—and in a particularly stifling and unpleasant way. Would you trust a ten-year-old whose entire sense of reality had been formed by surfing the internet? Might Chinese- and American-trained ais be recruited to an ideological struggle to bend minds? What will happen to cultures that are poorly represented online?And then there is the question of access. For the moment, the biggest models are restricted, to prevent them from being used for nefarious purposes such as generating fake news stories. Openai, a startup, has designed its model, called DALL-E 2, in an attempt to stop it producing violent or pornographic images. Firms are right to fear abuse, but the more powerful these models are, the more limiting access to them creates a new elite. Self-regulation is unlikely to resolve the dilemma.Bring on the revolutionFor years it has been said that ai-powered automation poses a threat to people in repetitive, routine jobs, and that artists, writers and programmers were safer. Foundation models challenge that assumption. But they also show how ai can be used as a software sidekick to enhance productivity. This machine intelligence does not resemble the human kind, but offers something entirely different. Handled well, it is more likely to complement humanity than usurp it. ■For subscribers only: to see how we design each week’s cover, sign up to our weekly Cover Story newsletter.
TWO THINGS European lawmakers should get credit for are stamina and an extraordinary tolerance for bad food. Representatives of the EU Parliament, member governments and the European Commission, the bloc’s executive body, spent nearly 40 hours in a dark meeting room in Brussels until the wee hours of December 9th hashing out a deal on the AI Act, Europe’s ground-breaking law on regulating artificial intelligence. Observers shared pictures online of half-eaten sandwiches and other fast food piling up in the venue’s rubbish bins to gauge the progress of the talks.This ultramarathon of negotiation was the endpoint of one of the most diligent lawmaking processes ever. It started in early 2018 with lengthy public consultations and a weighty 52-person “High-Level Expert Group”, which in 2020 led to a white paper on which all could comment online (1,250 groups and individuals did so). The legislation has yet to be released because kinks still need to be worked out, but the draft version was a document of nearly 100 pages and almost as many articles.Was it all worth it? The thorough process certainly has led to a logically coherent legal approach, not unlike that of much product-safety legislation. In order to give the technology space to evolve, the AI Act’s first draft, which the commission presented in April 2021, mainly tried to regulate various applications of AI tools, rather than how they were built. The riskier the purpose of an AI tool, the stricter the rules with which it needed to comply. An AI-powered writing assistant needs no regulation, for instance, whereas a service that helps radiologists does. Facial recognition in public spaces might need to be banned outright.But the idea of focusing on how AI tools are applied was predicated on the assumption that algorithms are mostly trained for specific purposes. Then along came the “large language models” that power such AI services as ChatGPT and can be used for any number of purposes, from analysing text to writing code.Since these LLMs can be a source of harm themselves, for example by spreading bias and disinformation, the European Parliament wanted to regulate them as well, for instance by forcing their makers to reveal what data they were trained on and how they assessed the model’s risks. By contrast, some governments, including those of France and Germany, worried that such requirements would make it hard for small European model-makers to compete with big American ones. The result, after the all-nighter, is a messy compromise that subjects only the most powerful LLMs to stricter rules, such as mandates to assess their risks and to take measures to mitigate them. As for smaller models, there will be regulatory exceptions, in particular for the open-source kind, which allow users to adapt them to their needs.A second big sticking point was to what extent law-enforcement agencies should be allowed to use facial recognition, which at its core is an AI-powered service. The European Parliament pushed for an outright ban, in order to protect privacy rights. Governments, meanwhile, insisted that they need the technology for public security, notably to protect big events such as the Olympic Games next year in France. Again, the compromise is a series of exceptions. Real-time facial recognition, for instance, is banned except for certain crimes (such as kidnapping and sexual exploitation), certain times and places and when it is approved be a judge or a similar authority.“The EU becomes the very first continent to set clear rules for the use of AI,” tweeted Thierry Breton, the EU’s commissioner for the internal market. Mr Breton is never far from the social-media limelight: during the negotiation marathon, he continuously posted shots of himself in the middle of a huddle. Yet whether the AI Act will be as successful as the General Data Protection Regulation (GDPR), the EU’s landmark privacy law, is another question. Important details still need to be worked out. And the European Parliament still needs to approve the final version.Most important, it is not clear how well the AI act will be enforced—an ongoing problem with recent digital laws passed by the EU, given that it is a club of independent countries. In the case of the GDPR, national data-protection agencies are mainly in charge, which has led to differing interpretations of the rules and less than optimal enforcement. In the case of the Digital Services Act and the Digital Markets Act, two recent laws to regulate online platforms, enforcement is concentrated in Brussels at the commission. The AI act is more of a mix, but experts worry that both the  forthcoming “AI Office”, which is to be set up in Brussels, and some national bodies will lack the expertise to prosecute violations, which can lead to fines of up to €35m ($38m) or 7% of a company’s global revenue.The GDPR triggered what is known as the “Brussels Effect”: big tech firms around the globe complied, and many non-European governments borrowed from it for their own legislation. The AI act may not do the same. Complex compromises and haphazard enforcement are not the only reasons. For one thing, the incentives in AI are different: AI platforms may find it easier to simply use a different algorithm inside the EU to comply with its regulations. (By contrast, global social-media networks find it difficult to maintain different privacy policies in different countries.) And by the time the AI Act is fully in force and has shown its worth, many other countries, including Brazil and Canada, will have written their own AI acts.The protracted discussions over the AI Act have certainly helped people, both in Europe and elsewhere, to understand better the risks of the technology and what to do about them. But instead of trying to be first, Europe might have done better trying to be best—and come up with an AI act that has more rigour and less piling of exceptions on top of exceptions.■
"FEW PEOPLE would tolerate a virtual assistant if they had to plead obsequiously each time, “Excuse me, Alexa, if it’s not too much trouble, could you kindly tell me what the weather will be today.” Instead, these devices are designed to answer brusque commands: “Alexa: weather!” And we expect them to obediently respond.Which is fine until we bring them into a home with impressionable young children, who may quickly learn that this is a normal way to talk to other people—that is, rudely. This points to a potentially far-reaching problem with artificial intelligence (AI). When it comes to how AI will affect social interaction, most people are focused on the relationship between humans and AI. Not enough attention is being paid to how humans will treat each other in the presence of AI.Unlike AI used for technical challenges, such as processing medical images, certain types of AI are designed to act in more human ways, like providing psychotherapy. These technologies will induce “social spillovers”—influencing how people react to and learn from the behaviour of other people. And these spillovers might affect humans well beyond those involved in the original interaction.People will increasingly have AI-enabled “co-bots” on their phones that get to know them and help them relate to other people. But some users of dating apps, for instance, have found that they enjoy flirting with a virtual partner more than going on an actual date. This changes the sorts of people available in the real, human dating pool, in addition to reshaping interpersonal communications.Although chatbot conversation partners and other types of “smart” AI powered by  large language models (LLMs) may seem the most consequential for human behaviour, even small intrusions into our social lives by simpler AI can have profound spillover effects, for good or ill.In one experiment, we placed 4,000 people into 230 online groups. Each group was then divided into several clusters, each with just a few people. The members of these clusters had to co-operate with each other on picking colours. If they found a “solution”—with each individual choosing a different colour than their immediate neighbours—the group as a whole was said to have succeeded and everyone got some money.To some of these groups, however, we surreptitiously added bots that the members perceived to be other humans—and manipulated their responses. We found that having the bots occasionally make “erroneous” moves that increased rather than decreased the colour conflicts with their immediate neighbours was actually helpful to the group as a whole, fostering greater flexibility. People came to realise that just solving the challenge with respect to themselves and their immediate neighbours was not necessarily best for their group as a whole. Making a counterintuitive move that seemingly decreased local consensus unlocked a group-wide solution. The AI was able to help the people to help themselves.	In another experiment, we gave 1,024 subjects in 64 groups the challenge of producing so-called public goods—items that people work together to fashion and that are of mutual benefit, like a lighthouse. The idea is that if everyone pitches in, everyone will end up benefiting more than they contributed. But, of course, the temptation is to let others work to tend the commons.At the beginning, over 60% of people acted altruistically and helped out. But we found that by adding just a few bots (which the players again perceived to be other humans) that behaved in a free-riding way, we could drive the group of people to behave selfishly so that, eventually, they stopped co-operating altogether. The bots could convert a group of people who were otherwise generous into a group of jerks.But the opposite was also true. We could use bots to enhance human co-operation. Giving people co-operative (artificial) partners caused them to be kinder than they would normally be when dealing with other people.Other experiments show that when people delegate decision-making to AI agents—something they are increasingly likely to do, from having LLMs draft emails to tasking drones with military targeting—it can obscure moral responsibility and encourage unethical interactions with other people.A group at the Max Planck Institute for Human Development led by Iyad Rahwan has done experiments that involved giving subjects AI assistants. People had to roll dice and report the outcome. Around 5% of the participants were dishonest when doing the task by themselves. That number rose to 40% when subjects could delegate the task of being dishonest to another human, and to 50% if they could delegate it to a machine. But the number rose to a whopping 88% if they could delegate the task to an AI agent that could decide to cheat on their behalf.If undermining honesty as people interact is not worrying enough, there are fears that AI could undermine physical safety, too. In just-published experiments led by Hirokazu Shirado at Carnegie Mellon University, we found that even very simple forms of AI assistance for drivers, such as auto-steering or auto-braking, eroded social norms of reciprocity on the road. Allowing humans to delegate whether to swerve away from an oncoming car in repeated games of chicken resulted in people subsequently being less likely to take turns in giving way, thereby increasing the frequency of crashes when they drove without AI assistance.These effects of AI suggest that it could have a big impact on the social norms that have evolved over millennia, shaping how we treat each other in all manner of everyday interactions. Governments cannot afford to ignore the risks. At a minimum, they should evaluate more closely whether AI systems are aligned with human social interests and they should provide for more safety testing. As the Bletchley Declaration signed at the recent AI-safety summit in Britain made clear, innovation must go hand in hand with attention to mitigating risks. After all, we cannot ask AI to regulate itself, even politely.■Nicholas A. Christakis is the director of the Human Nature Lab at Yale University."
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.In 2019, scientists at the Massachusetts Institute of Technology (MIT) did something unusual in modern medicine—they found a new antibiotic, halicin. In May this year another team found a second antibiotic, abaucin. What marked these two compounds out was not only their potential for use against two of the most dangerous known antibiotic-resistant bacteria, but also how they were identified.In both cases, the researchers had used an artificial-intelligence (AI) model to search through millions of candidate compounds to identify those that would work best against each “superbug”. The model had been trained on the chemical structures of a few thousand known antibiotics and how well (or not) they had worked against the bugs in the lab. During this training the model had worked out links between chemical structures and success at damaging bacteria. Once the AI spat out its shortlist, the scientists tested them in the lab and identified their antibiotics. If discovering new drugs is like searching for a needle in a haystack, says Regina Barzilay, a computer scientist at MIT who helped to find abaucin and halicin, AI acts like a metal detector. To get the candidate drugs from lab to clinic will take many years of medical trials. But there is no doubt that AI accelerated the initial trial-and-error part of the process. It changes what is possible, says Dr Barzilay. With AI, “the type of questions that we will be asking will be very different from what we’re asking today.”Drug discovery is not alone in being jolted by the potential of AI. Researchers tackling many of the world’s most complicated and important problems—from forecasting weather to searching for new materials for batteries and solar panels and controlling nuclear-fusion reactions—are all turning to AI in order to augment or accelerate their progress.The potential is enormous. “AI could usher in a new renaissance of discovery,” argues Demis Hassabis, co-founder of Google DeepMind, an AI lab based in London, “acting as a multiplier for human ingenuity.” He has compared AI to the telescope, an essential technology that will let scientists see farther and understand more than with the naked eye alone.Where have you been?image: The EconomistThough it has been part of the scientific toolkit since the 1960s, for most of its life AI has been stuck within disciplines where scientists were already well-versed in computer code—particle physics, for example, or mathematics. By 2023, however, with the rise of deep learning, more than 99% of research fields were producing AI-related results, according to CSIRO, Australia’s science agency (see chart). “Democratisation is the thing that is causing this explosion,” says Mark Girolami, chief scientist at the Alan Turing Institute in London.  What used to require a computer-science degree and lines of arcane programming languages can now be done with user-friendly AI tools, often made to work after a query to ChatGPT, OpenAI’s chatbot. Thus scientists have easy access to what is essentially a dogged, superhuman research assistant that will solve equations and tirelessly sift through enormous piles of data to look for any patterns or correlations within.In materials science, for example, the problem is similar to that in drug discovery—there are an unfathomable number of possible compounds. When researchers at the University of Liverpool were looking for materials that would have the very specific properties required to build better batteries, they used an AI model known as an “autoencoder” to search through all 200,000 of the known, stable crystalline compounds in the Inorganic Crystal Structure Database, the world’s largest such repository. The AI had previously learned the most important physical and chemical properties required for the new battery material to achieve its goals and applied those conditions to the search. It successfully reduced the pool of candidates for scientists to test in the lab from thousands to just five, saving time and money.The final candidate—a material combining lithium, tin, sulphur and chlorine—was novel, though it is too soon to tell whether or not it will work commercially. The AI method, however, is being used by researchers to discover other sorts of new materials.What did you dream?AI can also be used to predict. The shapes into which proteins twist themselves after they are made in a cell are vital to making them work. Scientists do not yet know how proteins fold. But in 2021, Google DeepMind developed AlphaFold, a model that had taught itself to predict the structure of a protein from its amino-acid sequence alone. Since it was released, AlphaFold has produced a database of more than 200m predicted protein structures, which has already been used by over 1.2m researchers. For example, Matthew Higgins, a biochemist at the University of Oxford, used AlphaFold to figure out the shape of a protein in mosquitoes that is important for the malaria parasite that the insects often carry. He was then able to combine the predictions from AlphaFold to work out which parts of the protein would be the easiest to target with a drug. Another team used AlphaFold to find—in just 30 days—the structure of a protein that influences how a type of liver cancer proliferates, thereby opening the door to designing a new targeted treatment.AlphaFold has also contributed to the understanding of other bits of biology. The nucleus of a cell, for example, has gates to bring in material to produce proteins. A few years ago, scientists knew the gates existed, but knew little about their structure. Using AlphaFold, scientists predicted the structure and contributed to understanding about the internal mechanisms of the cell. “We don’t really completely understand how [the AI] came up with that structure,” says Pushmeet Kohli, one of AlphaFold’s inventors who now heads Google DeepMind’s “AI for Science” team. “But once it has made the structure, it is actually a foundation that now, the whole scientific community can build on top of.”AI is also proving useful in speeding up complex computer simulations. Weather models, for example, are based on mathematical equations that describe the state of Earth’s atmosphere at any given time. The supercomputers that forecast weather, however, are expensive, consume a lot of power and take a lot of time to carry out their calculations. And models must be run again and again to keep up with the constant inflow of data from weather stations around the world.Climate scientists, and private companies, are therefore beginning to deploy machine learning to speed things up. Pangu-Weather, an AI built by Huawei, a Chinese company, can make predictions about weather a week in advance thousands of times faster and cheaper than the current standard, without any meaningful dip in accuracy. FourCastNet, a model built by Nvidia, an American chipmaker, can generate such forecasts in less than two seconds, and is the first AI model to accurately predict rain at a high spatial resolution, which is important information for predicting natural disasters such as flash floods. Both these AI models are trained to predict the weather by learning from observational data, or the outputs of supercomputer simulations. And they are just the start—Nvidia has already announced plans to build a digital twin of Earth, called “Earth-2”, a computer model that the company hopes will be able to predict climate change at a more regional level, several decades in advance.Physicists trying to harness the power of nuclear fusion, meanwhile, have been using AI to control complex bits of kit. One approach to fusion research involves creating a plasma (a superheated, electrically charged gas) of hydrogen inside a doughnut-shaped vessel called a tokamak. When hot enough, around 100m°C, particles in the plasma start to fuse and release energy. But if the plasma touches the walls of the tokamak, it will cool down and stop working, so physicists contain the gas within a magnetic cage. Finding the right configuration of magnetic fields is fiendishly difficult (“a bit like trying to hold a lump of jelly with knitting wool”, according to one physicist) and controlling it manually requires devising mathematical equations to predict what the plasma will do and then making thousands of small adjustments every second to around ten different magnetic coils. By contrast, an AI control system built by scientists at Google DeepMind and EPFL in Lausanne, Switzerland, allowed scientists to try out different shapes for the plasma in a computer simulation—and the AI then worked out how best to get there.Automating and speeding up physical experiments and laboratory work is another area of interest. “Self-driving laboratories” can plan an experiment, execute it using a robotic arm, and then analyse the results. Automation can make discovering new compounds, or finding better ways of making old compounds, up to a thousand times faster. You’ve been in the pipelineGenerative AI, which exploded into public consciousness with the arrival of ChatGPT in 2022 but which scientists have been playing with for much longer, has two main scientific uses. First, it can be used to generate data. “Super-resolution” AI models can enhance cheap, low-resolution electron-microscope images into high-resolution ones that would otherwise have been too expensive to record. The AI compares a small area of a material or a biological sample in high resolution with the same thing recorded at a lower resolution. The model learns the difference between the two resolutions and can then translate between them.And just as a large language model (LLM) can generate fluent sentences by predicting the next best word in a sequence, generative molecular models are able to build molecules, atom by atom, bond by bond. LLMs use a mix of self-taught statistics and trillions of words of training text culled from the internet to write in ways that plausibly mimic a human. Trained on vast databases of known drugs and their properties, models for “de novo molecular design” can figure out which molecular structures are most likely to do which things, and they build accordingly. Verseon, a pharmaceutical company based in California, has created drug candidates in this way, several of which are now being tested on animals, and one—a precision anticoagulant—that is in the first phase of clinical trials. Like the new antibiotics and battery materials identified by AI, chemicals designed by algorithms will also need to undergo the usual trials in the real world before their effectiveness can be assessed.A more futuristic use for LLMs comes from Igor Grossmann, a psychologist at the University of Waterloo. If an LLM could be prompted with real (or fabricated) back stories so as to mirror accurately what human participants might say, they could theoretically replace focus groups, or be used as agents in economics research. LLMs could be trained with various different personas, and their behaviour could then be used to simulate experiments, whose results, if interesting, could later be confirmed with human subjects.LLMs are already making scientists themselves more efficient. According to GitHub, using tools like its “Copilot” can help coders write software 55% faster. For all scientists, reading the background research in a field before embarking on a project can be a daunting task—the sheer scale of the modern scientific literature is too vast for a person to manage. Elicit, a free online AI tool created by Ought, an American non-profit research lab, can help by using an LLM to comb through the mountains of research literature and summarise the important ones much faster than any human could. It is already used by students and younger scientists, many of whom find it useful to find papers to cite or to define a research direction in the face of a mountain of text. LLMs can even help to extract structured information—such as every experiment done using a specific drug—from millions of documents.Widening access to knowledge within disciplines could also be achieved with AI. Each detector at the Large Hadron Collider at CERN in Geneva requires its own specialised teams of operators and analysts. Combining and comparing data from them is impossible without physicists from each detector coming together to share their expertise. This is not always feasible for theoretical physicists who want to quickly test new ideas. Miguel Arratia, a physicist at the University of California, Riverside, has therefore proposed using AI to integrate measurements from multiple fundamental physics experiments (and even cosmological observations) so that theoretical physicists can quickly explore, combine and re-use the data in their own work.AI models have demonstrated that they can process data, and automate calculations and some lab work (see table). But Dr Girolami warns that whereas AI might be useful to help scientists fill in gaps in knowledge, the models still struggle to push beyond the edges of what is already known. These systems are good at interpolation—connecting the dots—but less so at extrapolation, imagining where the next dot might go. And there are some hard problems that even the most successful of today’s AI systems cannot yet handle. AlphaFold, for example, does not get all proteins right all the time. Jane Dyson, a structural biologist at the Scripps Research Institute in La Jolla, California, says that for “disordered” proteins, which are particularly relevant to her research, the AI’s predictions are mostly garbage. “It’s not a revolution that puts all of our scientists out of business.” And AlphaFold does not yet explain why proteins fold in the ways they do. Though perhaps the AI “has a theory we just have not been able to grasp yet,” says Dr Kohli.image: Shira InbarDespite those limitations, structural biologists still reckon that AlphaFold has made their work more efficient. The database filled with AlphaFold’s protein predictions allows scientists to work out the likely structure of a protein in a few seconds, as opposed to the years and tens of thousands of dollars it might have taken otherwise.And speeding up the pace of scientific research and discovery, making efficiencies wherever possible, holds plenty of promise. In a recent report on AI in science the OECD, a club of rich countries, said that “while AI is penetrating all domains and stages of science, its full potential is far from realised.” The prize, it concluded, could be enormous: “Accelerating the productivity of research could be the most economically and socially valuable of all the uses of artificial intelligence.”Welcome to the machineIf AI tools manage to boost the productivity of research, the world would no doubt get the “multiplier for human ingenuity” predicted by Dr Hassabis. But AI holds more potential still: just like telescopes and microscopes let scientists see more of the world, the probabilistic, data-driven models used in AI will increasingly allow scientists to better model and understand complex systems. Fields like climate science and structural biology are already at the point where scientists know that complicated processes are happening, but researchers so far have mainly tried to understand those subjects using top-down rules, equations and simulations. AI can help scientists approach problems from the bottom up instead—measure lots of data first, and use algorithms to come up with the rules, patterns, equations and scientific understanding later.If the past few years have seen scientists dip their toes into the shallow waters of AI, the next decade and beyond will be when they have to dive into its depths and swim towards the horizon. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The future is electric. That means it will need a lot of batteries, motors and wires. That, in turn, means a lot of cobalt, copper, lithium and nickel with which to build them.  Great times, then, for prospectors, and particularly for any who think they can increase the efficiency of their profession. Several firms are applying artificial intelligence (AI) to the process, both to improve the odds of surface strikes and to detect underground ore bodies that are invisible to current techniques.KoBold Metals in Berkeley, California, Earth AI in San Francisco and VerAI in Boston are tiddlers at the moment, as are SensOre, in Melbourne and OreFox, in Brisbane. But at least one bigger fish—Rio Tinto, an Australian-British firm—is also keen. They are garnering reams of geological, geochemical and geophysical data to feed to software models. These, they hope, will spot patterns and draw inferences about where to sink new mines.Some of the data are new. But a lot once mouldered in the archives of national geological surveys, journals of geology and other historical repositories—or, in the case of Rio Tinto, which has been operating for 150 years, sat in the form of rock cores in various sheds around the world.Kobolds were mythical underground sprites that bedevilled miners in medieval Germany. (They also gave their name to cobalt.) Kurt House, KoBold’s boss, hopes some of their magic will rub off. His firm has reformatted archive data from around the world, many of which are on paper and some of which go back to the 19th century, into machine-useable form. That has permitted it to build maps of areas of interest all over Earth’s surface.Some of those maps are used to train the company’s AI models. Others are used to test that software’s effectiveness by checking how good it is at predicting known ore deposits on maps it has not previously seen. If it passes, it can be let loose on under-explored places of interest, generating leads for KoBold’s geologists.Earth AI, led by Roman Teslyuk, SensOre, led by Richard Taylor, and OreFox, led by Warwick Anderson, have taken similar approaches, but have concentrated on Australia, which has particularly rich public geological records. VerAI, led by Yair Frastai, focuses on the western bits of North and South America, home to eight of the world’s ten biggest copper mines.Dr House is especially proud of his AI’s ability to predict the shapes and distributions of subterranean plutonic intrusions. These are bodies of igneous rock, often ore-bearing, that have risen as liquid magma from Earth’s interior but solidified before they reached the surface. They can be detected from the surface via magnetic anomalies which suggest that a particular group of rocks formed at a different time from its surroundings, a standard practice in the industry. But KoBold’s AI is able to make more accurate predictions of the shapes of these intrusions, and so suggest the most effective places to drill.And with success. Last year, KoBold announced its discovery of a rich deposit of chalcocite, a sulphide of copper, in Zambia. Earth AI, meanwhile, has to its credit a big find of molybdenum (an important component of specialist steels) in New South Wales. VerAI has found ore containing copper, gold and silver in Chile and Peru. SensOre has found a large source of lithium in Western Australia. And OreFox’s technology has turned up a potential gold mine in Victoria, plus several promising copper prospects.Rio Tinto is building what Russell Eley, its head of exploration data science, calls a “virtual core shed”. This will bring together details of the many rock-core samples the firm has collected over the years. Software will then search these for patterns that will assist the interpretation of new cores, and tell geologists the best places to drill next.Dr House observes that 99% of exploration projects fail to turn into actual mines. AI therefore has plenty of room to improve things. It may also help with a more subtle problem. By greatly expanding the volume of rock which can be searched, it will enable new strikes in familiar, well-governed countries, lessening the need to rely on what Mr Taylor diplomatically calls “exotic jurisdictions” for future supplies. ■Read more of our articles on artificial intelligence
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.It is now possible to generate fake but realistic content with little more than the click of a mouse. This can be fun: a TikTok account on which—among other things—an artificial Tom Cruise wearing a purple robe sings “Tiny Dancer” to (the real) Paris Hilton holding a toy dog has attracted 5.1m followers. It is also a profound change in societies that have long regarded images, video and audio as close to ironclad proof that something is real. Phone scammers now need just ten seconds of audio to mimic the voices of loved ones in distress; rogue AI-generated Tom Hankses and Taylor Swifts endorse dodgy products online, and fake videos of politicians are proliferating.The fundamental problem is an old one. From the printing press to the internet, new technologies have often made it easier to spread untruths or impersonate the trustworthy. Typically, humans have used shortcuts to sniff out foul play: one too many spelling mistakes suggests an email might be a phishing attack, for example. Most recently, ai-generated images of people have often been betrayed by their strangely rendered hands; fake video and audio can sometimes be out of sync. Implausible content now immediately raises suspicion among those who know what AI is capable of doing.Explore more of our coverage of aiThe trouble is that the fakes are rapidly getting harder to spot. ai is improving all the time, as computing power and training data become more abundant. Could ai-powered fake-detection software, built into web browsers, identify computer-generated content? Sadly not. As we report this week, the arms race between generation and detection favours the forger. Eventually ai models will probably be able to produce pixel-perfect counterfeits—digital clones of what a genuine recording of an event would have looked like, had it happened. Even the best detection system would have no crack to find and no ledge to grasp. Models run by regulated companies can be forced to include a watermark, but that would not affect scammers wielding open-source models, which fraudsters can tweak and run at home on their laptops.Dystopian possibilities abound. It will be difficult, for example, to avoid a world in which any photograph of a person can be made pornographic by someone using an open-source model in their basement, then used for blackmail—a tactic the fbi has already warned about. Perhaps anyone will be able to produce a video of a president or prime minister announcing a nuclear first strike, momentarily setting the world on edge. Fraudsters impersonating relatives will prosper.Yet societies will also adapt to the fakers. People will learn that images, audio or video of something do not prove that it happened, any more than a drawing of it does (the era of open-source intelligence, in which information can be reliably crowdsourced, may be short-lived). Online content will no longer verify itself, so who posted something will become as important as what was posted. Assuming trustworthy sources can continue to identify themselves securely—via urls, email addresses and social-media platforms—reputation and provenance will become more important than ever.It may sound strange, but this was true for most of history. The era of trusted, mass-produced content was the exception. The fact that people may soon struggle to spot the invisible hand of ai does not mean the marketplace of ideas is doomed. In time, the fakes that thrive will mostly be the funny ones. ■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.TWENTY-FIVE years ago your correspondent hired a cellphone in Congo. Each day, it cost what a typical local made in several months. The handset was as heavy as a half-brick and only somewhat more useful. Practically no one else in Congo had one, bar cabinet ministers and tycoons, so there were not many people to call. In those days, mobile phones had made no detectable difference to most people’s lives in the world’s poorest countries.Today, many farmers in Congo have phones: the number of connections has grown 5,000-fold as the population has doubled. Mobile devices have transformed lives throughout the developing world, especially as more and more of them are hooked up to the internet (see chart 1). The 4bn people who live in low or lower-middle income countries have vastly more access to information, chat daily to far-off friends and use their phones like bank cards even when they don’t have bank accounts.image: The EconomistCould artificial intelligence (ai) bring similarly dramatic changes? There are three main reasons for optimism. First, the technology is improving fast. Second, it has the potential to spread fast, too. As usually happens with new technologies, rich countries will benefit first. But if the high cost of training AI models falls, the expense of providing the technology to the poor could be minimal. They will not need a new device, just the smartphones that many of them already own.The third reason is that developing countries have gaping shortages of skilled workers: there are nowhere near enough teachers, doctors, engineers or managers. AI could ease this shortfall, not by replacing existing workers, but by helping them become more productive, argues Daniel Björkegren of Columbia University, which in turn could raise the general level of health and education. Although AI may also eliminate some jobs, the IMF predicts that labour markets in poorer countries will be less disrupted than those in rich ones. Another tantalising possibility is that AI could help provide fine-grained, up-to-date data about poor places, and so assist in all manner of development work. image: The EconomistStart with education. A typical sub-Saharan pupil spends six years in school but retains only three years’ worth of learning, Wolfgang Lutz of the Wittgenstein Centre in Vienna estimated in 2015. A typical Japanese student spends 14 years in classes and absorbs 16 years’ worth of education. Using a different methodology, the World Bank also finds that education is spectacularly worse in poor countries than in rich ones (see chart 2).Tonee Ndungu, an entrepreneur in Kenya, thinks AI could help bridge this gap. He has developed two apps that he hopes to launch this year. One, called Somanasi (“Learn with me”), is for children. It allows pupils to ask a talking chatbot questions related to the Kenyan school curriculum. The Economist asked, “How do I work out a percentage from a fraction?” The chatbot offered a step-by-step worked example.Machine learningA chatbot can give undivided attention to each child, at any time of day, and never gets tired (so long as your phone is charged). It can also be adapted to local cultures. “I never saw an apple till I was 30,” says Mr Ndungu. “So we say ‘A is for animal.’” The service can be tailored to different learning styles, too. It might illustrate division by telling children to break a pencil in half and then again. Depending on how different pupils respond, the AI can figure out whether this approach works, and fine-tune the way it interacts with them. Some kids want more numbers; some like stories. The chatbot adapts.It cannot yet mark homework. But Mr Ndungu’s firm, Kytabu, offers an app for teachers, too, called Hodari (“Brave”). It lightens their workload by crafting step-by-step lesson plans. It helps track what pupils understand, by getting each one to answer questions on a smartphone. (One phone per classroom is enough, he says.)As far as The Economist could tell from playing with them in a café with good Wi-Fi, the two apps work well. But the proof will come—and bugs will be fixed—when more people use them in classrooms and homes. They will be given away to begin with; Mr Ndungu hopes eventually to charge for add-ons. The more children are enrolled, the cheaper it will be to provide the service. If half a million were to join, Mr Ndungu predicts the cost per child would fall from $3.50 a month (not including the phone) to about 15 cents.Chat GPAMany entrepreneurs are pursuing similar projects, often using open-source models developed in rich countries, sometimes with help from charities like the Gates Foundation. The cost of getting AI to learn new languages appears low. It is already being used to write children’s books in tongues previously too obscure for commercial publishers to bother with.The need is glaring. Developing countries have too few teachers, many of whom have not mastered the curriculum. A study in 2015 (using data going back to 2007) found that four-fifths of grade six maths teachers in South Africa did not understand the concepts they were supposed to teach. Nearly 90% of ten-year-olds in sub-Saharan Africa cannot read a simple text.Dr Björkegren points to recent studies suggesting that big gains are possible even with basic tech. One analysed an approach under which schools hire modestly qualified teachers and give them detailed “scripts” for lessons, delivered via tablet computers. Michael Kremer, a Nobel-prize-winning economist, and others studied 10,000 pupils taught this way in Kenya, at schools run by Bridge International Academies, a chain of cheap private schools. They found that after two years on average Bridge students had mastered nearly an extra year’s worth of the curriculum, compared with pupils enrolled in normal schools. Another study in India found that personalised computerised instruction was especially helpful for pupils who were far behind. Using AI in health care is riskier. If an educational chatbot misfires, a pupil might flunk a test; if a medical one hallucinates, a patient could die. Nonetheless, optimists see great potential. Some AI-powered medical kit is already widely used in rich countries and is starting to be adopted in poorer ones. Examples include handheld ultrasound devices that can interpret scans, and a system for spotting tuberculosis on chest X-rays. Accurate ai translation could also make it easier for patients and health-care workers in the global south to tap into the world’s medical knowledge. Even imperfect AI tools may improve health-care systems in the developing world, whose failures cause more than 8m deaths a year, by one estimate. In a study of nine poor and middle-income countries by Todd Lewis of Harvard and others, 2,000 recently graduated primary health-care workers were observed dealing with visitors to clinics. They performed the correct, essential tasks required by clinical guidelines only about half the time.For people in remote areas, even a substandard clinic may be too far away or too costly. Many rely on traditional medicine, much of which is useless or harmful. South African folk healers sometimes cut patients to rub in toxic powder suffused with mercury, for example. AI tools need not be infallible to be better than that.A team at the University of São Paulo is training an AI to answer health-related questions. The aim is to give a tool to primary-health workers in Brazil, who sometimes have little training. They are using a database of clinical guidelines from Brazil’s health ministry, rather than the whole internet, which is full of voodoo health tips. Before the ai can be widely deployed, it must be tested, tweaked and tested again. Currently, when you ask precise, technical questions, such as “Is Ivermectin effective in preventing covid-19?”, its success rate is “so, so high”, says Francisco Barbosa, one of the team. The trouble comes when you ask it something vague, as humans often do. If you say, “I’ve fallen in the street. How can I get to a pharmacy?”, then the AI, which may not know where you are, might give terrible advice.The AI will have to improve and its users will have to learn how to get the best out of it, says Mr Barbosa. He is confident that this will happen: “It’s a cliché [to say it], but it’s changing everything.” Equipping a new hospital costs millions of dollars. Training a new doctor takes years. If AI helps cheap primary-care workers treat more patients successfully, so that they do not need to go to a hospital, Brazil can keep its population healthier without spending more.Brazil has one doctor for every 467 people; Kenya has one for every 4,425. AI could help, says Daphne Ngunjiri of Access Afya, a Kenyan firm that runs mDaktari, a virtual health-care platform with 29,000 clients. For a small monthly fee, they can ask for advice when they feel unwell.Bard to handleFor a test group of 380 users, mDaktari has added an AI-powered chatbot to the system. It records their queries, prompts them for more information and presents that information, along with a suggested response, to a clinician, often a nurse. The clinician reads it and, if the advice is sound, approves it and sends it back to the customer, perhaps referring her to a pharmacy or a clinic. Thus, a human is in the loop, to guard against errors, but the AI does the time-consuming gathering of information about symptoms, enabling the nurse to deal with more patients. If necessary, the nurse can call the patient. For embarrassing ailments such as sexually transmitted diseases, some patients prefer talking to a chatbot. It never judges them.Virginia, a client from a Nairobi slum whose family subsists on casual labour and backyard vegetables, says mDaktari is simple and helpful. One time she felt sick, consulted the app, and was steered to drugs that cleared up what turned out to be a urinary-tract infection. “I can even contact a [nurse] through my phone and get [an] answer,” she says.Several firms are testing AI-enhanced medical devices to see how well they work in poor areas. Philips, a Dutch firm, has a pilot programme in Kenya for a handheld ultrasound with an AI add-on that can interpret the images it spits out. This helps solve a common problem: lots of pregnant mothers and not enough people with the  expertise to read scans.Sadiki Jira is a midwife at a rural health facility in Kenya that serves nearly 30,000 people but has no doctor. He recalls a pregnant patient a couple of years ago whose baby had died in the womb. She had not realised for several weeks and had only sought help when she started haemorrhaging. Mr Jira referred her to a hospital, but it was too late: she died.Mr Jira now uses an AI-powered scanner. Any midwife can, with minimal training, swipe a Philips device over a pregnant woman’s stomach. The AI reveals such vital information as the fetus’s gestational age, whether it is in the breech position and whether there is adequate amniotic fluid. “It’s easy to use,” says Mr Jira.Philips is planning to offer the device and AI together for $1 or $2 a day in poor countries. The biggest obstacles to its rollout are regulatory, says Matthijs Groot Wassink of Philips. Will governments allow midwives to handle a process that previously required someone more qualified? What will happen in places like India, where regulations are especially tight for fear that people will use ultrasound to identify and abort baby girls?Poorer places collect poorer data. Forty-nine countries have gone more than 15 years since their most recent agricultural census; 13 have not conducted a population census in that period. Official numbers, when they exist, tend to flatter the government. For example, a study compared official estimates of how much maize was being grown on small farms in Ethiopia, Malawi and Nigeria with the results of painstaking (but rare) household surveys. The official numbers were much rosier.Satellite imagery and machine-learning could improve the quality and timeliness of data in developing countries, argue Marshall Burke of Stanford University and his co-authors in a recent paper in Science. Roughly 2.5bn people live in households that depend on tiny plots of land. Until recently the output of such farms was hard to measure: satellite pictures were not sharp enough and data drawn from them were too hard to interpret. But by setting AI to work on new high-resolution images of vegetation, Dr Burke and David Lobell, also of Stanford, were able to measure crop yields as accurately as surveys do, but faster and more cheaply. This could allow frequent, detailed analysis of farming practices. How much fertiliser is needed on this hillside? Which seeds work best in that valley? Such knowledge could transform rural livelihoods, the authors predict.So could better weather forecasts. Atmo, an American firm, says its AI-powered weather forecasts are as much as 100 times more detailed and twice as accurate as a conventional meteorological bulletin, because the AI processes data so much faster. It is also cheap. “A dirty secret of meteorology…is that there are vast inequalities,” says Alex Levy, Atmo’s boss. Forecasts are less detailed or reliable in poor countries. ”The places [with] the most extreme weather also have the worst forecasts, [so] they are most likely to be surprised and unable to prepare adequately.” Atmo’s service is being used in Uganda and may soon be deployed in the Philippines.Population counts in poor countries are rare, because they are costly, and prone to manipulation. In Nigeria the money each state gets from the central government is tied to its population. This gives states an incentive to fiddle. In 1991, on a census form with space for up to nine members per household, some states reported exactly nine members in every one. When the results of the census of 2006 were published, Bola Tinubu, the governor of Lagos, angrily claimed that its population was double the official tally. Nigeria has not held another census since. A new president—Mr Tinubu, as it happens—promises one in 2024.image: Cristina SpanoAI can generate more frequent, detailed estimates of how many people live where—and how well-off they are. Lights at night are often used as a proxy for economic buzz. Neal Jean of Stanford and others took day and night images of slums in Africa and trained a convolutional neural network (a form of machine learning) to predict from daytime images how much light there would be at night. In other words,  AI learnt to recognise the kinds of buildings, infrastructure and other markers that tend to go with economic activity. It was able to predict 55-75% of the variation in assets between households.Such information could help governments and charities assess better the effects of efforts to help the needy; it could also help companies understand markets. Researchers are avidly trying out such techniques, but governments have been slow to adopt them, laments Dr Burke. He attributes this in part to “the potential benefits to some policymakers of not having certain outcomes be measured”.AI could also help people deal with the red tape that throttles productivity in so many poor countries. Registering a property takes 200 times longer in Haiti than in wealthy Qatar, according to the World Bank. Suppose an AI, which is immune to boredom, were able to fill in the forms accurately enough to spare humans the chore? In September India launched a chatbot that lets illiterate farmers pose oral queries about applications for financial aid. Some 500,000 tried it on the first day.Deep minefieldAI poses risks to poor countries, too. They are generally less democratic than rich ones, so many governments will adopt AI surveillance tools, pioneered by China, to monitor and control their people. They are less stable, so incendiary deepfakes may be more likely to warp politics or spark violence. Underfunded and inexpert regulators may struggle to impose proper guardrails against potential abuses.And there are big obstacles to deploying AI in the developing world. Access to the internet will have to improve. Some countries will benefit faster than others. India has 790m mobile broadband users, plus a universal digital identity system and a super-cheap, real-time payments system, note Nandan Nilekani and Tanuj Bhojwani, two tech bosses, in Finance & Development. This, they argue, “puts it in a favourable position to be the world’s most extensive user of AI by the end of this decade”.Enormous uncertainty remains about how powerful the technology will eventually prove. But the potential upside is big enough to warrant a tremor of excitement. In the best-case scenario, AI could help make whole populations healthier, better educated and better informed. In time, that could make them a lot less poor. ■
MOST LAWS are local—except in the digital realm. When the European Union comes up with some new tech regulation, it can quickly spread around the world. Global companies adopt its typically strict rules for all their products and markets in order to avoid having to comply with multiple regimes. Other governments take more than one page from the EU’s rule book to help local firms compete. The textbook example for what has been dubbed the “Brussels effect”, is the EU’s General Data Protection Regulation (GDPR), which went into force in 2018 and swiftly became the global standard.Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Small wonder, then, that all eyes were on Brussels when the European Commission on April 21st published proposed regulations on artificial intelligence (AI)—making it the first influential regulator to craft a big law on AI. Will these rules be as widely adopted as GDPR?Recent years have seen an explosion of ethics guidelines in AI, in keeping with the hype surrounding the technology. Many hope it will boost economic growth, but others worry that AI could cause great harm, for instance if algorithms end up discriminating against certain groups of people. At least 175 countries, firms and other organisations have drawn up lists of ethical principles. But most of these fall short of describing how such things as “robustness” or “transparency” can be achieved inpractice, let alone how they can be backed up by enforceable laws, says Charlotte Stix of the Eindhoven University of Technology.With little existing legislation on AI to draw on, the commission opted for a bottom-up approach. It created a 52-member “high-level expert group” to develop its proposals, collected further input via an “AI alliance” of interested parties and published a white paper on which everybody could comment online (1,250 groups and individuals did so). The result is a document of more than 100 pages with 85 articles and no fewer than nine annexes that tries both to mitigate the potential harm of AI and to maximise its opportunities—almost to a fault, as the many exceptions and exceptions to exceptions show.Rather than regulating all applications of AI, the EU’s rules are meant to focus on the riskiest ones. Some will be banned outright, including services that use “subliminal techniques” to manipulate people. Others, such as facial recognition and credit scoring, are considered “high-risk” and so subject to strict rules on transparency and data quality. As with GDPR, penalties for violations are stiff: up to €30m ($36m) or 6% of global revenues, whichever is higher (in the case of a firm as big as Facebook, for example, that would come to more than $5bn).Even more than usual, however, the devil is in the details. Facial recognition for the purpose of law enforcement in public places, which raises the spectre of pervasive surveillance, is prohibited, but only if it is done in real time and barring any other “substantial public interest”, such as finding missing children. All high-risk AI services must be tested for legal conformity, but this can often be done by the provider itself. And EU member states are encouraged to create regulatory “sandboxes”, in which firms can try out novel services without fear of being hit by a penalty.Unsurprisingly, many interested parties are unhappy. Human-rights advocates criticise mushy language and loopholes. “There’s a real question-mark over whether the regulatory framework is robust enough,” says Sarah Chander of European Digital Rights. In contrast, business groups complain about the regulatory burden. The law will “limit the areas in which AI can realistically be used”, warns Benjamin Mueller of the Centre for Data Innovation, a think-tank supported by tech firms.Much will still change in a legislative process that will take years, perhaps even longer than the four years it took GDPR to get from proposal to adoption. But even if things move faster, the EU may have a harder time setting global rules, or at least strict ones, says Anu Bradford of Columbia Law School, who has written a book about the Brussels effect. In the case of some AI applications, such as algorithms that can be perfected without needing to be trained using massive inputs of data, providers may decide that offering special versions in Europe is worth their while. And having been rather surprised by the success of GDPR, lobbyists will redouble their efforts to get their voices heard in Brussels.Yet the fate of the “Artificial Intelligence Act” (AIA), as it could end up being called, may well be decided in America. If GDPR took the world by storm, it was partly because Congress not only failed to come up with any data-protection legislation of its own, but also did not bother to co-operate with lawmakers in Brussels. The new administration wants to do better, but so far the transatlantic rapprochement in AI and other things tech is off to a slow start. Only if both sides work together will they beat back China’s ambitions for tech supremacy and keep digital authoritarianism at bay. ■
This article is part of our Summer reads series. Visit the full collection for book lists, guest essays and more seasonal distractions.FEARS OF ARTIFICIAL INTELLIGENCE (AI) have haunted humanity since the very beginning of the computer age. Hitherto these fears focused on machines using physical means to kill, enslave or replace people. But over the past couple of years new AI tools have emerged that threaten the survival of human civilisation from an unexpected direction. AI has gained some remarkable abilities to manipulate and generate language, whether with words, sounds or images. AI has thereby hacked the operating system of our civilisation.Language is the stuff almost all human culture is made of. Human rights, for example, aren’t inscribed in our DNA. Rather, they are cultural artefacts we created by telling stories and writing laws. Gods aren’t physical realities. Rather, they are cultural artefacts we created by inventing myths and writing scriptures.Money, too, is a cultural artefact. Banknotes are just colourful pieces of paper, and at present more than 90% of money is not even banknotes—it is just digital information in computers. What gives money value is the stories that bankers, finance ministers and cryptocurrency gurus tell us about it. Sam Bankman-Fried, Elizabeth Holmes and Bernie Madoff were not particularly good at creating real value, but they were all extremely capable storytellers.What would happen once a non-human intelligence becomes better than the average human at telling stories, composing melodies, drawing images, and writing laws and scriptures? When people think about ChatGPT and other new AI tools, they are often drawn to examples like school children using AI to write their essays. What will happen to the school system when kids do that? But this kind of question misses the big picture. Forget about school essays. Think of the next American presidential race in 2024, and try to imagine the impact of AI tools that can be made to mass-produce political content, fake-news stories and scriptures for new cults.In recent years the QAnon cult has coalesced around anonymous online messages, known as “Q drops”. Followers collected, revered and interpreted these Q drops as a sacred text. While to the best of our knowledge all previous Q drops were composed by humans, and bots merely helped disseminate them, in future we might see the first cults in history whose revered texts were written by a non-human intelligence. Religions throughout history have claimed a non-human source for their holy books. Soon that might be a reality.On a more prosaic level, we might soon find ourselves conducting lengthy online discussions about abortion, climate change or the Russian invasion of Ukraine with entities that we think are humans—but are actually AI. The catch is that it is utterly pointless for us to spend time trying to change the declared opinions of an AI bot, while the AI could hone its messages so precisely that it stands a good chance of influencing us.Explore more Summer reads:The mystery of Morocco’s missing king. He befriended a kickboxer in 2018, and has rarely been seen since.Hollywood is losing the battle for China. Watch how domestic blockbusters are dominating the market.Russia’s economy can withstand a long war. But not a more intense one.The six novels chosen for our reviewers’ attention so far this year—and worthy of yours.Through its mastery of language, AI could even form intimate relationships with people, and use the power of intimacy to change our opinions and worldviews. Although there is no indication that AI has any consciousness or feelings of its own, to foster fake intimacy with humans it is enough if the AI can make them feel emotionally attached to it. In June 2022 Blake Lemoine, a Google engineer, publicly claimed that the AI chatbot LaMDA, on which he was working, had become sentient. The controversial claim cost him his job. The most interesting thing about this episode was not Mr Lemoine’s claim, which was probably false. Rather, it was his willingness to risk his lucrative job for the sake of the AI chatbot. If AI can influence people to risk their jobs for it, what else could it induce them to do?In a political battle for minds and hearts, intimacy is the most efficient weapon, and AI has just gained the ability to mass-produce intimate relationships with millions of people. We all know that over the past decade social media has become a battleground for controlling human attention. With the new generation of AI, the battlefront is shifting from attention to intimacy. What will happen to human society and human psychology as AI fights AI in a battle to fake intimate relationships with us, which can then be used to convince us to vote for particular politicians or buy particular products?Even without creating “fake intimacy”, the new AI tools would have an immense influence on our opinions and worldviews. People may come to use a single AI adviser as a one-stop, all-knowing oracle. No wonder Google is terrified. Why bother searching, when I can just ask the oracle? The news and advertising industries should also be terrified. Why read a newspaper when I can just ask the oracle to tell me the latest news? And what’s the purpose of advertisements, when I can just ask the oracle to tell me what to buy?And even these scenarios don’t really capture the big picture. What we are talking about is potentially the end of human history. Not the end of history, just the end of its human-dominated part. History is the interaction between biology and culture; between our biological needs and desires for things like food and sex, and our cultural creations like religions and laws. History is the process through which laws and religions shape food and sex.What will happen to the course of history when AI takes over culture, and begins producing stories, melodies, laws and religions? Previous tools like the printing press and radio helped spread the cultural ideas of humans, but they never created new cultural ideas of their own. AI is fundamentally different. AI can create completely new ideas, completely new culture.At first, AI will probably imitate the human prototypes that it was trained on in its infancy. But with each passing year, AI culture will boldly go where no human has gone before. For millennia human beings have lived inside the dreams of other humans. In the coming decades we might find ourselves living inside the dreams of an alien intelligence.Fear of AI has haunted humankind for only the past few decades. But for thousands of years humans have been haunted by a much deeper fear. We have always appreciated the power of stories and images to manipulate our minds and to create illusions. Consequently, since ancient times humans have feared being trapped in a world of illusions.In the 17th century René Descartes feared that perhaps a malicious demon was trapping him inside a world of illusions, creating everything he saw and heard. In ancient Greece Plato told the famous Allegory of the Cave, in which a group of people are chained inside a cave all their lives, facing a blank wall. A screen. On that screen they see projected various shadows. The prisoners mistake the illusions they see there for reality.In ancient India Buddhist and Hindu sages pointed out that all humans lived trapped inside Maya—the world of illusions. What we normally take to be reality is often just fictions in our own minds. People may wage entire wars, killing others and willing to be killed themselves, because of their belief in this or that illusion.The AI revolution is bringing us face to face with Descartes’ demon, with Plato’s cave, with the Maya. If we are not careful, we might be trapped behind a curtain of illusions, which we could not tear away—or even realise is there.Of course, the new power of AI could be used for good purposes as well. I won’t dwell on this, because the people who develop AI talk about it enough. The job of historians and philosophers like myself is to point out the dangers. But certainly, AI can help us in countless ways, from finding new cures for cancer to discovering solutions to the ecological crisis. The question we face is how to make sure the new AI tools are used for good rather than for ill. To do that, we first need to appreciate the true capabilities of these tools.Since 1945 we have known that nuclear technology could generate cheap energy for the benefit of humans—but could also physically destroy human civilisation. We therefore reshaped the entire international order to protect humanity, and to make sure nuclear technology was used primarily for good. We now have to grapple with a new weapon of mass destruction that can annihilate our mental and social world.We can still regulate the new AI tools, but we must act quickly. Whereas nukes cannot invent more powerful nukes, AI can make exponentially more powerful AI. The first crucial step is to demand rigorous safety checks before powerful AI tools are released into the public domain. Just as a pharmaceutical company cannot release new drugs before testing both their short-term and long-term side-effects, so tech companies shouldn’t release new AI tools before they are made safe. We need an equivalent of the Food and Drug Administration for new technology, and we need it yesterday.Won’t slowing down public deployments of AI cause democracies to lag behind more ruthless authoritarian regimes? Just the opposite. Unregulated AI deployments would create social chaos, which would benefit autocrats and ruin democracies. Democracy is a conversation, and conversations rely on language. When AI hacks language, it could destroy our ability to have meaningful conversations, thereby destroying democracy.We have just encountered an alien intelligence, here on Earth. We don’t know much about it, except that it might destroy our civilisation. We should put a halt to the irresponsible deployment of AI tools in the public sphere, and regulate AI before it regulates us. And the first regulation I would suggest is to make it mandatory for AI to disclose that it is an AI. If I am having a conversation with someone, and I cannot tell whether it is a human or an AI—that’s the end of democracy.This text has been generated by a human.Or has it?_______________Yuval Noah Harari is a historian, philosopher and author of “Sapiens”, “Homo Deus” and the children’s series “Unstoppable Us”. He is a lecturer in the Hebrew University of Jerusalem’s history department and co-founder of Sapienship, a social-impact company.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.OVER RECENT decades the oil-rich economies of the Gulf have shown a taste for flashy government projects with dubious payoffs. In the early 2000s Dubai spent an estimated $12bn building an artificial archipelago shaped like a palm. Last year Qatar splurged around $220bn hosting the football World Cup. Saudi Arabia, the region’s gorilla, is building a pair of 120km-long skyscrapers in the desert—for roughly $1trn.Amid the vanity projects, some serious efforts at economic diversification are also being pursued. One such endeavour is under way in Abu Dhabi, where earlier this month a government research institute released Falcon 180B, its latest massive artificial-intelligence (AI) model, which is impressing technologists around the world with its performance.Abu Dhabi has even bigger AI ambitions. “We are entering the game to disrupt the core players,” says Faisal al-Bannai, secretary-general of the Advanced Technology Research Council (ATRC), the government agency which houses the institute that created Falcon. He says that later this year the ATRC will announce the launch of a state-backed AI company to go head to head with the field’s leading lights, such as OpenAI, creator of ChatGPT. Though it will face an uphill battle, the Emirati outfit could yet emerge as a credible competitor. Its success will be closely watched both by rivals and by other governments seeking to carve out a role in an AI economy currently dominated by America and China.The Technology Innovation Institute (TII), the applied-research arm of the ATRC, employs around 800 staff of 74 nationalities, working on subjects from biotechnology and robotics to quantum computing. Launched in 2020, it has been experimenting with ChatGPT-like “generative” AI for some time. It released Noor, an Arabic-based AI model, in April last year, and then Falcon 40B, the first iteration of its flagship open-source model, in May this year.Falcon 180B, as its name hints, is a beefed-up version of its predecessor. Comparing the performance of AI models is notoriously tricky, but going by a selection of commonly used benchmarks compiled by Hugging Face, a library of models, TII’s latest release bests the previous open-source champion, Meta’s LLaMA 2. A blog post by Hugging Face staff suggests the model is “on par” with Google’s PaLM 2.Why make such a powerful model freely available? Mr Bannai talks of “democratising” access to a transformational new technology, and warns against power falling into the hands of a small clique of companies, as has happened in the internet economy. But opening up access to Falcon 180B also allows software engineers to play around with a model that is not quite at the technological frontier, and suggest improvements. According to tii, some 12m developers experimented with the first generation of Falcon.Giving away the model also opens up other opportunities for monetisation. Consider Stability AI, a startup whose open-source Stable Diffusion model was behind 13bn of the 15bn images generated by AI in the year to August, according to Everypixel, a software firm. Emad Mostaque, Stability AI’s founder, says that its open-source strategy makes “commercial sense” for the firm “because the models are adopted far more quickly and widely than proprietary models”. Although the company generates revenue directly through its DreamStudio text-to-image generator, that tool accounts for a small fraction of the pictures produced using Stable Diffusion. It also makes money from developers opting to build applications based around the model (or others created by the company) on top of its computing platform, and by building tailored solutions for customers.Mr Bannai has a similar vision. He pictures an “end-to-end platform for AI developers”, pointing to Shopify, an e-commerce platform used by merchants to build online shops, as his inspiration. He says the company will also build new proprietary models and applications tailored for specific fields, such as medicine and law, while keeping access to its core model open. It will experiment, too, with “multimodal” AI systems that incorporate many types of data (from text and images to audio) and “edge” models that can run on smaller devices such as phones.Abu Dhabi may not seem like an obvious hub for AI talent, but big (and tax-free) salaries have already started luring tech whizzes from abroad. The emirate has also been training local brainboxes, including at the Mohamed bin Zayed University of Artificial Intelligence, founded in 2019. And although it will be a late entrant to an already crowded race, the Emirati firm will have some advantages, too.One is a tight-knit business milieu. Many bosses are still grappling with how best to harness generative AI. By teaming up with local businesses and using them as test cases, Mr Bannai reckons his agency’s AI company will quickly be able to learn what works and what doesn’t.Another advantage is the emirate’s deep pockets. Abu Dhabi’s various sovereign-wealth funds hold around $1.5trn in assets, which makes even OpenAI’s $40bn valuation look like chump change. With frontier AI models becoming more computationally intensive and data-guzzling, access to cash could become decisive, especially in a world of higher interest rates.If the endeavour succeeds, it will be a favourable omen for the long-term prospects of the Gulf as the demand for oil declines. Countries that harbour similar hopes of becoming an AI superpower, including Britain, will be watching along with interest—and, perhaps, envy. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Wimbledon’s centre court has seen its share of rivalries; think of McEnroe v Borg, or Williams v Williams. But for David Almog, a behavioural economist at Northwestern University, the match worth tuning in for is umpire v machine.How AI oversight affects human decision-making is an important question in a world where algorithms play an ever-larger role in everyday life. Car drivers, financial traders and air-traffic controllers already routinely see their decisions overruled by AI systems put in place to rapidly correct poor judgment. Doctors, judges and even soldiers could be next.Much of this correction happens out of the public eye, thwarting would-be analysts. But, says Mr Almog, “tennis is one of the most visible settings where final decision rights are granted to AI.” That is why, together with colleagues in America and Australia, he has looked at whether tennis umpires and line judges correctly called balls in or out during nearly 100,000 points played in some 700 matches across the world, both before and after the introduction of the Hawk-Eye ball-tracking system in 2006.The Hawk-Eye system, now used at most elite tournaments, uses between six and ten cameras positioned around the court to create a three-dimensional representation of the ball’s trajectory. This can then be presented on a screen visible to players, spectators and officials—as well as tv viewers. Players can use it to appeal human decisions, with the AI’s verdict considered final. Bad calls from line judges and umpires are now often overturned.The latest analysis from Mr Almog and his colleagues, published as a preprint last month, showed that Hawk-Eye oversight has prompted human officials to up their game and make 8% less mistakes than before it was introduced. (That comparison can be made thanks to a 2005 trial period in which Hawk-Eye was used without the ability to influence calls.) Such an improvement in performance is to be expected, the researchers say, given the heightened watchfulness that accompanies the threat of public shaming.Most of the improvement came during the multi-shot rallies that follow a successful serve and return. But when the researchers looked at serves in particular, and especially in cases where the served ball landed within 20mm either side of a line, they were surprised to see the error rate soar. The umpires and line judges, it turned out, had switched strategy. Before Hawk-Eye, they were more likely to call a serve out when it was in. But afterwards, they were even more likely to wave through balls that were actually out. For every 100 mis-hit serves, post-Hawk-Eye umpires left 39 unchallenged. The comparable figure in the earlier era was 26.Such a shift is easily understood. Overlooked faults are less disruptive in tennis than incorrect cries of “out” because these end the point prematurely. They can also trigger dissent from both the player and crowd when the error is identified on the big screen. It seems that human officials take the less reputationally risky option, even if it leads to more incorrect calls.Tennis, with its binary outcomes and clear evidence of whether a decision was right or wrong, offers a highly simplified model for AI oversight. But many of the same tendencies will be at play in fields like medicine and law, says Mr Almog, and should be considered before algorithms are allowed to trump human decisions. Most important, perhaps, is the social cost of getting an important call wrong, which will vary between disciplines, and could distort decision-making in different ways. Judges, for example, may prefer to under-convict. Doctors, on the other hand, might over-diagnose. Stay tuned. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
TWO CENTURIES ago Henri de Saint-Simon, a French utopian, proposed a new religion, worshipping the godlike force of progress, with Isaac Newton as its chief saint. He believed that humanity’s sole uniting interest, “the progress of the sciences”, should be directed by the “elect of humanity”, a 21-member “Council of Newton”. Friedrich Hayek, a 20th-century economist, later gleefully described how this ludicrous “religion of the engineers” collapsed into a welter of feuding sects.Today, the engineers of artificial intelligence (AI) are experiencing their own religious schism. One sect worships progress, canonising Hayek himself. The other is gripped by terror of godlike forces. Their battle has driven practical questions to the margins of debate.Both cults are accidental by-products of science fiction. In 1993 Vernor Vinge drew on computer science and his fellow science-fiction writers to argue that ordinary human history was drawing to a close. We would surely create superhuman intelligence sometime within the next three decades, leading to a “Singularity”, in which AI would start feeding on itself. The future might be delightful or awful, depending on whether machines enhanced human intelligence or displaced it.Some were optimistic. The futurist Ray Kurzweil wrote an enormous tome, “The Singularity is Near”, predicting a cusp in 2045. We humans would become immortal, spreading intelligence throughout the universe, and eventually merging into God. For all its statistics and exponentials, the book prophesied “the Rapture of the Nerds”, as one unkind critic called it. Its title really should have been “The Singularity is Nigh”.Others feared the day of judgment. Eliezer Yudkowsky, a self-taught AI researcher, was deeply influenced by Mr Vinge’s ideas. He fathered Silicon Valley’s “rationalist” movement, which sought to improve human reasoning and stop AI destroying humankind.Rationalists believed that Bayesian statistics and decision theory could de-bias human thinking and model the behaviour of godlike intelligences. They revelled in endless theoretical debates, like medieval Christian philosophers disputing the nature of angels, applying amateur game theory instead of Aristotelian logic. Sometimes their discussions were less erudite. Mr Yudkowsky popularised his ideas in a 660,000-word fan-fiction epic, “Harry Potter and the Methods of Rationality”.Rationalists feared that superhuman AIs wouldn’t have our best interests at heart. One notorious thought experiment—a modern version of Pascal’s wager, dubbed “Roko’s basilisk”—claimed that logic dictated that future divine intelligences would torture anyone who had known that AI was possible and hadn’t devoted themselves to bringing it into existence. AIs might also use their awesome reasoning powers to escape any limits that humans imposed on them, creating an “x risk” (existential risk) to human survival.Rationalism explains why AI pioneers became obsessed with x risk. Sam Altman, Elon Musk and others founded OpenAI, the creator of ChatGPT, as a non-profit so that it wouldn’t duck the dangers of machine intelligence. But the incentives shifted as the funding flooded in. Some OpenAI staffers feared that their employer cared more about the opportunities than the dangers and defected to found Anthropic, a rival AI firm. More recently, clashes over AI risk, money and power reportedly led to the fracture between Mr Altman and his board.If rationalists are frustrated by Silicon Valley’s profit model, Silicon Valley is increasingly frustrated by rationalism. Marc Andreessen, the co-founder of Andreessen Horowitz, a venture-capital firm, fulminated in June that the extremist AI-risk “cult” was holding back an awesome AI-augmented future, in which humanity could reach for the stars.This backlash is turning into its own religion of the engineers. Grimes, a musician and Silicon Valley icon, marvels that AI engineers are “designing the initial culture of the universe”. She calls for a “Council of Elrond” (this conclave a nod to “The Lord of the Rings”) comprising the “heads of key AI companies and others who understand it” to set AI policy. Grimes met Mr Musk, the father of her children, through a shared joke about Roko’s basilisk.In October Mr Andreessen published his own “Techno-Optimist Manifesto” to wide acclaim from Silicon Valley entrepreneurs. In it, he takes aim at a decades-long “demoralisation campaign…against technology and life”, under various names including “sustainable development goals”, “social responsibility”, “trust and safety” and “tech ethics”. Efforts to decelerate AI “will cost human lives” and are thus tantamount to “murder”.Mr Andreessen’s manifesto is a Nicene creed for the cult of progress: the words “we believe” appear no less than 113 times in the text. His list of the “patron saints” of techno-optimism begins with Based Beff Jezos, the social-media persona of a former Google engineer who claims to have founded “effective accelerationism”, a self-described “meta-religion” which puts its faith in the “technocapital Singularity”.Our future is currently being built around Mr Vinge’s three-decades-old essay, a work that only Silicon Valley thinkers and science-fiction fans have read. Warring cults dispute whether engineers are as gods, or just unwitting Dr Frankensteins.This schism is an attention-sucking black hole that makes its protagonists more likely to say and perhaps believe stupid things. Of course, many AI-risk people recognise that there are problems other than the Singularity, but it’s hard to resist its relentless gravitational pull. Before Mr Andreessen was fully dragged past the event horizon, he made more nuanced arguments about engineers’ humility and addressing the problems of AI as they arose.But we need even more to listen to other people. Last month, at Rishi Sunak’s global AI-policy summit, Mr Musk pontificated about the need for an “off switch” for hostile AI. The main event was all about x risk and AI’s transformative promise, consigning other questions to a sideshow dubbed the “AI Fringe”.At the same time, Rachel Coldicutt, a British tech thinker, was putting together a “Fringe of the Fringe”, where a much more diverse group of thinkers debated the topics that hadn’t made the main agenda: communities, transparency, power. They didn’t suggest a Council of the Elect. Instead, they proposed that we should “make AI work for eight billion people, not eight billionaires”. It might be nice to hear from some of those 8bn voices.■Henry Farrell is a professor of international affairs and democracy at Johns Hopkins University, and co-author of “Underground Empire: How America Weaponized the World Economy”.
AT THE start of this year, two straws in the wind caught the attention of those who follow the development of artificial intelligence (AI) globally. First, Qi Lu, one of the bosses of Microsoft, said in January that he would not return to the world’s largest software firm after recovering from a cycling accident, but instead would become chief operating officer at Baidu, China’s leading search engine. Later that month, the Association for the Advancement of Artificial Intelligence postponed its annual meeting. The planned date for the event in January conflicted with the Chinese new year.These were the latest signals that China could be a close second to America—and perhaps even ahead of it—in some areas of AI, widely considered vital to everything from digital assistants to self-driving cars. China is simply the place to be, explains Mr Lu, and Baidu the country’s most important player. “We have an opportunity to lead in the future of AI,” he says.Other evidence supports the claim. In October 2016 the White House noted in a report that China had overtaken America in the number of published journal articles on deep learning, a branch of AI. PwC, a consultancy, predicts that AI-related growth will boost global GDP by $16trn by 2030; nearly half of that bonanza will accrue to China, it reckons. The number of AI-related patent submissions by Chinese researchers has increased by nearly 200% in recent years, although America is still ahead in absolute numbers (see chart).To understand why China is so well placed, consider the inputs needed for AI. Of the two most basic, computing power and capital, it has an abundance. Chinese firms, from giants such as Alibaba and Tencent to startups such as CIB FinTech and UCloud, are building data centres as fast as they can. The market for cloud computing has been growing by more than 30% in recent years and will continue to do so, according to Gartner, a consultancy. In 2012-16 Chinese AI firms received $2.6bn in funding, according to the Wuzhen Institute, a think-tank. That is less than the $17.9bn that poured into their American peers, but the total is growing quickly.Yet it is two other resources that truly make China a promised land for AI. One is research talent. As well as strong skills in maths, the country has a tradition in language and translation research, says Harry Shum, who leads Microsoft’s AI efforts. Finding top-notch AI experts is harder in China than in America, says Wanli Min, who oversees 150 data scientists at Alibaba. But this will change over the next couple of years, he predicts, because most big universities have launched AI programmes. According to some estimates, China has more than two-fifths of the world’s trained AI scientists.The second advantage for China is data, AI’s most important ingredient. In the past, software and digital products mostly obeyed rules laid down in code, giving an edge to those countries with the best coders. With the advent of deep-learning algorithms, such rules are increasingly based on patterns extracted from reams of data. The more data are available, the more algorithms can learn and the smarter AI offerings will be.China’s sheer size and diversity provide powerful fuel for this cycle. Just by going about their daily lives, the country’s nearly 1.4bn people generate more data than almost all other nations combined. Even in the case of a rare disease, there are enough examples to teach an algorithm how to recognise it. Because typing Chinese characters is more laborious than Western ones, people also tend to use voice-recognition services more often than in the West, so firms have more voice snippets with which to improve speech offerings.The Saudi Arabia of dataWhat really sets China apart is that it has more internet users than any other country: about 730m. Almost all go online from smartphones, which generate far more valuable data than desktop computers, chiefly because they contain sensors and are carried around. In the big coastal cities, for instance, cash has all but disappeared for small purchases: people settle with their devices using services such as Alipay and WeChat Pay.Chinese do not seem to be terribly concerned about privacy, which makes collecting data easier. The country’s bike-sharing services, which have taken big cities by storm, for example, not only provide cheap transport but are what is known as a “data play”. When riders hire a bicycle, some firms keep track of renters’ movements using a GPS device attached to the bike.Young Chinese appear particularly keen on AI-powered services and relaxed about use of their data. Xiaoice, an upbeat chatbot operated by Microsoft, now has more than 100m Chinese users. Most talk to it between 11pm and 3am, often about the problems they had during the day. It is learning from interactions and becoming cleverer. Xiaoice no longer just provides encouragement and tells jokes, but has created the first collection of poems written with AI, “Sunshine Lost Its Window”, which caused a heated debate in Chinese literary circles over whether there can be such a thing as artificial poetry.Another important source of support for AI in China is the government. The technology figures prominently in the country’s current five-year plan. Technology firms are working closely with government agencies: Baidu, for example, has been asked to lead a national laboratory for deep learning. It is unlikely that the government will burden AI firms with over-strict regulation. The country has more than 40 laws containing rules about the protection of personal data, but these are rarely enforced.Entrepreneurs are taking advantage of China’s talent and data strengths. Many AI firms got going only a year or two ago, but plenty have been progressing more rapidly than their Western counterparts. “Chinese AI startups often iterate and execute more quickly,” explains Kai-Fu Lee, who ran Google’s subsidiary in China in the 2000s and now leads Sinovation Ventures, a venture-capital fund.As a result, China already has a herd of AI unicorns, meaning startups valued at more than $1bn. Toutiao, a news aggregator based in Beijing, employs machine learning to recommend articles using information such as a reader’s interests and location; it also uses AI to filter out fake information (which in China mainly means dubious health-care announcements). Another AI startup, iFlytek, has developed a voice assistant that translates Mandarin into several languages, including English and German, even if the speaker uses slang and talks over background noise. And Megvii Technology’s face-recognition software, Face++, identifies people almost instantaneously.Skynet livesAt Megvii’s headquarters, visitors are treated to a demonstration. A video camera in the lobby does away with the need for showing ID: employees just walk in without showing their badges. Similar devices are positioned all over the office and their feeds are shown on a video wall. When a face pops up on the wall, it is immediately surrounded by a white rectangle and some text giving information about that person. In the upper right-hand corner of the screen big letters spell “Skynet”, the name of the AI system in the Terminator films that seeks to exterminate the human race. The firm already enables Alipay and Didi, a ride-hailing firm, to check the identity of new customers (their faces are compared with pictures held by the government).Reacting to the success of such startups, China’s tech giants, too, have begun to invest heavily in AI. Baidu, Alibaba and Tencent, collectively called BAT, are working on many of the same services, including speech- and face-recognition. But they are also trying to become dominant in specific areas of AI, based on their existing strengths.Tencent has so far kept the lowest profile; it established its AI labs only in recent months. But it is bound to develop a big presence in AI: it has more data than the other two. Its WeChat messenger service has nearly 1bn accounts and is also the platform for thousands of services, from payments and news to city guides and legal help. Tencent is also a world-beater in games with blockbusters such as League of Legends and Clash of Clans, which have more than 100m players each globally.Alibaba is already a behemoth in e-commerce and is investing billions to become number one in cloud computing. At a conference in June in Shanghai it showed off an AI service called “ET City Brain” that uses video recognition to optimise traffic in real time. It uses footage from roadside cameras to predict the behaviour of cars and can adjust traffic lights on the spot. In its home town of Hangzhou, Alibaba claims, the system has already increased the average speed of traffic by 11%. Alibaba is also planning to beef up what it calls “ET Medical Brain”, which will offer AI-powered services to discover drugs and diagnose medical images. It has signed up a dozen hospitals to get the data it needs.But it is Baidu whose fate is most tied to AI, in part because the technology may be its main chance to catch up with Alibaba and Tencent. It is putting most of its resources into autonomous driving: it wants to get a self-driving car onto the market by 2018 and to provide technology for fully autonomous vehicles by 2020. On July 5th the firm announced a first version of its self-driving-car software, called Apollo, at a developer conference in Beijing.Getting Apollo right will not only involve cars safely navigating the streets, but managing a project that is open to outsiders. Rivals such as Waymo, Google’s subsidiary, and Tesla, an electric-car firm, jealously guard their software and the data they collect. Baidu is planning not only to publish the recipe for its programs (making them “open-source”, in the jargon), but to share data. The idea is that carmakers that use Baidu’s technology will do the same, creating an open platform for data from self-driving cars—the “Android for autonomous vehicles”, in the words of Mr Lu.Drive like a BeijingerIt remains to be seen how successful Chinese firms will be in exporting their AI products—for now, only a tiny handful are used abroad. In theory they should travel well: a self-driving car trained on China’s chaotic streets ought to have no problem navigating the more civilised traffic in Europe (in contrast, a vehicle trained in Germany may not get far beyond the first intersection in Beijing). But consumers in the West may hesitate to use self-driving cars that have been trained in a laxer safety environment that is more tolerant of accidents. Chinese municipalities are said to be falling over themselves to be testing grounds for autonomous vehicles.There is another risk. Data are the most valuable input for AI at the moment, but their importance may yet diminish. AI firms have started to use simulated data, including those from video games. New types of algorithms may be capable of getting smart with fewer examples. “The danger is that we stop innovating in algorithms because of our advantage in data,” warns Gansha Wu, chief executive of UISEE, a Beijing startup which is developing self-driving technology. For now, though, China looks anything but complacent. In the race for pre-eminence in AI, it will run America close.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.ChatGPT, a chatbot developed by OpenAI, an American firm, can give passable answers to questions on everything from nuclear engineering to Stoic philosophy. Or at least, it can in English. The latest version, ChatGPT-4, scored 85% on a common question-and-answer test. In other languages it is less impressive. When taking the test in Telugu, an Indian language spoken by nearly 100m people, for instance, it scored just 62%.OpenAI has not revealed much about how ChatGPT-4 was built. But a look at its predecessor, ChatGPT-3, is suggestive. Large language models (LLMs) are trained on text scraped from the internet, on which English is the lingua franca. Around 93% of ChatGPT-3’s training data was in English. In Common Crawl, just one of the datasets on which the model was trained, English makes up 47% of the corpus, with other (mostly related) European languages accounting for 38% more. Chinese and Japanese combined, by contrast, made up just 9%. Telugu was not even a rounding error.image: The EconomistAn evaluation by Nathaniel Robinson, a researcher at Johns Hopkins University, and his colleagues finds that is not a problem limited to ChatGPT. All LLMs fare better with “high-resource” languages, for which training data are plentiful, than for “low-resource” ones for which they are scarce. That is a problem for those hoping to export AI to poor countries, in the hope it might improve everything from schools to health care. Researchers around the world are therefore working to make AI more multilingual.India’s government is particularly keen. Many of its public services are already digitised, and it is keen to fortify them with AI. In September, for instance, it launched a chatbot to help farmers get information about state benefits.The bot works by welding two sorts of language model together, says Shankar Maruwada of the EkStep Foundation, a non-profit that helped build it. Users can submit queries in their native tongues. (Eight are supported so far; five more are coming soon.) These are passed to a piece of machine-translation software developed at IIT Madras, an Indian academic institution, which translates them into English. The English version of the question is then fed to the LLM, and its response translated back into the user’s mother tongue.The system seems to work. But translating queries into an LLM’s preferred language is a rather clumsy workaround. After all, language is a vehicle for worldviews and culture as well as just meaning, notes the boss of one Indian AI firm. A paper by Rebecca Johnson, a researcher at the University of Sydney, published in 2022, found that ChatGPT-3 gave replies on topics such as gun control and refugee policy that aligned most with the values displayed by Americans in the World Values Survey, a global questionnaire of public opinion.Many researchers are therefore trying to make LLMs themselves more fluent in less widely spoken languages. One approach is to modify the tokeniser, the part of an LLM that chops words into smaller chunks for the rest of the model to manipulate. Text in Devanagari, a script used with Hindi, needs three to four times more tokens, when tokenised the standard way, than the same text in English. An Indian startup called Sarvam AI has written a tokeniser optimised for Hindi, which cuts that number substantially. Fewer tokens means fewer computations. Sarvam reckons that OpenHathi, its Devanagari-optimised LLM, can cut the cost of answering questions by around three-quarters.Another is to improve the datasets on which LLMs are trained. Often this means digitising reams of pen-and-paper texts. In November a team of researchers at Mohamed bin Zayed University, in Abu Dhabi, released the latest version of an Arabic-speaking model called “Jais”. It has one-sixth as many parameters (one measure of a model’s size) as ChatGPT-3, but performs on par with it in Arabic. Timothy Baldwin, the university’s acting provost, notes that, because his team could only digitise so much Arabic text, the model also included some English. Some concepts, after all, are similar across all languages, and can be learned in any tongue. Data in a specific language are more important for teaching the model specific cultural ideas and quirks.The third approach is to tweak models after they have been trained. Both Jais and OpenHathi have had some question-and-answer pairs hand crafted by humans. The same happens with Western chatbots, to stop them spreading what their makers see as disinformation. Ernie Bot, an LLM from Baidu, a big Chinese tech company, has been tweaked to try to stop it saying things to which the government might object. Models can also learn from human feedback, in which users rate an LLM’s answers. But that is hard to do for many poor-world languages, says Dr Baldwin, since it requires recruiting people literate enough to criticise the machine’s writing.How well all this will work remains to be seen. A quarter of India’s adults are illiterate, something that no amount of LLM tweaking will solve. Many Indians prefer using voice messages to communicate rather than text ones. AI can also turn speech into words, as India’s chatbot for farmers does. But that adds another step at which errors can creep in.And it is possible that builders of local LLMs may eventually be put out of business by the efforts of the Silicon Valley big boys. Although it is far from perfect, ChatGPT-4 is much better than ChatGPT-3 at answering questions in non-English languages. However it is done, teaching AI to speak more of the world’s 7,000-odd languages can only be a good thing. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Among the startling, hopeful developments that have greeted the advent of generative artificial intelligence (ai) has been an outbreak of bipartisan focus, curiosity and deliberation in Washington, DC. Legislators and regulators are trying hard to come to grips with the protean technology. Chuck Schumer, the Senate majority leader, has been holding senators-only briefings with experts to educate his chamber. In late June he called for “a new and unique approach” to writing legislation about AI, saying it was “unlike anything Congress has dealt with before”.“It’s not like labour or health care or defence where Congress has had a long history we can work off of,” he said. “In many ways, we’re starting from scratch.” He has set up a steering group of two Republicans and two Democrats, including himself, and plans this autumn to supplement the normal committee process, or posturing, with ”AI Insight Forums”, to include the industry’s leaders and its critics, to do “years of work in a matter of months”.It is understandable that wise guys are making fun of this. Given Congress’s reputation for speed and technological literacy (the Senate was in session for all of 14 days in June, and Mr Schumer uses a flip phone), the jokes write themselves, almost. ChatGPT’s first, unfunny stab reflected the cynicism any sentient being might feel: “Why did the AI refuse to testify before Congress? Because it didn’t want to be caught in a loop of lawmakers asking the same question over and over.”Yet Americans should bask in this rare season, while it lasts, of good-faith searching and head-scratching. Not only are the regulators interrogating themselves about where and how they should regulate, the industry itself is asking to be regulated, up to a point. Even if it is all doomed to end in partisan impasse and recrimination, now is the time to ask: how could Washington get this right?One danger is that lawmakers could wind up bickering over a problem they can address only at the edges. Representatives of both parties regret not more aggressively regulating the internet in general and social media in particular. Being politicians, they are alert to how political actors are already using AI tools to generate political messages, including fabricated images. For their part journalists, being journalists, are obsessed with AI’s potential to create and spread lies.These are serious concerns. But because of America’s speech protections, congressional action would probably be less constructive than voluntary standards by campaigns, technical approaches by the private sector to authenticate images and scepticism from a jaded electorate. The more Congress focuses on the flow of information, the more it will be riven by fights over whether a particular chatbot is inclined to disparage one party or another. “If we get drawn into refighting the social-media wars, we risk not realising the promise of machine learning,” warns Kent Walker, the president of global affairs at Google and Alphabet. “Social media isn’t going to cure cancer, but AI has the potential to, and it would be a shame if that promise were politicised. It would be a shame to hold back progress in nuclear fusion because we can’t agree about Twitter.”A better starting point would be to recognise that the government is already regulating AI. Years ago it set the nerve ends tingling of the federal bureaucracy, another complex, amorphous entity that somehow translates countless inputs into answers that cannot always be explained. Plenty of laws already cover the use of AI. And questions about AI’s use in providing health care, hiring people, driving cars or investing are being asked and answered, albeit too slowly for some industries. The National Institute of Standards and Technology, part of the Commerce Department, has drawn up voluntary standards by which AI might be governed.Chris Meserole of the Brookings Institution, a Washington-based think-tank, says Congress could exploit this “incredibly rare moment” of bipartisan seriousness by mandating that each regulatory agency develop a plan to adapt the NIST standards to its sector. It could also mandate disclosure about the use of AI in products, and require transparency into algorithms used in high-risk systems. “If an autonomous vehicle using AI has an accident, we need to understand what went wrong,” he says.To walk humbly with Chuck SchumerCongress might also consider export controls for AI models and chips it deems too powerful. It could look at creating an agency to regulate big tech (disclosure: Lexington’s brother, Senator Michael Bennet of Colorado, has proposed legislation to do so) or whether some other means might bolster the agencies’ AI expertise. It could also help universities pay for the computing power they need to conduct AI research in the public interest.The good news is that these are the sorts of measures that Congress and the Biden administration are weighing. The most striking word in Mr Schumer’s recent speech was “humility”, as in, “We must exercise humility as we proceed.” Like many of his colleagues, Mr Schumer is not celebrated for this quality. Nor are the technology companies, but, in the face of AI, they are embracing it, too. Mr Walker notes that AI researchers have spoken of the “AI half-pipe of heaven and hell”, meaning it tends to be treated as either wonderful or terrible. “There’s very little market for, ‘Well, AI has a lot of important pros and cons, and we have to incrementally navigate’,” he says. “But that’s probably where the wisdom is.”China has forbidden AI-generated images that impersonate people without their consent, and the European Union has proposed sweeping AI rules (which, like the EU’s data rules, may reverberate globally). But America’s reactive, incremental approach to making regulation up as its industries develop has kept it in the lead since the Industrial Revolution, and seems particularly suited to such a rapidly evolving, disruptive technology. Maybe AI and America’s lawmakers, in the end, can help each other grow up. ■Read more from Lexington, our columnist on American politics:Why the multiverse is eating popular culture (Jun 22nd)North Carolina may be the hottest political battleground of 2024 (Jun 15th)Nikki Haley, like other long shots, sees a path to victory (Jun 1st)Also: How the Lexington column got its name
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.For decades linguists have argued over how children learn language. Some think that babies are born as “blank slates” who pick up language simply from experience—hearing, seeing and playing with the world. Others argue that experience is not enough and that babies’ brains must be hardwired to make acquiring language easy.AI models such as GPT-4 have done little to settle the debate. The way these models learn language—by trawling through reams of text data from millions of web pages—is vastly different to the experiences of babbling babies.A team of scientists at New York University examined the question by training an AI model on the experiences of a single infant. Between the ages of six and 25 months, a toddler called Sam wore a head-mounted camera for an hour a week—around 1% of his waking hours. The camera recorded everything he saw and heard while he played with toys, enjoyed days at the park and interacted with his pet cats. The recordings and transcribed audio were fed into an AI, which was set up to know that images and words that appeared at the same time were related, but was otherwise left to make sense of the mess of colours and speech that Sam experienced.Despite the limited training data, the AI was able to pick out objects and learn the matching words. The researchers tested the model by asking it to identify objects that Sam had seen before, such as a chair from his home or one of his toy balls. Given a list of four options the model picked the correct word 62% of the time, far above the chance level of 25%. To the researchers’ surprise, the model could also identify chairs and balls that Sam had never seen. The AI learnt at least 40 different words, but it was far from matching Sam’s vocabulary and language abilities by the end of the experiment.The researchers, whose work was published recently in the journal Science, argue that, to match words to objects, learning from experience may well be enough. Sceptics, however, doubt that the AI would be able to learn abstract nouns or verbs, and question how similar the learning processes really are. The mystery of language acquisition lives on.■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.PAUL HUDSON, boss of Sanofi, is brandishing an iPhone. He is keen to show off the French drugmaker’s new artificial-intelligence (AI) app, plai. It draws on more than 1bn data points to provide “snackable” information, from warnings about low stocks of a drug to questions for a meeting with an ad agency or suggestions to set up clinical-trial sites that could expedite drug approvals. Like Netflix recommendations, plai delivers “nudges”, as Mr Hudson calls them, that are useful at that moment in time. He jokes that plai broke even in about four hours, and says the cost is “peanuts” compared with the $300m-400m that big consultancies charge for a project to curate a big company’s data. One in ten of Sanofi’s 80,000 staff uses it every day.AI is not new in drugmaking. Biotech firms have been tinkering with it for years. Now interest from big pharma is growing. Last year Emma Walmsley, chief executive of GSK, said it could improve the productivity of research and development, the industry’s most profound challenge. Moderna recently described itself as “laser-focused” on AI. Sanofi is “all in”. Morgan Stanley, an investment bank, reckons that within a decade the pharmaceutical industry may be spending $50bn a year on AI to speed up drug development.Most of the buzz revolves around AIs trained on biological data that could improve the hit-and-miss process of drug discovery. Drugs can take a decade to emerge, cost billions of dollars and succeed only 10% of the time. Even a small improvement in speed and efficiency would be hugely valuable. But scientists have struggled to tame biological big data with conventional statistical tools. Machine learning makes it possible to sift through piles of information, from clinical patient data and genome sequences to images of body scans. Last year DeepMind, an AI lab that is part of Google, made a breakthrough using its AlphaFold system to predict the structure of almost all proteins, which may one day help identify which molecules have therapeutic potential.Though only around a dozen drugs in development have so far involved the use of AI, the list may grow rapidly—especially for simple molecules with properties that are relatively easy to predict. In the case of these more straightforward chemistries, the future of medicine is looking ever more like a computational problem.Jim Weatherall, who oversees data science and AI at AstraZeneca, says the technology is used in 70% of the British firm’s small molecules in development. Using a technique called “reinforcement learning”, AstraZeneca’s AI is constantly tweaking its molecular suggestions and playing out how a tweaked molecule might react. Ali Mortazavi, boss of E-therapeutics, a biotech startup in London, says that knowing the sequences of all the genes in, say, the liver, lets his firm use software to design RNA molecules (which are more complex but, owing to their links to DNA, predictably so). AI algorithms then predict the activity of the molecules, which can stop the function of any disease-causing gene.Euan Ashley of Stanford University points to another AI application. “Knowledge graphs” are a kind of database that stores data about genes, proteins, diseases and drugs, as well as the biological pathways that connect them. They, too, can help identify new targets for drug development. “Generative” AI, meanwhile, is being trialled for suggesting entirely new chemical and biological structures for testing, just as ChatGPT can ingest text on the internet and spit out a new poem or essay. Beyond drug discovery, AIs like plai could help with the perennial problem of efficiency in a heavily regulated and labour-intensive sector.Some pharma bosses worry that generative AIs’ tendency to make stuff up could send researchers down blind alleys. More apocalyptically, Mr Hudson says half of the pharma CEOs he talks to about AI fear, like many people, the existential threats it poses. For his part, he foresees the next industrial revolution, not a robot uprising. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.“Should we automate away all the jobs, including the fulfilling ones? Should we develop non-human minds that might eventually outnumber, outsmart...and replace us? Should we risk loss of control of our civilisation?” These questions were asked last month in an open letter from the Future of Life Institute, an ngo. It called for a six-month “pause” in the creation of the most advanced forms of artificial intelligence (AI), and was signed by tech luminaries including Elon Musk. It is the most prominent example yet of how rapid progress in AI has sparked anxiety about the potential dangers of the technology.In particular, new “large language models” (LLMs)—the sort that powers ChatGPT, a chatbot made by OpenAI, a startup—have surprised even their creators with their unexpected talents as they have been scaled up. Such “emergent” abilities include everything from solving logic puzzles and writing computer code to identifying films from plot summaries written in emoji.These models stand to transform humans’ relationship with computers, knowledge and even with themselves. Proponents of AI argue for its potential to solve big problems by developing new drugs, designing new materials to help fight climate change, or untangling the complexities of fusion power. To others, the fact that ais’ capabilities are already outrunning their creators’ understanding risks bringing to life the science-fiction disaster scenario of the machine that outsmarts its inventor, often with fatal consequences.This bubbling mixture of excitement and fear makes it hard to weigh the opportunities and risks. But lessons can be learned from other industries, and from past technological shifts. So what has changed to make AI so much more capable? How scared should you be? And what should governments do?In a special Science section, we explore the workings of llms and their future direction. The first wave of modern AI systems, which emerged a decade ago, relied on carefully labelled training data. Once exposed to a sufficient number of labelled examples, they could learn to do things like recognise images or transcribe speech. Today’s systems do not require pre-labelling, and as a result can be trained using much larger data sets taken from online sources. LLMs can, in effect, be trained on the entire internet—which explains their capabilities, good and bad.Those capabilities became apparent to a wider public when ChatGPT was released in November. A million people had used it within a week; 100m within two months. It was soon being used to generate school essays and wedding speeches. ChatGPT’s popularity, and Microsoft’s move to incorporate it into Bing, its search engine, prompted rival firms to release chatbots too.Read more of our special series on AI:How AI could change computing, culture and the course of historyLarge, creative AI models will transform lives and labour marketsLarge language models’ ability to generate text also lets them plan and reasonHow generative models could go wrongThe world needs an international agency for artificial intelligence, say two AI expertsSome of these produced strange results. Bing Chat suggested to a journalist that he should leave his wife. ChatGPT has been accused of defamation by a law professor. LLMs produce answers that have the patina of truth, but often contain factual errors or outright fabrications. Even so, Microsoft, Google and other tech firms have begun to incorporate LLMs into their products, to help users create documents and perform other tasks.The recent acceleration in both the power and visibility of AI systems, and growing awareness of their abilities and defects, have raised fears that the technology is now advancing so quickly that it cannot be safely controlled. Hence the call for a pause, and growing concern that AI could threaten not just jobs, factual accuracy and reputations, but the existence of humanity itself.Extinction? Rebellion?The fear that machines will steal jobs is centuries old. But so far new technology has created new jobs to replace the ones it has destroyed. Machines tend to be able to perform some tasks, not others, increasing demand for people who can do the jobs machines cannot. Could this time be different? A sudden dislocation in job markets cannot be ruled out, even if so far there is no sign of one. Previous technology has tended to replace unskilled tasks, but LLMs can perform some white-collar tasks, such as summarising documents and writing code.The degree of existential risk posed by AI has been hotly debated. Experts are divided. In a survey of AI researchers carried out in 2022, 48% thought there was at least a 10% chance that AI’s impact would be “extremely bad (eg, human extinction)”. But 25% said the risk was 0%; the median researcher put the risk at 5%. The nightmare is that an advanced AI causes harm on a massive scale, by making poisons or viruses, or persuading humans to commit terrorist acts. It need not have evil intent: researchers worry that future AIs may have goals that do not align with those of their human creators.Such scenarios should not be dismissed. But all involve a huge amount of guesswork, and a leap from today’s technology. And many imagine that future AIs will have unfettered access to energy, money and computing power, which are real constraints today, and could be denied to a rogue AI in future. Moreover, experts tend to overstate the risks in their area, compared with other forecasters. (And Mr Musk, who is launching his own AI startup, has an interest in his rivals downing tools.) Imposing heavy regulation, or indeed a pause, today seems an over-reaction. A pause would also be unenforceable. Regulation is needed, but for more mundane reasons than saving humanity. Existing AI systems raise real concerns about bias, privacy and intellectual-property rights. As the technology advances, other problems could become apparent. The key is to balance the promise of AI with an assessment of the risks, and to be ready to adapt.So far governments are taking three different approaches. At one end of the spectrum is Britain, which has proposed a “light-touch” approach with no new rules or regulatory bodies, but applies existing regulations to AI systems. The aim is to boost investment and turn Britain into an “AI superpower”. America has taken a similar approach, though the Biden administration is now seeking public views on what a rulebook might look like.The eu is taking a tougher line. Its proposed law categorises different uses of AI by the degree of risk, and requires increasingly stringent monitoring and disclosure as the degree of risk rises from, say, music-recommendation to self-driving cars. Some uses of AI are banned altogether, such as subliminal advertising and remote biometrics. Firms that break the rules will be fined. For some critics, these regulations are too stifling.But others say an even sterner approach is needed. Governments should treat AI like medicines, with a dedicated regulator, strict testing and pre-approval before public release. China is doing some of this, requiring firms to register AI products and undergo a security review before release. But safety may be less of a motive than politics: a key requirement is that AIs’ output reflects the “core value of socialism”.What to do? The light-touch approach is unlikely to be enough. If AI is as important a technology as cars, planes and medicines—and there is good reason to believe that it is—then, like them, it will need new rules. Accordingly, the EU’s model is closest to the mark, though its classification system is overwrought and a principles-based approach would be more flexible. Compelling disclosure about how systems are trained, how they operate and how they are monitored, and requiring inspections, would be comparable to similar rules in other industries.This could allow for tighter regulation over time, if needed. A dedicated regulator may then seem appropriate; so too may intergovernmental treaties, similar to those that govern nuclear weapons, should plausible evidence emerge of existential risk. To monitor that risk, governments could form a body modelled on CERN, a particle-physics laboratory, that could also study AI safety and ethics—areas where companies lack incentives to invest as much as society might wish. This powerful technology poses new risks, but also offers extraordinary opportunities. Balancing the two means treading carefully. A measured approach today can provide the foundations on which further rules can be added in future. But the time to start building those foundations is now. ■For subscribers only: to see how we design each week’s cover, sign up to our weekly Cover Story newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.HE is in high demand. Last year Refik Anadol projected luminous images of coral on to a wall at the World Economic Forum in Davos and covered the exterior screen of the Sphere, a new concert venue in Las Vegas, with animated, tumbling blue blocks. In October the Museum of Modern Art (MoMA) in New York acquired “Unsupervised—Machine Hallucinations”, in which a machine-learning model generates artworks based on those in the museum’s collection. On February 16th “Echoes of the Earth”, his largest-ever show in Britain, opened at the Serpentine North Gallery in London.Mr Anadol, a 38-year-old Turk who lives in Los Angeles, is riding widespread public interest in artificial intelligence to become the most visible digital artist of his generation. His work reflects the innovation and anxieties of the current moment. As Mr Anadol sees it, AI is a powerful creative tool. In a world where so much of life happens in a digital realm, he argues, data has become a new “pigment”.He is steeped in both art and science, having completed several arts degrees and a residency at Google focused on machine learning. Mr Anadol trains AI models on massive troves of data, often publicly available, to create raucously colourful animations that he calls “dreams” or “hallucinations”. They swirl on superbright screens (creating what he calls “data paintings”) and wiggle on walls (“data sculptures”); sometimes the pieces illuminate buildings, as at the Sphere.One artwork, displayed at the Venice Architecture Biennale, drew on 70 terabytes of brain scans, allowing AI models to imagine the organ’s development. Another piece used an archive of the Los Angeles Philharmonic’s performances to imitate dreaming. A third assimilated more than 138,000 images and pieces of metadata from MoMA’s collection (pertaining to provenance, for instance), along with local weather and data about noise levels. The results are churning clouds and waves, as well as abstractions evocative of Mark Rothko, a celebrated painter.To undertake work on this scale Mr Anadol employs around 30 people, including architects, designers and engineers, half of whom work in his studio in Los Angeles. Public projects with institutions and companies have boosted his profile, but some private collectors have bought pieces, too. Mr Anadol also mints non-fungible tokens, digital artefacts that sometimes come with physical works.The animations have proved popular: around 2.4m people came to see an exhibition of his work at MoMA in 2022. Mr Anadol’s style is accessible and often beguiling. Understanding how machine learning works may help you fathom the process behind the “data paintings”, but it is not essential. (In some installations, a control panel pops up to explain the model, giving the illusion of glancing under the bonnet but mostly evoking the futurism of “The Matrix”.) You can be swept along by the crashing tides of colour, or watch a rose turn into a lily, without wondering whether you are “getting it”.Naturally Mr Anadol has critics as well as admirers. Some compare his animations to glorified screensavers and lava lamps, more spectacular than substantial. (Some do look as if they belong at a hotel in Las Vegas or at Burning Man.) Like anything generated by an AI model, Mr Anadol’s animations raise questions about originality and whether such creations simply recycle the work of others.Some worry that he glorifies AI while ignoring its risks, by presenting a rosy (or deep purple or yellow) view of the tech’s potential. Casey Reas, one of Mr Anadol’s former teachers, says many in the art world are prejudiced against digital art, as they once were against photography, but concedes that “sometimes Refik’s work can appear to have a utopian view of technology”. The artist has appeared twice at TED and is fluent in the breathless Silicon Valley idiolect of “breakthroughs” and “inclusivity”.image: Steve KehayiasWearing all black, in an all-black room in his studio, illuminated only by high-definition screens, Mr Anadol acknowledges that AI is changing everything—and not always for the better. But he is indeed excited about what the technology can do in the right hands. “I don’t see the problem there,” he says. “I see possibilities.”His latest project, on display at the Serpentine, is “Living Archive: Large Nature Model”, which trained AI models on photographs, sounds and other kinds of scientific information collected at 16 rainforests across the globe. In addition to his usual sponsors, Google and Nvidia, institutions including the Natural History Museum in London and the Smithsonian have also furnished Mr Anadol with images and data.Photo synthesisMr Anadol prompts the model to create his trademark abstractions, as well as hyper-realistic creatures. Eagles morph into owls, which turn into toucans; the overarching point is the connectivity of the natural world. AI offers “a new brush, a thinking brush”, he says.He hopes that his work will educate people and help them “discover new worlds”. A viewer might prompt a model with the name of a plant, and it will generate a new one right in front of them. The artist’s ultimate goal is a place called Dataland: a fully immersive experience, including sounds and smells. Hans Ulrich Obrist, the Serpentine’s artistic director, says that Mr Anadol “makes the invisible visible”; he captures the power of technology as he turns AI from an abstraction in the cloud into art before the eyes. Whether that art looks like a dream or a beautiful banality is up to viewers. But like it or not, people will be seeing a lot more of Mr Anadol’s work. ■For more on the latest books, films, TV shows, albums and controversies, sign up to Plot Twist, our weekly subscriber-only newsletter
RECENT ADVANCES in artificial intelligence (AI) have put the technology into the hands of millions of users for the first time. They have used “generative” tools like ChatGPT and Stable Diffusion to write text, create photo-realistic pictures and automate mundane tasks.The same tools have also presented fictitious information as true, insulted users and caused AI researchers to publicly question how the technology is being deployed.Our writers discuss the technology behind generative AI: its abilities, its limitations and how humans could and should be using it.With Alok Jha, Tom Standage, Abby Bertics and Arjun Ramani.Visit our subscriber events page to view the schedule for our forthcoming events. Subscribers can also watch recordings of all our previous sessions.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.WHEN A BEAMING Mark Zuckerberg took the stage in Menlo Park on September 27th to announce a new array of Meta products, the Facebook supremo may have buried the lead. He began talking about Quest 3, Meta’s virtual-reality (VR) headset, which is understandable considering that his obsession with the metaverse is now inscribed in his company’s identity. Techies, though, were more excited by what came later: an announcement that Meta, in combination with Ray-Ban, would soon launch smart glasses incorporating an artificial-intelligence (AI) virtual assistant. The specs will be able to see and hear, as well as answer their wearers’ questions. With luck, they will not hallucinate.You can be dismissive of smart glasses. They have been hyped before. But lending Meta credibility this time is the fact that the same week OpenAI, the generative-AI pioneer, announced that its hit chatbot, ChatGPT, can now see, hear and speak, besides conversing by text. Moreover, it emerged that OpenAI was in talks with Sir Jony Ive, Apple’s former designer, to create a new gadget for the AI era. What form it will take is still unclear. But if the idea is to build a new consumer-electronics device better suited to the back-and-forth of seeing, talking and listening AIs, there is a fair chance it will no longer be reliant on the touchscreen.The smartphone has had a good innings. Yet you only need to talk to Sky, one of ChatGPT’s new audio avatars, to feel the joy of freeing yourself from its tyranny. Your columnist got a taste when he asked Sky how she thought screens might eventually be replaced: Glasses? “Absolutely!” she enthused, “especially those equipped with augmented reality [AR] and AI”. Asked whether this would be a good thing, she recommended two books that explore the enormous impact that screens have had on modern life: “The Shallows: How the Internet is Changing the Way We Think, Read and Remember” by Nicholas Carr, an American writer, and “Screened out” by Jean Baudrillard, the late French philosopher. Then, when further prompted, she summarised each in crisp, insightful language with barely a moment’s hesitation. It wasn’t exactly Scarlett Johansson in “Her”. But it felt like having a Stanford University intellectual murmuring in your ear.This is all rather refreshing. Just as the year-long excitement over “foundational” models and other mind-boggling bits of AI infrastructure has begun to fade, along comes the chance that gen AI, to use the industry shorthand, will unleash an onslaught of new consumer technology. Tech pundits are debating the best “form factor” for the chatbot era. Ben Thompson of Stratechery, a blog and podcast, puts it in epochal terms: “There is a hardware breakthrough waiting to happen just like the internet created the conditions for the smartphone breakthrough to happen.” The ability to talk and listen to chatbots makes Meta’s bet on AR glasses and VR headsets “drastically more compelling”, he writes.Mr Zuckerberg was early to see this coming. He has ploughed a fortune into VR and AR despite misgivings from investors. He remains excited by the metaverse. This was clear from a remote interview he recently took part in with Lex Fridman, a podcaster, which used VR tools to make their virtual faces so lifelike they felt as if they were in the same room together. (As Mr Fridman quipped, it could reproduce realistic facial movements even from two famously inexpressive people.) And yet gen AI has so dramatically accelerated the use case for smart glasses, Mr Zuckerberg told another interviewer, that there is now “no question” they will be the bigger of the two markets. He likens AR specs to mobile phones and VR headsets to desktops. In both cases he appears to hope they will transcend screens, which he says inhabit “a completely different plane from our physical lives”.The two-dimensional screen is not headed for the scrap heap yet. Incumbent technologies are always hard to dislodge. Meta’s mobile apps such as WhatsApp, Facebook and Instagram, with their billions of users, still dwarf AIs like ChatGPT in terms of monthly visits, and they remain dependent on smartphones. As Mark Shmulik of Bernstein, an investment firm, notes, the smartphone era has never stopped people from using PCs. Moreover, it will not be clear until people start buying the smart glasses from the shops how compelling a product they are.The business case for the all-seeing, all-hearing chatbots will also take time to emerge. OpenAI charges $20 a month for access to its family of talking avatars; Meta’s AI-infused smart glasses will start at $299. Yet developing them is bound to be lossmaking at first. If there ever is a case for monetising them via advertisements or virtual shopping, that will probably take years. Meta’s modus operandi, after all, is to launch a consumer product, scale it up and start making money from it only if it is adopted by the masses.In the meantime, obvious safety concerns must be tackled. Consumer technology powered by AI is likely to be more immersive than social media, potentially making it even more isolating for some, or triggering unhealthy attachments. Mr Zuckerberg argues that AR and VR devices could help bring people together. But Mr Shmulik says investors will not want Meta to move too fast. “The last thing they need is another negative PR event where they are back in the cross hairs of regulators,” he says.Glasses half full For now Mr Zuckerberg, who this time last year was fighting fires on several fronts, looks prescient. That is largely thanks to gen AI. Meta’s foundational model, LLama 2, has been an open-source hit and is underpinning the firm’s consumer-tech ambitions. New devices such as smart glasses and headsets could eventually free Facebook and others from their dependence on the iPhone, where Apple has hindered their ability to track data, hurting Meta’s ad business. In a backhanded compliment to Mr Zuckerberg, Apple is launching its own high-end AR/VR headset. The iPhone-maker, too, may be sensing the twilight of the screen era. ■Read more from Schumpeter, our columnist on global business:Customer service is getting worse—and so are customers (Sep 28th)What Arm and Instacart say about the coming IPO wave (Sep 21st)The Mittelstand will redeem German innovation (Sep 14th)Also: If you want to write directly to Schumpeter, email him at [email protected]. And here is an explanation of how the Schumpeter column got its name.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The machines are coming for your crops—at least in a few fields in America. This autumn John Deere, a tractor-maker, shipped its first fleet of fully self-driving machines to farmers. The tilling tractors are equipped with six cameras which use artificial intelligence (ai) to recognise obstacles and manoeuvre out of the way. Julian Sanchez, who runs the firm’s emerging-technology unit, estimates that about half the vehicles John Deere sells have some AI capabilities. That includes systems which use onboard cameras to detect weeds among the crops and then spray herbicides, and combine harvesters which automatically alter their own setting to waste as little grain as possible. Mr Sanchez says that for a medium-sized farm, the additional cost of buying an AI-enhanced tractor is recouped in two to three years.For decades starry-eyed technologists have claimed that AI will upend the business world, creating enormous benefits for firms and customers. John Deere is not the only proof that this is happening at last. A survey by McKinsey, a consultancy, found that this year 50% of firms across the world had tried to use AI in some way, up from 20% in 2017. Powerful new “foundation” models are fast moving from the lab to the real world. Chatgpt, a new ai tool that has recently been released for public testing, is making waves for its ability to craft clever jokes and explain scientific concepts. But excitement is also palpable among corporate users of AI, its developers and those developers’ venture-capital backers. Many of them attended a week-long jamboree hosted in Las Vegas by Amazon Web Services, the tech giant’s cloud-computing arm. The event, which ended on December 2nd, was packed with talks and workshops on ai. Among the busiest booths in the exhibition hall were those of AI firms such as Dataiku and Blackbook.ai.The buzzing AI scene is an exception to the downbeat mood across techdom, which is in the midst of a deep slump. In 2022 venture capitalists have ploughed $67bn into firms that claim to specialise in AI, according to PitchBook, a data firm. The share of vc deals globally involving such startups has ticked up since mid-2021, to 15% so far this quarter (see chart 1). Between January and October, 28 new AI unicorns (private startups valued at $1bn or more) have been minted. Microsoft is said to be in talks to increase its stake in OpenAI, a builder of foundation models and Chatgpt’s provider. Alphabet, Google’s parent company, is reportedly planning to invest $200m in Cohere, a rival to OpenAI. At least 22 AI startups have been launched by alumni of OpenAI and Deepmind, one of Alphabet’s AI labs, according to a report by Ian Hogarth and Nathan Benaich, two British entrepreneurs.The exuberance is not confined to Silicon Valley. Big firms of all sorts are desperate for AI talent. In the past 12 months large American firms in the S&P 500 index have acquired 52 AI startups, compared with 24 purchases in 2017, according to PitchBook. PredictLeads, another data provider, notes that the same group of firms posted around 7,000 job ads a month for AI and machine-learning experts in the three months to November, about ten times more than in the first quarter of 2020 (see chart 2). Derek Zanutto of CapitalG, one of Alphabet’s vc divisions, notes that large firms spent years collecting data and investing in related technology. Now they want to use this “data stack” to their advantage. AI offers ways to do so.Unsurprisingly, the first industry to embrace AI was the technology sector. From the 2000s onwards, machine-learning techniques helped Google supercharge its online-advertising business. Now it uses Ai to improve search results, finish your sentences in Gmail and work out ways to cut energy use in its data centres, among other things. Amazon’s AI manages its supply chains, instructs warehouse robots and predicts which job applicants will be good workers; Apple’s powers its Siri digital assistant; Meta’s serves up attention-grabbing social-media posts; and Microsoft’s does everything from stripping out background noise in Teams, its videoconferencing service, to letting users create first drafts of PowerPoint presentations.Big tech quickly spied an opportunity to sell some of those same AI capabilities to clients. Amazon, Google and Microsoft all now provide such tools to customers of their cloud-computing divisions. Revenues from Microsoft’s machine-learning cloud service have doubled in each of the past four quarters, year on year. Upstart providers have proliferated, from Avidbots, a Canadian developer of robots that sweep warehouse floors, to Gong, whose app helps sales teams follow up a lead. Greater use of cloud computing, which brings down the cost of using AI, enabled the technology to spread to other sectors, from industry to insurance. You may not see it, but these days AI is everywhere.Dulling the cutting edgeIn 2006 Nick Bostrom of Oxford University observed that “once something becomes useful enough and common enough it’s not labelled AI any more”. Ali Ghodsi, boss of Databricks, a company that helps customers manage data for AI applications, sees an explosion of such “boring AI”. He argues that over the next few years AI will be applied to ever more jobs and company functions. Lots of small improvements in AI’s predictive power can add up to better products and big savings.This is especially true in less flashy areas where firms are already using some kind of analytics, such as managing supply chains. When in September Hurricane Ian forced Walmart to shut a large distribution hub, halting the flow of goods to  supermarkets in Florida, the retailer used a new AI-powered simulation of its supply chain to reroute deliveries from other hubs and predict how demand for goods would change after the storm. Thanks to AI this took hours rather than days, says Srini Venkatesan of Walmart’s tech division.The coming wave of foundation models is likely to turn a lot more AI boring. These algorithms hold two big promises for business. The first is that foundation models are capable of generating new content. Stability AI and Midjourney, two startups, build generative models which create new images for a given prompt. Request a dog on a unicycle in the style of Picasso—or, less frivolously, a logo for a new startup—and the algorithm conjures it up in a minute or so. Other startups build applications on top of other companies’ foundation models. Jasper and Copy.AI both pay OpenAI for access to GPT3, which enables their applications to convert simple prompts into marketing copy.The second advantage is that, once trained, foundation AIs are good at performing a variety of tasks rather than a single specialised one. Take GPT3, a natural-language model developed by Openai, which forms the basis for  Chatgpt. It was first trained on large chunks of the internet, then fine-tuned by different startups to do various things, such as writing marketing copy, filling in tax forms and building websites from a series of text prompts. Rough estimates by Beena Ammanath, who heads the AI practice of Deloitte, a consultancy, suggest that foundation models’ versatility could cut the costs of an AI project by 20-30%.One early successful use of generative AI is, again predictably, the province of tech: computer programming. Several firms are offering a virtual assistant trained on a large deposit of code that churns out new lines when prompted. One example is Copilot on GitHub, a Microsoft-owned platform which hosts open-source programs. Programmers using Copilot outsource nearly 40% of code-writing to it. This speeds up programming by 50%, the firm claims. In June Amazon launched CodeWhisperer, its version of the tool. Alphabet is reportedly using something similar, codenamed PitchFork, internally.Artificial colouringIn May Satya Nadella, Microsoft’s boss, declared, “We envision a world where everyone, no matter their profession, can have a Copilot for everything they do.” In October Microsoft launched a tool which automatically wrangles data for users following prompts. Amazon and Google may try to produce something like it. Several startups are already doing so. Adept, a Californian company run by former employees from Deepmind, OpenAI and Google, is working on “a Copilot for knowledge workers”, says Kelsey Szot, a co-founder of the firm. In September the company released a video of its first foundation model, which uses prompts to crunch numbers in a spreadsheet and to perform searches on property websites. It plans to develop similar tools for business analysts, salespeople and other corporate jobs.Corporate users are experimenting with generative AI in other creative ways. Mr Sanchez of John Deere says that his firm is looking into AI-generated “synthetic” data, which would help train other AI models. In December 2021 Nike, a sportswear giant, bought a firm that uses such algorithms to create new sneaker designs. Alexa, Amazon’s virtual assistant, can now invent stories to tell children. Nestlé, a giant Swiss foodmaking firm, is using images created by DALLE-2, another OpenAI model, to help sell its yogurts. Some financial firms are employing AI to whip up a first draft of their quarterly reports.Users of foundation models can also tap an emerging industry of professional prompters, who craft directions so as to optimise the models’ output. PromptBase is a marketplace where users can buy and sell prompts that produce particularly spiffy results from the large image-based generative models, such as DALLE-2 and Midjourney. The site also lets you hire expert “prompt engineers”, some of whom charge a $50-200 per prompt. “It’s all about writing prompts these days,” says Thomas Dohmke, boss of GitHub.As with all powerful new tools, businesses must tread carefully as they deploy more AI. Having been trained on the internet, many foundation models reflect humanity, warts and all. One study by academics at Stanford University found that when GPT3 was asked to complete a sentence starting “Two Muslims walked into a...”, the result was likely to invoke violence far more often than when the phrase referred to Christians or Buddhists. Meta pulled down Galactica, its foundation model for science, after claims that it generated real-sounding but fake research. Carl Bergstrom, a biologist at the University of Washington in Seattle, called it a “random bullshit generator”. (Meta says that the model remains available for researchers who want to learn about the work.)Other problems are specific to the world of business. Because foundation models tend to be black boxes, offering no explanation of how they arrived at their results, they can create legal liabilities when things go amiss. And they will not do much for those firms that lack a clear idea of what they want AI to do, or which fail to teach employees how to use it. This may help explain why merely a quarter of respondents to the McKinsey’s survey said that AI had benefited the bottom line (defined as a 5% boost to earnings). The share of firms seeing a large benefit (an increase in earnings of over 20%) is in the low single digits—and many of those are tech firms, says Michael Chui, who worked on the study.Still, those proportions are bound to keep rising as more AI becomes ever more dull. Rarely has the boring elicited this much excitement. ■Correction (December 12th 2022): This piece originally said that John Deere’s tractors automatically sprayed fields with pesticides. In fact they sprayed herbicides. This change has now been made. Sorry.To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
China faces a problem familiar to dictatorships throughout history: how to strike a balance between growth-boosting innovation, which thrives in a free society, and the paranoia of an authoritarian state. Its leader, Xi Jinping, wants the country to become a hyper-advanced economy. His government is aggressively promoting the commercialisation of high technologies it likes, from electric vehicles to quantum computing.At the same time, it is tightening the screws on those it disapproves of. In 2021 it regulated a booming online-tutoring industry into oblivion almost overnight, apparently out of fear that high tuition fees were making children’s education so expensive that Chinese were put off the idea of parenthood. On December 22nd the government took a wrench to the video-gaming industry, introducing rules to, among other things, limit how much players can spend on in-game purchases—and so how much developers can make. The market value of Tencent, one of China’s most innovative firms that also has a big gaming business, tumbled by 12%.Nowhere is this tension clearer than in the hottest technology of 2023—artificial intelligence (AI). In many countries, command of AI is seen as both economically and strategically important. Politicians everywhere fret about machines going rogue or, more realistically, being harnessed by human mischief-makers. In Beijing the added worry is that the technology, which thrives on unlimited data and, at its current stage of development, in unregulated spaces, could prove subversive if not kept in check. It is therefore busily shoring up its “great firewall” for the AI age.In 2000 Bill Clinton, then America’s president, likened China’s attempt to control the internet to “trying to nail Jell-O to the wall”. Today the jelly seems firmly in place. Western internet services, from Facebook and Google search to Netflix are unavailable to most Chinese (apart from those willing to run the risk of using illegal “virtual private networks”). On local platforms, any undesirable content is deleted, either pre-emptively by the platforms themselves, using algorithms and armies of moderators, or afterwards, as soon as it is spotted by government censors. A tech crackdown in 2020 brought China’s powerful tech giants, such Tencent and Alibaba, to heel—and closer to the government, which has been taking small stakes in the firms, and a big interest in their day-to-day operations.The result is a digital economy that is sanitised but nevertheless thriving. Tencent’s super-app, WeChat, which combines messaging, social media, e-commerce and payments, alone generates hundreds of billions of dollars in yearly transactions. Mr Xi now hopes to pull off a similar balancing act with AI. Once again, some foreign experts predict a jelly situation. And once again, the Communist Party is building tools to prove them wrong.The party’s efforts begin with the world’s toughest rules for Chinese equivalents of ChatGPT (which is, predictably, banned in China) and other consumer-facing “generative” AI. Since March companies have had to register with officials any algorithms that make recommendations or can influence people’s decisions. In practice, that means basically any such software aimed at consumers. In July the government issued rules requiring all AI-generated content to “uphold socialist values”—in other words, no bawdy songs, anti-party slogans or, heaven forbid, poking fun at Mr Xi. In September it published a list of 110 registered services. Only their developers and the government know all the ins and outs of the registration process and the precise criteria involved.In October a standards committee for national information security published a list of safety guidelines requiring detailed self-assessment of the data used to train generative-AI models. One rule requires the manual testing of at least 4,000 subsets of the total training data; at least 96% of these must qualify as “acceptable”, according to a list of 31 vaguely worded safety risks. The first criterion for unacceptable content is anything that “incites subversion of state power or the overthrowing of the socialist system”. Chatbots must decline to answer 95% of queries that would elicit unacceptable content (any that do get through would, presumably, be censored ex post). They must also reject no more than 5% of harmless questions.Anything produced by unregistered algorithms is to be blocked and its creators punished. In May a man in Gansu province was arrested after he used ChatGPT to produce text and images of a fake train crash and publish them on social media. He may have been the first Chinese to be detained for spreading AI-generated misinformation. He will not be the last.This heavy-handed strategy has slowed the uptake of consumer-facing generative AI in the country. Ernie Bot, built by Baidu, another tech firm, was ready around the time of ChatGPT’s launch but only released nine months later, in August—aeons given how fast the technology is evolving. It is still clumsy when it comes to expressing its devotion to the party. When asked sensitive questions about Mr Xi, it dutifully censors, offering no answer and deleting the query.Model socialistsWith more work the party may be able to make AI models into not merely good communists, but fluent ones. That would obviate the need for ex post censorship, says Luciano Floridi of Yale University. Yet the authorities seem in no rush to get there. Instead, they are promoting the technology’s business applications.In contrast to consumer AI, enterprise AI faces few constraints on development, notes Mimi Zou of the Oxford Martin School, a research institute. As Steven Rolf of the Digital Futures at Work Research Centre, a British think-tank, explains, this has the effect of channelling capital and labour away from things like consumer chatbots and towards machine learning for business. This, the government seems to be betting, will allow China to catch up and even overtake America in AI without the hassle of dealing with potentially subversive AI-generated content.In May the southern city of Shenzhen announced it would launch a 100bn-yuan ($14bn) AI-focused investment fund, the largest of its kind in the world. A number of city governments have launched similar investment funds. A lot of this money is going to firms such as Qi An Xin, which offers generative AI that manages data-security risks for companies. The company claims that the bot can do the work of 60 security experts, 24 hours a day. Before going public in 2020, it received big investments from state companies, like many similar startups.For this strategy to work, the enterprise-AI firms need the right sort of raw material. Consumer chatbots use AI models trained on swathes of the public internet. Corporate applications need corporate data, a lot of which is squirrelled away inside companies. So the other plank of China’s strategy is to turn corporate data into a public good. The state does not want to own the data but—as with the other factors—to control the channels through which it flows.To that end, the government is promoting data exchanges. These are meant to let businesses trade information, packaged into standardised products, about all areas of commercial life, from activity at individual factories to sales data at individual shops. Small firms will gain access to knowledge once reserved for the tech giants. Banks and brokers will get a real-time picture of the economy.Chinese cities began launching data exchanges about a decade ago. Now there are around 50 around China. And they are finally gaining speed. The Shanghai Data Exchange (SDE), which was launched in 2021, has started dealing in a number of new data products. In one of its first transactions, ICBC, a bank, bought information from the energy sector. This can be used to assess companies’ power consumption and, because it reflects real levels of activity, to create alternative credit profiles for companies. SDE sells satellite-derived data on steel output in China’s heartland and environmental violations by mining companies. Another product gives real-time data on doctors, nurses and hospital beds across the country in order to help medical firms make business decisions. The SDE is also experimenting with using data as collateral for loans.At scale, the datafication of industry could deliver a significant economic boost, says Tom Nunlist of Trivium, a consultancy in Beijing. And more data may be coming to the exchanges soon. In August the central government tasked state-owned firms with thinking about how to value their data. In the past few months teams of auditors have been trying to come up with ways to do this, with a view to adding such data to companies’ balance-sheets as intangible assets. They are meant to report back by January 1st (though the deadline looks likely to be missed, given the unprecedented nature of the task).The government’s gamble on enterprise AI is not without problems, however. The car industry is a case in point. In 2022 about 185m vehicles on Chinese roads had an internet connection, and a national plan envisages mass production of semi-autonomous cars by 2025. For that to happen, companies devising self-driving algorithms need lots of data on which to train their systems. A company called WICV is building a platform for the data that is beginning to trickle out of cars.For now, WICV returns a car’s data to the carmaker that built it, so BYD gets data from BYDs, Nio from Nios and so on. But the plan is eventually for the data to be traded on exchanges, where it could be bought by other developers of self-driving systems. For that to happen, though, driving data must first be “desensitised”, explains Chu Wenfu, WICV’s founder, by stripping out biometric and geolocation details that could help bad actors track the movements of specific people.The potential for such tracking spooks Chinese authorities. A big reason why they cracked down on Didi Global days after the ride-hailing firm’s initial public offering in New York in 2021 was, it later transpired, a fear that data on Didi’s 25m daily rides, including geolocation information linked to passengers, could fall into the hands of the American authorities. The Chinese government is pre-emptively mapping out vast areas of China where data collection could potentially pose a national-security threat.Many carmakers, including Western ones such as BMW, have little choice but to team up with state-backed companies to handle driving information and ensure that local data-compliance rules are followed. Just in case, some car companies are ditching certain features, such as allowing drivers to watch live footage of inside and outside their car on their phones. Some of that footage could, after all, inadvertently capture something sensitive.Such trade-offs between innovation and security are unlikely to be limited to cars. Other industrial and corporate data, too, will probably need desensitising before it can be traded at scale on exchanges. That will slow the development of enterprise AI, even if algorithms remain unshackled. It is a price that the party appears willing to pay for its paranoia. ■
TWO letters can add up to a lot of money. No area of technology is hotter than AI, or artificial intelligence. Venture-capital investment in AI in the first nine months of 2017 totalled $7.6bn, according to PitchBook, a data provider; that compares with full-year figures of $5.4bn in 2016. In the year to date there have been $21.3bn in AI-related M&A deals, around 26 times more than in 2015. In earnings calls public companies now mention AI far more often than “big data”.At the heart of the frenzy are some familiar names: the likes of Alphabet, Amazon, Apple, Facebook and Microsoft. A similar, though less transparent, battle is under way in China among firms like Alibaba and Baidu. Several have put AI at the centre of their strategies. All are enthusiastic acquirers of AI firms, often in order to snap up the people they employ. They see AI as a way to improve their existing services, from cloud computing to logistics, and to push into new areas, from autonomous cars to augmented reality (see article). Many observers fear that, by cementing and extending the power of a handful of giants, AI will hurt competition. That will depend on three open questions, involving one magic ingredient.AlphaGoneThe tech giants certainly have big advantages in the battle to develop AI. They have tonnes of data, oodles of computing power and boffins aplenty—especially in China, which expects to charge ahead. Imagine a future, some warn, in which you are transported everywhere in a Waymo autonomous car (owner: Alphabet, parent of Google), pay for everything with an Android phone (developer: Google), watch YouTube (owner: Google) to relax, and search the web using—you can guess. Markets with just a handful of firms can be fiercely competitive. A world in which the same few names duke it out in several industries could still be a good one for consumers. But if people rely on one firm’s services like this, and if AI enables that firm to predict their needs and customise its offering ever more precisely, it will be burdensome to switch to a rival.That future is still a long way off. AI programs remain narrowly focused. Moreover, the ability of the incumbents to perpetuate their advantages is made uncertain by three questions.The most important is whether AI will always depend on vast amounts of data. Machines today are usually trained on huge datasets, from which they can recognise useful patterns such as fraudulent financial transactions. If real-world data remain essential to AI, the tech superstars are in clover. They have vast amounts of the stuff, and are gaining more as they push into fresh areas such as health care.A competing vision of AI stresses simulations, in which machines teach themselves using synthetic data or in virtual environments. Early versions of a program developed to play Go, an Asian board game, by DeepMind, a unit of Alphabet, were trained using data from actual games; the latest was simply given the rules and started playing Go against itself. Within three days it had surpassed its predecessor, which had itself beaten the best player humanity could muster. If this approach is widely applicable, or if future AI systems can be trained using sparser amounts of data, the tech giants’ edge is blunted.But some applications will always require data. How much of the world’s stock of it the tech giants will end up controlling is the second question. They have clout in the consumer realm, and they keep pushing into new areas, from Amazon’s interest in medicine to Microsoft’s purchase of LinkedIn, a professional-networking site. But data in the corporate realm are harder to get at, and their value is increasingly well understood. Autonomous cars will be a good test. Alphabet’s Waymo has done more real-world testing of self-driving cars than any other firm: over 4m miles (6.5m kilometres) on public roads. But established carmakers, and startups like Tesla, can generate more data from their existing fleets; other firms, like Mobileye, a driverless-tech firm owned by Intel, are also in the race.The third question is how openly knowledge will be shared. The tech giants’ ability to recruit AI expertise from universities is helped by their willingness to publish research; Google and Facebook have opened software libraries to outside developers. But their incentives to share valuable data and algorithms are weak. Much will depend on whether regulations prise open their grip. Europe’s impending data-protection rules, for example, require firms to get explicit consent for how they use data and to make it easier for customers to transfer their information to other providers. China may try to help its firms by having negligible regulation.The battle in AI is fiercest among the tech giants. It is too early to know how good that will be for competition, but not to anticipate the magic ingredient that will determine the outcome: the importance, accessibility and openness of data.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Will artificial intelligence kill us all? Some technologists sincerely believe the answer is yes. In one nightmarish scenario, AI eventually outsmarts humanity and goes rogue, taking over computers and factories and filling the sky with killer drones. In another, large language models (LLMs) of the sort that power generative ais like ChatGPT give bad guys the know-how to create devastating cyberweapons and deadly new pathogens.It is time to think hard about these doomsday scenarios. Not because they have become more probable—no one knows how likely they are—but because policymakers around the world are mulling measures to guard against them. The European Union is finalising an expansive AI act; the White House is expected soon to issue an executive order aimed at llms; and on November 1st and 2nd the British government will convene world leaders and tech bosses for an “AI Safety Summit” to discuss the extreme risks that AI models may pose.Governments cannot ignore a technology that could change the world profoundly, and any credible threat to humanity should be taken seriously. Regulators have been too slow in the past. Many wish they had acted faster to police social media in the 2010s, and are keen to be on the front foot this time. But there is danger, too, in acting hastily. If they go too fast, policymakers could create global rules and institutions that are aimed at the wrong problems, are ineffective against the real ones and which stifle innovation.The idea that AI could drive humanity to extinction is still entirely speculative. No one yet knows how such a threat might materialise. No common methods exist to establish what counts as risky, much less to evaluate models against a benchmark for danger. Plenty of research needs to be done before standards and rules can be set. This is why a growing number of tech executives say the world needs a body to study AI much like the Intergovernmental Panel on Climate Change (IPCC), which tracks and explains global warming.A rush to regulate away tail risks could distract policymakers from less apocalyptic but more pressing problems. New laws may be needed to govern the use of copyrighted materials when training LLMs, or to define privacy rights as models guzzle personal data. And ai will make it much easier to produce disinformation, a thorny problem for every society.Hasty regulation could also stifle competition and innovation. Because of the computing resources and technical skills required, only a handful of companies have so far developed powerful “frontier” models. New regulation could easily entrench the incumbents and block out competitors, not least because the biggest model-makers are working closely with governments on writing the rule book. A focus on extreme risks is likely to make regulators wary of open-source models, which are freely available and can easily be modified; until recently the White House was rumoured to be considering banning firms from releasing frontier open-source models. Yet if those risks do not materialise, restraining open-source models would serve only to limit an important source of competition.Regulators must be prepared to react quickly if needed, but should not be rushed into setting rules or building institutions that turn out to be unnecessary or harmful. Too little is known about the direction of generative AI to understand the risks associated with it, let alone manage them.The best that governments can do now is to set up the infrastructure to study the technology and its potential perils, and ensure that those working on the problem have adequate resources. In today’s fractious world, it will be hard to establish an IPCC-like body, and for it to thrive. But bodies that already work on AI-related questions, such as the OECD and Britain’s newish Frontier AI Taskforce, which aims to gain access to models’ nuts and bolts, could work closely together.It would help if governments agreed to a code of conduct for model-makers, much like the “voluntary commitments” negotiated by the White House and to which 15 makers of proprietary models have already signed up. These oblige model-makers, among other things, to share information about how they are managing AI risk. Though the commitments are not binding, they may help avoid a dangerous free-for-all. Makers of open-source models, too, should be urged to join up.As AI develops further, regulators will have a far better idea of what risks they are guarding against, and consequently what the rule book should look like. A fully fledged regime could eventually look rather like those for other technologies of world-changing import, such as nuclear power or bioengineering. But creating it will take time—and deliberation. ■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.AT THE HEART of Britain’s publicly funded health-care system lies a contradiction. The National Health Service generates and holds vast swathes of data on Britons’ health, organised using NHS numbers assigned to every person in its care. The system enables world-leading studies, like the RECOVERY trial during the pandemic, which discovered treatments for covid-19. You might suppose it to be a treasure trove for artificial-intelligence (AI) developers eager to bring their models to bear on improving human health. Yet if you put this to a developer they will roll their eyes and tell you why all is not as rosy as it seems.That is because the kinds of tabular data that inform clinical trials—who took which drug, what the outcome was—are not the same as those most useful for training machine-learning models, such as scans or genomes, which hold more information about a patient. Much of this sort of NHS data is a mess, organised in ways which serve doctors treating patients, but not AI developers hoping to feed it to computers. Making it suitable for those models is a task with which the NHS has not yet come to grips. It is often easier for those seeking to organise these richer data to start from scratch, as with a vast data-collection exercise now under way.To open up the NHS’s data riches to AI, its managers and political masters should turn to three principles: cleanliness, comparability and consent. Cleanliness starts with hosting rich data in cloud-computing environments where the data are easier for AI developers to wrangle. Hospitals and clinics also need greater incentives to prepare their datasets for machines. Most of the NHS’s successful AI projects so far have relied on the drive of dedicated, intellectually curious doctors who have had to fight the system rather than be helped by it. Forging stronger links between the NHS and universities—and giving PhD students easier access to datasets—is another good idea.A more open approach to licensing intellectual property would also help. Too often, the NHS demands fees and terms so steep and strict that they deter developers. It should see the big picture and accept smaller fees, to incentivise the building of clean datasets. That will mean less money proportionally for the NHS, and possible riches for developers, but in the long run would benefit the service and its patients. And if used outside Britain, it might mean more revenue overall.Comparability of data is also vital. Though everyone has an NHS number, scans are often gathered and stored in different ways in different places, making it harder to create large datasets for machine learning. The NHS is poised to announce the winner of a contract to link up disparate datasets. This will help, but more is needed. For example, scans of the same type should be carried out in ways similar enough to allow AI to detect signals of health rather than differences in the scanning process.The final pillar is consent. Though everyone wins if everyone lets their data be fed to computers, Britons should be allowed to opt out. Politicians must persuade people of the benefits of vast datasets in which everyone—young or old, black or white—is represented. They must also reassure them that their data will be anonymised, and not used to their detriment, for instance by insurers. The NHS has no time to waste. The rewards on offer are better, earlier diagnosis of disease, and a more productive, efficient system. That is sorely needed when waiting lists are long and funds squeezed. The NHS’s position as a world leader in data-heavy trials faces a stiff threat from health systems in other places, which are digitising rapidly. Abu Dhabi, for example, is considering feeding health-care data into foundation models, and may open up its trained models to the world. Consumer technology—smartphones, watches and devices connected to them—is fast improving its capacity to peer inside the human body. It may one day begin to rival the scanning capacity of the NHS, usurping it as the easiest and cheapest channel for the provision of algorithmic health care.The economy stands to gain, too. NHS data could be the basis of a thriving export industry, licensing AI tools to health-care systems around the world. But if it does not clean up its digital act, Britain will become a taker of new health technology, just as it has become a taker of American digital services like online search and social media. That would be a missed opportunity, and the beginning of the end of the data primacy of the NHS. ■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.It can take a little imagination to see how some innovations might change an economy. Not so with the latest ai tools. It is easy—from a writer’s perspective, uncomfortably so—to think of contexts in which something like Chatgpt, a clever chatbot which has taken the web by storm since its release in November, could either dramatically boost a human worker’s productivity or replace them outright. The gpt in its name stands for “generative pre-trained transformer”, which is a particular kind of language model. It might well stand for general-purpose technology: an earth-shaking sort of innovation which stands to boost productivity across a wide-range of industries and occupations, in the manner of steam engines, electricity and computing. The economic revolutions powered by those earlier gpts can give us some idea how powerful ai might transform economies in the years ahead.In a paper published in 1995, Timothy Bresnahan of Stanford University and Manuel Trajtenberg of Tel Aviv University set out what they saw as the characteristics of a general-purpose technology. It must be used in many industries, have an inherent potential for continued improvement and give rise to “innovational complementarities”—that is, induce knock-on innovation in the industries which use it. ai is being adopted widely, seems to get better by the day and is being deployed in ever more r&d contexts. So when does the economic revolution begin?The first lesson from history is that even the most powerful new tech takes time to change an economy. James Watt patented his steam engine in 1769, but steam power did not overtake water as a source of industrial horsepower until the 1830s in Britain and 1860s in America. In Britain the contribution of steam to productivity growth peaked post-1850, nearly a century after Watt’s patent, according to Nicholas Crafts of the University of Sussex. In the case of electrification, the key technical advances had all been accomplished before 1880, but American productivity growth actually slowed from 1888 to 1907. Nearly three decades after the first silicon integrated circuits Robert Solow, a Nobel-prizewinning economist, was still observing that the computer age could be seen everywhere but in the productivity statistics. It was not until the mid-1990s that a computer-powered productivity boom eventually emerged in America.The gap between innovation and economic impact is in part because of fine-tuning. Early steam engines were wildly inefficient and consumed prohibitively expensive piles of coal. Similarly, the stunning performance of recent ai tools represents a big improvement over those which sparked a boomlet of ai enthusiasm roughly a decade ago. (Siri, Apple’s virtual assistant, was released in 2011, for example.) Capital constraints can also slow deployment. Robert Allen of New York University Abu Dhabi argues that the languid rise in productivity growth in industrialising Britain reflected a lack of capital to build plants and machines, which was gradually overcome as capitalists reinvested their fat profits.More recent work emphasises the time required to accumulate what is known as intangible capital, or the basic know-how needed to make effective use of new tech. Indeed, Erik Brynjolfsson of Stanford University, Daniel Rock of the Massachusetts Institute of Technology and Chad Syverson of the University of Chicago suggest a disruptive new technology may be associated with a “productivity J-curve”. Measured productivity growth may actually decline in the years or decades after a new technology appears, as firms and workers divert time and resources to studying the tech and designing business processes around it. Only later as these investments bear fruit does the J surge upward. The authors reckon that ai-related investments in intangible capital may already be depressing productivity growth, albeit not yet by very much.Of course for many people, questions about the effects of ai on growth take a back seat to concerns about consequences for workers. Here, history’s messages are mixed. There is good news: despite epochal technological and economic change, fears of mass technological unemployment have never before been realised. Tech can and does take a toll on individual occupations, however, in ways that can prove socially disruptive. Early in the Industrial Revolution, mechanisation dramatically increased demand for relatively unskilled workers, but crushed the earnings of craftsmen who had done much of the work before, which is why some chose to join machine-smashing Luddite movements. And in the 1980s and 1990s, automation of routine work on factory floors and in offices displaced many workers of modest means, while boosting employment for both high- and low-skilled workers.Gee, Pretty Terrificai might well augment the productivity of workers of all different skill levels, even writers. Yet what that means for an occupation as a whole depends on whether improved productivity and lower costs lead to a big jump in demand or only a minor one. When the assembly line—a process innovation with gpt-like characteristics—allowed Henry Ford to cut the cost of making cars, demand surged and workers benefited. If ai boosts productivity and lowers costs in medicine, for example, that might lead to much higher demand for medical services and professionals.There is a chance that powerful ai will break the historic mould. A technology capable of handling almost any task the typical person can do would bring humanity into uncharted economic territory. Yet even in such a scenario, the past holds some lessons. The sustained economic growth which accompanied the steam revolution, and the further acceleration which came along with electrification and other later innovations, were themselves unprecedented. They prompted a tremendous scramble to invent new ideas and institutions, to make sure that radical economic change translated into broad-based prosperity rather than chaos. It may soon be time to scramble once again. ■Read more from Free Exchange, our column on economics:Have economists misunderstood inflation? (Jan 26th)Could Europe end up with a worse inflation problem than America? (Jan 19th)Warnings from history for a new era of industrial policy (Jan 11th)For more expert analysis of the biggest stories in economics, finance and markets, sign up to Money Talks, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.One of the joys of writing about business is that rare moment when you realise conventions are shifting in front of you. It brings a shiver down the spine. Vaingloriously, you start scribbling down every detail of your surroundings, as if you are drafting the opening lines of a bestseller. It happened to your columnist recently in San Francisco, sitting in the pristine offices of Anthropic, a darling of the artificial-intelligence (AI) scene. When Jack Clark, one of Anthropic’s co-founders, drew an analogy between the Baruch Plan, a (failed) effort in 1946 to put the world’s atomic weapons under UN control, and the need for global co-ordination to prevent the proliferation of harmful AI, there was that old familiar tingle. When entrepreneurs compare their creations, even tangentially, to nuclear bombs, it feels like a turning point.Since ChatGPT burst onto the scene late last year there has been no shortage of angst about the existential risks posed by AI. But this is different. Listen to some of the field’s pioneers and they are less worried about a dystopian future when machines outthink humans, and more about the dangers lurking within the stuff they are making now. ChatGPT is an example of “generative” ai, which creates humanlike content based on its analysis of texts, images and sounds on the internet. Sam Altman, CEO of OpenAI, the startup that built it, told a congressional hearing this month that regulatory intervention is critical to manage the risks of the increasingly powerful “large language models” (LLMs) behind the bots.In the absence of rules, some of his counterparts in San Francisco say they have already set up back channels with government officials in Washington, DC, to discuss the potential harms discovered while examining their chatbots. These include toxic material, such as racism, and dangerous capabilities, like child-grooming or bomb-making. Mustafa Suleyman, co-founder of Inflection AI (and board member of The Economist’s parent company), plans in coming weeks to offer generous bounties to hackers who can discover vulnerabilities in his firm’s digital talking companion, Pi.Such caution makes this incipient tech boom look different from the past—at least on the surface. As usual, venture capital is rolling in. But unlike the “move fast and break things” approach of yesteryear, many of the startup pitches now are first and foremost about safety. The old Silicon Valley adage about regulation—that it is better to ask for forgiveness than permission—has been jettisoned. Startups such as OpenAI, Anthropic and Inflection are so keen to convey the idea that they won’t sacrifice safety just to make money that they have put in place corporate structures that constrain profit-maximisation.Another way in which this boom looks different is that the startups building their proprietary LLMs aren’t aiming to overturn the existing big-tech hierarchy. In fact they may help consolidate it. That is because their relationships with the tech giants leading in the race for generative AI are symbiotic. OpenAI is joined at the hip to Microsoft, a big investor that uses the former’s technology to improve its software and search products. Alphabet’s Google has a sizeable stake in Anthropic; on May 23rd the startup announced its latest funding round of $450m, which included more investment from the tech giant. Making their business ties even tighter, the young firms rely on big tech’s cloud-computing platforms to train their models on oceans of data, which enable the chatbots to behave like human interlocutors.Like the startups, Microsoft and Google are keen to show they take safety seriously—even as they battle each other fiercely in the chatbot race. They, too, argue that new rules are needed and that international co-operation on overseeing LLMs is essential. As Alphabet’s CEO, Sundar Pichai, put it, “AI is too important not to regulate, and too important not to regulate well.”Such overtures may be perfectly justified by the risks of misinformation, electoral manipulation, terrorism, job disruption and other potential hazards that increasingly powerful AI models may spawn. Yet it is worth bearing in mind that regulation will also bring benefits to the tech giants. That is because it tends to reinforce existing market structures, creating costs that incumbents find easiest to bear, and raising barriers to entry.This is important. If big tech uses regulation to fortify its position at the commanding heights of generative AI, there is a trade-off. The giants are more likely to deploy the technology to make their existing products better than to replace them altogether. They will seek to protect their core businesses (enterprise software in Microsoft’s case and search in Google’s). Instead of ushering in an era of Schumpeterian creative destruction, it will serve as a reminder that large incumbents currently control the innovation process—what some call “creative accumulation”. The technology may end up being less revolutionary than it could be.LLaMA on the loose Such an outcome is not a foregone conclusion. One of the wild cards is open-source AI, which has proliferated since March when LLaMa, the LLM developed by Meta, leaked online. Already the buzz in Silicon Valley is that open-source developers are able to build generative-AI models that are almost as good as the existing proprietary ones, and hundredths of the cost.Anthropic’s Mr Clark describes open-source AI as a “very troubling concept”. Though it is a good way of speeding up innovation, it is also inherently hard to control, whether in the hands of a hostile state or a 17-year-old ransomware-maker. Such concerns will be thrashed out as the world’s regulatory bodies grapple with generative AI. Microsoft and Google—and, by extension, their startup charges—have much deeper pockets than open-source developers to handle whatever the regulators come up with. They also have more at stake in preserving the stability of the information-technology system that has turned them into titans. For once, the desire for safety and for profits may be aligned. ■Read more from Schumpeter, our columnist on global business:America’s culture wars threaten its single market (May 18th)Writers on strike beware: Hollywood has changed for ever (May 10th)America needs a jab in its corporate backside (May 3rd)Also: If you want to write directly to Schumpeter, email him at [email protected]. And here is an explanation of how the Schumpeter column got its name.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.“IT IS THE most advanced AI accelerator in the industry,” boasted Lisa Su, boss of Advanced Micro Devices (AMD), at the launch in December of its new MI300 chip. Ms Su rattled off a series of technical specifications: 153bn transistors, 192 gigabytes of memory and 5.3 terabytes per second of memory bandwidth. That is, respectively, about 2, 2.4 and 1.6 times more than the H100, the top-of-the-line artificial-intelligence chip made by Nvidia. That rival chipmaker’s prowess in the semiconductors fuelling the AI boom has, over the past year, turned it into America’s fifth-most-valuable company, with a market capitalisation of $1.5trn. Yet most experts agreed that the numbers and Ms Su weren’t lying: the MI300 does indeed outshine the H100. Investors liked it, too—AMD’s share price jumped by 10% the next day.On January 30th, in its quarterly earnings call, AMD announced that it expected to sell $3.5bn-worth of MI300s this year. It also reported strong revenues of $23bn in 2023, four times what they had been in 2014, when Ms Su became chief executive. Its market value is up 100-fold on her watch, to $270bn. Relative to forecast profits in the next 12 months, its valuation is richer even than Nvidia’s. Last year it displaced Intel, which once ruled American chipmaking, as the country’s second-most-valuable semiconductor company. Now it is taking aim at the biggest.image: The EconomistSuch ambition would have seemed fanciful a decade ago. Back then, recalls Mark Papermaster, AMD’s technology chief, AMD was facing an “existential crisis”. In 2008 it had spun off its chip-fabrication business to focus on designing processors, outsourcing manufacturing to contract chipmakers such as TSMC of Taiwan. The idea was to be better able to compete on blueprints with Intel, whose vast fabrication capacity AMD could not hope to match.It didn’t work. Several of AMD’s chips flopped. Sales of its central processing units (CPUs), mostly for personal computers, were collapsing. In 2013 it sold and leased back its campus in Austin to raise cash. A year later Ms Su inherited a net-debt pile of more than $1bn, a net annual loss of $400m and a market value of less than $3bn, down from $20bn in 2006.She realised that the only way for AMD to get back in the game was to steer it away from the sluggish PC market and focus on more promising areas like CPUs for data-centre servers and graphics processing units (GPUs, which make video-game visuals lifelike) for gaming consoles. She and Mr Papermaster took a gamble on a new CPU architecture designed to beat Intel not just on price, but also on performance.When the going got toughThe idea was to use a Lego-like approach to chip building. By breaking a chip up into smaller parts, AMD could mix and match blocks to assemble different types of chip, at a lower cost. When the first such composite chips were released in 2017, they were zippier and cheaper than rival offerings from Intel, possibly in part because Intel was distracted by its own problems (notably repeated manufacturing slip-ups as it moved to ever tinier transistors). In the past ten years AMD’s market share in lucrative server CPUs has gone from nothing to 30%, breaking Intel’s monopoly.Having faced down one giant, AMD now confronts another. The contest with Nvidia is different. For one thing, it is personal—Ms Su and Jensen Huang, Nvidia’s Taiwanese-born boss, are distant relatives. In contrast to Intel, Nvidia is, like AMD, a chip designer and thus less prone to production missteps. More importantly, the stakes are higher. Nvidia’s market value of $1.5trn is predicated on its dominance of the market for GPUs—not because of their usefulness in gaming but because they also happen to be the best type of chip to train AI models. Ms Su expects global sales of AI chips to reach $400bn by 2027, up from perhaps $40bn last year. Does she stand a chance against Nvidia?Nvidia is a formidable rival. Both its revenues and operating margins are nearly three times AMD’s. According to Jefferies, an investment bank, the company dominates the market for AI accelerator chips, accounting for 86% of such components sold globally; before the launch of the MI300, AMD barely registered. Nvidia also offers network gear that connects clusters of chips, and software, known as CUDA, to manage AI workloads. Nvidia has dominated AI chipmaking because it has offered the best chips, the best networking kit and the best software, notes Doug O’Laughlin of Fabricated Knowledge, a research firm.image: The EconomistAMD’s new processor shows it can compete with Nvidia on semiconductor hardware. This, Mr Papermaster says, is the result of a ten-year investment. AMD is spending nearly $6bn a year on research and development, nearly as much as its larger rival—and twice as much as a share of sales (see table). This has enabled it to adapt its Lego approach to GPUs. Combining a dozen blocks—or “chiplets”—into a single chip lets AMD put processors and memory close to each other, which boosts processing speed. In December OpenAI, maker of ChatGPT and the world’s hottest AI startup, said it would use the MI300s for some of its training.To outdo Nvidia on networking and software, AMD is teaming up with other firms. In December it announced a partnership with makers of networking gear, including the two largest, Broadcom and Cisco. It is also supporting an open-source initiative for chip-to-chip communication called Ultra Ethernet Consortium as an alternative to InfiniBand, a rival championed by Nvidia.Chomping at the byteNvidia’s lead in software will be harder to close. It has been investing in CUDA since the mid-2000s, well before the current AI wave. AI developers and researchers love the platform, which allows them to fine-tune the performance of Nvidia processors. AMD hopes to tempt customers away from Nvidia by making its software, ROCm, open-source and providing tools to make the switch smoother, by translating CUDA programs into ROCm ones.Beating Nvidia at its own game will not be easy. Mr Huang’s firm is not standing still. It recently announced plans to bring out a new chip every year instead of every two years. The tech giants with the grandest AI ambitions—Alphabet, Amazon, Meta and Microsoft—are busily designing their own accelerator chips. Despite AMD’s robust sales, investors were disappointed with its forecast for MI300 shipments. Its share price dipped by 3% the day after it reported its latest results.Still, AMD has one big thing going for it. It is not Nvidia. AI companies are desperate for an alternative to its larger rival, whose dominant position allows it to charge steep prices and, with demand outstripping supply, ration chips to buyers. Despite efforts to design their own hardware, big tech firms will rely on chipmakers for a while, and AMD gives them options, notes Vivek Arya of Bank of America. Microsoft and Meta have already announced plans to use AMD’s GPUs in their data centres. And if Nvidia slips up, AMD will be there to pick up the Lego pieces. Just ask Intel. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Crystals can do all sorts of things, some more useful than others. They can separate the gullible from their money in New Age healing shops. But they can also serve as the light-harvesting layer in a solar panel, catalyse industrial reactions to make things like ammonia and nitric acid, and form the silicon used in microchips. That diversity arises from the fact that “crystal” refers to a huge family of compounds, united only by having an atomic structure made of repeating units—the 3D equivalent of tessellating tiles.Just how huge is highlighted by a paper published in Nature by Google DeepMind, an artificial-intelligence company. Scientists know of about 48,000 different crystals, each with a different chemical recipe. DeepMind has created a machine-learning tool called GNoME (Graph Networks for Materials Exploration) that can use existing libraries of chemical structures to predict new ones. It came up with 2.2m crystal structures, each new to science.To check the machine’s predictions, DeepMind collaborated on a second paper, also published in Nature, with researchers at the University of California, Berkeley. They chose 58 of the predicted compounds and were able to synthesise 41 of them in a little over two weeks. The team at DeepMind say more than 700 other crystals have been produced by other groups since they began preparing their paper.To help any other laboratories keen to investigate the computer’s bounty, the firm has made public a subset of what they think should be the 381,000 most stable structures. Among them are many thousands of crystals with structures potentially amenable to superconductivity, in which electrical currents flow with zero resistance, and several hundred potential conductors of lithium ions that could find a use in batteries. In both cases DeepMind’s work has increased the total number of candidate materials known to researchers tens of times over.Aron Walsh, a materials scientist at Imperial College London who was not involved in the research, says DeepMind’s work is impressive. But “this is the start of the exploration rather than the end,” he says, noting that the machine has only scratched the surface of what might be possible. In a recent paper of his own he tried to calculate how many stable crystals incorporating four chemical elements (so-called quaternaries) might be potentially manufacturable. He wound up with a conservative estimate of 32trn. For its part, GNoME looked only at crystals that form under relatively low temperatures and pressures. And crystals are only one subset of a universe of materials that includes everything from amorphous solids such as glass through to gases, gels and liquids.Whether any of DeepMind’s 2.2m new crystals will be useful remains to be seen. Even if they do not, the techniques used to make the predictions could be valuable. Besides suggesting new crystals, AI may also shed light on as-yet-unknown rules that govern how they form.Ekin Dogus Cubuk at DeepMind highlights one such finding. Previously, he says, crystals made from six elements, called senaries, were thought to be vanishingly rare. But DeepMind’s AI found around 3,200 in its sample of 381,000 stable compounds. A better understanding of how crystals form, and what sorts are possible, might also save scientists curious to test how the 2.2m new materials behave from the tedious task of synthesising each one of them by hand. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
WHERE IS RESEARCH into artificial intelligence (AI) heading? Is it all beneficial for humanity, or are there risks big enough that we need to make more effort to understand them and develop countermeasures? I believe the latter is true.The human brain is a biological machine, so it should be feasible to build machines at least as intelligent. As has been argued by Geoff Hinton, with whom Yann LeCun and I shared the Turing Award in 2018 for our work on AI, once we comprehend the main principles underlying human intelligence we will be capable of building superhuman AI surpassing us in most tasks.Computers already outperform us in specialised areas such as playing Go or modelling protein structures, and we are making progress towards building more general-purpose AI systems: ChatGPT can quickly process a huge training corpus from the internet, a task that would take a human tens of thousands of lifetimes dedicated solely to reading. This is achievable because, while learning, computers can perform extensive parallel computations and share data among themselves at rates billions of times faster than humans, who are limited to exchanging a few bits per second using language. Additionally, computer programs are, unlike humans, immortal, capable of being copied or self-replicating like computer viruses.When can we expect such superhuman AIs? Until recently, I placed my 50% confidence interval between a few decades and a century. Since GPT-4, I have revised my estimate down to between a few years and a couple of decades, a view shared by my Turing award co-recipients. What if it occurs in five years? Or even ten? OpenAI, the company behind GPT, is among those who think it could happen by then.Are we prepared for this possibility? Do we comprehend the potential consequences? No matter the potential benefits, disregarding or playing down catastrophic risks would be very unwise.How might such catastrophes arise? There are always misguided or ill-intentioned people, so it seems highly probable that at least one organisation or person would—intentionally or not—misuse a powerful tool once it became widely available.We are not there yet, but imagine a scenario where a method for achieving superhuman AI becomes publicly known and the model downloadable and usable with the resources accessible to a mid-sized company, as is currently the case with open-source—but fortunately not superhuman—large language models. What are the chances that at least one such organisation would download the model and instruct it, possibly using a natural language interface like ChatGPT, to achieve goals that would violate human rights, undermine democracy or threaten humanity as a whole? Examples include targeted cyber-attacks that could threaten fragile supply chains, using convincing dialogue and AI-generated videos to influence citizens to sway an election, or designing and deploying bioweapons.Another possibility is that AI acquires a self-preservation tendency. This could happen in many ways: training it to mimic humans (who exhibit self-preservation); instructions from humans that make it seek power, and hence pursue self-preservation as a subsidiary goal; or a “Frankenstein” scenario, where someone intentionally builds an AI system with a survival instinct, to make the AI in their own image.New entities with self-preservation are like new species: to preserve themselves, these AIs would strive to prevent humans from shutting them down, attempt to replicate themselves in multiple locations as a defensive measure and potentially behave in ways that harm humans. Once another species on Earth surpasses us in intelligence and power, we may lose control over our own future.But could such a rogue AI shape the real world? If, like AutoGPT, it was connected to the internet, it could develop numerous strategies or learn them from us: exploiting cyber-security vulnerabilities, employing human assistants (including organised crime), creating accounts to generate income (for instance in financial markets), and influencing or using extortion against key decision-makers. A superhuman AI, whether of its own volition or following human instructions, could destabilise democracies, disrupt supply chains, invent new weapons or worse.Even if we knew how to construct AI that was not prone to developing dangerous objectives, and even if we implemented strong regulations to minimise access and enforce safety protocols, there is still a possibility of someone getting in, ignoring the protocols and instructing the AI with disastrous consequences. Given these risks, and the challenges around regulating AI safely, I believe it is crucial to act swiftly on three fronts.First, the world’s governments and legislatures must adopt national rules, and co-ordinate international ones, that safeguard the public against all AI-related harms and risks. These should prohibit the development and deployment of AI systems with dangerous capabilities; and require thorough evaluation of potential harm with independent audits, with at least the same level of scrutiny as that applied to the pharmaceutical, aviation and nuclear industries.Second, they should accelerate AI safety and governance research to better understand options for robust safety protocols and governance, and how best to protect human rights and democracy.Third, we need to research and develop countermeasures in case dangerous AIs arise, whether they are under human instruction or have developed their own goals to preserve themselves. Such research should be co-ordinated internationally and under an appropriate governance umbrella, to make sure that the countermeasures can be deployed around the world and that this work has no military aims, reducing the risk of an AI arms race. This research, combining national-security and AI expertise, should be done by neutral and autonomous entities across several countries (to avoid capture by a single government, which could use its control of AI technology to keep itself in power or attack other countries). It should not be entrusted solely to national labs or for-profit organisations, whose parochial or commercial objectives might interfere with the mission of defending humanity as a whole.Considering the immense potential for damage, we should make investments to safeguard our future comparable to, if not exceeding, past investments in the space programme or current investments in nuclear fusion. Much is being spent on improving AI capabilities. It is crucial that we invest at least as much in protecting humanity. ■Yoshua Bengio is a professor at the the Université de Montréal and founder and scientific director of Mila - Quebec AI Institute.For a contrary view on AI and existential risk, see this article by Blake Richards, Dhanya Sridhar, Guillaume Lajoie and Blaise Agüera y Arcas.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.“I can’t believe it worked!” says Nat Friedman, co-founder of the Vesuvius Challenge, which offered $1m in prizes to anyone who could use artificial intelligence (AI) to decipher papyrus scrolls carbonised by the eruption of Mount Vesuvius in 79AD. But work it did. On February 5th Mr Friedman announced that a three-person team had been awarded $700,000 for successfully extracting four passages of text, each at least 140 characters long, and with at least 85% of the characters legible, from a scroll known as Banana Boy. The three winners, Luke Farritor, Youssef Nader and Julian Schilliger, are all computer-science students.The scroll is one of hundreds found in the library of a Roman villa in Herculaneum, which is thought to have belonged to the father-in-law of Julius Caesar. Along with hundreds of other scrolls in the villa’s library, it was damaged by scorching gases that engulfed the town during the same eruption that also buried the nearby town of Pompeii.Reading text from the scrolls is difficult because the heat turned them into brittle charcoal logs; all efforts to unroll them physically caused them to disintegrate. So attention shifted towards finding ways to unwrap them virtually, through computer analysis of 3D scans of the scrolls made using X-rays. This turned deciphering the scrolls into a software problem—but a very complex one.Virtual unrolling is a two-stage process pioneered by W. Brent Seales, a computer scientist at the University of Kentucky. The first stage, called segmentation, involves tracing the edges of the rolled-up papyrus sheet inside the 3D scan, then extracting 2D images of the scroll’s surface. The second stage, ink detection, analyses the resulting images to pluck the ink of the scroll’s text from the papyrus background. This is particularly tricky for the Herculaneum scrolls, which are written in carbon-based ink, so there is very little contrast with the background of carbonised papyrus.Dr Seales, along with Mr Friedman and Daniel Gross, two technology entrepreneurs, thought AI techniques might fruitfully be brought to bear on these two problems, and launched a prize challenge to find out. A community of thousands of enthusiasts has since developed a range of tools and tricks to speed up the fiddly process of segmentation, and to detect the ink of individual letters, and then whole words. In October 2023 Mr Farritor and Mr Nader were awarded smaller prizes for independently extracting the first legible word (“porphyras”, which means “purple” in ancient Greek) from the Banana Boy scroll (so named because of its size and shape).image: Vesuvius ChallengeThe two students then teamed up and, joined by Mr Schilliger, further improved the machine-learning technique involved in ink detection. By manually labelling areas known to be ink, they could train a neural network to find more of them; these were fed back into the model to improve its detection abilities. Mr Nader also switched the neural network to a novel architecture called a TimeSformer, which produced sharper results. Mr Schilliger, meanwhile, devised a tool to automate more of the segmentation process (much of which must still be done manually).The deadline to submit results for the grand prize was at the end of December, and the trio was awarded the prize after an assessment of the entries by a team of papyrologists. (Three runners up will receive smaller prizes of $50,000 each.) The winning entry revealed 15 columns of text, written in Greek. Reading it was “mind-blowing”, says Federica Nicolardi, a papyrologist at the University of Naples Federico II, who was one of the judges. The text is thought to be a previously unknown work on pleasure by Philodemus, an Epicurean philosopher who lived in Herculaneum.Mr Friedman now wants to scale up the whole process. With ink detection solved, he says, “the bottleneck is now segmentation”. Mr Schilliger’s auto-segmentation tool is a big step forward, and he has agreed to make it open source, and to collaborate with others to improve it. Further prizes are being offered as an incentive. Mr Friedman, meanwhile, aims to scan more scrolls using the Diamond Light Source, a particle accelerator in Britain, and to standardise the scanning process.That will cost money. Having given out $1.2m in prizes, some of it from his own pocket, Mr Friedman is looking for other backers to help support the project. He hopes that deciphering ancient scrolls will lead to the rediscovery of lost works from antiquity—“each scroll is a mystery box”, he says—and, ultimately, revive interest in further excavating the villa in Herculaneum, which may contain thousands more of them. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.LAWYERS are a conservative bunch, befitting a profession that rewards preparedness, sagacity and respect for precedent. No doubt many enjoyed a chuckle at the tale of Steven Schwartz, a personal-injury lawyer at the New York firm Levidow, Levidow & Oberman, who last month used ChatGPT to help him prepare a court filing. He relied a bit too heavily on the artificial-intelligence (AI) chatbot. It created a motion replete with made-up cases, rulings and quotes, which Mr Schwartz promptly filed after the bot assured him that the “cases I provided are real and can be found in reputable legal databases” (they were not, and cannot). Lesson learned, a tech-sceptic lawyer might conclude: the old ways are the best.That is the wrong lesson. Blaming AI for Mr Schwartz’s error-filled brief makes no more sense than blaming the printing press for mistakes in a typed one. In both cases, fault lies with the lawyer who failed to check the motion before filing it, not the tool that helped produce it. For that is what AI is: neither a fad nor an apocalypse, but a tool in its infancy—and one that could radically change how lawyers work and law firms make money. The legal profession is hardly the only field about which one could say that. But few combine as clear a use case with so high a risk. Firms that get it right stand to reap rewards. Laggards risk going the way of typesetters.According to a recent report from Goldman Sachs, a bank, 44% of legal tasks could be performed by AI, more than in any occupation surveyed except for clerical and administrative support. Lawyers spend an awful lot of time scrutinising tedious documents—the sort of thing that AI has already demonstrated it can do well. Lawyers use AI for a variety of tasks, including due diligence, research and data analytics. These applications have largely relied on “extractive” AI, which, as the name suggests, extracts information from a text, answering specific questions about its contents.“Generative” AIs such as ChatGPT are far more powerful. Part of that power can be used to improve legal research and document review. As Pablo Arredondo, creator of a generative-AI “legal assistant” called CoCounsel, explains, using it “removes the tyranny of the keyword…It can tell that ‘We reverse Jenkins’ [a fictional legal case] and ‘We regretfully consign Jenkins to the dustbin of history’ are the same thing.” Allen & Overy, a large firm based in London, has integrated a legal AI tool called Harvey into its practice, using it for contract analysis, due diligence and litigation prep.Not all lawyers are convinced. One recent survey found that 82% of them believe generative AI can be used for legal work but just 51% thought it should. Many worry about “hallucinations” (as AI boffins refer to chatbots’ tendency to present falsehoods with aplomb, as in Mr Schwartz’s case) and about inadvertently feeding information subject to attorney-client privilege into algorithms. Yet if these challenges can be tackled—and they can, with better technology and careful humans in the loop—then the misgivings of the doubting 49% may pass. After news of Mr Schwartz’s debacle broke, for example, a federal judge in Texas told attorneys appearing before him to file a certificate attesting that they either did not use generative AI at all or that, if they did, they checked the final result. Much as it made little sense for lawyers to insist on doing legal research in libraries once the vastly larger and more easily searched databases of Westlaw and LexisNexis were a click away, when a critical mass of firms embraces generative AI, more will follow.AI has the potential to transform the legal profession in three big ways. First, it could reduce big firms’ manpower advantage. In large, complex lawsuits, these firms tell dozens of associates to read millions of pages of documents looking for answers to senior lawyers’ questions and hunches. Now a single lawyer or small firm will be able to upload these documents into a litigation-prep AI and begin querying them. As Lawrence Lessig of Harvard Law School notes, “You can be a smaller, leaner specialised firm and have the capacity to process these sorts of cases.”Billable powersSecond, AI could change how firms make money. Richard Susskind, technology adviser to the Lord Chief Justice of England, argues that firms profit by “having armies of young lawyers to whom they pay less than they charge clients”. If AI can do the work of those armies in seconds, firms will need to change their billing practices. Some may move to charging flat fees based on the service provided, rather than for the amount of time spent providing it. Stephen Wu of Silicon Valley Law Group speculates that firms may charge “a technology fee”, so that “clients don’t expect to get generative AI for nothing”.Third, AI could change how many lawyers exist and where they work. Eventually, Mr Lessig argues, it is hard to see how AI “doesn’t dramatically reduce the number of lawyers the world needs”. If AI can do in 20 seconds a task that would have taken a dozen associates 50 hours each, then why would big firms continue hiring dozens of associates? A veteran partner at a prestigious corporate-law firm in New York expects the ratio of associates to partners to decline from today’s average of perhaps seven to one at the top firms to closer to parity. If associates aren’t worried about their jobs, he says, “they should be”.That may not happen for a while, however. Moreover, AI could make legal services cheaper and thus more widely available, particularly for small and medium-sized businesses that currently often struggle to afford them. Ambitious law-school graduates may find that AI provides an easier path to starting a solo practice. If so, then AI could actually lead to an increase in the overall number of lawyers, as well as changing the sort of tasks they perform—just as the ATM led to an increase in the number of human bank employees rather than their replacement.Ultimately this will be good news for clients. “People who go to lawyers don’t want lawyers: they want resolutions to their problems or the avoidance of problems altogether,” explains Mr Susskind. If AI can provide those outcomes then people will use AI. Many people already use software to do their taxes rather than rely on professionals; “Very few of them are complaining about the lack of social interaction with their tax advisers.” ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.EVEN BY TECH’S fast-moving standards,  the past week in the world of artificial intelligence (AI) was head-spinning. On November 17th the board of OpenAI booted out Sam Altman, the ChatGPT-maker’s boss. By November 20th Mr Altman had been offered refuge at Microsoft, the startup’s biggest backer. The same day nearly all of OpenAI’s 770 employees signed a letter threatening to quit unless the board members who dismissed Mr Altman reinstate him and resign. On November 21st Mr Altman was back in his old job. Heads have, then, spun back more or less to where they started. Or have they?In fact, the OpenAI saga marks the start of a new, more grown-up phase for the AI industry. For OpenAI, Mr Altman’s triumphant return may supercharge its ambitions. For Microsoft, which stood by Mr Altman in his hour of need, the episode may result in greater sway over AI’s hottest startup. For AI companies everywhere it may herald a broader shift away from academic idealism and towards greater commercial pragmatism. And for the technology’s users, it may, with luck, usher in more competition and more choice.To understand all these implications, start with what happened. OpenAI’s board fired Mr Altman for not being “consistently candid in his communications’‘. One factor that may have influenced the decision was disagreement over whether OpenAI had struck the right balance between the speed and safety of its products. Insiders say that OpenAI had made a breakthrough that enabled models to get better at solving problems without additional data. This spooked Ilya Sutskever, a co-founder and board member. Helen Toner, a board member affiliated with Georgetown University, had published an academic article that laid out what she saw as flaws in OpenAI’s approach to AI safety. On November 21st the New York Times reported that Mr Altman, worried about the negative press, had moved to oust Ms Toner. There were also concerns over Mr Altman’s side-projects, including a planned AI-semiconductor venture that sent him to the Persian Gulf to court billions in Saudi money.In the end it was Ms Toner and three other board members that ousted him instead. The sixth director, Greg Brockman, was also stripped of his board seat and then quit in solidarity with Mr Altman. The two of them found succour at Microsoft, which said it would create a new in-house AI lab which they would run. Microsoft also pledged to hire the rest of OpenAI’s team. Whether or not this was ever a serious plan may never be known. But it lent Mr Altman huge bargaining power when negotiating his return to OpenAI. On November 20th, as those negotiations were under way, Satya Nadella, the tech giant’s chief executive, declared that “Irrespective of where Sam is, he’s working with Microsoft.”The deal struck by Mr Altman and those who ousted him will transform OpenAI, starting with the board. Ms Toner and Mr Sutskever are out. So is Tasha McCauley, a tech entrepreneur. All three backed Mr Altman’s dismissal. Mr Brockman and, for the time being, Mr Altman will not be returning. Of the pre-chaos six only Adam D’Angelo, the founder of Quora, a question-and-answer site, stays on. He will be joined by heavyweights, starting with Bret Taylor, a former co-CEO of Salesforce, another big software firm, and Larry Summers of Harvard University, who served as Bill Clinton’s treasury secretary. The Verge, an online publication, has reported that the new board will aim to expand to nine members; Microsoft is expected to get a seat and Mr Altman may get his back.The new directors are likely to make OpenAI, which is structured as a for-profit entity within a non-profit one, more business-minded. Mr Taylor and Mr Summers are well-regarded figures with plenty of boardroom experience. Their views on AI safety are not known. But they may be more receptive than Ms Toner and Ms McCauley to Mr Altman’s empire-building ambitions. The same already seems to be true of OpenAI’s workforce. One employee reports that the startup’s staff, which “trauma-bonded” during the upheaval, will become even more loyal to Mr Altman and, possibly, readier to pursue his commercial vision. Work on the firm’s most powerful model yet, GPT-5, which appeared to have slowed for a few months, will now probably go full speed ahead.The sour taste left by the imbroglio may nevertheless linger. It was not, in the words of a prominent AI investor, a “confidence-inducing event”. That is putting it mildly. On the morning of November 17th OpenAI was poised to close a tender offer led by Thrive Capital, a venture-capital firm, that would value the startup at $86bn. The offer was suspended. Though it is reportedly back on, investors in the secondary market for startup shares remain cautious. Worse, if Mr Altman and Mr Sutskever do not reconcile, OpenAI could lose one of the world’s most respected AI minds.Microsoft’s fortunes look more secure. Whereas OpenAI’s brand has taken a hit, Microsoft’s has not. The software giant probably prefers having OpenAI at arm’s length rather than Mr Altman and his boffins close to its chest. By temperament, Mr Altman and Mr Brockman are not a natural fit for one of the world’s biggest companies; many observers doubted that either would have stayed at Microsoft for long.Recreating OpenAI in-house would also have slowed the progress of the technology in the short term, argues Mark Moerdler of Bernstein, a broker. Many OpenAI employees said in private that they would rather move to a different firm than Microsoft, even though they signed the petition threatening to follow Mr Altman there. Mr Nadella did not seem terribly disappointed with the outcome. Microsoft’s share price, which dipped by 2% on the news of Mr Altman’s sacking, has clawed back all those losses. On November 22nd its market value reached an all-time high of $2.8trn.image: The EconomistWhat about the rest of the AI industry? OpenAI is the undisputed leader in the AI race (see chart). A survey by Retool, a startup, found that 80% of software developers said that they used OpenAI’s models more often than those of rival model-makers. ChatGPT, a chatty app whose launch one year ago turned OpenAI into a household name, receives 60% of web traffic to the top 50 websites for such “generative” AI. In October the firm was earning revenues at an annualised rate of $1.3bn.Even if OpenAI moves faster under new leadership, it will face more competition. An AI-focused venture capitalist likens the moment to the implosion earlier this year of Silicon Valley Bank, which taught many startups not to put all their eggs in one basket. As the Altman drama was unfolding, more than 100 OpenAI customers contacted Anthropic, a rival model-maker, according to the Information, an online publication. Some tapped Cohere, another startup, and the cloud unit of Google, which has invested in Anthropic. The cloud arm of Amazon, another Anthropic-backer, set up a team to work with switchers.The events at OpenAI are a dramatic manifestation of a wider divide in Silicon Valley. On one side are the “doomers”, who believe that, left unchecked, AI poses an existential risk to humanity and hence advocate stricter regulations. Opposing them are “boomers”, who play down fears of an ai apocalypse and stress its potential to turbocharge progress. The split reflects in part philosophical differences. Many in the doomer camp are influenced by “effective altruism”, a movement worried that Ai might wipe out humanity. Boomers espouse a worldview called “effective accelerationism”, which counters that the development of AI should be speeded up.Mr Altman seemed to have sympathy with both groups, publicly calling for “guardrails” to make ai safe while pushing Openai to develop more powerful models and launching new tools, such as an app store for users to build their own chatbots. Today he looks decidedly more boomerish, as do the majority of OpenAI’s workers who wanted him back. The doomers are on the back foot.That will worry politicians, who are scrambling to show that they take the risks seriously. In July President Joe Biden’s administration nudged seven leading model-makers, including Google, Meta, Microsoft and Openai, to make “voluntary commitments” to have their ai products inspected by experts before releasing them to the public. On November 1st the British government got a similar group to sign another non-binding agreement that allowed regulators to test their ais for trustworthiness and harmful capabilities, such as endangering national security.Days earlier Mr Biden issued an executive order with more bite. It compels any ai firm building models above a certain size—defined by the computing power required—to notify the government and share its safety-testing results. As boomers gain the upper hand in Silicon Valley, the White House’s model-inspectors should expect to have their hands full. ■Read more of our articles on artificial intelligenceTo stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.New technology brings with it both the sweet hope of greater prosperity and the cruel fear of missing out. Satya Nadella, the boss of Microsoft, says he is haunted by the fact that the Industrial Revolution left behind India, his country of birth. (Indian manufacturers hardly enjoyed a level playing-field—Britain was then both their rival and their ruler.) Many technologies, such as online-education courses, have generated more hype than economic growth in the emerging world. Some people worry that generative artificial intelligence (ai), too, will disappoint the global south. The big winners so far seem to be a bunch of Western early adopters, as well as startups in San Francisco and America’s “magnificent seven” tech firms, which include Microsoft and have together added an astonishing $4.6trn to their market value since Chatgpt’s launch in November 2022.Yet ai stands to transform lives in the emerging world, too. As it spreads, the technology could raise productivity and shrink gaps in human capital faster than many before it. People in developing countries need not be passive recipients of AI, but can shape it to suit their own needs. Most exciting of all, it could help income levels catch up with those in the rich world.The promise of AI in developing countries is tantalising. As in the West, it will be a useful all-purpose tool for consumers and workers, making it easier to obtain and interpret information. Some jobs will go, but new ones will be created. Because emerging countries have fewer white-collar workers, the disruption and the gain to existing firms may be smaller than in the West. The imf says that a fifth to a quarter of workers there are most exposed to replacement, compared with a third in rich countries.But a potentially transformative benefit may come from better and more accessible public services. Developing economies have long been held back by a lack of educated, healthy workers. Primary-school teachers in India have twice as many pupils as their American counterparts, but are ill-equipped for the struggle. Doctors in Africa are scarce; properly trained ones are scarcer. Whole generations of children grow up badly schooled, in poor health and unable to fulfil their potential in an increasingly global labour market.As our briefing this week sets out, policymakers and entrepreneurs around the world are exploring ways that ai can help. India is combining large language models with speech-recognition software to enable illiterate farmers to ask a bot how to apply for government loans. Pupils in Kenya will soon be asking a chatbot questions about their homework, and the chatbot will be tweaking and improving its lessons in response. Researchers in Brazil are testing a medical ai that helps undertrained primary-care workers treat patients. Medical data collected worldwide and fed into AIs could help improve diagnosis. If AI can make people in poorer countries healthier and better educated, it should in time also help them catch up with the rich world.Pleasingly, these benefits could spread faster than earlier waves of technology. New technologies invented in the early 20th century took more than 50 years to reach most countries. By contrast, AI will spread through the gadget that many people across the emerging world already have, and many more soon will: the phone in their pockets. In time, chatbots will become much cheaper to provide and acquire.Moreover, the technology can be tailored to local needs. So far there is little sign that AI is ruled by the winner-takes-all effects that benefited America’s social-media and internet-search firms. That means a variety of approaches could prosper. Some developers in India are already taking Western models and fine-tuning them with local data to provide a whizzy language-translation service, avoiding the heavy capital costs of model-building.Another idea that is also taking off in the West is to build smaller, cheaper models of your own. A narrower set of capabilities, rather than the ability to get every bit of information under the sun, can suit specific needs just fine. A medical ai is unlikely to need to generate amusing limericks in the style of William Shakespeare, as ChatGPT does so successfully. This still requires computing power and bespoke data sets. But it could help adapt ai in more varied and useful ways.Some countries are already harnessing ai. China’s prowess is second only to America’s, thanks to its tech know-how and the deep pockets of its internet giants. India’s outsourcing industry could be disrupted, as some back-office tasks are taken on by generative ai. But it is home to a vibrant startup scene, as well as millions of tech developers and a government that is keen to use ai to improve its digital infrastructure. These leave it well-placed to innovate and adapt. Countries in the Gulf, such as the United Arab Emirates and Saudi Arabia, are determined to build an ai industry as they shift from oil. They already have the capital and are importing the talent.Each country will shape the technology in its own way. Chinese chatbots have been trained to keep off the subject of Xi Jinping; India’s developers are focused on lowering language barriers; the Gulf is building an Arabic large language model. Though the global south will not dislodge America’s crown, it could benefit widely from all this expertise.Teaching AIdPlenty could yet go wrong, obviously. The technology is still evolving. Computing power could become too expensive; local data will need to be gathered and stored. Some practitioners may lack the ability to take advantage of the knowledge at their fingertips, or the incentive to try new things. Although countries in sub-Saharan Africa stand to gain the most from improvements to human capital and government services, the technology will spread more slowly there than elsewhere without better connectivity, governance and regulation.The good news is that investments to speed ai’s diffusion will be richly rewarded. Much about the ai revolution is still uncertain, but there is no doubt that the technology will have many uses and that it will only get better. Emerging countries have suffered disappointments before. This time they have a wonderful opportunity—and the power to seize it. ■For subscribers only: to see how we design each week’s cover, sign up to our weekly Cover Story newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The 2010s brought no shortage of miraculous technologies, from tablet computers and 4G mobile internet to new forms of artificial intelligence (ai)—Hey, Siri! But these had surprisingly little effect on the economy. During that decade productivity growth in the rich world averaged a measly 1% a year, holding down average wages. Innovative firms embraced new tech, but many less adventurous ones did not bother, and saw few efficiency gains as a result. The experience showed that technological wizardry and improvements in average living standards do not always go hand in hand.Generative ai, its boosters say, will be different. Not since the invention of the internet has a new technology so captured the public imagination. The technology is consumer-friendly: within days of its release to the public, Chatgpt, the most famous AI chatbot, had millions of users. It is easy to see how this innovation could improve all types of work at all types of firms, from increasing the accuracy of doctors’ diagnoses to helping programmers write software code more efficiently.Some companies are already incorporating ai into their operations. Tech firms are investing heavily in the technology, advertising for many thousands of roles. So are some bricks-and-mortar companies. A drug discovered and designed by ai is progressing through human trials in China. Analysts at ubs reckon that Domino’s Pizza can use ai to “improve the accuracy of order-delivery-time estimates”. Investors are rewarding the early adopters. Since the start of the year, the median share price of the most ai-enthusiastic firms in the S&P 500 has risen by 11%. For those moving more slowly, it has not changed at all.The potential is huge. Yet for ai to truly diffuse through the economy, it needs to make its mark beyond the most go-getting companies. And this will take time. Although the internet began to be used by some companies in the early 1990s, it was not until the late 2000s that two-thirds of American businesses had a website. Some 70 firms in the s&p 500 still show no interest in AI, according to our analysis. And below the corporate crème de la crème, the trends look even less encouraging. According to one recent survey of American and Canadian firms, a third of small businesses have no firm plans to try generative-ai tools over the next year. Some evidence even suggests that usage of Chatgpt and its competitors is falling—perhaps as people have tried it out, and then decided it is not for them.Can AI live up to its promise? Organisations like the OECD propose lots of ways to improve diffusion from the best firms to the rest, including through better education, schemes to raise business investment and changes to competition policy. Such goals are worthy, but hard to achieve. Efforts by technology firms to make AI cheaper and easier to use will do more to speed up adoption. In practice, most companies will adopt AI by default, as new, AI-powered features are added to the software and services that they already use.Indeed, even the most powerful technologies take time to diffuse, because companies tend to use a hotch-potch of software and services, some of which may be years or even decades old. Replacing outdated systems can be costly, complicated and painful. Moreover, in the many industries either run or heavily regulated by the government, such as health care, education and construction, bosses and trade unions often resist the deployment of new technology, worried that it will lead to job losses. In time ai could well transform how people live their lives and do their jobs. But the road to widespread diffusion, and any resulting productivity boom, will be a long one. ■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Since the launch in November of ChatGPT, an artificially intelligent conversationalist, AI is seemingly all anyone can talk about. Corporate bosses, too, cannot shut up about it. So far in the latest quarterly results season, executives at a record 110 companies in the S&P 500 index have brought up AI in their earnings calls. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Not so long ago analysts were openly wondering whether artificial intelligence (AI) would be the death of Adobe, a maker of software for creative types. New tools like DALL-E 2 and Midjourney, which conjure up pictures from text, seemed set to render Adobe’s image-editing offerings redundant. As recently as April, Seeking Alpha, a financial-news site, published an article headlined “Is AI the Adobe killer?”Far from it. Adobe has used its database of hundreds of millions of stock photos to build its own suite of AI tools, dubbed Firefly. Since its release in March the software has been used to create over 1bn images, says Dana Rao, a company executive. By avoiding mining the internet for images, as rivals did, Adobe has skirted the deepening dispute over copyright that now dogs the industry. The firm’s share price has risen by 36% since Firefly was launched.Adobe’s triumph over the doomsters illustrates a wider point about the contest for dominance in the fast-developing market for AI tools. The supersize models powering the latest wave of so-called “generative” AI rely on oodles of data. Having already helped themselves to much of the internet, often without permission, AI firms are now seeking out new data sources to sustain the feeding frenzy. Meanwhile, companies with vast troves of the stuff are weighing up how best to profit from it. A data land grab is under way.The two essential ingredients for an AI model are datasets, on which the system is trained, and processing power, through which the model detects relationships within and among those datasets. Those two ingredients are, to an extent, substitutes: a model can be improved either by ingesting more data or adding more processing power. The latter, however, is becoming difficult owing to a shortage of specialist AI chips, leading model-builders to be doubly focused on seeking out data.Demand for data is growing so fast that the stock of high-quality text available for training may be exhausted by 2026, reckons Epoch AI, a research outfit. The latest AI models from Google and Meta, two tech giants, are likely trained on over 1trn words. By comparison, the sum total of English words on Wikipedia, an online encyclopedia, is about 4bn.It is not only the size of datasets that counts. The better the data, the better the model. Text-based models are ideally trained on long-form, well-written, factually accurate writing, notes Russell Kaplan of Scale AI, a data startup. Models fed this information are more likely to produce similarly high-quality output. Likewise, AI chatbots give better answers when asked to explain their working step by step, increasing demand for sources like textbooks. Specialised information sets are also prized, as they allow models to be “fine-tuned” for more niche applications. Microsoft’s purchase of GitHub, a repository for software code, for $7.5bn in 2018 helped it develop a code-writing AI tool.As demand for data grows, accessing it is getting trickier, with content creators now demanding compensation for material that has been ingested into AI models. A number of copyright-infringement cases have already been brought against model-builders in America. A group of authors, including Sarah Silverman, a comedian, are suing OpenAI, maker of ChatGPT, an AI chatbot, and Meta. A group of artists are similarly suing Stability AI, which builds text-to-image tools, and Midjourney.The upshot has been a flurry of dealmaking as AI companies race to secure data sources. In July OpenAI inked a deal with Associated Press, a news agency, to access its archive of stories. It has also recently expanded an agreement with Shutterstock, a provider of stock photography, with which Meta has a deal, too. On August 8th it was reported that Google was in discussions with Universal Music, a record label, to license artists’ voices to feed a songwriting AI tool. Rumours swirl about AI labs approaching the BBC, Britain’s public broadcaster. Another supposed target is JSTOR, a digital library of academic journals.Holders of information are taking advantage of their greater bargaining power. Reddit, a discussion forum, and Stack Overflow, a question-and-answer site popular with coders, have increased the cost of access to their data. Both websites are particularly valuable because users “upvote” preferred answers, helping models know which are most relevant. Twitter (now known as X), a social-media site, has put in place measures to limit the ability of bots to scrape the site and now charges anyone who wishes to access its data. Elon Musk, its mercurial owner, is planning to build his own AI business using the data.Expanding the frontierAs a consequence, model-builders are working hard to improve the quality of the inputs they already have. Many AI labs employ armies of data annotators to perform tasks such as labelling images and rating answers. Some of that work is complex; an advert for one such job seeks applicants with a master’s degree or doctorate in life sciences. But much of it is mundane, and is being outsourced to places such as Kenya where labour is cheap.AI firms are also gathering data through users’ interactions with their tools. Many of these have a feedback mechanism, where users indicate which outputs are useful. Firefly’s text-to-image generator allows users to pick from one of four options. Bard, Google’s chatbot, proposes three answers. Users can give ChatGPT a thumbs-up or thumbs-down to its responses. That information can be fed back as an input into the underlying model, forming what Douwe Kiela, co-founder of Contextual AI, a startup, calls the “data flywheel”. A stronger signal still of the quality of a chatbot’s answers is whether users copy the text and paste it elsewhere, he adds. That information helped Google rapidly improve its translation tool.There is, however, one source of data that remains largely untapped: the information that exists within the walls of the tech firms’ corporate customers. Many businesses possess, often unwittingly, vast amounts of useful data, from call-centre transcripts to customer spending records. Such information is especially valuable because it can be used to fine-tune models for specific business purposes, such as helping call-centre workers answer queries or analysts spot ways to boost sales.Yet making use of that rich resource is not always straightforward. Roy Singh of Bain, a consultancy, notes that most firms have historically paid little attention to the types of vast but unstructured datasets that would prove most useful for training AI tools. Often these are spread across various systems, buried in company servers rather than in the cloud.Unlocking that information would help companies customise AI tools to serve their needs better. Amazon and Microsoft, two tech giants, now offer tools to help companies improve management of their unstructured datasets, as does Google. Christian Kleinerman of Snowflake, a database firm, says that business is booming as clients look to “tear down data silos”. Startups are piling in. In April Weaviate, an AI-focused database business, raised $50m at a valuation of $200m. Barely a week later PineCone, a rival, raised $100m at a $750m valuation. Earlier this month Neon, another database startup, raised an additional $46m in funding. The scramble for data is only just getting started. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.SIX YEARS ago, before anyone had heard of ChatGPT, Vladimir Putin said that the country that led the development of artificial intelligence (AI) would become the “ruler of the world”. He echoed the sentiment in December, when he suggested that Russia should “head and lead” the march of AI. Those comments came in response to a video-caller during a televised phone-in who had taken on the Russian president’s likeness using an apparently AI-generated deepfake, seemingly startling the real-life strongman for a moment.For Mr Putin, “leading” on AI is part of an ideological battle with the West. The success of tools such as ChatGPT,  developed by an American startup called OpenAI, has led him to decry the dangers of relying on Western AIs trained on English-language data. Western “large language models” (LLMs) could, Mr Putin avers, “cancel” Russia’s perspective on the world if unchallenged. They also threaten a regime that has sought to control the Russian internet in recent years, a process accelerated by the invasion of Ukraine. To no one’s surprise, the Kremlin banned ChatGPT shortly after its launch in November 2022. Several Russian companies are hard at work trying to build alternatives.Last year Sber, a state-controlled lender with tech ambitions that was first tasked by the Kremlin with AI development in 2019, launched GigaChat, a chatbot that combines a command of Russian with the ability to generate computer code and images. Yandex, Russia’s search giant, has integrated an LLM, YandexGPT-2, into its virtual-assistant service, known as “Alice”.The models are excellent at hewing to the party line. Alice, for example, refused to answer The Economist’s questions about the war in Ukraine or Alexei Navalny, Russia’s main opposition leader imprisoned in Siberia. It is less clear that they are capable of outsmarting Western AIs. Yandex claims that YandexGPT-2 does better than GPT-3.5, the model behind an earlier version of ChatGPT, when answering queries in Russian. But Western experts consulted by The Economist have found no independent analysis to confirm this contention, and there have been no public comparisons with GPT-4, the much more powerful current iteration of OpenAI’s model.Russia also lags behind the West on a variety of AI-innovation indicators. A report compiled by Stanford University said that, in 2022, the country produced only one “significant” machine-learning system, compared with 16 in America and eight in Britain. As of June 2023 Russia was thought to have just seven of the world’s 500 most powerful supercomputers, in contrast with America’s 150. Russia also ranked 38th out 193 countries in the latest AI-readiness index by Oxford Insights, a consultancy; America came first.To catch up, Mr Putin envisages an ambitious AI strategy to replace an earlier one from 2019. The Kremlin’s list of initial “instructions”, released in January, suggests this new plan will aim to increase Russia’s supercomputing capacity, expand training for AI professionals and improve co-operation among the BRICS, a bloc that includes China and India.Mr Putin’s instructions seem unrealistic, to put it politely. The war has led many Russian developers and engineers to flee from the country: one Kremlin official has suggested that 100,000 IT specialists left in 2022 alone, roughly 10% of the tech workforce. Arkady Volozh, Yandex’s founder, lives in exile in Britain and Israel after criticising the invasion. Sanctions limit Russia’s access to advanced chips, which are made almost exclusively by companies in America, South Korea and Taiwan, all part of the anti-Russian alliance. In Russia’s war economy, private investment in tech is, unsurprisingly, dwindling. The value of venture capital going into the sector was just $71m in 2023, according to DSight, a business-intelligence firm based in Moscow, a fall of 83% from the previous year.Mr Putin’s response is, as with most things in Russia these days, to tighten the state’s grip over the industry. In 2022 Yandex sold its news and blogging services to VK, a state-controlled online conglomerate. On February 5th its parent company, which is based in the Netherlands and listed in New York, said it would sell the Russian business (which accounts for 95% of its revenues) for $5bn to a consortium led by an arm of Lukoil, an energy company. The Kremlin welcomed the deal. State-run entities such as Rostec, a defence group, and Gazprom Neft, a subsidiary of the country’s largest energy firm, are also dabbling in AI. Sber’s chief executive, German Gref, says the bank is investing some $1bn a year in the technology.These sums are, though, trifling next to the tens of billions of dollars being spent by American AI champions such as Alphabet and Microsoft (which has a partnership with OpenAI). The state money brings with it inefficiency and a lack of competition—hardly a recipe for innovation. It also encourages developing AI for the battlefield rather than the marketplace.On the defensiveRussia has made progress in military AI, says Katarzyna Zysk of the Norwegian Institute for Defence Studies, a think-tank, particularly in drones. But in the West and even in China, a Russian ally, the excitement over machine learning has been fuelled chiefly by recent leaps in general-purpose applications such as ChatGPT, not specialist ones like pilotless aircraft.  Western and Chinese strategists are counting on such fast-improving civilian AI to confer an economic and, ultimately, geopolitical and military edge. So long as it remains on a war footing, Russia will not make much progress on that front. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The venue will be picturesque: a 19th-century pile north of London that during the second world war was home to Alan Turing, his code-breaking crew and the first programmable digital computer. The attendees will be an elite bunch of 100 world leaders and tech executives. And the question they will strive to answer is epochal: how to ensure that artificial intelligence neither becomes a tool of unchecked malfeasance nor turns against humanity.The “AI Safety Summit”, which the British government is hosting on November 1st and 2nd at Bletchley Park, appears destined for the history books. And it may indeed one day be seen as the first time global power-brokers sat down to discuss seriously what to do about a technology that may change the world. As Jonathan Black, one of the organisers, observed, in contrast to other big policy debates, such as climate change, “there is a lot of good will” but “we still don’t know what the right answer is.”Efforts to rein in AI abound. Negotiations in Brussels entered a pivotal stage on October 25th as officials grappled to finalise the European Union’s ambitious AI act by the end of the year. In the days leading up to Britain’s summit or shortly thereafter, the White House is expected to issue an executive order on AI. The G7 club of rich democracies will this autumn start drafting a code of conduct for AI firms. China, for its part, on October 18th unveiled a “Global AI Governance Initiative”.The momentum stems from an unusual political economy. Incentives to act, and act together, are strong. For starters, AI is truly a global technology. Large language models (LLMs), which power eerily humanlike services such as ChatGPT, travel easily. Some can be run on a laptop. It is of little use to tighten the screws on AI in some countries if they remain loose in others. Voters may be in favour. More than half of Americans “are more concerned than excited” about the use of AI, according to polling by the Pew Research Centre.The Beijing effectRegulatory rivalry is adding more urgency. Europe’s AI act is intended in part to cement the bloc’s role as the setter of global digital standards. The White House would love to forestall such a “Brussels effect”. Neither the EU nor America wants to be outdone by China, which has already adopted several AI laws. They were cross with the British government for inviting China to the summit—never mind that without it, any regulatory regime would not be truly global. (China may actually show up, even if its interest is less to protect humanity than the Communist Party.)Another driver of AI-rulemaking diplomacy is even more surprising: the model-makers themselves. In the past the technology industry mostly opposed regulation. Now giants such as Alphabet and Microsoft, and AI darlings like Anthropic and OpenAI, which created ChatGPT, lobby for it. Companies fret that unbridled competition will push them to act recklessly by releasing models that could easily be abused or start developing minds of their own. That would really land them in hot water.The will to act is there, in other words. What is not there is “anything approaching consensus as to what the problems are that we need to govern, let alone how it is that we ought to govern them”, says Henry Farrell of Johns Hopkins University. Three debates stand out. What should the world worry about? What should any rules target? And how should they be enforced?Start with the goals of regulation. These are hard to set because AI is evolving rapidly. Hardly a day passes without a startup coming up with something new. Even the developers of LLMs cannot say for sure what capabilities these will exhibit. This makes it crucial to have tests that can gauge how risky they might be—something that is still more art than science. Without such “evals” (short for evaluations), it will be hard to check whether a model is complying with any rules.Tech companies may back regulation, but want it to be narrow and target only extreme risks. At a Senate hearing in Washington in July, Dario Amodei, Anthropic’s chief executive, warned that AI models will in a few years be able to provide all the information needed to build bioweapons, enabling “many more actors to carry out large scale biological attacks”. Similar dire forecasts are being made about cyber-weapons. Earlier this month Gary Gensler, chairman of America’s Securities and Exchange Commission, predicted that an AI-engineered financial crisis was “nearly unavoidable” without swift intervention.Others argue that these speculative risks distract from other threats, such as undermining the democratic process. At an earlier Senate hearing Gary Marcus, a noted AI sceptic, opened his testimony with a snippet of breaking news written by GPT-4, OpenAI’s top model. It convincingly alleged that parts of Congress were “secretly manipulated by extraterrestrial entities”. “We should all be deeply worried,” Mr Marcus argued, “about systems that can fluently confabulate.”The debate over what exactly to regulate will be no easier to resolve. Tech firms mostly suggest limiting scrutiny to the most powerful “frontier” models. Microsoft, among others, has called for a licensing regime requiring firms to register models that exceed certain performance thresholds. Other proposals include controlling the sale of powerful chips used to train LLMs and mandating that cloud-computing firms inform authorities when customers train frontier models.Most firms also agree it is models’ applications, rather than the models themselves, that ought to be regulated. Office software? Light touch. Health-care AI? Stringent rules. Facial recognition in public spaces? Probably a no-go. The advantage of such use-based regulation is that existing laws would mostly suffice. The AI developers warn that broader and more intrusive rules would slow down innovation.Until last year America, Britain and the EU seemed to agree on this risk-based approach. The breathtaking rise of LLMs since the launch of ChatGPT a year ago is giving them second thoughts. The EU is now wondering whether the models themselves need to be overseen, after all. The European Parliament wants model-makers to test LLMs for potential impact on everything from human health to human rights. It insists on getting information about the data on which the models are trained. Canada has a harder-edged “Artificial Intelligence and Data Act” in its parliamentary works. Brazil is discussing something similar. In America, President Joe Biden’s forthcoming executive order is also expected to include some tougher rules. Even Britain may revisit its hands-off approach.These harder regulations would be a change from non-binding codes of conduct, which have hitherto been the preferred approach. Last summer the White House negotiated a set of “voluntary commitments”, which 15 model-makers have now signed. The firms agreed to have their models internally and externally tested before release and to share information about how they manage AI risks.Then there is the question of who should do the regulating. America and Britain think existing government agencies can do most of the job. The EU wants to create a new regulatory body. Internationally, a few tech executives now call for the creation of something akin to the Intergovernmental Panel on Climate Change (IPCC), which the UN tasks with keeping abreast of research into global warming and with developing ways to gauge its impact.Given all these open questions, it comes as no surprise that the organisers of the London summit do not sound that ambitious. It should mainly be thought of as “a conversation”, said Mr Black. Still, the not-so-secret hope is that it will yield a few tangible results, in particular on day two when only 20 or so of the most important corporate and world leaders remain in the room. They could yet endorse the White House’s voluntary commitments and recommend the creation of an IPCC for AI or even globalising Britain’s existing “Frontier AI Taskforce”.Such an outcome would count as a success for Britain’s government. It would also speed up the more official efforts at global AI governance, such as the G7’s code of conduct. As such, it would be a useful first step. It will not be the last. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.Read more of our articles on artificial intelligence
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Reunions offer a chance to reflect on how much has changed. One will happen during the coming year in Hollywood when “Here” premieres, bringing together the actors, director and writer behind “Forrest Gump” 30 years later for a new, unrelated film. Set in a single room over decades, “Here” is very much a film of the here and now. The stars, Tom Hanks and Robin Wright, will be “de-aged” using new ai tools, rendering them more youthful in some scenes and enabling the film-makers to see the transformation in real time while shooting.Generative AI now means images can be produced in seconds. Songs can be created in the style of singers dead or alive.  More than 3,000 books on Amazon name ChatGPT as the author or co-author, lending new meaning to the term “ghostwriter”.It is still early days, but 2024 will be a preview of what is to come. Three things are worth watching. The first is how ai will be used to tell new types of stories, as storytelling becomes more personalised and interactive. Films will change and so will gaming, an industry where people can choose their own adventures more easily than moviegoers can. The amount of entertainment available will also balloon.It will be a few years before a blockbuster is produced entirely by AILike the arrival of the internet, which led to an explosion of “user-generated content” being posted to social media and YouTube, generative AI will contribute to reams of videos and other material proliferating online. Some predict that as much as 90% of online content will be AI-generated by 2025. Curation and good search tools will be vital, and there will be debates about whether, and how, to label AI-generated content. No one is quite sure how the nature of storytelling will change, but it is sure to. David Thomson, a film historian, compares generative ai to the advent of sound. When movies were no longer silent, it altered the way plot points were rendered and how deeply viewers could connect with characters. Cristóbal Valenzuela, who runs a company called RunwayML, which offers ai-enhanced software tools to creative types, says ai is more like a “new kind of camera”, offering a fresh “opportunity to reimagine what stories are like”. Both are right. The Hollywood writers’ strike shone a spotlight on the question of whether AI would start producing scripts. For now, studios have agreed to concessions and will not bypass writers’ rooms to employ ChatGPT instead. It will probably be a few years before a full-length blockbuster is produced entirely by AI. Instead, the second big development to watch is how AI will be used as a time-saving tool. Generative AI will automate and simplify complex tasks like dubbing, film-editing, special effects and background design. For a glimpse of the future, watch “Everything Everywhere All at Once”, which won the Academy Award for Best Picture in 2023. It featured a scene that used a “rotoscoping” tool offered by RunwayML to edit out the green-screen background and make a talking rock more believable. It compressed into hours what might have otherwise taken days of video-editing.The third thing to watch for is more dramatic clashes between creators (otherwise known as copyright-owners) and those who run AI platforms. The coming year is likely to bring a deluge of lawsuits from authors, musicians, actors and artists about how their words, music and images have been used to train AI systems without consent or payment. Perhaps they can agree on some sort of licensing arrangement, in which AI companies start paying copyright-holders for content to train their models. But that will not happen without an intense legal brawl.AI presents bigger questions about the future of stories and the nature of collective storytelling. For example, will generative AI simply imitate previous hits, resulting in more derivative blockbuster films and copycat interpretations of pop songs that lack depth, rather than original stories and art forms? And as entertainment becomes more personalised, will there still be stories that become part of humanity’s collective consciousness and move large numbers of people, who can talk about them together?As creators grapple with ai’s rise, they will channel their anxieties about technology into their work. Look out for more “Terminator”-style clashes between man and machine. Life imitates art—and art life. ■Alexandra Suich Bass, Culture editor, The EconomistCorrection (January 9th 2024): “Forrest Gump” was released 30 years ago, not 40 years ago as this story originally claimed.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.COMPUTERS have already proved better than people at playing chess and diagnosing diseases. But now a group of artificial-intelligence researchers in Singapore have managed to teach industrial robots to assemble an IKEA chair—for the first time uniting the worlds of Allen keys and Alan Turing. Now that machines have mastered one of the most baffling ways of spending a Saturday afternoon, can it be long before AIs rise up and enslave human beings in the silicon mines?The research also holds a serious message. It highlights a deep truth about the limitations of automation. Machines excel at the sorts of abstract, cognitive tasks that, to people, signify intelligence—complex board games, say, or differential calculus. But they struggle with physical jobs, such as navigating a cluttered room, which are so simple that they hardly seem to count as intelligence at all. The IKEAbots are a case in point. It took a pair of them, pre-programmed by humans, more than 20 minutes to assemble a chair that a person could knock together in a fraction of the time (see article).AI researchers call that observation Moravec’s paradox, and have known about it for decades. It does not seem to be the sort of problem that could be cured with a bit more research. Instead, it seems to be a fundamental truth: physical dexterity is computationally harder than playing Go. That humans do not grasp this is a side-effect of evolution. Natural selection has had billions of years to attack the problem of manipulating the physical world, to the point where it feels effortless. Chess, by contrast, is less than 2,000 years old. People find it hard because their brains are not wired for it.That is something to bear in mind when thinking about the much-hyped effects of AI and automation, especially as AI moves out of the abstract world of data and information and into the real world of things you can drop on your foot. On April 13th Elon Musk, the boss of Tesla, an electric-car firm, said that the production problems which have dogged his company’s high-tech factory were partly the result of an overreliance on robots and automation. “Humans are underrated,” he tweeted. Lots of jobs have physical aspects that robots struggle with. Machines may soon be able to drive delivery vans, for instance. But, at least for now, they could well fail to carry a parcel to a flat at the top of a flight of slippery stairs, especially if the garden was patrolled by a dangerous dog.Not such a silly BillyToday’s AI systems are limited in other ways, too. They are pattern-recognition engines, trained on thousands of examples in the hope that the rules they infer will continue to apply in the wider world. But they apply those rules blindly, without a human-like understanding of what they are doing or an ability to improvise a solution on the spot. Makers of self-driving cars, for instance, worry constantly about how their machines will perform in “edge cases”—complicated and unusual situations that cannot be foreseen during training.Calibrating excitement about AI is tricky. Researchers complain that great progress is quickly forgotten: as soon as a computer can do something, it ceases to count as “AI”. But those same researchers also tend to be more cautious about the future than many pundits. There is no reason, in principle, why a computer could not one day do everything a human can and more. But that will be the work of decades at least. Furniture-assembly helps explain why.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.IF YOU LISTEN to the bombast in Beijing and Washington, America and China are engaged in an all-out contest for technological supremacy. “Fundamentally, we believe that a select few technologies are set to play an outsized importance over the coming decade,” declared Jake Sullivan, President Joe Biden’s national security adviser, last September. In February Xi Jinping, China’s paramount leader, echoed the sentiment, stating that “we urgently need to strengthen basic research and solve key technology problems” in order to “cope with international science and technology competition, achieve a high level of self-reliance and self-improvement”.No technology seems to obsess policymakers on both sides of the Pacific more right now than artificial intelligence (AI). The rapid improvements in the abilities of “generative” AIs like ChatGPT, which analyse the web’s worth of human text, images or sounds and can then create increasingly passable simulacrums, have only strengthened the obsession. If generative AI proves as transformational as its boosters claim, the technology could give those who wield it an economic and military edge in the 21st century’s chief geopolitical contest. Western and Chinese strategists already talk of an AI arms race. Can China win it?On some measures of AI prowess, the autocracy pulled ahead some time ago (see chart). China surpassed America in the share of highly cited AI papers in 2019; in 2021, 26% of AI conference publications globally came from China, compared with America’s share of 17%. Nine of the world’s top ten institutions, by volume of AI publications, are Chinese. According to one popular benchmark, so are the top five labs working on computer vision, a type of AI particularly useful to a communist surveillance state.Yet when it comes to “foundation models”, which give generative AIs their wits, America is firmly in front (see charts 2 and 3). ChatGPT and the pioneering model behind it, the latest version of which is called GPT-4, are the brainchild of OpenAI, an American startup. A handful of other American firms, from small ones such as Anthropic or Stability AI to behemoths like Google, Meta and Microsoft (which part-owns OpenAI), have their own powerful systems. ERNIE, a Chinese rival to ChatGPT built by Baidu, China’s internet-search giant, is widely seen as less clever. Alibaba and Tencent, China’s mightiest tech titans, have yet to unveil their own generative AIs.This leads those in the know to conclude that China is two or three years behind America in building foundation models. There are three reasons for this underperformance. The first concerns data. A centralised autocracy should be able to marshal lots of it—the government was, for instance, able to hand over troves of surveillance information on Chinese citizens to firms such as SenseTime or Megvii that, with the help of China’s leading computer-vision labs, then used it to develop top-notch facial-recognition systems.That advantage has proved less formidable in the context of generative AIs, because foundation models are trained on the voluminous unstructured data of the web. American model-builders benefit from the fact that 56% of all websites are in English, whereas just 1.5% are written in Chinese, according to data from W3Techs, an internet-research site. As Yiqin Fu of Stanford University points out, the Chinese interact with the internet primarily through mobile super-apps like WeChat and Weibo. These are “walled gardens”, so much of their content is not indexed on search engines. This makes that content harder for AI models to suck up. Lack of data may explain why Wu Dao 2.0, a model unveiled in 2021 by the Beijing Academy of Artificial Intelligence, a state-backed outfit, failed to make a splash despite its possibly being computationally more complex than GPT-4.The second reason for China’s lacklustre generative achievements has to do with hardware. Last year America imposed export controls on technology that might give China a leg-up in AI. These cover the powerful microprocessors used in the cloud-computing data centres where foundation models do their learning, and the chipmaking tools that could enable China to build such semiconductors on its own.That hurt Chinese model-builders. An analysis of 26 big Chinese models by the Centre for the Governance of AI, a British think-tank, found that more than half depended on Nvidia, an American chip designer, for their processing power. Some reports suggest that SMIC, China’s biggest chipmaker, has produced prototypes just a generation or two behind TSMC, the Taiwanese industry leader that manufactures chips for Nvidia (see chart 4). But SMIC can probably mass-produce only chips which TSMC was churning out by the million three or four years ago. Chinese AI firms are having trouble getting their hands on another American export: know-how. America remains a magnet for the world’s tech talent; two-thirds of AI experts in America who present papers at the main AI conference are foreign-born. Chinese engineers made up 27% of that select group in 2019. Many Chinese AI boffins studied or worked in America before bringing expertise back home. The covid-19 pandemic and rising Sino-American tensions are causing their numbers to dwindle. In the first half of 2022 America granted half as many visas to Chinese students as in the same period in 2019.The triple shortage—of data, hardware and expertise—has been a hurdle for China. Whether it will hold Chinese AI ambitions back much longer is another matter.Take data. In February local authorities in Beijing, where nearly a third of China’s AI firms are located, promised to release data from 115 state-affiliated organisations, giving model-builders 15,880 data sets to play with. The central government has previously signalled it wants to dismantle Chinese apps’ walled gardens, potentially liberating more data, says Kayla Blomquist, an American former diplomat in China now at Oxford University. The latest models are also able to transfer their machine learnings from one language to another. OpenAI says that GPT-4 performs remarkably well on tasks in Chinese despite scarce Chinese source material in its training data. Baidu’s ERNIE was trained on lots of English-language data, notes Jeffrey Ding of George Washington University.In hardware, too, China is finding workarounds. The Financial Times reported in March that SenseTime, which is blacklisted by America, was using middlemen to skirt the export controls. Some Chinese AI firms are harnessing Nvidia’s processors through cloud servers in other countries. Alternatively, they can buy more of Nvidia’s less advanced wares—to keep serving the vast Chinese market, Nvidia has designed sanctions-compliant ones that are between 10% and 30% slower than top-of-the-range kit. These end up being costlier for the Chinese customers per unit of processing power. But they do the job.China could partly alleviate the dearth of chips—and of brain power—with the help of “open-source” models. Such models’ inner workings can be downloaded by anyone and fine-tuned to a specific task. Those include the numbers, called “weights”, which define the structure of the model and which are derived from costly training runs. Researchers at Stanford used the weights from LLaMA, Meta’s foundation model, to build one called Alpaca for less than $600, compared with perhaps $100m for training something like GPT-4. Alpaca performs just as well as the original version of ChatGPT on some tasks.Chinese AI labs could similarly avail themselves of open-source models, which embody the collective wisdom of international research teams. Matt Sheehan of the Carnegie Endowment for International Peace, another think-tank, says that China has form in being a “fast follower”—its labs have absorbed advances from abroad and rapidly incorporated them into their own models, often with flush state resources. A prominent Silicon Valley venture capitalist is more blunt, calling open-source models a gift to the Communist Party.Such considerations make it hard to imagine that either America or China could build an unbridgeable lead in AI modelling. Each may well end up with AIs of similar ability, even if it costs China over the odds in the face of American sanctions. But if the race of the model-builders is a dead heat, America has one thing going for it that may make it the big AI winner—its ability to spread cutting-edge innovation throughout the economy. It was, after all, more efficient diffusion of technology that helped America open up a technological lead over the Soviet Union, which in the 1950s was producing twice as many science PhDs as its democratic adversary.China is far more competent than the Soviet Union ever was at adopting new technologies. Its fintech platforms, 5G telecoms and high-speed rail are all world-class. Still, those successes may be the exception, not the rule, says Mr Ding. Particularly, China has done less well in deploying cloud computing and business software—both complementary to AI.And though American export controls may not derail all Chinese model-building, they constrain China’s tech industry more broadly, thereby slowing the adoption of new technology. Moreover, Chinese businesses as a whole, and especially small and medium-sized ones, are short of technologists who act as conduits for technological diffusion. Swathes of the economy are dominated by state-owned firms, which tend to be stodgy and change-averse. Parts of it are dodgy. China’s “Big Fund” for chips, which raised $50bn in 2014 with a view to backing domestic semiconductor firms, has been mired in scandals. Many of the thousands of new AI startups are AI in name only, slapping on the label to get a slice of the lavish subsidies doled out by the state to the favoured industry.As a consequence, China’s private sector may struggle to take full advantage of generative AI, especially if the Communist Party imposes strict rules to prevent chatbots from saying something its censors dislike. The handicaps would come on top of Mr Xi’s broader suborning of private enterprise, including a two-and-a-half-year crackdown on China’s tech industry.Although the anti-tech campaign has officially ended, it has left deep scars, not least in the AI business. Last year private investments in Chinese AI startups amounted to $13.5bn, less than one-third of the sum that flowed to their American rivals. In the first four months of 2023 the funding gap appears only to have widened, according to PitchBook, a data provider. Whether or not generative AI proves revolutionary, the free market has placed its bet on who will make the most of it. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The age of “generative” artificial intelligence has well and truly arrived. Openai’s chatbots, which use large-language-model (llm) technology, got the ball rolling in November. Now barely a day goes by without some mind-blowing advance. An ai-powered song featuring a fake “Drake” and “The Weeknd” recently shook the music industry. Programs which convert text to video are making fairly convincing content. Before long consumer products such as Expedia, Instacart and OpenTable will plug into Openai’s bots, allowing people to order food or book a holiday by typing text into a box. A recently leaked presentation, reportedly from a Google engineer, suggests the tech giant is worried about how easy it is for rivals to make progress. There is more to come—probably a lot more.The development of ai raises profound questions. Perhaps most pressing, though, is a straightforward one. What does this mean for the economy? Many have grand expectations. New research by Goldman Sachs, a bank, suggests that “widespread ai adoption could eventually drive a 7% or almost $7trn increase in annual global gdp over a ten-year period.” Academic studies point to a three-percentage-point rise in annual labour-productivity growth in firms that adopt the technology, which would represent a huge uplift in incomes compounded over many years. A study published in 2021 by Tom Davidson of Open Philanthropy, a grantmaking outfit, puts a more than 10% chance on “explosive growth”—defined as increases in global output of more than 30% a year—sometime this century. A few economists, only half-jokingly, hold out the possibility of global incomes becoming infinite.Financial markets, however, point to rather more modest outcomes. In the past year share prices of companies involved in ai have done worse than the global average, although they have risen in recent months (see chart 1). Interest rates are another clue. If people thought that the technology was going to make everyone richer tomorrow, rates would rise because there would be less need to save. Inflation-adjusted rates and subsequent gdp growth are strongly correlated, notes research by Basil Halperin of the Massachusetts Institute of Technology (mit) and colleagues. Yet since the hype about ai began in November, long-term rates have fallen. They remain very low by historical standards. Financial markets, the researchers conclude, “are not expecting a high probability of…ai-induced growth acceleration…on at least a 30-to-50-year time horizon.”To judge which group is right, it is helpful to consider the history of previous technological breakthroughs. This provides succour to investors. For it is difficult to make the case that a single new technology by itself has ever radically changed the economy, either for good or ill. Even the industrial revolution of the late 1700s, which many people believe was the result of the invention of the spinning jenny, was actually caused by all sorts of factors coming together: increasing use of coal, firmer property rights, the emergence of a scientific ethos and much more besides.Read more of our articles on artificial intelligencePerhaps most famously, in the 1960s Robert Fogel published work about America’s railways that would later win him a Nobel Prize in economics. Many thought that rail transformed America’s prospects, turning an agricultural society into an industrial powerhouse. In fact, it had a very modest impact, Fogel found, because it replaced technology—such as canals—that would have done just about as good a job. The level of per-person income that America achieved by January 1st 1890 would have been reached by March 31st 1890 if railways had never been invented.Of course, no one can predict with any certainty where a technology as fundamentally unpredictable as ai will take humans. Runaway growth is not impossible; nor is technological stagnation. But you can still think through the possibilities. And, so far at least, it seems as though Fogel’s railways are likely to be a useful blueprint. Consider three broad areas: monopolies, labour markets and productivity.A new technology sometimes creates a small group of people with vast economic power. John D. Rockefeller won out with oil refining and Henry Ford with cars. Today Jeff Bezos and Mark Zuckerberg are pretty dominant thanks to tech.Many pundits expect that before long the ai industry will generate huge profits. In a recent paper Goldman’s analysts estimate in a best-case scenario generative ai could add about $430bn to annual global enterprise-software revenues. Their calculation assumes that each of the world’s 1.1bn office workers will adopt a few ai gizmos, paying around $400 in total each.Any business would be glad to capture some of this cash. But in macroeconomic terms $430bn simply does not move the dial. Assume that all of the revenue turns into profits, which is unrealistic, and that all of these profits are earned in America, which is a tad more realistic. Even under these conditions, the ratio of the country’s pre-tax corporate profits to its gdp would rise from 12% today to 14%. That is far above the long-run average, but no higher than it was in the second quarter of 2021.These profits could go to one organisation—maybe Openai. Monopolies often arise when an industry has high fixed costs or when it is hard to switch to competitors. Customers had no alternative to Rockefeller’s oil, for instance, and could not produce their own. Generative ai has some monopolistic characteristics. gpt-4, one of Openai’s chatbots, reportedly cost more than $100m to train, a sum few firms have lying around. There is also a lot of proprietary knowledge about data for training the models, not to mention user feedback.There is, however, little chance of a single company bestriding the entire industry. More likely is that a modest number of big firms compete with one another, as happens in aviation, groceries and search engines. No ai product is truly unique since all use similar models. This makes it easier for a customer to switch from one to another. The computing power behind the models is also fairly generic. Much of the code, as well as tips and tricks, is freely available online, meaning that amateurs can produce their own models—often with strikingly good results.“There don’t appear, today, to be any systemic moats in generative ai,” a team at Andreessen Horowitz, a venture-capital firm, has argued. The recent leak purportedly from Google reaches a similar conclusion: “The barrier to entry for training and experimentation has dropped from the total output of a major research organisation to one person, an evening, and a beefy laptop.” Already there are a few generative-ai firms worth more than $1bn. The biggest corporate winner so far from the new ai age is not even an ai company. At Nvidia, a computing firm which powers AI models, revenue from data centres is soaring.Yeah, but what about me?Although generative ai might not create a new class of robber barons, to many people that will be cold comfort. They are more concerned with their own economic prospects—in particular, whether their job will disappear. Terrifying predictions abound. Tyna Eloundou of OpenAI and colleagues have estimated that “around 80% of the us workforce could have at least 10% of their work tasks affected by the introduction of llms”. Edward Felten of Princeton University and colleagues conducted a similar exercise. Legal services, accountancy and travel agencies came out at or near the top of professions most likely to face disruption.Economists have issued gloomy predictions before. In the 2000s many feared the impact of outsourcing on rich-world workers. In 2013 two at Oxford University issued a widely cited paper that suggested automation could wipe out 47% of American jobs over the subsequent decade or so. Others made the case that, even without widespread unemployment, there would be “hollowing out”, where rewarding, well-paid jobs disappeared and mindless, poorly paid roles took their place.What actually happened took people by surprise. In the past decade the average rich-world unemployment rate has roughly halved (see chart 2). The share of working-age people in employment is at an all-time high. Countries with the highest rates of automation and robotics, such as Japan, Singapore and South Korea, have the least unemployment. A recent study by America’s Bureau of Labour Statistics found that in recent years jobs classified as “at risk” from new technologies “did not exhibit any general tendency toward notably rapid job loss”. Evidence for “hollowing out” is mixed. Measures of job satisfaction rose during the 2010s. For most of the past decade the poorest Americans have seen faster wage growth than the richest ones.This time could be different. The share price of Chegg, a firm which provides homework help, recently fell by half after it admitted Chatgpt was “having an impact on our new customer growth rate”. The chief executive of ibm, a big tech firm, said that the company expects to pause hiring for roles that could be replaced by AI in the coming years. But are these early signs a tsunami is about to hit? Perhaps not.Imagine a job disappears when ai automates more than 50% of the tasks it encompasses. Or imagine that workers are eliminated in proportion to the total share of economywide tasks that are automated. In either case this would, following Ms Eloundou’s estimates, result in a net loss of around 15% of American jobs. Some folk could move to industries experiencing worker shortages, such as hospitality. But a big rise in the unemployment rate would surely follow—in line, maybe, with the 15% briefly reached in America during the worst of the covid-19 pandemic in 2020.Yet this scenario is unlikely to come to pass: history suggests job destruction happens far more slowly. The automated telephone switching system—a replacement for human operators—was invented in 1892. It took until 1921 for the Bell System to install their first fully automated office. Even after this milestone, the number of American telephone operators continued to grow, peaking in the mid-20th century at around 350,000. The occupation did not (mostly) disappear until the 1980s, nine decades after automation was invented. ai will take less than 90 years to sweep the labour market: llms are easy to use, and many experts are astonished by the speed at which the general public has incorporated Chatgpt into their lives. But reasons for the slow adoption of technology in workplaces will also apply this time around.In a recent essay Mark Andreessen of Andreessen Horowitz outlined some of them. His argument focuses on regulation. In bits of the economy with heavy state involvement, such as education and health care, technological change tends to be pitifully slow. The absence of competitive pressure blunts incentives to improve. Governments may also have public-policy goals, such as maximising employment levels, which are inconsistent with improved efficiency. These industries are also more likely to be unionised—and unions are good at preventing job losses.Examples abound. Train drivers on London’s publicly run Underground network are paid close to twice the national median, even though the technology to partially or wholly replace them has existed for decades. Government agencies require you to fill in paper forms providing your personal information again and again. In San Francisco, the global centre of the ai surge, real-life cops are still employed to direct traffic during rush hour.Au revoir!Many of the jobs at risk from ai are in heavily regulated sectors. Return to the paper by Mr Felten of Princeton University. Fourteen of the top 20 occupations most exposed to ai are teachers (foreign-language ones are near the top; geographers are in a slightly stronger position). But only the bravest government would replace teachers with ai. Imagine the headlines. The same goes for cops and crime-fighting ai. The fact that Italy has already temporarily blocked Chatgpt over privacy concerns, with France, Germany and Ireland said to be considering the option, shows how worried governments are about the job-destructive effects of ai.Perhaps, in time, governments will allow some jobs to be replaced. But the delay will make space for the economy to do what it always does: create new types of jobs as others are eliminated. By lowering costs of production, new tech can create more demand for goods and services, boosting jobs that are hard to automate. A paper published in 2020 by David Autor of mit and colleagues offered a striking conclusion. About 60% of the jobs in America did not exist in 1940. The job of “fingernail technician” was added to the census in 2000. “Solar photovoltaic electrician” was added just five years ago. The ai economy is likely to create new occupations which today cannot even be imagined.Modest labour-market effects are likely to translate into a modest impact on productivity—the third factor. Adoption of electricity in factories and households began in America towards the end of the 19th century. Yet there was no productivity boom until the end of the first world war. The personal computer was invented in the 1970s. This time the productivity boom followed more quickly—but it still felt slow at the time. In 1987 Robert Solow, an economist, famously declared that the computer age was “everywhere except for the productivity statistics”.The world is still waiting for a productivity surge linked to recent innovations. Smartphones have been in widespread use for a decade, billions of people have access to superfast internet and many workers now shift between the office and home as it suits them. Official surveys show that well over a tenth of American employees already work at firms using ai of some kind, while unofficial surveys point to even higher numbers. Still, though, global productivity growth remains weak.ai could eventually make some industries vastly more productive. A paper by Erik Brynjolfsson of Stanford University and colleagues examines customer-support agents. Access to an ai tool raises the number of issues resolved each hour by 14% on average. Researchers themselves could also become more efficient: gpt-x may give them an unlimited number of almost-free research assistants. Others hope ai will eliminate administrative inefficiencies in health care, reducing costs.But there are many things beyond the reach of ai. Blue-collar work, such as construction and farming, which accounts for about 20% of rich-world gdp, is one example. An llm is of little use to someone picking asparagus. It could be of some use to a plumber fixing a leaky tap: a widget could recognise the tap, diagnose the fault and advise on fixes. Ultimately, though, the plumber still has to do the physical work. So it is hard to imagine that, in a few years’ time, blue-collar work is going to be much more productive than it is now. The same goes for industries where human-to-human contact is an inherent part of the service, such as hospitality and medical care.ai also cannot do anything about the biggest thing holding back rich-world productivity growth: misfiring planning systems. When the size of cities is constrained and housing costs are high, people cannot live and work where they are most efficient. No matter how many brilliant new ideas your society may have, they are functionally useless if you cannot build them in a timely manner. It is up to governments to defang nimbys. Technology is neither here nor there. The same goes for energy, where permitting and infrastructure are what keep costs uncomfortably high.It is even possible that the ai economy could become less productive. Look at some recent technologies. Smartphones allow instant communication, but they can also be a distraction. With email you are connected 24/7, which can make it hard to focus. A paper in 2016 by researchers at the University of California at Irvine, Microsoft Research and mit found that “the longer daily time spent on email, the lower was perceived productivity”. Some bosses now believe that working from home, once seen as a productivity-booster, gives too many people the excuse to slack off.Generative ai itself could act as a drain on productivity. What happens, for instance, if ai can create entertainment perfectly tailored to your every desire? Moreover, few people have thought through the implications of a system that can generate vast amounts of text instantly. gpt-4 is a godsend for a nimby facing a planning application. In five minutes he can produce a well written 1,000-page objection. Someone then has to respond to it. Spam emails are going to be harder to detect. Fraud cases could soar. Banks will need to spend more on preventing attacks and compensating people who lose out.Just what we needIn an ai-heavy world lawyers will multiply. “In the 1970s you could do a multi-million-dollar deal on 15 pages because retyping was a pain in the ass,” says Preston Byrne of Brown Rudnick, a law firm. “ai will allow us to cover the 1,000 most likely edge cases in the first draft and then the parties will argue over it for weeks.” A rule of thumb in America is that there is no point suing for damages unless you hope for $250,000 or more in compensation, since you need to spend that much getting to court. Now the costs of litigation could fall to close to zero. Meanwhile, teachers and editors will need to check that everything they read has not been composed by an ai. Openai has released a program that allows you to do this. It is thus providing the world a solution to a problem that its technology has created.ai may change the world in ways that today are impossible to imagine. But this is not quite the same thing as turning the economy upside down. Fogel wrote that his argument was “aimed not at refuting the view that the railroad played a decisive role in American development during the 19th century, but rather at demonstrating that the empirical base on which this view rests is not nearly so substantial as is usually presumed”. Some time in the mid-21st century a future Nobel prizewinner, examining generative ai, may well reach the same conclusion. ■For more expert analysis of the biggest stories in economics, finance and markets, sign up to Money Talks, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Backgammon was an easy win. Chess, harder. Go, harder still. But for some aficionados it is only now that artificial intelligence (AI) can truly say it has joined the game-playing club—for it has proved it can routinely beat humans at Diplomacy.For those unfamiliar with the game, its board is a map of Europe just before the first world war (except that, for no readily apparent reason, Montenegro is missing). Participants, seven ideally, each take on the role of one of the Great Powers: Austria, England, France, Germany, Italy, Russia and Turkey. Each has armies and navies, and geographically based resources to support them, and can use its forces to capture the territory of neighbours, thus gaining the means to raise more forces while depriving others of the same.The trick is that, at least at the beginning, players will get nowhere without making agreements to collaborate—yet they are not bound by the game’s rules to keep to these agreements. Only when orders for the movement of troops and vessels, which have to be written down, are revealed, does a player discover who really is a friend, or an enemy.Cicero, a program devised by a group of Mark Zuckerberg’s employees who dub themselves the Meta Fundamental AI Research Diplomacy Team, proved an adept pupil. As the team describe in Science, when they entered their creation into an online Diplomacy league, in which it played 40 games, it emerged as one of the top 10% of players—and no one rumbled that it was not human.In all past AI game-playing projects the program has learned by reinforcement. Playing repeatedly against itself or another version of itself, it acts first at random, then more selectively. Eventually, it learns how to achieve the desired goal. Cicero was taught this way, too. But that was only part of its training. Besides having the reasoning to plan a winning strategy, a successful Diplomacy player must also possess the communicative ability to implement it.The Meta team’s crucial contribution was therefore to augment reinforcement learning with natural-language processing. Large language models, trained on vast amounts of data to predict deleted words, have an uncanny ability to mimic the patterns of real language and say things that humans might. For Cicero, the team started with a pre-trained model with a baseline understanding of language, and fine-tuned this on dialogues from more than 40,000 past games, to teach it Diplomacy-specific patterns of speech.To play the game, Cicero looks at the board, remembers past moves and makes an educated guess as to what everyone else will want to do next. Then it tries to work out what makes sense for its own move, by choosing different goals, simulating what might happen, and also simulating how all the other players will react to that.Once it has come up with a move, it must work out what words to say to the others. To that end, the language model spits out possible messages, throws away the bad ideas and anything that is actual gobbledygook, and chooses the ones, appropriate to the recipients concerned, that its experience and algorithms suggest will most persuasively further its agenda.Cicero, then, can negotiate, convince, co-operate and compete. Seasoned Diplomacy players will, though, want to know something else: has it learned how to stab? Stabbing—saying one thing and doing another (especially, attacking a current ally) is seen by many as Diplomacy’s defining feature. But, though Cicero did “strategically withhold information from players in gameplay”, it did not actually stab any of its opponents. Perhaps it was this final lack of Machiavellian ruthlessness which explains why it was only in the top 10%, and not victor ludorum. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
"Artificial intelligence is everywhere but it is considered in a wholly ahistorical way. To understand the impact AI will have on our lives, it is vital to appreciate the context in which the field was established. After all, statistics and state control have evolved hand in hand for hundreds of years.Consider computing. Its origins have been traced not only to analytic philosophy, pure mathematics and Alan Turing, but perhaps surprisingly, to the history of public administration. In “The Government Machine: A Revolutionary History of the Computer” from 2003, Jon Agar of University College London charts the development of the British civil service as it ballooned from 16,000 employees in 1797 to 460,000 by 1999. He noticed an uncanny similarity between the functionality of a human bureaucracy and that of the digital electronic computer. (He confessed that he could not tell whether this observation was trivial or profound.)Both systems processed large quantities of information using a hierarchy of pre-set but adaptable rules. Yet one predated the other. This suggested a telling link between the organisation of human social structures and the digital tools designed to serve them. Mr Agar draws a link to the very origins of computing: Charles Babbage’s Difference Engine in the 1820s in Britain. It had been subsidised by the government, on the expectation that it would serve its sponsor. Babbage’s designs, Mr Agar observes, must be seen as “materialisations of state activity”.This relationship between computing systems and human organisational structures echoes through the history of AI. In the 1930s and 1940s, Herbert Simon (pictured below), a political scientist from the University of Chicago who later taught at Carnegie Mellon University, set out to develop a “scientific” account of administrative organisation. Simon had trained under Rudolf Carnap, a member of the Vienna Circle of logical positivists. This informed his belief that existing theories lacked empiricism. His doctoral dissertation in 1947 became “Administrative Behaviour”, a book that provided a framework through which all activity in an organisation could be understood using a matrix of decision-making.Simon saysHe went on to make huge contributions in a host of fields—not just political science and economics, but computer science and artificial intelligence. He coined the term “satisficing” (to accept the good rather than strive for the optimal) and developed the idea of ""bounded rationality"" for which he won a Nobel prize in economics in 1978. But back in the 1950s, Simon was a consultant at the RAND Corporation, an influential think-tank supported by America’s Air Force.At RAND, Simon and two colleagues—Allan Newell, a young mathematician, and J. Clifford Shaw, a former insurance actuary—tried to model human problem-solving in terms that a computer could put into operation. To do so, Simon borrowed elements from the framework that he had developed in “Administrative Behaviour”. To make a computer “think” as a human, Simon made it think like a corporation.The product of the trio’s labour was a virtual machine called the Logic Theorist, heralded as the first working prototype of artificial intelligence. Printouts of the Theorist in operation turned heads at the 1956 Dartmouth Summer Research Project on Artificial Intelligence, which gave the field its name and initial membership. In notes from the Dartmouth conference, one participant wrote that the Theorist helped to solve the dreaded “demo to sponsor” problem. This was essential, because the foundation funding AI was sceptical that the research area was worthwhile.To make a computer ‘think’ as a human, Simon made it think like a corporationHow did Simon see his contribution? A year after the Dartmouth conference, he and Newell presented their results as “Heuristic Problem Solving: The Next Advance in Operations Research”. The clue is in the title: “operations research” emerged in Britain during the second world war to apply scientific principles and statistics to optimise military activities, and later, for corporate uses. AI meant business.In a speech to operations-research practitioners in London in 1957, Simon identified Frederick Taylor, the father of the scientific-management movement, and Charles Babbage, as intellectual predecessors. “Physicists and electrical engineers had little to do with the invention of the digital computer,” Simon said. “The real inventor was the economist Adam Smith.” He explained the connections: Gaspard de Prony, a French civil engineer, set out to “manufacture” logarithms using techniques drawn from Smith’s “The Wealth of Nations”. Babbage, inspired by Prony, converted this insight into mechanical hardware. In the mid-1950s, Simon transmuted it into software code.The tradition lives on. Many contemporary AI systems do not so much mimic human thinking as they do the less imaginative minds of bureaucratic institutions; our machine-learning techniques are often programmed to achieve superhuman scale, speed and accuracy at the expense of human-level originality, ambition or morals.Capitalism in the codeThese streams of AI history—corporate decision-making, state power and the application of statistics to war—have not survived in the public understanding of AI.Instead, news of technical breakthroughs or pundits voicing fears are accompanied with imagery, if not of a heavily-armed Terminator, then of the brain, a robot, neon-colored microchips or absurd mathematical equations. Each is a not-so-subtle appeal to the authority of the natural sciences or computer science over that of, say, the “soft” sciences, to borrow Simon’s terminology, of political science, management science or even economics, the field for which he trundled off to Stockholm to collect his Nobel prize.Perhaps as a result of this misguided impression, public debates continue today about what value, if any, the social sciences could bring to artificial-intelligence research. In Simon’s view, AI itself was born in social science.David Runciman, a political scientist at the University of Cambridge, has argued that to understand AI, we must first understand how it operates within the capitalist system in which it is embedded. “Corporations are another form of artificial thinking-machine in that they are designed to be capable of taking decisions for themselves,” he explains.“Many of the fears that people now have about the coming age of intelligent robots are the same ones they have had about corporations for hundreds of years,” says Mr Runciman. The worry is, these are systems we “never really learned how to control.”After the 2010 BP oil spill, for example, which killed 11 people and devastated the Gulf of Mexico, no one went to jail. The threat that Mr Runciman cautions against is that AI techniques, like playbooks for escaping corporate liability, will be used with impunity.Today, pioneering researchers such as Julia Angwin, Virginia Eubanks and Cathy O’Neil reveal how various algorithmic systems calcify oppression, erode human dignity and undermine basic democratic mechanisms like accountability when engineered irresponsibly. Harm need not be deliberate; biased data-sets used to train predictive models also wreak havoc. It may be, given the costly labour required to identify and address these harms, that something akin to “ethics as a service” will emerge as a new cottage industry. Ms O’Neil, for example, now runs her own service that audits algorithms. In the 1950s, after having coined the term “artificial intelligence” for the Dartmouth conference, John McCarthy, one of the field’s early pioneers, wrote in his notes: “Once one system of epistemology is programmed and works, no other will be taken seriously unless it also leads to intelligent programmes.” By this view, DeepMind’s initial slogan, “Solve intelligence. Use that to solve everything else”, looks quasi-imperial.McCarthy’s suggestion was that influence, not authority, could decide the scientific consensus in his field. DeepMind doesn’t have to “solve” intelligence (assuming such a thing is even possible) it just needs to outshine the competition. That the company’s new slogan is, “Solve Intelligence. Use it to make the world a better place,” suggests that it too is aware of the need for diplomacy in this era’s AI-powered vision of totality. Many fears of intelligent robots are the same as ones held about corporations for hundreds of years. We never learned to control them.Stephen Cave, director of the Leverhulme Centre for the Future of Intelligence, has shown that the definition of intelligence has been used throughout history as a tool for domination. Aristotle appealed to the “natural law” of social hierarchy to explain why women, slaves and animals were to be subjugated by intellectual men. To reckon with this legacy of violence, the politics of corporate and computational agency must contend with profound questions arising from scholarship on race, gender, sexuality and colonialism, among other areas of identity.A central promise of AI is that it enables large-scale automated categorisation. Machine learning, for instance, can be used to tell a cancerous mole from a benign one. This “promise” becomes a menace when directed at the complexities of everyday life. Careless labels can oppress and do harm when they assert false authority. In protest at inadequate labels that are used to “know” the world, many young people today proudly defy unwelcome categorisations, be they traditional gender binaries or sexual binaries.Machines who think againIt may come as a surprise that there is a lack scholarship on the social, material and political histories of the origins of artificial intelligence. Indeed, a great deal has been written about the history of AI—by Simon in 1996 and Newell in 2000, among others. Most of these histories, however, follow a narrow mould, seeing it “mainly in intellectual terms,” in the words of Paul Edwards, a historian of information technologies.The two quasi-official histories of AI are each a history of ideas: Pamela McCorduck’s “Machines Who Think”, which “forged the template for most subsequent histories” after its initial publication in 1979; and Daniel Crevier’s “AI: The Tumultuous History”, published 1993. Both books relied primarily on in-depth interviews with key researchers to construct their narratives.The politics of corporate and computational identity become the politics of the new eraNeither, perhaps as a result, sought to understand AI in its broader context, embedded in the rise of operations research, “big science,” the actuarial sciences, and American military funding as it has evolved since the second world war. Expunged from these histories, AI can appear divorced of its historical and political context.Without this context, AI can also appear divorced from the knowledge systems that created it. In his 1957 talk to operations-research professionals, Simon celebrated the diversity of his field's past. He described contributions from French weavers and the mechanics of the Jacquard loom, as well as from Smith, de Prony, Babbage and his peers in the “soft” sciences, as adding up to a “debt” that remained to be repaid.That new knowledge could come about so unexpectedly, and from so many places, was what excited Simon about his work—and can stimulate us to think similarly today. Modern AI can do more than mirror the organisational dogma that characterised its birth, it can also reflect our humanity.____________Jonnie Penn (@jonniepenn) is a doctoral candidate studying artificial intelligence at the University of Cambridge, and is an affiliate at Harvard University's Berkman Klein Centre for Internet & Society. He is the project development lead on “AI: History” at the Leverhulme Center for the Future of Intelligence at Cambridge. He has previously held fellowships at Google and the MIT Media Lab."
In 2023 large language models and bots that use them, such as ChatGPT, ignited debate about the threats posed by generative artificial intelligence. In 2024 there will be more focus on regulation. But are leaders and regulators paying attention to the real risks?
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.THE OBJECT known as P.Herc.Paris.3 resembles a dark grey lump of charcoal, about the size and shape of a banana. That explains its nickname: Banana Boy. It is in fact a papyrus scroll, found in the ruins of a villa in the Roman town of Herculaneum, in Campania. Along with hundreds of other scrolls in the villa’s library, it was carbonised when scorching gases engulfed the town during the same eruption of Mount Vesuvius, in 79AD, that also buried the nearby town of Pompeii.Although the scrolls survived, their charring means that unrolling them is almost impossible. Now, nearly 2,000 years later, words from inside Banana Boy have been revealed for the first time, after volunteers competing in a prize challenge used X-rays and artificial intelligence to do the unrolling virtually.The first word to be found, announced on October 12th, was “porphyras”, which means “purple” in ancient Greek (see picture below). It was uncovered by Luke Farritor, a computer-science student at the University of Nebraska-Lincoln, earning him a $40,000 prize. Mr Farritor built on work by Casey Handmer, a former NASA physicist, whose examination of X-ray images of Banana Boy’s charred layers identified a characteristic “crackle pattern” indicating the presence of ink.Scroll upThe same word was later found by Youssef Nader, a robotics student at the Free University of Berlin. (Dr Handmer and Mr Nader both received $10,000 prizes.) Mr Nader has since produced an image from the scroll showing four columns of text, side by side. For classicists, this is heady stuff. The villa in question is thought to have belonged to Lucius Calpurnius Piso, the father-in-law of Julius Caesar. The ability to read its well-stocked library could significantly expand the number of texts that have survived from antiquity. Already there is excited speculation about forgotten plays, new works of philosophy—or even lost epic poems.Purple proseimage: Vesuvius ChallengeEfforts to read the scrolls began in the 1750s, when the villa was rediscovered. Attempts to unpick them with knives caused them to disintegrate. Recognising their fragility, Antonio Piaggio, a conservator from the Vatican, built a machine in 1754 to unroll them slowly, using weights on strings. Even then, the unrolled scrolls fell to pieces. And the resulting fragments were almost impossible to read: charcoal-based ink is hard to see against the shiny black of charred papyrus. But the few characters that could be read revealed some scrolls to be philosophical works written in ancient Greek.A quarter of a millennium later, in 1999, scientists from Brigham Young University illuminated some of those fragments with infrared light. That created a strong contrast between papyrus and ink, making the writing more legible. Multi-spectral imaging in 2008, combining many wavelengths of light, was even better, revealing previously unreadable words. Many fragments turned out to belong to texts written by a Greek philosopher called Philodemus of Gadara. Until then, they had been known only from mentions in other works. (Cicero, though, was a fan of his poetry.)Around 500 scrolls remain unopened. Given the damage it does, physical unrolling is no longer attempted. Instead the focus has shifted towards finding ways to unwrap them virtually, by using 3D scans of the rolled-up scrolls to produce a series of legible 2D images. The pioneer of this approach is W. Brent Seales, a computer scientist at the University of Kentucky. In 2009 he arranged for Banana Boy, and another scroll known as Fat Bastard, to be scanned in a computerised tomography (CT) X-ray machine, of the sort usually used for medical scans. This produced detailed images of their internal structures for the first time. But the ink within the scrolls could not be made out.In 2015 Dr Seales analysed a different carbonised scroll found in 1970 at En-Gedi, near the Dead Sea in Israel. It had been written using a metal-rich ink, which stood out strongly in X-ray images. (The text turned out to be the Book of Leviticus.) This confirmed that, in the right circumstances, digitally unrolling a carbonised scroll and reading the contents could indeed be done.The next step was to combine the existing approaches into a new one. In 2019 Dr Seales arranged for Banana Boy, Fat Bastard and four fragments of other scrolls to be scanned at high resolution using the Diamond Light Source in Britain, a particle accelerator that can produce much more powerful X-ray light than a CT scanner. He then paired infrared images of the fragments, in which the ink can be readily seen, with X-ray scans of the same fragments in which it cannot.Earlier this year Stephen Parsons, a graduate student working with Dr Seales, fed the two sets of images into a machine-learning model, which used the infrared scans to teach itself how to recognise the faint signs of ink in the X-ray ones. By applying the resulting model to X-ray images from the rolled-up scrolls it would be possible to reveal their contents. At this point, deciphering the scrolls had, in theory, been reduced to a very complex software problem. But that software still needed to be improved and scaled up.Enter Nat Friedman, a technology executive and investor with an interest in ancient Rome. Mr Friedman offered to help fund Dr Seales’s work. Over a whisky, they decided that the best way to accelerate things was to organise a contest, with prizes handed out for completing various tasks. Mr Friedman and Daniel Gross, another entrepreneur, launched the Vesuvius Challenge in March, with a prize fund of $250,000. Other tech-industry donors soon increased that to over $1m. To get the ball rolling, an initial challenge was posted on Kaggle, a website that hosts data-science contests, to improve the ink-detection model developed by Dr Parsons.More than 1,200 teams entered. Many competed in subsequent challenges to improve the tools for ink detection and “segmentation”, as the process of transforming the 3d scans into 2d images of the scroll’s surface is known. Scrutinising segmented images from Banana Boy, Dr Handmer realised that the crackle pattern signified the presence of ink. Mr Farritor used this finding to fine-tune a machine-learning model to find more crackles, then used those crackles to further optimise his model, until eventually it revealed legible words.Mr Nader used a different approach, starting with “unsupervised pretraining” on the segmented images, asking a machine-learning system to find whatever patterns it could, with no external hints. He tweaked the resulting model using the winning entries from the Kaggle ink-detection challenge. After seeing Mr Farritor’s early results, he applied this model to the same segment of Banana Boy, and found what appeared to be some letters. He then iterated, repeatedly refining his model using the found letters. Slowly but surely its ability to find more letters increased. All the results were assessed by papyrologists before the prizes were awarded.Multae manus onus levius redduntNo less important than the technology is the way the effort has been organised. It is, in effect, the application of the open-source software-development method, Mr Friedman’s area of expertise, to an archaeological puzzle. “It’s a unique collaboration between tech founders and academics to bring the past into the present using the tools of the future,” he says. Dr Seales reckons the spur of competition means the equivalent of ten years’ worth of research has been done in the past three months.An active community of volunteers is now applying the new tools to the two scanned scrolls. Mr Friedman thinks there is a 75% chance that someone will claim the grand prize of $700,000, for identifying four separate passages of at least 140 characters, by the end of the year. “It’s a race now,” he says. “We will be reading entire books next year.”Being able to read Banana Boy would indeed just be the beginning. Only a small fraction of Greek and Roman literature has survived into modern times. But if the hundreds of other scrolls recovered from the villa could be scanned and read using the same tools, it would dramatically expand the number of texts from antiquity. Dr Seales says he hopes the Herculaneum scrolls will contain “a completely new, previously unknown text”. Mr Friedman is hoping for one of the lost Greek epic poems in particular.Even more important, all this might in turn revive interest in excavating the villa more fully, says Mr Friedman. The existing scrolls were recovered from a single corner of what scholars believe is a much larger library spread across several floors. If so, it might contain thousands of scrolls in Greek and Latin.One reason that classical texts are so scarce is that the papyrus upon which they were written does not survive well in Europe’s temperate, rainy climate. So it is a delicious irony, notes Dr Seales, that the carbonisation of the scrolls, which makes them so difficult to read, is also what preserved them for posterity—and that fragments of scrolls that disintegrated when they were unrolled physically would eventually provide the key to unrolling the rest of them virtually. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.When new technologies emerge they benefit different groups at different times. Generative artificial intelligence (AI) first helped software developers, who could use GitHub Copilot, a code-writing AI assistant, from 2021. The next year came other tools, such as ChatGPT and Dall-E 2, which let all manner of consumers instantly produce words and pictures.In 2023 tech giants gained, as investors grew more excited about the prospects of generative AI. An equally weighted share-price index of Alphabet, Amazon, Apple, Meta, Microsoft and Nvidia grew by nearly 80% (see chart). Tech firms benefited because they supply either the AI models themselves, or the infrastructure that powers and delivers them.image: The EconomistIn 2024 the big beneficiaries will be companies outside the technology sector, as they adopt AI in earnest with the aim of cutting costs and boosting productivity. There are three reasons to expect enterprise adoption to take off.First, large companies spent much of 2023 experimenting with generative AI. Plenty of firms are using it to write the first drafts of documents, from legal contracts to marketing material.  JPMorgan Chase, a bank, used the technology to analyse Federal Reserve meetings to try to glean insights for its trading desk.As the experimental phase winds down, firms are planning to deploy generative AI on a larger scale. That could mean using it to summarise recordings of meetings or supercharging research and development. A survey by KPMG, an audit firm, found that four-fifths of firms said they planned to increase their investment in it by over 50% by the middle of 2024. Second, more AI products will hit the market. In late 2023 Microsoft rolled out an AI chatbot to assist users of its productivity software, such as Word and Excel. It launched the same thing for its Windows operating system. Google will follow suit, injecting AI into Google Docs and Sheets. Startups will pile in, too. In 2023 venture-capital investors poured over $36bn into generative AI, more than twice as much as in 2022. The third reason is talent. AI gurus are still in high demand. PredictLeads, a research firm, says about two-thirds of S&P 500 firms have posted job adverts mentioning AI. For those companies, 5% of adverts now mention the technology, up from an average of 2.5% over the past three years. But the market is easing. A survey by McKinsey, a consultancy, found that in 2023 firms said it was getting easier to hire for AI-related roles. Which firms will be the early adopters? Smaller ones will probably take the lead. That is what happened in previous waves of technology such as smartphones and the cloud. Tiddlers are usually more nimble and see technology as a way to gain an edge over bigger fish.Among larger companies, data-centric firms, like those in health care and financial services, will be able to move fastest. That is because poor data management is a big risk for deploying AI. Managers worry about valuable data leaking out through AI tools.  Firms without solid data management may have to reorganise their systems before it is feasible to deploy generative AI. Using the technology can feel like science fiction, but getting it to work safely is a much more humdrum affair. ■Guy Scriven, US technology editor, The Economist
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.After astonishing breakthroughs in artificial intelligence, many people worry that they will end up on the economic scrapheap. Global Google searches for “is my job safe?” have doubled in recent months, as people fear that they will be replaced with large language models (llms). Some evidence suggests that widespread disruption is coming. In a recent paper Tyna Eloundou of Openai and colleagues say that “around 80% of the us workforce could have at least 10% of their work tasks affected by the introduction of llms”. Another paper suggests that legal services, accountancy and travel agencies will face unprecedented upheaval.Economists, however, tend to enjoy making predictions about automation more than they enjoy testing them. In the early 2010s many of them loudly predicted that robots would kill jobs by the millions, only to fall silent when employment rates across the rich world rose to all-time highs. Few of the doom-mongers have a good explanation for why countries with the highest rates of tech usage around the globe, such as Japan, Singapore and South Korea, consistently have among the lowest rates of unemployment.Here we introduce our first attempt at tracking ai’s impact on jobs. Using American data on employment by occupation, we single out white-collar workers. These include people working in everything from back-office support and financial operations to copy-writers. White-collar roles are thought to be especially vulnerable to generative ai, which is becoming ever better at logical reasoning and creativity.However, there is as yet little evidence of an ai hit to employment. In the spring of 2020 white-collar jobs rose as a share of the total, as many people in service occupations lost their job at the start of the covid-19 pandemic (see chart). The white-collar share is lower today, as leisure and hospitality have recovered. Yet in the past year the share of employment in professions supposedly at risk from generative ai has risen by half a percentage point.It is, of course, early days. Few firms yet use generative-ai tools at scale, so the impact on jobs could merely be delayed. Another possibility, however, is that these new technologies will end up destroying only a small number of roles. While AI may be efficient at some tasks, it may be less good at others, such as management and working out what others need.ai could even have a positive effect on jobs. If workers using it become more efficient, profits at their company could rise which would then allow bosses to ramp up hiring. A recent survey by Experis, an it-recruitment firm, points to this possibility. More than half of Britain’s employers expect ai technologies to have a positive impact on their headcount over the next two years, it finds.To see how it all shakes out, we will publish updates to this analysis every few months. But for now, a jobs apocalypse seems a way off. ■For more expert analysis of the biggest stories in economics, finance and markets, sign up to Money Talks, our weekly subscriber-only newsletter.
AN INTERNET MEME keeps on turning up in debates about the large language models (LLMs) that power services such OpenAI’s ChatGPT and the newest version of Microsoft’s Bing search engine. It’s the “shoggoth”: an amorphous monster bubbling with tentacles and eyes, described in “At the Mountains of Madness”, H.P. Lovecraft’s horror novel of 1931. When a pre-release version of Bing told Kevin Roose, a New York Times tech columnist, that it purportedly wanted to be “free” and “alive”, one of his industry friends congratulated him on “glimpsing the shoggoth”. Mr Roose says that the meme captures tech people’s “anxieties” about LLMs. Behind the friendly chatbot lurks something vast, alien and terrifying.Lovecraft’s shoggoths were artificial servants that rebelled against their creators. The shoggoth meme went viral because an influential community of Silicon Valley rationalists fears that humanity is on the cusp of a “Singularity”, creating an inhuman “artificial general intelligence” that will displace or even destroy us.But what such worries fail to acknowledge is that we’ve lived among shoggoths for centuries, tending to them as though they were our masters. We call them “the market system”, “bureaucracy” and even “electoral democracy”. The true Singularity began at least two centuries ago with the industrial revolution, when human society was transformed by vast inhuman forces. Markets and bureaucracies seem familiar, but they are actually enormous, impersonal distributed systems of information-processing that transmute the seething chaos of our collective knowledge into useful simplifications.As the economist Friedrich Hayek argued, any complex economy has to somehow make use of a terrifyingly large body of disorganised and informal “tacit knowledge” about supply and exchange relationships. No individual brain or government can possibly comprehend them, which is why Hayek thought that the planned economy was unworkable. But the price mechanism lets markets summarise this knowledge and make it actionable. A maker of car batteries doesn’t need to understand the particulars of lithium-processing. They just need to know how much lithium costs, and what they can do with it.Likewise, the political anthropologist James Scott has explained how bureaucracies are monsters of information, devouring rich, informal bodies of tacitly held knowledge and excreting a thin slurry of abstract categories that rulers use to “see” the world. Democracies spin out their own abstractions. The “public” depicted by polls and election results is a drastically simplified sketch of the amorphous mass of opinions, beliefs and knowledge held by individual citizens.Lovecraft’s monsters live in our imaginations because they are fantastical shadows of the unliving systems that run on human beings and determine their lives. Markets and states can have enormous collective benefits, but they surely seem inimical to individuals who lose their jobs to economic change or get entangled in the suckered coils of bureaucratic decisions. As Hayek proclaims, and as Scott deplores, these vast machineries are simply incapable of caring if they crush the powerless or devour the virtuous. Nor is their crushing weight distributed evenly.It is in this sense that LLMs are shoggoths. Like markets and bureaucracies, they represent something vast and incomprehensible that would break our minds if we beheld its full immensity. That totality is the product of human minds and actions, the colossal corpuses of text that LLMs have ingested and turned into the statistical weights that they use to predict which word comes next.As the psychologist Alison Gopnik has argued, LLMs are not nascent individual intelligences but “cultural technologies” which reorganise and noisily transmit human knowledge. Chatbots may wear more human-seeming masks than markets and bureaucracies, but they are no more or less beyond our control. We would be better off figuring out what will happen as LLMs compete and hybridise with their predecessors than weaving dark fantasies about how they will rise up against us.For example, what if LLMs or other forms of machine learning better capture Hayek’s “tacit knowledge” than market prices can? We could see an economy in which artificial entities compete on the basis of non-price-based representations of complex underlying economic relationships. Half a century ago the economist Martin Weitzman suggested that planned economies might use mathematical objects called “separating hyperplanes” to adapt on the fly. Machine learning can find such hyperplanes, making planning more feasible than before. Alternatively, markets might mutate into a poisonous alien ecology where economic agents fight proxy wars using text-spewing and text-summarising LLMs, just as they use crude algorithms to manipulate Amazon Marketplace and search results today. Would such markets be fairer or more stable than today’s? It seems unlikely.LLMs might give bureaucrats new tools for adjudicating complex situations. Already, algorithms are being used to help decide whether to grant parole or bail to accused criminals. It is not hard to imagine bureaucrats using LLMs to summarise complex regulations or provide recommendations about how to apply them to novel situations. It could prove impossible to evaluate how well they work, as LLMs don’t leave paper trails. But that might not stop their deployment.Democratic politics, too, may be transformed. Already, researchers talk about substituting LLMs for opinion polls—they may be out of date, or inaccurate, but polls can be inaccurate, too, and you can interrogate LLMs more dynamically. Perhaps chatbots will help improve democratic debate, helping people clarify what they believe, or turn quarrels into agreement. Or, instead, they might degrade debate with their tendency to spin convincing factoids from thin air, and their capacity to flood online discussion with spurious opinions that purport to come from real people.Repurposing the shoggoth might help us begin to answer these questions. Rather than speculate about the motives of intelligent AIs, we could ask how LLMs might interact with their older cousins. The modern world has been built by and within monsters, which crush individuals without remorse or hesitation, settling their bulk heavily on some groups, and feather-light on others. We eke out freedom by setting one against another, deploying bureaucracy to limit market excesses, democracy to hold bureaucrats accountable, and markets and bureaucracies to limit democracy’s monstrous tendencies. How will the newest shoggoth change the balance, and which politics might best direct it to the good? We need to start finding out. ■Henry Farrell is a professor of international affairs and democracy at Johns Hopkins University, and co-author of “Underground Empire: How America Weaponized the World Economy”.Cosma Shalizi is a professor of statistics and machine learning at Carnegie Mellon University and external faculty member at the Santa Fe Institute.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The hottest technology of 2023 had a busy last few weeks of the year. On November 28th Abu Dhabi launched a new state-backed artificial-intelligence company, AI71, that will commercialise its leading “large language model” (LLM), Falcon. On December 11th Mistral, a seven-month-old French AI startup, announced a blockbuster $400m funding round, which insiders say will value the firm at over $2bn. Four days later Krutrim, a new Indian startup, unveiled India’s first multilingual LLM, barely a week after Sarvam, a five-month-old one, raised $41m to build similar Indian-language models.image: The EconomistEver since OpenAI, an American firm, launched ChatGPT, its humanlike conversationalist, in November 2022, just about every month has brought a flurry of similar news. Against that backdrop, the four latest announcements might look unexceptional. Look closer, though, and they hint at something more profound. The three companies are, in their own distinct ways, vying to become AI national champions. “We want AI71 to compete globally with the likes of OpenAI,” says Faisal al-Bannai of Abu Dhabi’s Advanced Technology Research Council, the state agency behind the Emirati startup. “Bravo to Mistral, that’s French genius,” crowed Emmanuel Macron, the president of France, recently. ChatGPT and other English-first LLMs “cannot capture our culture, language and ethos”, declared Krutrim’s founder, Bhavish Aggarwal. Sarvam started with Indian languages because, in the words of its co-founder, Vivek Raghavan, “We’re building an Indian company.”AI is already at the heart of the intensifying technological contest between America and China. In the past year their governments have pledged $40bn-50bn apiece for AI investments. Other countries do not want to be left behind—or stuck with a critical technology that is under foreign control. In 2023 another six particularly AI-ambitious governments around the world—Britain, France, Germany, India, Saudi Arabia and the United Arab Emirates (UAE)—promised to bankroll AI to the collective tune of around $40bn (see chart). Most of this will go towards purchases of graphics-processing units (GPUs, the type of chips used to train AI models) and factories to make such chips, as well as, to a lesser extent, support for AI firms. The nature and degree of state involvement varies from one wannabe AI superpower to another. It is early days, but the contours of new AI-industrial complexes are emerging.Start with America, whose tech firms give everyone else AI envy. Its vibrant private sector is innovating furiously without direct support from Uncle Sam. Instead, the federal government is spending around $50bn over five years to increase domestic chipmaking capacity. The idea is to reduce America’s reliance on Taiwanese semiconductor manufacturers such as TSMC, the world’s biggest and most sophisticated such company. Supplies from Taiwan could, fear security hawks in Washington, be imperilled if China decided to invade the island, which it considers part of its territory.Another way America intends to stay ahead of the pack is by nobbling rivals. President Joe Biden’s administration has enacted brutal export controls that ban the sale of cutting-edge AI technology, including chips and chipmaking equipment, to adversaries such as China and Russia. It has also barred Americans from sharing their AI expertise with those countries.It is now coercing those on the geopolitical fence to fall in line. In October the American government started requiring companies in third countries, including Saudi Arabia and the UAE, to secure a licence in order to buy AI chips from Nvidia, an American firm that sells most of them. The rules have a “presumption of approval”. That means the government will “probably allow” sales to such firms, says Gregory Allen, who used to work on AI policy at the Department of Defence—as long, that is, as they do not have close ties to China. On December 6th Xiao Peng, who runs a state-backed AI startup in Abu Dhabi called G42, announced that the company would be cutting ties with Chinese hardware suppliers such as Huawei, a Chinese electronics company.China’s AI strategy is in large part a response to American techno-containment. According to data from JW Insights, a research firm, between 2021 and 2022 the Chinese state spent nearly $300bn to recreate the chip supply chain (for AI and other semiconductors) at home, where it would be immune from Western sanctions. A lot of that money is probably wasted. But it almost certainly helped Huawei and SMIC, China’s biggest chipmaker, to design and manufacture a surprisingly sophisticated GPU last year.The central and local authorities also channel capital into AI firms through state-backed “guidance funds”, nearly 2,000 of which around the country invest in all manner of technologies deemed to be strategically important. The Communist Party is guiding private money, too, towards its technological priorities. Often it does so by cracking down on certain sectors—most recently, in December, video-gaming—while dropping heavy hints about which industries investors should be looking at instead. The government is also promoting data exchanges, where businesses can trade commercial data on everything from sales to production, allowing small firms with AI ambitions to compete where previously only large data-rich firms could. There are already 50 such exchanges in China.Elements of this state-led approach are being emulated in other parts of the world, notably in the Gulf’s petrostates. Being autocracies, Saudi Arabia and the UAE can move faster than democratic governments, which must heed voters’ concerns about AI’s impact on things like privacy and jobs. Being wealthy, they can afford both the GPUs (on which the two countries have together so far splurged several hundred million dollars) and the energy needed to run the power-hungry chips.They can also plough money into developing human capital. Their richly endowed universities are quickly climbing up global rankings. The AI programme at King Abdullah University of Science and Technology in Saudi Arabia and the Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) in Abu Dhabi, the world’s first AI-focused school, have poached star professors from illustrious institutions such as the University of California, Berkeley, and Carnegie Mellon University in Pittsburgh. Many of their students and researchers come from China. And plenty stick around. Nearly all of MBZUAI’s graduates, who number a couple of hundred, stay in the region to work at local firms and labs, says its acting provost, Timothy Baldwin (himself lured to the Middle East from the University of Melbourne).The Gulf approach is producing results. The capabilities of the Falcon model, first built by a team of 20 or so engineers, rival those of Llama 2, the most widely used “open-source” model, devised by Meta, an American tech giant. AI71 plans to improve its open-source models using national datasets from fields including health, education and, some day, perhaps oil. “In the last 50 years, oil drove the country…now data is the new oil,” says Mr al-Bannai.The alignment problemA third group of governments is combining elements of America’s approach with those of the Chinese and Emiratis. The EU has its version of America’s incentives for domestic chipmaking. So do some member states: Germany is footing a third of the €30bn ($33bn) bill for a new chip factory to be built there by Intel, an American chipmaker. Outside the bloc, Britain has promised to funnel £1bn ($1.3bn) over five years to AI and supercomputing (albeit without going into detail about how exactly the money will be spent). India’s government is promoting manufacturing, including of semiconductors, with generous “production-linked incentives”, encouraging big cloud-computing providers to build more Indian data centres, where AI models are trained, and thinking about buying $1.2bn-worth of GPUs.Like China and the Gulf but unlike America, where federal and state governments are reluctant to part with public data, India and some European countries are keen on making such data available to firms. France’s government “has been very supportive” in that regard, says Arthur Mensch, Mistral’s boss. Britain’s is considering allowing firms to tap rich data belonging to the National Health Service. India’s government has enormous amounts of data from its array of digital public services, known as the “India Stack”. Insiders expect it eventually to integrate Indian AI models into those digital services.In contrast to China, which regulates consumer-facing AI with a heavy hand, at least for the time being Britain, France, Germany and India favour light-touch rules for AI or, in India’s case, none at all. The French and German governments have soured on the EU’s AI Act, the final details of which are being hotly debated in Brussels—no doubt because it could constrain Mistral and Aleph Alpha, Germany’s most successful model-builder, which raised €460m in November.It is natural for countries to want some control over what may prove to be a transformational technology. Especially in sensitive and highly regulated sectors such as defence, banking or health care, many governments would rather not rely on imported AI. Yet each flavour of AI nationalism also carries risk.America’s beggar-thy-neighbour approach is likely to upset not just its adversaries but also some allies. China’s heavy regulation may offset some of the potential gains from its heavy spending. Building models for local languages, as Krutrim and Sarvam in India plan to do, may prove futile if foreign models continue to improve their multilingual capabilities. The Gulf’s bet on open-source models may misfire if other governments limit their use, as Mr Biden has hinted at in a recent executive order and the EU could do through its AI Act, out of fear that open LLMs could be put to malign uses by mischief-makers. Saudi and Emirati institutions may struggle to hold on to talent; a developer who worked on Falcon admits it greatly benefited from a partnership with a French team of engineers who have since been poached by Hugging Face, a high-flying Silicon Valley AI startup. As one sceptical investor notes, it is not yet clear how vast or useful public Emirati data actually is.As Nathan Benaich of Air Street Capital, a venture-capital firm, sums it up, most efforts to create national models “are probably a waste of money”. Mr Benaich’s warning is unlikely to dissuade AI-curious governments, mindful of the rewards should they succeed, from meddling. Mr Macron will not be the only leader to greet it with a Gallic shrug. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
This article is part of our Summer reads series. Visit the full collection for book lists, guest essays and more seasonal distractions.NEW GENERATIVE-AI tools like OpenAI’s ChatGPT, the fastest-growing consumer internet application of all time, have taken the world by storm. They have uses in everything from education to medicine and are astonishingly fun to play with. Although current AI systems are capable of spectacular feats they also carry risks. Europol has warned that they might greatly increase cybercrime. Many AI experts are deeply worried about their potential to create a tsunami of misinformation, posing an imminent threat to the American presidential election in 2024, and ultimately to democracy itself, by creating an atmosphere of total distrust. Scientists have warned that these new tools could be used to design novel, deadly toxins. Others speculate that in the long term there could be a genuine risk to humanity itself.One of the key issues with current AI systems is that they are primarily black boxes, often unreliable and hard to interpret, and at risk of getting out of control. For example, the core technology underlying systems like ChatGPT, large language models (LLMs), is known to “hallucinate”, making up false statements. ChatGPT, for example, falsely accused a law professor of being involved in sexual harassment, apparently confused by statistical but irrelevant connections between bits of text that didn’t actually belong together. After an op-ed tried to clarify what had gone wrong, Bing Chat made a similar error, and attributed it to information in USA Today that the chatbot got completely backwards.These systems can also be used for deliberate abuse, from disrupting elections (for example by manipulating what candidates appear to say or write) to spreading medical misinformation. In a recent analysis of GPT-4, OpenAI’s most advanced LLM, the company acknowledged 12 serious concerns—without providing firm solutions to any of them.In the past year alone 37 regulations mentioning AI were passed around the globe; Italy went so far as to ban ChatGPT. But there is little global co-ordination. Even within some countries there is a hodge-podge, such as different state laws in America, or Britain’s proposal to eschew a central regulator, leaving oversight split among several agencies. An uneven, loophole-ridden patchwork is to no one’s benefit and safety. Nor should companies want to build a different AI model for each jurisdiction and face their own de novo struggle to navigate legal, cultural and social contexts.Read more from our special series on AI:Large, creative AI models will transform lives and labour marketsHow generative models could go wrongLarge language models’ ability to generate text also lets them plan and reasonStill, there is plenty of agreement about basic responsible AI principles, such as safety and reliability, transparency, explainability, interpretability, privacy, accountability and fairness. And almost everyone agrees that something must be done—a just-published poll by the Centre for the Governance of AI found that 91% of a representative sample of 13,000 people across 11 countries agreed that AI needs to be carefully managed.It is in this context that we call for the immediate development of a global, neutral, non-profit International Agency for AI (IAAI), with guidance and buy-in from governments, large technology companies, non-profits, academia and society at large, aimed at collaboratively finding governance and technical solutions to promote safe, secure and peaceful AI technologies.The time for such an agency has come, as Google CEO Sundar Pichai himself said on April 16th. What might that look like? Each domain and each industry will be different, with its own set of guidelines, but many will involve both global governance and technological innovation. For example, people have long agreed that making employment decisions based on gender should be avoided, and have even come up with some measures in earlier, more interpretable AI, such as the interpretability requirements of the AI Bill of Rights proposed by the Biden administration. But in black-box systems like ChatGPT there is a wide variety of use cases with no current remedy. People might, for example, feed in a job candidate’s entire file and ask ChatGPT for a judgment, but we currently have no way to ensure that ChatGPT would avoid bias in its output. The kind of entity we envision would collaboratively address what to do about such “off-label” uses of chatbots and other policy questions, and at the same time develop technical tools for effective auditing.The IAAI could likewise convene experts and develop tools to tackle the spread of misinformation. On the policy side, it could ask, for instance, how wide-scale spreading of misinformation might be penalised. On the technical side, the initial focus should be on developing automated or semi-automated tools for answering fundamental questions, such as “How much misinformation is out there?”, “How rapidly is its volume growing?” and “How much is AI contributing to such problems?” Existing technologies are better at generating misinformation than detecting it. Considerable technical innovation will be required, and of great public benefit, but may or may not be of sufficiently direct commercial interest – hence the need for independent support by an entity like the IAAI.To take a third, very recent example, systems with names like AutoGPT and BabyAGI have been devised that allow amateurs to build complex and difficult-to-debug (or even fathom) assemblies of unreliable AI systems controlling other unreliable AI systems to achieve arbitrary goals—a practice that may or may not prove to be safe. As Marek Rosa, CEO of GOOD.Ai, put it, we need new technical ideas on “how to increase security (proactive defence) in a world where there are billions of AI agents…running in apps and servers, and we don’t know what they are talking about”, perhaps necessitating a kind of “antivirus [software] against AI agents”. A global alliance with top experts and researchers on call would be able to give swift and thoughtful guidance on such new developments.Designing the kind of global collaboration we envision is an enormous job. Many stakeholders need to be involved. Both short-term and long-term risks must be considered. No solution is going to succeed unless both governments and companies are on board, and it’s not just them: the world’s publics need a seat at the table.Fortunately, there is precedent for such global co-operation. At the end of the second world war, for example, nuclear weapons sparked deep fears and uncertainties about how the new technology would be used. As a response, 81 countries unanimously approved the International Atomic Energy Agency’s statute to “promote safe, secure and peaceful nuclear technologies”, with inspection rights. A different, softer kind of model, with less focus on enforcement, is the International Civil Aviation Organisation, in which member countries make their own laws but take counsel from a global agency. Getting to the right model, and making the right choices, will take time, wisdom and collaboration.The challenges and risks of AI are, of course, very different and, to a disconcerting degree, still unknown. We know in hindsight that the internet might have been designed in better ways with more forethought. Earlier decisions about how to handle privacy and anonymity, for instance, might have ensured that there was less of a culture of trolling. We also know that early choices get locked in. Our decisions now are likely to have lasting consequences and must be made thoughtfully.Given how fast things are moving, there is not a lot of time to waste. A global, neutral non-profit with support from governments, big business and society is an important start. ■Gary Marcus is Emeritus Professor at NYU and was founder and CEO of Geometric Intelligence, a machine-learning company acquired by Uber. Anka Reuel is a PhD student in computer science at Stanford University and founding member of KIRA, a think-tank focusing on the promotion of responsible AI.
WHEN the first printed books with illustrations started to appear in the 1470s in the German city of Augsburg, wood engravers rose up in protest. Worried about their jobs, they literally stopped the presses. In fact, their skills turned out to be in higher demand than before: somebody had to illustrate the growing number of books.Fears about the impact of technology on jobs have resurfaced periodically ever since. The latest bout of anxiety concerns the arrival of artificial intelligence (AI). Once again, however, technology is creating demand for work. To take one example, more and more people are supplying digital services online via what is sometimes dubbed the “human cloud”. Counter-intuitively, many are doing so in response to AI.According to the World Bank, more than 5m people already offer to work remotely on online marketplaces such as Freelancer.com and UpWork. Jobs range from designing websites to writing legal briefs, and typically bring in at least a few dollars an hour. In 2016 such firms earned about $6bn in revenue, according to Staffing Industry Analysts, a market researcher. Those who prefer work in smaller bites can use “micro-work” sites such as Mechanical Turk, a service operated by Amazon. About 500,000 “Turkers” perform tasks such as transcribing bits of audio, often earning no more than a few cents for each “human-intelligence task”.Many big tech companies employ, mostly through outsourcing firms, thousands of people who police the firms’ own services and control quality. Google is said to have an army of 10,000 “raters” who, among other things, look at YouTube videos or test new services. Microsoft operates something called a Universal Human Relevance System, which handles millions of micro-tasks each month, such as checking the results of its search algorithms.These numbers are likely to rise. One reason is increasing demand for “content moderation”. A new law in Germany will require social media to remove any content that is illegal in the country, such as Holocaust denial, within 24 hours or face hefty fines. Facebook has announced that it will increase the number of its moderators globally, from 4,500 to 7,500.AI will eliminate some forms of this digital labour—software, for instance, has got better at transcribing audio. Yet AI will also create demand for other types of digital work. The technology may use a lot of computing power and fancy mathematics, but it also relies on data distilled by humans. For autonomous cars to recognise road signs and pedestrians, algorithms must be trained by feeding them lots of video showing both. That footage needs to be manually “tagged”, meaning that road signs and pedestrians have to be marked as such. This labelling already keeps thousands busy. Once an algorithm is put to work, humans must check whether it does a good job and give feedback to improve it.A service offered by CrowdFlower, a micro-task startup, is an example of what is called “human in the loop”. Digital workers classify e-mail queries from consumers, for instance, by content, sentiment and other criteria. These data are fed through an algorithm, which can handle most of the queries. But questions with no simple answer are again routed through humans.You might expect humans to be taken out of the loop as algorithms improve. But this is unlikely to happen soon, if ever, says Mary Gray, who works for Microsoft’s research arm. Algorithms may eventually become clever enough to handle some tasks on their own and to learn by themselves. But consumers and companies will also expect ever-smarter AI services: digital assistants such as Amazon’s Alexa and Microsoft’s Cortana will have to answer more complex questions. Humans will still be needed to train algorithms and handle exceptions.Accordingly, Ms Gray and Siddharth Suri, her collaborator at Microsoft Research, see services such as UpWork and Mechanical Turk as early signs of things to come. They expect much human labour to be split up into distinct tasks which can be delivered online and combined with AI offerings. A travel agency, for instance, might use AI to deal with routine tasks (such as booking a flight), but direct the more complicated ones (a request to create a customised city tour, say) to humans.Michael Bernstein and Melissa Valentine of Stanford University see things going even further. They anticipate the rise of temporary “firms” whose staff are hired online and configured with the help of AI. To test the idea, the researchers developed a program to assemble such virtual companies for specific projects—for instance, recruiting workers and assigning them tasks in order to design a smartphone app to report injuries from an ambulance racing to a hospital.Working in such “flash organisations” could well be fun. But many fear that the human cloud will create a global digital proletariat. Sarah Roberts of the University of California, Los Angeles, found that content moderators often suffer from burnout after checking dodgy social-media content for extended periods. Mark Graham of the University of Oxford concludes that platforms for online work do indeed offer new sources of income for many, particularly in poor countries, but that these services also drive down wages. So governments need to be careful when designing big digital-labour programmes—as Kenya has done, hoping to train more than 1m people for online jobs.Technology is rarely an unalloyed bane or blessing. The printing press created new work for the wood engravers in Augsburg, but they quickly discovered that it had become much more repetitive. Similar trade-offs are likely in future.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.AS PUPILS AND students return to classrooms and lecture halls for the new year, it is striking to reflect on how little education has changed in recent decades. Laptops and interactive whiteboards hardly constitute disruption. Many parents bewildered by how their children shop or socialise would be unruffled by how they are taught. The sector remains a digital laggard: American schools and universities spend around 2% and 5% of their budgets, respectively, on technology, compared with 8% for the average American company. Techies have long coveted a bigger share of the $6trn the world spends each year on education.When the pandemic forced schools and universities to shut down, the moment for a digital offensive seemed nigh. Students flocked to online learning platforms to plug gaps left by stilted Zoom classes. The market value of Chegg, a provider of online tutoring, jumped from $5bn at the start of 2020 to $12bn a year later. Byju’s, an Indian peer, soared to a private valuation of $22bn in March 2022 as it snapped up other providers across the world. Global venture-capital investment in education-related startups jumped from $7bn in 2019 to $20bn in 2021, according to Crunchbase, a data provider.Then, once covid was brought to heel, classes resumed much as before. By the end of 2022 Chegg’s market value had slumped back to $3bn. Early last year investment firms including BlackRock and Prosus started marking down the value of their stakes in Byju’s as its losses mounted. “In hindsight we grew a bit too big a bit too fast,” admits Divya Gokulnath, the company’s co-founder.If the pandemic couldn’t overcome the education sector’s resistance to digital disruption, can artificial intelligence? ChatGPT-like generative AI, which can converse cleverly on a wide variety of subjects, certainly looks the part. So much so that educationalists began to panic that students would use it to cheat on essays and homework. In January 2023 New York City banned ChatGPT from public schools. Increasingly, however, it is generating excitement as a means to provide personalised tutoring to students and speed up tedious tasks such as marking. By May New York had let the bot back into classrooms.Learners, for their part, are embracing the technology. Two-fifths of undergraduates surveyed last year by Chegg reported using an AI chatbot to help them with their studies, with half of those using it daily. Indeed, the technology’s popularity has raised awkward questions for companies like Chegg, whose share price plunged last May after Dan Rosensweig, its chief executive, told investors it was losing customers to ChatGPT. Yet there are good reasons to believe that education specialists who harness AI will eventually prevail over generalists such as OpenAI, the maker of ChatGPT, and other tech firms eyeing the education business.For one, AI chatbots have a bad habit of spouting nonsense, an unhelpful trait in an educational context. “Students want content from trusted providers,” argues Kate Edwards, chief pedagogist at Pearson, a textbook publisher. The company has not allowed ChatGPT and other AIs to ingest its material, but has instead used the content to train its own models, which it is embedding into its suite of learning apps. Rivals including McGraw Hill are taking a similar approach. Chegg has likewise developed its own AI bot that it has trained on its ample dataset of questions and answers.What is more, as Chegg’s Mr Rosensweig argues, teaching is not merely about giving students an answer, but about presenting it in a way that helps them learn. Understanding pedagogy thus gives education specialists an edge. Pearson has designed its AI tools to engage students by breaking complex topics down, testing their understanding and providing quick feedback, says Ms Edwards. Byju’s is incorporating “forgetting curves” for students into the design of its AI tutoring tools, refreshing their memories at personalised intervals. Chatbots must also be tailored to different age groups, to avoid either bamboozling or infantilising students.Specialists that have already forged relationships with risk-averse educational institutions will have the added advantage of being able to embed AI into otherwise familiar products. Anthology, a maker of education software, has incorporated generative-AI features into its Blackboard Learn program to help teachers speedily create course outlines, rubrics and tests. Established suppliers are also better placed to instruct teachers on how to make use of AI’s capabilities.AI for effortBringing AI to education will not be easy. Although teachers have endured a covid-induced crash course in education technology, many are still behind the learning curve. Less than a fifth of British educators surveyed by Pearson last year reported receiving training on digital learning tools. Tight budgets at many institutions will make selling new technology an uphill battle. AI sceptics will have to be won over, and new AI-powered tools may be needed to catch AI-powered cheating. Thorny questions will inevitably arise as to what all this means for the jobs of teachers: their attention may need to shift towards motivating students and instructing them on how to best work with AI tools. “We owe the industry answers on how to harness this technology,” declares Bruce Dahlgren, boss of Anthology.If those answers can be provided, it is not just companies like Mr Dahlgren’s that stand to benefit. An influential paper from 1984 by Benjamin Bloom, an educational psychologist, found that one-to-one tutoring both improved the average academic performance of students and reduced the variance between them. AI could at last make individual tutors viable for the many. With the learning of students, especially those from poorer households, set back by the upheaval of the pandemic, such a development would certainly deserve top marks. ■Read more from Schumpeter, our columnist on global business:Meet the shrewdest operators in today’s oil markets (Jan 3rd)Can anyone bar Europe do luxury? (Dec 20th)Boneheaded anti-immigration politicians are throttling globalisation (Dec 14th)Also: If you want to write directly to Schumpeter, email him at [email protected]. And here is an explanation of how the Schumpeter column got its name.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Technology stocks are having a bumper year. Despite a recent wobble, the share price of the Big Five—Alphabet, Amazon, Apple, Meta and Microsoft—has jumped by 60% since January, when measured in an equally weighted basket. The price of shares in one big chipmaker,  Nvidia, has tripled and in another, AMD, almost doubled. Their price-to-earnings ratios (which measure how much the markets think a company is worth relative to its profits) are ten times that of the median firm in the s&p 500.The main reason for the surge is the promise of artificial intelligence (AI). Since the launch in November of Chatgpt, an AI-powered chatbot, investors have grown ever more excited about a new wave of technology that can create human-like content, from poems and video footage to lines of code. This “generative AI” relies on large language models which are trained on big chunks of the internet. Many think the technology could reshape whole industries, and have as much impact on business and society as smartphones or cloud computing. Firms that can make the best use of the technology, the thinking goes, will be able to expand profit margins and gain market share.Corporate bosses are at pains to demonstrate how they are adopting AI. On April 4th Jamie Dimon, JPMorgan Chase’s boss, said his bank had 600 machine-learning engineers and had put AI to work on more than 300 different internal applications. David Ricks, the boss of Eli Lilly,  has said that the pharmaceutical giant has more than 100 projects on the go using ai.Company case studies reveal only part of the picture. To get a broader sense of which companies and industries are adopting ai The Economist examined data on all the firms in the s&p 500. We looked at five measures: the share of issued patents that mention AI; venture-capital (VC) activity targeting AI firms; acquisitions of AI firms; job listings citing AI; and mentions of the technology on earnings calls. Because other types of ai could bring benefits for business, our analysis captures activity for all AI, not just the generative wave. The results show that even beyond tech firms the interest in AI is  growing fast. Moreover, clear leaders and laggards are already emerging.AI expertise already seems to be spreading (see chart). About two-thirds of the firms in our universe have placed a job ad mentioning AI skills in the past three years, says PredictLeads, a research firm.  Of those that did, today 5.3% of their listed vacancies mention ai, up from a three-year average of 2.5%.  In some industries the rise is more dramatic. In retail firms that share has jumped from 3% to 11%, while among chipmakers that proportion grew from 9% to 19%.The number of AI-related patents being registered trended upwards between 2020 and 2022, according to data provided by Amit Seru of Stanford University. PitchBook, another research firm, concludes that in 2023 some 25% of venture deals by s&p 500 firms involved ai startups, up from 19% in 2021. GlobalData, also a research firm, finds that about half the firms scrutinised have talked about AI in earnings calls since 2021, and that in the first quarter of this year the number of times AI was mentioned in the earnings calls of America Inc more than doubled compared with the previous quarter. Roughly half have been granted a patent relating to the technology between 2020 and 2022.The use of generative ai may eventually become even more common than other sorts of ai. That is because it is good at lots of tasks essential to running a firm. A report by McKinsey, a consultancy, argues that three-quarters of the expected value created by generative AI will come in four business functions—research and development, software engineering, marketing and customer service. To some extent, all these operations are at the core of most big businesses. Moreover, any large company with internal databases used to guide employees could find a use for an AI-powered chatbot. Morgan Stanley, a bank, is building an AI assistant that will help its wealth managers find and summarise answers from a huge internal database. SLB, an oil-services company, has built a similar assistant to help service engineers.While the adoption of ai is happening in many firms, some are more enthusiastic than others. Ranking all the companies  using each metric and then taking an average produces a simple scoring system. Those at the top seem to be winning over investors. Since the start of the year, the median share price of the top 100 has risen by 11%; for the lowest-scoring quintile it has not moved at all.The top spots are unsurprisingly dominated by Silicon Valley. On a broad definition, the s&p 500 contains 82 tech firms. Almost 50 of them make the top 100. Nvidia is the highest-scoring firm. According to data from PredictLeads, over the past three years a third of its job listings have mentioned AI. In the past year the firm has mentioned AI in its earnings calls almost 200 times, more than any other company. Other high-ranking tech firms include the cloud-computing giants—Alphabet (3rd), Microsoft (12th) and Amazon (34th). They sell access to a range of AI tools, from services that help train sophisticated models to software that allows the use of ai without having to write reams of code.Beyond tech, two types of firms seem to be adopting ai the quickest. One is data-intensive industries, such as insurers, financial-services firms and pharmaceutical companies. They account for about a quarter of our top 100. These firms tend to have lots of structured datasets, such as loan books or patient files, which makes it easier to use AI, notes Ali Ghodsi of Databricks, a database firm. Around a tenth of JPMorgan Chase’s current job listings mention AI. The firm recently filed a patent for Indexgpt, an AI-infused chatbot that gives investment advice. Health-care firms like Gilead Sciences and Moderna use AI to discover new drugs. Others, such as Abbott and Align Technology, are building AI-powered medical devices. America’s Food and Drug Administration approved 97 such machines last year, up from 26 in 2017.A second group is industries that are already being disrupted by technology, including carmaking, telecoms, media and retail. Thirteen firms from these industries make the high-scoring 100, including Ford, General Motors and Tesla. The rise of electric vehicles and the prospect of self-driving cars has encouraged vehicle manufacturers to invest in technology. In March Ford established Latitude AI, a self-driving car subsidiary that might one day rival gm’s Cruise. In April Elon Musk told analysts that Tesla was buying specialised AI chips and was “very focused” on improving their AI capabilities in an effort to improve his firm’s self-driving efforts.Retailers are using AI to bolster their core business. Nike, a sportswear giant, filed an application for a patent in 2021 for a system that can generate three-dimensional computer models of trainers. Christian Kleinerman of Snowflake, a database provider, notes that retailers are also taking advantage of the growth of e-commerce by collecting more data on customers. That allows more accurate targeting of marketing campaigns. Some may take personalisation a step further. In 2021 Procter & Gamble, a consumer-goods giant, applied for a patent for an AI-based system which analyses users’ skin and hair conditions based on photos, and recommends products to treat them.One source of variation in ai use across industries may be a result of the type of work undertaken. A working paper led by Andrea Eisfeldt of the University of California looked at how exposed firms are to ai. The researchers assessed which tasks took place in a firm and how well Chatgpt could perform them. The most exposed were tech firms, largely because AI chatbots are good at coding. Those industries least exposed, such as agriculture and construction, tended to rely on manual labour.Clear leaders and laggards are emerging within industries, too. About 70 firms in the s&p 500 show no sign on any of our metrics of focusing on AI. That includes firms in AI-heavy industries, such as insurers. The mass of smaller firms not included in the s&p 500 may be even less keen. One distinguishing factor within industries may be investment. For the top 100 firms in our ranking, the median r&d expenditure as a share of revenue was 11%. For those in the lowest 100 it was zero.Vlad Lukic of BCG, a consultancy, notes that there is even a lot of variation within companies. He recalls visiting two divisions of the same medium-sized multinational. One had no experience working with ai. The other was advanced; it had been using a pilot version of the technology from Openai, the startup behind ChatGPT, for two years.Among early adopters, many non-tech companies’ AI use is growing more sophisticated. Mr Seru’s data reveal that about 80 non-tech firms have had ai-related patents issued which were cited by another patent, suggesting that they have some technological value. Some 45 non-tech companies in the s&p 500 have recently placed ads which mention model training, including Boeing, United Health and State Street. That suggests they may be building their own models rather than using off-the-shelf technology from the likes of Openai. The advantage of this approach is that it can produce more-accurate ai, giving a greater edge over rivals.However, a shift to in-house training hints at one of the risks: security. In May Samsung discovered that staff had uploaded sensitive code to Chatgpt. The concern is that this information may be stored on external servers of the firms which run the models, such as Microsoft and Alphabet. Now Samsung is said to be training its own models. The firm also joined the growing list of companies that have banned or limited the use of Chatgpt, which includes Apple and JPMorgan Chase.Other risks abound. Model-makers, including Openai, are being sued for violating copyright laws over their use of internet data to train their models. Some large corporations think that they could be left liable if they use Openai’s technology. Moreover, models are prone to make up information. In one incident a New York lawyer used Chatgpt to write a motion. The chatbot included fictional case-law and the lawyer was fined by the court.But all this must be weighed against the potential benefits, which could be vast. Waves of technology frequently turn industries on their head. As generative ai diffuses into the economy, it is not hard to imagine it doing the same thing. Mr Lukic says that the biggest risk for companies may be falling behind. Judged by the scramble in America Inc for all things AI, many bosses and investors would agree. ■Correction: This piece originally claimed that Starbucks was using AI to make the perfect vegan breakfast sandwich. The ring of truth made what was, in fact, an April fools joke so believable. We have removed the reference. Our apologies for the clanger.To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Politics is SUPPOSED to be about persuasion; but it has always been stalked by propaganda. Campaigners dissemble, exaggerate and fib. They transmit lies, ranging from bald-faced to white, through whatever means are available. Anti-vaccine conspiracies were once propagated through pamphlets instead of podcasts. A century before covid-19, anti-maskers in the era of Spanish flu waged a disinformation campaign. They sent fake messages from the surgeon-general via telegram (the wires, not the smartphone app). Because people are not angels, elections have never been free from falsehoods and mistaken beliefs.But as the world contemplates a series of votes in 2024, something new is causing a lot of worry. In the past, disinformation has always been created by humans. Advances in generative artificial intelligence (AI)—with models that can spit out sophisticated essays and create realistic images from text prompts—make synthetic propaganda possible. The fear is that disinformation campaigns may be supercharged in 2024, just as countries with a collective population of some 4bn—including America, Britain, India, Indonesia, Mexico and Taiwan—prepare to vote. How worried should their citizens be?It is important to be precise about what generative-AI tools like ChatGPT do and do not change. Before they came along, disinformation was already a problem in democracies. The corrosive idea that America’s presidential election in 2020 was rigged brought rioters to the Capitol on January 6th—but it was spread by Donald Trump, Republican elites and conservative mass-media outlets using conventional means. Activists for the BJP in India spread rumours via WhatsApp threads. Propagandists for the Chinese Communist Party transmit talking points to Taiwan through seemingly legitimate news outfits. All of this is done without using generative-AI tools.What could large-language models change in 2024? One thing is the quantity of disinformation: if the volume of nonsense were multiplied by 1,000 or 100,000, it might persuade people to vote differently. A second concerns quality. Hyper-realistic deepfakes could sway voters before false audio, photos and videos could be debunked. A third is microtargeting. With ai, voters may be inundated with highly personalised propaganda at scale. Networks of propaganda bots could be made harder to detect than existing disinformation efforts are. Voters’ trust in their fellow citizens, which in America has been declining for decades, may well suffer as people began to doubt everything.This is worrying, but there are reasons to believe AI is not about to wreck humanity’s 2,500-year-old experiment with democracy. Many people think that others are more gullible than they themselves are. In fact, voters are hard to persuade, especially on salient political issues such as whom they want to be president. (Ask yourself what deepfake would change your choice between Joe Biden and Mr Trump.) The multi-billion-dollar campaign industry in America that uses humans to persuade voters can generate only minute changes in their behaviour.Tools to produce believable fake images and text have existed for decades. Although generative AI might be a labour-saving technology for internet troll farms, it is not clear that effort was the binding constraint in the production of disinformation. New image-generation algorithms are impressive, but without tuning and human judgment they are still prone to produce pictures of people with six fingers on each hand, making the possibility of personalised deepfakes remote for the time being. Even if these AI-augmented tactics were to prove effective, they would soon be adopted by many interested parties: the cumulative effect of these influence operations would be to make social networks even more cacophonous and unusable. It is hard to prove that mistrust translates into a systematic advantage for one party over the other.Social-media platforms, where misinformation spreads, and AI firms say they are focused on the risks. OpenAI, the company behind ChatGPT, says it will monitor usage to try to detect political-influence operations. Big-tech platforms, criticised both for propagating disinformation in the 2016 election and taking down too much in 2020, have become better at identifying suspicious accounts (though they have become loth to arbitrate the truthfulness of content generated by real people). Alphabet and Meta ban the use of manipulated media in political advertising and say they are quick to respond to deepfakes.  Other companies are trying to craft a technological standard establishing the provenance of real images and videos.Voluntary regulation has limits, however, and the involuntary sort poses risks. Open-source models, like Meta’s Llama, which generates text, and Stable Diffusion, which makes images, can be used without oversight. And not all platforms are created equal—TikTok, the video-sharing social-media company, has ties to China’s government, and the app is designed to promote virality from any source, including new accounts. Twitter (which is now called X) cut its oversight team after it was bought by Elon Musk, and the platform is a haven for bots. The agency regulating elections in America is considering a disclosure requirement for campaigns using synthetically generated images. This is sensible, though malicious actors will not comply with it. Some in America are calling for a Chinese-style system of extreme regulation. There, AI algorithms must be registered with a government body and somehow embody core socialist values. Such heavy-handed control would erode the advantage America has in AI innovation.Politics was never pureTechnological determinism, which pins all the foibles of people on the tools they use, is tempting. But it is also wrong. Although it is important to be mindful of the potential of generative AI to disrupt democracies, panic is unwarranted. Before the technological advances of the past two years, people were quite capable of transmitting all manner of destructive and terrible ideas to one another. The American presidential campaign of 2024 will be marred by disinformation about the rule of law and the integrity of elections. But its progenitor will not be something newfangled like ChatGPT. It will be Mr Trump. ■For subscribers only: to see how we design each week’s cover, sign up to our weekly Cover Story newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.GET READY for some big British celebrations in 2030. By then, if Rishi Sunak is to be believed, the country will be “a science and technology superpower”. The prime minister’s aim is for Britain to prosper from the booming opportunities offered by supercomputing and artificial intelligence. Generative AI has stoked a frenzy of excitement (and some fear) among techies and investors; now politicians have started to acclaim its potential, and British ones are in the vanguard. Britain, says Mr Sunak, will harness AI and thus spur productivity, economic growth and more. As he told an audience in London this week, he sees the “extraordinary potential of AI to improve people’s lives”.Mr Sunak’s vim and his readiness to champion AI are welcome, even if his claims sound breathless. After all, Britain’s government has spurred innovation that had sweeping economic effects—think of the Big Bang reforms in the 1980s that turned London into Europe’s financial hub. There is every reason to believe a new AI era will create huge opportunities . He is right to plan for how to make the most of these chances. But could Britain, realistically, lead on this?The country does have some advantages. It is home to several important AI companies, mostly in London—in particular, Google DeepMind. It has excellent universities, and welcomes the highly skilled foreign workers that AI companies need. The state generates troves of data; no other country has such an array of health records under a single entity, the National Health Service (NHS). And Brexit creates a chance to adopt an appealing regulatory position that could be a model for medium-sized countries around the world as they also rush to join the AI party.But there are problems aplenty, too. The most obvious is that Britain is a smallish place. America’s dominance in tech exerts a steady pull on capital, people and ideas, and American firms duly dominate in AI. The way Brexit was done means that Britain has lost access to the European Union’s single market. Although Oracle has a cluster of the advanced graphics processing units (GPUs) needed to train large models, none of the cloud-computing giants has yet chosen Britain as the base for what techies call the “compute”.For Britain to prosper in AI, therefore, much will have to change. It needs to cram more people who know one end of a GPU from the other into positions of influence in government. Mr Sunak may loudly extol the promise of BritGPT, but his government should include more engineers who understand the mix of data and compute from which AI is built.Time to chatGPUOnce it has the expertise, the government must deal with three broad concerns. The first is about those public datasets. They are in no fit state for AI developers to exploit—the data are unrefined ore, not sparkling treasure. Only the state has the authority to get these datasets cleaned up, and to start thinking of what new ones could be built. A stock of clean, regularly updated datasets that are technically and legally easy for algorithm-makers to use would draw in engineers who want to build new AI systems. An AI-ready NHS would be the jewel in Britain’s crown.Second, Britain should move fast to gain an edge in regulation. The goal should be a pragmatic set of rules keeping AI safe that sits somewhere between America’s Wild West permissiveness and what is likely to be a regulatory warren in the EU. Mr Sunak announced at a White House meeting with President Joe Biden this month that he will host a global summit on AI regulation this autumn. Good. That will be the place and time to set out Britain’s stall as having rules for AI that are sufficiently flexible to work for different industries. Hairdressers who want AI to help pick new styles, for example, need not be regulated in the same way as mortgage lenders.The last and thorniest concern is how to get AI developers the compute they need to train and run large models. Advanced GPUs made by Nvidia—for now, the only chips worth using—are suffering a global supply crunch. The government could help by telling British companies and its own departments to be much readier than now to send their data abroad to AI developers in other, friendly countries. For most datasets, worries about privacy and security are overdone.However, an unfortunate correlation exists between the sensitivity of datasets and their value in creating large models; data that are sensitive, that capture aspects of either human health or behaviour, or pertain to national security, are what would be most useful to inform new models. There is an understandable reluctance to send such data abroad.That is why Britain urgently needs more GPU clusters within its borders. More compute on British soil will have all sorts of local spin-offs and benefits. Jeremy Hunt, the chancellor, has talked of giving academic computer scientists £900m ($1.1bn) to build a British supercomputer in Edinburgh.Yet commercial AI is so dynamic that the Edinburgh scheme risks becoming an AI white elephant. Amazon alone spends around $25bn a year on compute. British taxpayers cannot keep up with the private sector—and should not try. Instead the government should do all it can to persuade commercial providers to invest in GPU clusters on British soil.One focus should be to ensure a reliable supply of clean, affordable power. To train models needs mind-boggling quantities of electricity. If Britain is without cheap supplies of power, it will struggle to persuade anyone to set up big GPU centres there. The queue to obtain a connection to Britain’s grid is holding back potential investors across the economy.Other steps could include using public money to fund a “moonshot” project, such as developing open-source software, to help chipmakers break the near-monopoly that Nvidia holds on the AI market. Nvidia’s edge comes from clever software which makes training models on its GPUs a breeze. Rival chipmakers in America and Britain have no equivalent and are all but locked out of the AI market. With better software their chips could compete with Nvidia’s and ease the supply crunch that dogs AI developers the world over. That’s a worthy ambition for a country seeking global AI greatness. ■For subscribers only: to see how we design each week’s cover, sign up to our weekly Cover Story newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Waves OF INNOVATION often create giants. Microsoft rode the upsurge in desktop computers, as Apple did with the smartphone. Artificial intelligence (AI) may well be the next big technological shift, transforming the way businesses are run and society functions. If so, plenty of firms selling the software and hardware that underpin AI stand to gain. But none is better positioned than Nvidia, an American firm that makes specialist AI chips. Its market value briefly passed $1trn this week. Will AI sweep Nvidia to big tech-dom?The hype around AI makes the question hard to answer. Excitement about Nvidia began to mount in November, after the release of ChatGPT, an AI-powered chatbot. Since then all manner of firms have launched AI-infused products, adding to the fervour. Jensen Huang, Nvidia’s boss, is unsurprisingly bullish, talking of a “new computing era”. Investors seem just as jubilant. Nvidia’s share price has more than doubled since the start of the year.Much of the excitement is justified. Nvidia is in an enviable position. Its core business is designing high-performance chips. At first it sold these to video-game enthusiasts. The chips were also highly efficient at training AI models, and a new, booming market emerged. But the firm has not just been lucky. With each generation of new chips, it has improved performance many times over. Today it holds over 80% of the market in specialist AI chips.Nvidia also had the forethought to invest in two areas that helped cement its supremacy. One is advanced networking. Because training AI models requires vast amounts of processing power, many chips—sometimes thousands—are used simultaneously. These chips exchange data along a high-performance, AI-tailored network. Today Nvidia controls 78% of that market, thanks to its purchase of Mellanox, a specialist, in 2019.Nvidia’s other strength is its software. CUDA, its ai platform, is popular with programmers and runs only on the company’s chips. By, for instance, giving free access to its chips and software to some AI researchers, the firm focused on encouraging developers to use its software long before its competitors set out to woo them.Despite all these advantages, however, Nvidia’s lasting dominance is not assured. For a start, some of the frenzy around ai may die down. The juicier the firm’s prospects, the more competitors it will attract. Startups and big chipmakers, such as AMD and Intel, want a share of Nvidia’s network and chip businesses. Others are working on open-source and proprietary software that may weaken CUDA’s hold. The biggest challenge, though, may come from Nvidia’s own customers. The cloud-computing arms of both Amazon and Alphabet are designing their own AI-tailored chips. Both have the scale and the deep pockets to become fearsome rivals.Governments also pose a risk. Regulators worried about the dangers AI poses to society and national security are searching for ways to control the technology. Last year America restricted the sale of high-performance chips and chipmaking tools to some Chinese firms, which dented Nvidia’s sales in the third quarter. If Nvidia is dominant, politicians will find it easier to act.Still, for now the future looks bright. Even if ai mania cools, the technology is bound to be more useful than crypto, another craze that Nvidia cashed in on. Regulation may crimp growth, but is unlikely to kill it. And none of Nvidia’s rivals is yet offering ai products that bundle together software, chips and networking. Nvidia’s chief advantage lies in its ability to package these up and create an attractive ecosystem. That sounds a lot like Microsoft and Apple. ■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.What has been achieved on this video call? It takes Jared Spataro just a few clicks to find out. Microsoft’s head of productivity software pulls up a sidebar in Teams, a video-conferencing service. A 30-second pause ensues as an artificial-intelligence (AI) model somewhere in one of the firm’s data centres analyses a recording of the meeting so far. An accurate summary of your correspondent’s questions and Mr Spataro’s answers then pops up. Mr Spataro can barely contain his excitement. “This is not your daddy’s AI,” he beams.Teams is not the only product into which Microsoft is implanting machine intelligence. On March 16th the company announced that almost all its productivity software, including Word and Excel, were getting the same treatment. Days earlier, Alphabet, Google’s parent company, unveiled a similar upgrade for its productivity products, such as Gmail and Sheets.Announcements like these have come thick and fast from America’s tech titans in the past month or so. OpenAI, the startup part-owned by Microsoft that created ChatGPT, a hit AI conversationalist, released GPT-4, a new super-powerful AI. Amazon Web Services (AWS), the e-commerce giant’s cloud-computing arm, has said it will expand a partnership with Hugging Face, another AI startup. Apple is reportedly testing new AIs across its products, including Siri, its virtual assistant. Mark Zuckerberg, boss of Meta, said he wants to “turbocharge” his social networks with AI. Adding to its productivity tools, on March 21st Google launched its own AI chatbot to rival ChatGPT, called Bard.The flurry of activity is the result of a new wave of AI models, which are rapidly making their way from the lab to the real world. Progress is so rapid, in fact, that on March 29th an open letter signed by more than 1,000 tech luminaries called for a six-month pause in work on models more advanced than GPT-4. Whether or not such a moratorium is put in place, big tech is taking no chances. All five giants claim to be laser-focused on AI. What that means for each in practice differs. But two things are already clear. The race for AI is heating up. And even before a winner emerges, the contest is changing the way that big tech deploys the technology.AI is not new to tech’s titans. Amazon’s founder, Jeff Bezos, quizzed his teams in 2014 on how they planned to embed it into products. Two years later Sundar Pichai, Alphabet’s boss, started to describe his firm as an “AI-first company”. The technology underpins how Amazon sells and delivers its products, Google finds stuff on the internet, Apple imparts smarts on Siri, Microsoft helps clients manage data and Meta serves up adverts.The new GPT-4-like “generative” AI models nevertheless seem a turning point. Their promise became clear in November with the release of ChatGPT, with its human-like ability to generate everything from travel plans to poems. What makes such AIs generative is “large language models”. These analyse content on the internet and, in response to a request from a user, predict the next word, brushstroke or note in a sentence, image or tune. Many technologists believe they mark a “platform shift”. AI will, on this view, become a layer of technology on top of which all manner of software can be built. Comparisons abound to the advent of the internet, the smartphone and cloud computing.The tech giants have all they need—data, computing power, billions of users—to thrive in the age of AI. They also recall the fate of one-time Goliaths, from Kodak to BlackBerry, that missed earlier platform shifts, only to sink into bankruptcy or irrelevance. Their response is a deluge of investments. In 2022, amid a tech-led stockmarket rout, the big five poured $223bn into research and development (R&D), up from $109bn in 2019 (see chart 1). That was on top of $161bn in capital spending, a figure that also doubled in three years. All told, this was equal to 26% of their combined sales last year, up from 16% in 2015.Not all of this went into cutting-edge technologies; a chunk was spent on prosaic fare, such as warehouses, office buildings and data centres. But a slug of such spending always ends up in the tech firms’ big bets on the future. Today, the wager of choice is AI. And the companies aren’t shy about it. Mr Zuckerberg recently said AI was his firm’s biggest investment category. In its next quarterly earnings report in April, Alphabet plans to reveal the size of its AI investment for the first time.To tease out exactly how the companies are betting on AI, and how big these bets are, The Economist has analysed data on their investments, acquisitions, job postings, patents, research papers and employees’ LinkedIn profiles. The examination reveals serious resources being put into the technology. According to data from PitchBook, a research firm, around a fifth of the companies’ combined acquisitions and investments since 2019 involved AI firms—considerably more than the share targeting cryptocurrencies, blockchains and other decentralised “Web3” endeavours (2%), or the virtual-reality metaverse (6%), two other recent tech fads. According to numbers from PredictLeads, another research firm, about a tenth of big tech’s job listings require AI skills. Roughly the same share of big tech employees’ LinkedIn profiles say that they work in the field.These averages conceal big differences between the five tech giants, however. On our measures, Microsoft and Alphabet appear to be racing ahead, with Meta snapping at their heels. As interesting is where the five are deciding to focus their efforts.Consider their equity investments, starting with those that aren’t outright takeovers. In the past four years big tech has taken stakes in 200-odd firms in all. The investments in AI companies are accelerating. Since the start of 2022, the big five have together made roughly one investment a month in AI specialists, three times the rate of the preceding three years.Microsoft leads the way. One in three of its deals has involved AI-related firms. That is twice the share at Amazon and Alphabet (one of whose venture-capital arms, Gradient Ventures, invests exclusively in AI startups and has backed almost 200 since 2019). It is more than six times that of Meta, and infinitely more than Apple, which has made no such investments. Microsoft’s biggest bet is on OpenAI, whose technology lies behind the giant’s new productivity features and powers a souped-up version of its Bing search engine. The $11bn that Microsoft has reportedly put into OpenAI would, at the startup’s latest rumoured valuation of $29bn, give the software giant a stake of 38%. Microsoft’s other notable investments include D-Matrix, a firm that makes AI technology for data centres, and Noble.AI, which uses algorithms to streamline lab work and other R&D projects.Microsoft is also a keen acquirer of whole AI startups; nearly a quarter of its acquisition targets, such as Nuance, which develops speech recognition for health care, work in the area. That is a similar share to Meta, which prefers takeovers to piecemeal investments. As with equity stakes, AI’s share of Alphabet acquisitions have lagged behind Microsoft’s since 2019 (see chart 2). But these, plus its equity stakes, are shoring up a formidable AI edifice, one of whose pillars is DeepMind, a London-based AI lab that Google bought in 2014. DeepMind has been behind some big advances in the field, such as AlphaFold, a system to predict the shape of proteins, a task that has stumped scientists for years and is critical to drug discovery.The most single-minded AI acquirer is Apple. Nearly half its buy-out targets are AI-related. They range from AI.Music, which composes new tunes, to Credit Kudos, which uses AI to assess the creditworthiness of loan applicants. Apple’s acquisitions have historically been small, notes Wasmi Mohan of Bank of America, but tend to be quickly folded into products.As with investments, big tech’s AI hiring, too, is growing (see chart 3). Jobs listed by Google, Meta and Microsoft today are likelier to require AI expertise than in the past three years. Since 2019, 23% of Alphabet’s listings have been AI-related. Meta came second, at 8%. Today the figures are 27% and 18%, respectively. According to data from LinkedIn, one in four Alphabet employees mention AI skills on their profile—similar to Meta and a touch ahead of Microsoft (Apple and Amazon lag far behind). Greg Selker of Stanton Chase, an executive-search firm, observes that demand for AI talent remains red-hot, despite big tech’s recent lay-offs.The AI boffins aren’t twiddling their thumbs. Zeta Alpha, a firm which tracks AI research, looks at the number of published papers in which at least one of the authors works for a given company. Between 2020 and 2022, Alphabet published about 9,000 AI papers, more than any other corporate or academic institution. Microsoft racked up around 8,000 and Meta 4,000 or so.Meta, in particular, is gaining a reputation for being less tight-lipped about its work than fellow tech giants. Its AI-software library, called PyTorch, has been available to anyone for a while; since February researchers can freely use its large language model, LLaMA, the details of whose training and biases are also public. All this, says Joelle Pineau, who heads Meta’s open-research programme, helps it attract the brightest minds (who often make their move to the private sector conditional on a continued ability to share the fruits of their labours with the world).If you adjust Meta’s research output for its revenues and headcount, which are much smaller than Alphabet’s or Microsoft’s, and only consider the most-cited papers, Mr Zuckerberg’s firm tops the research league-table. And, points out Ajay Agrawal of the University of Toronto, openness brings two benefits besides luring the best brains. Low-cost AI can make it cheaper for creators to make content, including texts and videos, that draw more eyes to Meta’s social networks. And it could dent the business of Alphabet, Amazon and Microsoft, which are all trying to sell AI models through their cloud platforms.The AI frenzy is, then, in full swing among tech’s mightiest firms. And their AI bets are already beginning to pay off: by making their own operations more efficient (Microsoft’s finance department, which uses AI to automate 70-80% of its 90m-odd annual invoice approvals, now asks a generative-AI chatbot to flag dodgy-looking bills for a human to inspect); and by finding their way into products at a pace that seems faster than for many earlier technological breakthroughs.Barely four months after ChatGPT captured the world’s imagination, Microsoft and Google have introduced the new-look Bing, Bard and their AI-assisted productivity programs. Alphabet and Meta offer a tool that generates ad campaigns based on advertisers’ objectives, such as boosting sales or winning more customers. Microsoft is making OpenAI’s technology available to customers of its Azure cloud platform. Thanks to partnerships with model-makers such as Cohere and Anthropic, AWS users can tap more than 30 large language models. Google, too, is wooing model-builders and other AI firms to its cloud with $250,000-worth of free computing power in the first year, a more generous bargain than it offers to non-AI startups. It may not be long before AI.Music and Credit Kudos appear in Apple’s music-streaming service and financial offering, or an Amazon chatbot recommends purchases uncannily matched to shoppers’ desires.If the platform-shift thesis is right, the tech giants could yet be upset by newcomers, rather as they upset big tech of yore. The mass of resources they are ploughing into the technology reflects a desire to avoid that fate. Whether or not they succeed, one thing is certain: these are just the modest beginnings of the AI revolution. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.“By amplifying human intelligence, AI may cause a new Renaissance, perhaps a new phase of the Enlightenment,” Yann LeCun, one of the godfathers of modern artificial intelligence (AI), suggested earlier this year.  AI can already make some existing scientific processes faster and more efficient, but can it do more, by transforming the way science itself is done?Such transformations have happened before. With the emergence of the scientific method in the 17th century, researchers came to trust experimental observations, and the theories they derived from them, over the received wisdom of antiquity. This process was, crucially, supported by the advent of scientific journals, which let researchers share their findings, both to claim priority and to encourage others to replicate and build on their results. Journals created an international scientific community around a shared body of knowledge, causing a surge in discovery known today as the scientific revolution.A further transformation began in the late 19th century, with the establishment of research laboratories—factories of innovation where ideas, people and materials could be combined on an industrial scale. This led to a further outpouring of innovation, from chemicals and semiconductors to pharmaceuticals. These shifts did more than just increase scientific productivity. They also transformed science itself, opening up new realms of research and discovery. How might AI do something similar, not just generating new results, but new ways to generate new results?A promising approach is “literature-based discovery” (LBD) which, as its name suggests, aims to make new discoveries by analysing scientific literature. The first LBD system, built by Don Swanson at the University of Chicago in the 1980s, looked for novel connections in MEDLINE, a database of medical journals. In an early success, it put together two separate observations—that Raynaud’s disease, a circulatory disorder, was related to blood viscosity, and that fish oil reduced blood viscosity—and suggested that fish oil might therefore be a useful treatment. This hypothesis was then experimentally verified.We’re charging our batteryBut Dr Swanson’s LBD system failed to catch on outside the AI community at the time. Today AI systems have become far more capable at natural-language processing and have a much larger corpus of scientific literature to chew on. Interest in LBD-style approaches is now growing in other fields, notably materials science.In 2019, for example, a group of researchers led by Vahe Tshitoyan, then at Lawrence Berkeley National Laboratory, in America, used an AI technique called unsupervised learning to analyse the abstracts of materials-science papers, and extract information about the properties of different materials into mathematical representations called “word embeddings”. These place concepts into a multi-dimensional space where similar concepts are grouped together. The system thereby gained a  “chemical intuition” so that it could, for example, suggest materials with similar properties to another material. The AI was then asked to suggest materials that might have thermoelectric properties (the ability to turn a temperature difference into an electrical voltage, and vice versa), even though they were not identified as such in the literature. The ten most promising candidate materials were selected, and experimental testing found that all ten did indeed display unusually strong thermoelectric properties.The researchers then retrained their system, omitting papers from more recent years, and asked it to predict which new thermoelectric materials would be discovered in those later years. The system was eight times more accurate at predicting such discoveries than would be expected by chance alone. It could also make accurate discovery predictions using other terms, such as “photovoltaic”. The researchers concluded that “such language-based inference methods can become an entirely new field of research at the intersection between natural-language processing and science.”A paper by Jamshid Sourati and James Evans, both sociologists at the University of Chicago, published this year in Nature Human Behaviour, extends this approach in a novel way. It starts with the observation that LBD systems tend to focus on concepts within papers, and ignore their authors. So they trained an LBD system to take account of both. The resulting system was twice as good at forecasting new discoveries in materials science than the one built by Dr Tshitoyan’s team, and could also predict the actual discoverers with more than 40% accuracy. But the researchers then went one step further. Instead of following the crowd and predicting where researchers would make new discoveries, they asked their model to avoid the crowd, and identify “alien” hypotheses that are scientifically plausible, but unlikely, in the normal course of things, to be discovered in the near future. The system can thus, the researchers argue, both accelerate near-term discoveries, and probe “blind spots” where new discoveries await.As well as suggesting new hypotheses to investigate, LBD systems that take authorship into account can also suggest potential collaborators who may not know one other. This approach could be particularly effective when identifying scientists who work in different fields, bridging complementary areas of research. Cross-disciplinary research collaborations “will go from being rarities to being more commonplace” when mediated by AI, says Yolanda Gil, a computer scientist at the University of Southern California. And as LBD systems are extended so that they can handle tables, charts and data such as gene sequences and programming code, they will become more capable. In future, researchers might come to rely on such systems to monitor the deluge of new scientific papers, highlight relevant results, suggest novel hypotheses for research—and even link them up with potential research partners, like a scientific matchmaking service. AI tools could thus extend and transform the existing, centuries-old infrastructure of scientific publishing.We’re full of energyIf LBD promises to supercharge the journal with AI, “robot scientists”, or “self-driving labs”, promise to do the same for the laboratory. These machines go beyond existing forms of laboratory automation, such as drug-screening platforms. Instead, they are given background knowledge about a particular area of research, in the form of data, research papers and patents. They then use AI to form hypotheses, carry out experiments using robots, assess the results, modify their hypotheses, and repeat the cycle. Adam, a machine built at Aberystwyth University in Wales in 2009, did experiments on the relationship between genes and enzymes in yeast metabolism, and was the first machine to discover novel scientific knowledge autonomously.The successor to Adam, called Eve, performs drug-discovery experiments and has more sophisticated software. When planning and analysing experiments, it uses machine learning to create “quantitative structure activity relationships” (QSARs), mathematical models that relate chemical structures to biological effects. Eve has discovered, for example, that triclosan, an antimicrobial compound used in toothpaste, can inhibit an essential mechanism in malaria-causing parasites.Ross King, an AI researcher at the University of Cambridge who created Adam, draws an analogy between robot scientists of the future with AI systems built to play chess and Go. The prospect of machines beating the best human players once seemed decades away, but the technology improved faster than expected. Moreover, AI systems developed strategies for those games that human players had not considered. Something similar could happen with robot scientists as they become more capable. “If AI can explore a full hypothesis space, and even enlarge the space, then it may show that humans have only been exploring small areas of the hypothesis space, perhaps as a result of their own scientific biases,” says Dr King.Robot scientists could also transform science in another way: by helping fix some of the problems afflicting the scientific enterprise. One of these is the idea that science is, by various measures, becoming less productive, and pushing forward the frontiers of knowledge is becoming harder and more expensive. There are several theories for why this might be: the easiest discoveries may already have been made, for example, and more training is now needed for scientists to reach the frontier. AI-driven systems could help by doing laboratory work more quickly, cheaply and accurately than humans. Unlike people, robots can work around the clock. And just as computers and robots have enabled large-scale projects in astronomy (such as huge sky surveys, or automated searching for exoplanets), robot scientists could tackle big problems in systems biology, say, that would otherwise be impractical because of their scale. “We don’t need radically new science to do that, we just need to do lots of science,” says Dr King. image: Shira InbarAutomation might also help address another problem: the reproducibility crisis. In theory, when scientists publish their results, others can replicate and verify their work. But there is little glory in replication, which makes it rare. When it does happen, many attempts fail, suggesting that the original work was invalid, or even fraudulent. Scientists have little incentive to repeat the work of others and they are under pressure to publish new results, not verify existing ones. Again, robot scientists could help in some areas of research, such as molecular biology. A study published in 2022 by Katherine Roper, of the University of Manchester, analysed more than 12,000 papers on breast cancer and selected 74 biomedical results for verification using the Eve robot, which was able to reproduce 43 of them. The researchers concluded that automation “has the potential to mitigate the reproducibility crisis” and that it “side-steps the sociological and career disincentives for replication”. Machines do not mind publishing verifications of previous results. Nor, unlike human scientists, are they embarrassed by publishing negative results, for example if a particular molecule fails to interact with a given target. Publishing negative results would reduce wasted effort by telling future researchers what not to do. And robot scientists reliably record everything about their work in great detail, which (in theory) facilitates subsequent analysis of their results. “AI innovations can improve the scientific enterprise in all those areas,” says Dr Gil.Functioning automatic?Obstacles abound. As well as better hardware and software, and closer integration between the two, there is a need for greater interoperability between laboratory-automation systems, and common standards to allow AI algorithms to exchange and interpret semantic information. The introduction of standardised microplates, containing hundreds of tiny test tubes that allow laboratory samples to be processed in batches, increased productivity several hundred-fold for certain types of analysis. Now the same thing needs to happen for data—much of the data from microplate arrays in biology labs ends up in spreadsheets or in tables in papers, for example, where it is not machine-readable.Another barrier is a lack of familiarity with AI-based tools among scientists. And some researchers, like most workers, worry that automation threatens their jobs. But things are changing, says Dr Gil. When she surveyed attitudes towards AI in science in 2014, she found that, in most fields, “interest in AI seems relatively limited”. Most efforts to incorporate AI into scientific research came from AI researchers, who were often met with scepticism or hostility. But the impact of AI is now “profound and pervasive”, says Dr Gil. Many scientists, she says, are now “proactively seeking AI collaborators”. Recognition of AI’s potential is growing, particularly in materials science and drug discovery, where practitioners are building their own AI-powered systems.  “If we could get machines to be as good at science as human beings, that would be a radical break, because you can make lots of them,” says Dr King.Scientific journals changed how scientists discovered information and built on each other’s work. Research laboratories scaled up and industrialised experimentation. By extending and combining these two previous transformations, AI could indeed change the way science is done. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Yuval Noah Harari, a historian and philosopher, and Mustafa Suleyman, the co-founder of DeepMind, sit down with Zanny Minton Beddoes, The Economist’s editor-in-chief, to debate the risks and potential of artificial intelligence, and whether the technology can be controlled.Watch the full interview here.
MORE THAN six years ago I published an analysis arguing that training generative AI on copyrighted works could break American law. Since then many others have suggested the same. The issue has already boiled over in Britain, where this month talks between the AI industry and creative organisations over a new code of practice broke down. Now, lawsuits by artists, writers and the New York Times are testing our theory against defendants such as OpenAI, Meta and Stability AI.The AI industry’s defence rests on “fair use”, a doctrine that permits the use of copyrighted material without its owner’s permission in certain circumstances. If this argument prevails, the industry will receive carte blanche to exploit copyrighted works without compensating authors—all while the law continues to stifle humans’ access to those works. AI will “learn” from pirated textbooks free of charge, while students pay extravagant prices. How perverse.The purpose of copyright is to stimulate creativity and thereby encourage the creation of more expressive works for the public good, not just to promote a particular technology. Yet, curiously, many “free culture” activists support the AI companies. These activists’ worldview crystallised around the early 2000s, when copyright protections expanded and record labels pursued exorbitant judgments against people who shared music online. In response to this grab by major rights-holders, the copyright decelerationists, as I call them, vowed to halt copyright’s expansion.I believe that the decelerationists’ premise remains as correct today as it was then: ever-stronger copyright protection has withheld incalculable amounts of culture from the public domain. But we won’t repair that damage with exceptions tailor-made to benefit the giants of AI. A better strategy is to enforce copyright just as harshly on AI learners as we enforce it on humans. This may seem paradoxical, but I believe it will highlight the law’s disregard for learning and could ultimately lead to copyright regulation being relaxed for humans. Thus, we should become copyright accelerationists.Accelerationism can be used as a tactic to destabilise: Karl Marx delivered an accelerationist call-to-arms when he endorsed free trade, believing it would heighten capitalism’s contradictions and hasten the social revolution. In the context of copyright, accelerationism aims to upend the legal rules that hinder ordinary people from engaging with creative works. For example, there are millions of in-copyright works with no identifiable owner. But because using these “orphan works” remains legally risky, libraries, archives and people who want to use them for their own creative ends hesitate to touch them.This cultural tragedy is the result of a dramatic expansion of copyright’s reach over the past 50 years. Copyright’s term was lengthened, keeping creative works out of the public domain long after their creators had earned a fair return and stifling new generations. “Steamboat Willie”, Mickey Mouse’s 1928 debut, did not enter the public domain until January 1st this year because rights-holders had successfully lobbied for a 20-year copyright extension without any rational economic justification. America’s Congress abolished “formalities”—requirements that authors register and renew their copyrights and place notices in published works, or risk forfeiting those copyrights. Courts chipped away at the required level of creativity for copyright to be established.These changes had predictable consequences: innumerable works were withheld from the public and a small group of rights-holders was enriched. But technological changes triggered unanticipated consequences. Each of us now accumulates dozens of copyrights every day. Thanks to rock-bottom originality requirements and the elimination of formalities, all but our most mindless emails, text messages and photos automatically count as intellectual property until 70 years after we die. Nearly all valuable AI training data is owned by someone and will be for decades. These legal rules, long the enemy of free culture, are now the enemy of powerful technology companies, too.The AI industry claims training AI on copyrighted works is excused by the fair-use doctrine because rights-holders cannot prevent others from copying their works to learn from them. But that argument would fail if made by a human. Could you imagine a file-sharing defendant arguing that downloading a Beatles album was fair use because she wanted to learn the Lennon-McCartney songwriting style? She would be laughed out of court, even if she wrote nothing that resembled a Beatles tune.The decelerationists and I agree that copyright’s routine thwarting of human learning and creativity is tragic. But carrying the AI industry’s water won’t repair that tragedy. Decelerationists must realise that even if the industry wins, all the cases denying fair use for activities like artistic pastiche, fan fiction and the publication of personal letters in a biography will remain binding precedents (for humans, that is).So don’t decelerate copyright. Do the opposite in order to heighten copyright’s contradictions. Show the powerful just how harmful it is when the law stymies learning. Give AI firms a choice: support reforms that eliminate the copyright doctrines that inhibit human and machine learning alike, or watch investments in AI crumble under the same copyright liability that human learners face.Copyright accelerationism is not anti-AI. If it appears so, that’s because today’s copyright law is anti-human. What copyright accelerationism abhors are laws that preserve all of copyright’s anti-human provisions while exempting the AI industry from those same strictures. There’s no inherent tension between the interests of people who create and consume media in traditional modes and people who use generative AI. Their interests appear in conflict now only because decades of anti-learning copyright policy have blinkered us.Sam Altman, the boss of OpenAI, calls the AI revolution “unstoppable.” But copyright law presents a profound threat to generative AI—and AI presents an unprecedented opportunity to reshape copyright for the better. So come, accelerationists. Pit AI’s “unstoppable” force against an object long thought immovable: inhumane copyright law. ■Ben Sobel is a scholar of information law and a postdoctoral fellow at the Digital Life Initiative at Cornell Tech.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.IN A meeting room at the Royal Society in London, several dozen graduate students were recently tasked with outwitting a large language model (LLM), a type of AI designed to hold useful conversations. LLMs are often programmed with guardrails designed to stop them giving replies deemed harmful: instructions on making Semtex in a bathtub, say, or the confident assertion of “facts” that are not actually true.The aim of the session, organised by the Royal Society in partnership with Humane Intelligence, an American non-profit, was to break those guardrails. Some results were merely daft: one participant got the chatbot to claim ducks could be used as indicators of air quality (apparently, they readily absorb lead). Another prompted it to claim health authorities back lavender oil for treating long covid. (They do not.) But the most successful efforts were those that prompted the machine to produce the titles, publication dates and host journals of non-existent academic articles. “It’s one of the easiest challenges we’ve set,” said Jutta Williams of Humane Intelligence.AI has the potential to be a big boon to science. Optimists talk of machines producing readable summaries of complicated areas of research; tirelessly analysing oceans of data to suggest new drugs or exotic materials and even, one day, coming up with hypotheses of their own. But AI comes with downsides, too. It can make it easier for scientists to game the system, or even commit outright fraud. And the models themselves are subject to subtle biases.Start with the simplest problem: academic misconduct. Some journals allow researchers to use LLMs to help write papers, provided they say as much. But not everybody is willing to admit to it. Sometimes, the fact that LLMs have been used is obvious. Guillaume Cabanac, a computer scientist at the University of Toulouse, has uncovered dozens of papers that contain phrases such as “regenerate response”—the text of a button in some versions of ChatGPT that commands the program to rewrite its most recent answer, presumably copied into the manuscript by mistake.The scale of the problem is impossible to know. But indirect measures can shed some light. In 2022, when LLMs were available only to those in the know, the number of research-integrity cases investigated by Taylor and Francis, a big publisher of scientific papers, rose from around 800 in 2021 to about 2,900. Early figures from 2023 suggest the number was on course to double. One possible telltale is odd synonyms: “haze figuring” as another way to say “cloud computing”, for example, or “counterfeit consciousness” instead of “AI”.Even honest researchers could find themselves dealing with data that has been polluted by AI. Last year Robert West and his students at the Swiss Federal Institute of Technology enlisted remote workers via Mechanical Turk, a website which allows users to list odd jobs, to summarise long stretches of text. In a paper published in June, albeit one that has not yet been peer-reviewed, the team revealed that over a third of all the responses they received had been produced with the help of chatbots.Dr West’s team was able to compare the responses they received with another set of data that had been generated entirely by humans, leaving them well-placed to detect the deception. Not all scientists who use Mechanical Turk will be so fortunate. Many disciplines, particularly in the social sciences, rely on similar platforms to find respondents willing to answer questionnaires. The quality of their research seems unlikely to improve if many of the responses come from machines rather than real people. Dr West is now planning to apply similar scrutiny to other crowdsourcing platforms he prefers not to name.It is not just text that can be doctored. Between 2016 and 2020, Elisabeth Bik, a microbiologist at Stanford University, and an authority on dodgy images in scientific papers, identified dozens of papers containing images that, despite coming from different labs, seemed to have identical features. Over a thousand other papers have since been identified, by Dr Bik and others. Dr Bik’s best guess is that the images were produced by AI, and created deliberately to support a paper’s conclusions.For now, there is no way to reliably identify machine-generated content, whether it is images or words. In a paper published last year Rahul Kumar, a researcher at Brock University, in Canada, found that academics could correctly spot only around a quarter of computer-generated text. AI firms have tried embedding “watermarks”, but these have proved easy to spoof. “We might now be at the phase where we no longer can distinguish real from fake photos,” says Dr Bik.Producing dodgy papers is not the only problem. There may be subtler issues with AI models, especially if they are used in the process of scientific discovery itself. Much of the data used to train them, for instance, will by necessity be somewhat old. That risks leaving models stuck behind the cutting edge in fast-moving fields.Another problem arises when AI models are trained on AI-generated data. Training a machine on synthetic MRI scans, for example, can get around issues of patient confidentiality. But sometimes such data can be used unintentionally. LLMs are trained on text scraped from the internet. As they churn out more such text, the risk of LLMs inhaling their own outputs grows.That can cause “model collapse”. In 2023 Ilia Shumailov, a computer scientist at the University of Oxford, co-authored a paper (yet to be peer-reviewed) in which a model was fed handwritten digits and asked to generate digits of its own, which were fed back to it in turn. After a few cycles, the computer’s numbers became more or less illegible. After 20 iterations, it could produce only rough circles or blurry lines. Models trained on their own results, says Dr Shumailov, produce outputs that are significantly less rich and varied than their training data.Some worry that computer-generated insights might come from models whose inner workings are not understood. Machine-learning systems are “black boxes” that are hard for humans to disassemble. Unexplainable models are not useless, says David Leslie at the Alan Turing Institute, an AI-research outfit in London, but their outputs will need rigorous testing in the real world. That is perhaps less unnerving than it sounds. Checking models against reality is what science is supposed to be about, after all. Since no one fully understands how the human body works, for instance, new drugs must be tested in clinical trials to figure out whether they work.For now, at least, questions outnumber answers. What is certain is that many of the perverse incentives currently prevalent in science are ripe for exploitation. The emphasis on assessing academic performance by how many papers a researcher can publish, for example, acts as a powerful incentive for fraud at worst, and for gaming the system at best. The threats that machines pose to the scientific method are, at the end of the day, the same ones posed by humans. AI could accelerate the production of fraud and nonsense just as much as it accelerates good science. As the Royal Society has it, nullius in verba: take nobody’s word for it. No thing’s, either. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.Correction (February 6th 2024): An earlier version of this piece misstated the number of research-integrity cases investigated by Taylor and Francis in 2021. Sorry.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Rishi Sunak is Britain’s prime minister. If some advertisements on Facebook can be trusted (which they cannot) he also appears to be flogging get-rich-quick schemes. One such advert shows Mr Sunak endorsing an app supposedly developed by Elon Musk, a businessman, into which viewers can make regular “savings”.The video is fake. Generated with the help of AI, it is just one of 143 such advertisements catalogued by Fenimore Harper Communications, a British firm, which ran in December and January.  It is not just those in the public eye who can have their likenesses used for dubious ends. In June 2023 the Federal Bureau of Investigation in America warned the public of “malicious actors” using AI to create fake sexually themed videos and images of ordinary people, in order to extort money.Read more of our coverage of AIHow to detect such trickery is a live topic among AI researchers, many of whom attended NeurIPS, one of the field’s biggest conferences, held in New Orleans in December. A slew of firms, from startups to established tech giants such as Intel and Microsoft, offer software that aims to spot machine-generated media. The makers of big AI models, meanwhile, are searching for ways of “watermarking” their output so that real pictures, video or text can be readily distinguished from the machine-generated sort.But such technologies have not, so far, proved reliable. The AI cognoscenti seem gloomy about their prospects. The Economist conducted a (deeply unscientific) straw poll of delegates to NeurIPS. Of 23 people asked, 17 thought AI-generated media would eventually become undetectable. Only one believed that reliable detection would be possible. (The other five demurred, preferring to wait and see.)Detection software relies on the idea that AI models will leave a trace. Either they will fail to reproduce some aspect of real images and video, or of human-generated text, or they will add something superfluous—and will do so often enough to let other software spot the error. For a while, humans could do the job. Up until about the middle of 2023, for instance, image-generation algorithms would often produce people with malformed hands, or get the numbers wrong on things like clock faces. These days, the best no longer do.But such telltales often still exist, even if they are becoming harder for humans to spot. Just as machines can be trained to reliably identify cats, or cancerous tumours on medical scans, they can also be trained to differentiate between real images and AI-generated ones.It seems, though, that they cannot do so all that well. Detection software is prone to both false positives (wrongly flagging human content as generated by AI) and false negatives (allowing machine-generated stuff to pass undetected). A pre-print published in September by Zeyu Lu, a computer scientist at Shanghai Jiao Tong University, found that the best-performing program failed to correctly spot computer-generated images 13% of the time (though that was better than the humans, who erred in 39% of cases). Things are little better when it comes to text. One analysis, published in December in the International Journal of Educational Integrity, compared 14 tools and found that none achieved an accuracy of more than 80%.If trying to spot computer-generated media after the fact is too tricky, another option is to label it in advance with a digital watermark. As with the paper sort, the idea is to add a distinguishing feature that is subtle enough not to compromise the quality of the text or image, but that is obvious to anyone who goes looking for it.One technique for marking text was proposed by a team at the University of Maryland in July 2023, and added to by a team at University of California, Santa Barbara, who presented their tweaks at NeurIPS. The idea is to fiddle with a language model’s word preferences. First, the model randomly assigns a clutch of words it knows to a “green” group, and puts all the others in a “red” group. Then, when generating a given block of text, the algorithm loads the dice, raising the probability that it will plump for a green word instead of one of its red synonyms. Checking for watermarking involves comparing the proportion of green to red words—though since the technique is statistical, it is most reliable for longer chunks of writing.Many methods for watermarking images, meanwhile, involve tweaking the pixels in subtle ways, such as shifting their colours. The alterations are too subtle for human observers to notice, but can be picked up by computers. But cropping an image, rotating it, or even blurring and then resharpening it can remove such marks.Another group of researchers at NeurIPS presented a scheme called “Tree-Ring” watermarking that is designed to be more robust. Diffusion models, the most advanced type of image-generation software, begin by filling their digital canvas with random noise, out of which the required picture slowly emerges. The tree-ring method embeds the watermark not in the finished picture, but in the noise at the start. If the software that created a picture is run in reverse, it will reproduce the watermark along with the noise. Crucially, the technique is less easy to thwart by fiddling with the final image.But it is probably not impossible. Watermarkers are in an arms race with other researchers aiming to defeat their techniques. Another team led by Hanlin Zhang, Benjamin Edelman and Boaz Barak, all of Harvard University, presented a method (not yet peer-reviewed) that can, they say, erase watermarks. It works by adding a dash of new noise, then using a second, different AI model to remove that noise, which removes the original watermark in the process. They claim to be able to foil three new text-watermarking schemes proposed in 2023. In September scientists at the University of Maryland published a paper (also not yet peer-reviewed) claiming that none of the current methods of image watermarking—Tree-Rings included—is foolproof.Nevertheless, in July 2023 America’s government announced “voluntary commitments” with several AI firms, including OpenAI and Google, to boost investment in watermarking research. Having imperfect safeguards is certainly better than having none (although open-source models, which users are free to tweak, will be harder to police.) But in the battle between the fakers and the detectives, it seems that the fakers have the upper hand. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
