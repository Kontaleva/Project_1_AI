text
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The rainforests are alive with the sound of animals. Besides the pleasure of the din, it is also useful to ecologists. If you want to measure the biodiversity of a piece of land, listening out for animal calls is much easier than grubbing about in the undergrowth looking for tracks or spoor. But such “bioacoustic analysis” is still time-consuming, and it requires an expert pair of ears.In a paper published on October 17th in Nature Communications, a group of researchers led by Jörg Müller, an ecologist at the University of Würzburg, describe a better way: have a computer do the job. Smartphone apps already exist that will identify birds, bats or mammals simply by listening to the sounds they make. Their idea was to apply the principle to conservation work.The researchers took recordings from across 43 sites in the Ecuadorean rainforest. Some sites were relatively pristine, old-growth forest. Others were areas that had recently been cleared for pasture or cacao planting. And some had been cleared but then abandoned, allowing the forest to regrow.Sound recordings were taken four times every hour, over two weeks. The various calls were identified manually by an expert, and then used to construct a list of the species present. As expected, the longer the land had been free from agricultural activity, the greater the biodiversity it hosted.Then it was the computer’s turn. The researchers fed their recordings to artificial-intelligence models that had been trained, using sound samples from elsewhere in Ecuador, to identify 75 bird species from their calls. “We found that the AI tools could identify the sounds as well as the experts,” says Dr Müller.Of course, not everything in a rainforest makes a noise. Dr Müller and his colleagues used light-traps to capture night-flying insects, and DNA analysis to identify them. Reassuringly, they found that the diversity of noisy animals was a reliable proxy for the diversity of the quieter ones, too.The results may have relevance outside ecology departments, too. Under pressure from their customers, firms such as L’Oreal, a make-up company, and Shell, an oil firm, have been spending money on forest restoration projects around the world. Dr Müller hopes that an automated approach to checking on the results could help monitor such efforts, and give a standardised way to measure whether they are working as well as their sponsors say.■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Since ChatGPT took the world by storm last year, the internet has been littered with predictions of just how disruptive “generative” artificial intelligence (AI) will be. “Entire industries will reorient around it,” enthused Bill Gates in a blog post earlier this year, in which he declared the technology to be as disruptive as the internet and the microprocessor. From media and education to law and health care, vast areas of human endeavour are expected to be turned upside down.You may think that the losers from all this would be crusty old incumbents, rather as Kodak and Blockbuster were felled during past waves of technological upheaval. And, sure enough, a new wave of startups has sensed the chance to gain a foothold, crashing onto the scene with ai-powered legal chatbots, virtual doctors, writing assistants and so on. Some of these will make up a new industry of model-builders and innovators that soar to lofty valuations, rather as today’s tech giants ascended during the internet age. In the rest of the economy, however, it is far from clear that the upheaval will consign today’s corporate Goliaths to history. ai looks as likely to fortify reigning champions as to uproot them.One reason for this is incumbents’ advantages in distribution. That can help the giants maintain their dominance, even if they do not dream up the technology in the first place. Having paired with OpenAI, the creator of ChatGPT, for instance, Microsoft is souping up its ubiquitous Office software with AI features that let workers automate tasks such as writing emails and summarising documents. That will leave little space for rival upstarts. Salesforce and Zendesk, makers of software for sales reps and call-centre agents, respectively, are likewise embedding AI features in their tools. Whereas most companies may not be comfortable turning to a chatbot from an unknown startup for legal advice, they may try a large law firm like Allen & Overy, which is using one to help its lawyers speed up mundane tasks.Incumbents will also be helped by their access to proprietary datasets, which can be used to tailor AI models to specific markets. Bloomberg, a financial-data firm, has used its trove of information to train a chatbot to help with financial analysis. McKinsey, a consulting giant, has trained a bot on its corpus of intellectual property. Health-care providers could exploit their anonymised medical records, insurers their claims data, and media companies their archival film or print, putting them ahead of upstarts unable to draw on such data.Another reason to doubt that AI will upend the pecking order relates to how models are accessed. Whereas e-commerce required retailers to create an entirely new infrastructure for selling online, much AI development today is done by model-builders such as OpenAI and tech giants, including Alphabet and Amazon. Retailers, banks and others can link those models to their systems. By making it speedier for incumbents to develop AI-infused offerings, that will limit the opportunity for nimbler newcomers.A last reason to expect incumbents to prevail is history. Even during the technological upheaval of the past few decades, surprisingly few corporate giants were felled. Only 52 of the Fortune 500, America’s largest companies by revenue, were created since 1990. A mere seven were born after Apple unveiled the first iPhone in 2007. By contrast, 280 were founded before America entered the second world war. The average age of the Fortune 500 has steadily risen over the past three decades, from 75 to 90, defying the idea that the pace of disruption has accelerated in the internet era.Survival is not guaranteed, obviously. Those that dawdle in their adoption of AI will cede the advantage to faster rivals. Those that ignore it entirely may still go the way of Kodak or Blockbuster. For the Davids of the AI wave, however, the odds are nonetheless fearsome. ■
AT THE start of this year, two straws in the wind caught the attention of those who follow the development of artificial intelligence (AI) globally. First, Qi Lu, one of the bosses of Microsoft, said in January that he would not return to the world’s largest software firm after recovering from a cycling accident, but instead would become chief operating officer at Baidu, China’s leading search engine. Later that month, the Association for the Advancement of Artificial Intelligence postponed its annual meeting. The planned date for the event in January conflicted with the Chinese new year.These were the latest signals that China could be a close second to America—and perhaps even ahead of it—in some areas of AI, widely considered vital to everything from digital assistants to self-driving cars. China is simply the place to be, explains Mr Lu, and Baidu the country’s most important player. “We have an opportunity to lead in the future of AI,” he says.Other evidence supports the claim. In October 2016 the White House noted in a report that China had overtaken America in the number of published journal articles on deep learning, a branch of AI. PwC, a consultancy, predicts that AI-related growth will boost global GDP by $16trn by 2030; nearly half of that bonanza will accrue to China, it reckons. The number of AI-related patent submissions by Chinese researchers has increased by nearly 200% in recent years, although America is still ahead in absolute numbers (see chart).To understand why China is so well placed, consider the inputs needed for AI. Of the two most basic, computing power and capital, it has an abundance. Chinese firms, from giants such as Alibaba and Tencent to startups such as CIB FinTech and UCloud, are building data centres as fast as they can. The market for cloud computing has been growing by more than 30% in recent years and will continue to do so, according to Gartner, a consultancy. In 2012-16 Chinese AI firms received $2.6bn in funding, according to the Wuzhen Institute, a think-tank. That is less than the $17.9bn that poured into their American peers, but the total is growing quickly.Yet it is two other resources that truly make China a promised land for AI. One is research talent. As well as strong skills in maths, the country has a tradition in language and translation research, says Harry Shum, who leads Microsoft’s AI efforts. Finding top-notch AI experts is harder in China than in America, says Wanli Min, who oversees 150 data scientists at Alibaba. But this will change over the next couple of years, he predicts, because most big universities have launched AI programmes. According to some estimates, China has more than two-fifths of the world’s trained AI scientists.The second advantage for China is data, AI’s most important ingredient. In the past, software and digital products mostly obeyed rules laid down in code, giving an edge to those countries with the best coders. With the advent of deep-learning algorithms, such rules are increasingly based on patterns extracted from reams of data. The more data are available, the more algorithms can learn and the smarter AI offerings will be.China’s sheer size and diversity provide powerful fuel for this cycle. Just by going about their daily lives, the country’s nearly 1.4bn people generate more data than almost all other nations combined. Even in the case of a rare disease, there are enough examples to teach an algorithm how to recognise it. Because typing Chinese characters is more laborious than Western ones, people also tend to use voice-recognition services more often than in the West, so firms have more voice snippets with which to improve speech offerings.The Saudi Arabia of dataWhat really sets China apart is that it has more internet users than any other country: about 730m. Almost all go online from smartphones, which generate far more valuable data than desktop computers, chiefly because they contain sensors and are carried around. In the big coastal cities, for instance, cash has all but disappeared for small purchases: people settle with their devices using services such as Alipay and WeChat Pay.Chinese do not seem to be terribly concerned about privacy, which makes collecting data easier. The country’s bike-sharing services, which have taken big cities by storm, for example, not only provide cheap transport but are what is known as a “data play”. When riders hire a bicycle, some firms keep track of renters’ movements using a GPS device attached to the bike.Young Chinese appear particularly keen on AI-powered services and relaxed about use of their data. Xiaoice, an upbeat chatbot operated by Microsoft, now has more than 100m Chinese users. Most talk to it between 11pm and 3am, often about the problems they had during the day. It is learning from interactions and becoming cleverer. Xiaoice no longer just provides encouragement and tells jokes, but has created the first collection of poems written with AI, “Sunshine Lost Its Window”, which caused a heated debate in Chinese literary circles over whether there can be such a thing as artificial poetry.Another important source of support for AI in China is the government. The technology figures prominently in the country’s current five-year plan. Technology firms are working closely with government agencies: Baidu, for example, has been asked to lead a national laboratory for deep learning. It is unlikely that the government will burden AI firms with over-strict regulation. The country has more than 40 laws containing rules about the protection of personal data, but these are rarely enforced.Entrepreneurs are taking advantage of China’s talent and data strengths. Many AI firms got going only a year or two ago, but plenty have been progressing more rapidly than their Western counterparts. “Chinese AI startups often iterate and execute more quickly,” explains Kai-Fu Lee, who ran Google’s subsidiary in China in the 2000s and now leads Sinovation Ventures, a venture-capital fund.As a result, China already has a herd of AI unicorns, meaning startups valued at more than $1bn. Toutiao, a news aggregator based in Beijing, employs machine learning to recommend articles using information such as a reader’s interests and location; it also uses AI to filter out fake information (which in China mainly means dubious health-care announcements). Another AI startup, iFlytek, has developed a voice assistant that translates Mandarin into several languages, including English and German, even if the speaker uses slang and talks over background noise. And Megvii Technology’s face-recognition software, Face++, identifies people almost instantaneously.Skynet livesAt Megvii’s headquarters, visitors are treated to a demonstration. A video camera in the lobby does away with the need for showing ID: employees just walk in without showing their badges. Similar devices are positioned all over the office and their feeds are shown on a video wall. When a face pops up on the wall, it is immediately surrounded by a white rectangle and some text giving information about that person. In the upper right-hand corner of the screen big letters spell “Skynet”, the name of the AI system in the Terminator films that seeks to exterminate the human race. The firm already enables Alipay and Didi, a ride-hailing firm, to check the identity of new customers (their faces are compared with pictures held by the government).Reacting to the success of such startups, China’s tech giants, too, have begun to invest heavily in AI. Baidu, Alibaba and Tencent, collectively called BAT, are working on many of the same services, including speech- and face-recognition. But they are also trying to become dominant in specific areas of AI, based on their existing strengths.Tencent has so far kept the lowest profile; it established its AI labs only in recent months. But it is bound to develop a big presence in AI: it has more data than the other two. Its WeChat messenger service has nearly 1bn accounts and is also the platform for thousands of services, from payments and news to city guides and legal help. Tencent is also a world-beater in games with blockbusters such as League of Legends and Clash of Clans, which have more than 100m players each globally.Alibaba is already a behemoth in e-commerce and is investing billions to become number one in cloud computing. At a conference in June in Shanghai it showed off an AI service called “ET City Brain” that uses video recognition to optimise traffic in real time. It uses footage from roadside cameras to predict the behaviour of cars and can adjust traffic lights on the spot. In its home town of Hangzhou, Alibaba claims, the system has already increased the average speed of traffic by 11%. Alibaba is also planning to beef up what it calls “ET Medical Brain”, which will offer AI-powered services to discover drugs and diagnose medical images. It has signed up a dozen hospitals to get the data it needs.But it is Baidu whose fate is most tied to AI, in part because the technology may be its main chance to catch up with Alibaba and Tencent. It is putting most of its resources into autonomous driving: it wants to get a self-driving car onto the market by 2018 and to provide technology for fully autonomous vehicles by 2020. On July 5th the firm announced a first version of its self-driving-car software, called Apollo, at a developer conference in Beijing.Getting Apollo right will not only involve cars safely navigating the streets, but managing a project that is open to outsiders. Rivals such as Waymo, Google’s subsidiary, and Tesla, an electric-car firm, jealously guard their software and the data they collect. Baidu is planning not only to publish the recipe for its programs (making them “open-source”, in the jargon), but to share data. The idea is that carmakers that use Baidu’s technology will do the same, creating an open platform for data from self-driving cars—the “Android for autonomous vehicles”, in the words of Mr Lu.Drive like a BeijingerIt remains to be seen how successful Chinese firms will be in exporting their AI products—for now, only a tiny handful are used abroad. In theory they should travel well: a self-driving car trained on China’s chaotic streets ought to have no problem navigating the more civilised traffic in Europe (in contrast, a vehicle trained in Germany may not get far beyond the first intersection in Beijing). But consumers in the West may hesitate to use self-driving cars that have been trained in a laxer safety environment that is more tolerant of accidents. Chinese municipalities are said to be falling over themselves to be testing grounds for autonomous vehicles.There is another risk. Data are the most valuable input for AI at the moment, but their importance may yet diminish. AI firms have started to use simulated data, including those from video games. New types of algorithms may be capable of getting smart with fewer examples. “The danger is that we stop innovating in algorithms because of our advantage in data,” warns Gansha Wu, chief executive of UISEE, a Beijing startup which is developing self-driving technology. For now, though, China looks anything but complacent. In the race for pre-eminence in AI, it will run America close.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Reunions offer a chance to reflect on how much has changed. One will happen during the coming year in Hollywood when “Here” premieres, bringing together the actors, director and writer behind “Forrest Gump” 30 years later for a new, unrelated film. Set in a single room over decades, “Here” is very much a film of the here and now. The stars, Tom Hanks and Robin Wright, will be “de-aged” using new ai tools, rendering them more youthful in some scenes and enabling the film-makers to see the transformation in real time while shooting.Generative AI now means images can be produced in seconds. Songs can be created in the style of singers dead or alive.  More than 3,000 books on Amazon name ChatGPT as the author or co-author, lending new meaning to the term “ghostwriter”.It is still early days, but 2024 will be a preview of what is to come. Three things are worth watching. The first is how ai will be used to tell new types of stories, as storytelling becomes more personalised and interactive. Films will change and so will gaming, an industry where people can choose their own adventures more easily than moviegoers can. The amount of entertainment available will also balloon.It will be a few years before a blockbuster is produced entirely by AILike the arrival of the internet, which led to an explosion of “user-generated content” being posted to social media and YouTube, generative AI will contribute to reams of videos and other material proliferating online. Some predict that as much as 90% of online content will be AI-generated by 2025. Curation and good search tools will be vital, and there will be debates about whether, and how, to label AI-generated content. No one is quite sure how the nature of storytelling will change, but it is sure to. David Thomson, a film historian, compares generative ai to the advent of sound. When movies were no longer silent, it altered the way plot points were rendered and how deeply viewers could connect with characters. Cristóbal Valenzuela, who runs a company called RunwayML, which offers ai-enhanced software tools to creative types, says ai is more like a “new kind of camera”, offering a fresh “opportunity to reimagine what stories are like”. Both are right. The Hollywood writers’ strike shone a spotlight on the question of whether AI would start producing scripts. For now, studios have agreed to concessions and will not bypass writers’ rooms to employ ChatGPT instead. It will probably be a few years before a full-length blockbuster is produced entirely by AI. Instead, the second big development to watch is how AI will be used as a time-saving tool. Generative AI will automate and simplify complex tasks like dubbing, film-editing, special effects and background design. For a glimpse of the future, watch “Everything Everywhere All at Once”, which won the Academy Award for Best Picture in 2023. It featured a scene that used a “rotoscoping” tool offered by RunwayML to edit out the green-screen background and make a talking rock more believable. It compressed into hours what might have otherwise taken days of video-editing.The third thing to watch for is more dramatic clashes between creators (otherwise known as copyright-owners) and those who run AI platforms. The coming year is likely to bring a deluge of lawsuits from authors, musicians, actors and artists about how their words, music and images have been used to train AI systems without consent or payment. Perhaps they can agree on some sort of licensing arrangement, in which AI companies start paying copyright-holders for content to train their models. But that will not happen without an intense legal brawl.AI presents bigger questions about the future of stories and the nature of collective storytelling. For example, will generative AI simply imitate previous hits, resulting in more derivative blockbuster films and copycat interpretations of pop songs that lack depth, rather than original stories and art forms? And as entertainment becomes more personalised, will there still be stories that become part of humanity’s collective consciousness and move large numbers of people, who can talk about them together?As creators grapple with ai’s rise, they will channel their anxieties about technology into their work. Look out for more “Terminator”-style clashes between man and machine. Life imitates art—and art life. ■Alexandra Suich Bass, Culture editor, The EconomistCorrection (January 9th 2024): “Forrest Gump” was released 30 years ago, not 40 years ago as this story originally claimed.
ARTIFICIAL intelligence (AI) is barging its way into business. As our special report this week explains, firms of all types are harnessing AI to forecast demand, hire workers and deal with customers. In 2017 companies spent around $22bn on AI-related mergers and acquisitions, about 26 times more than in 2015. The McKinsey Global Institute, a think-tank within a consultancy, reckons that just applying AI to marketing, sales and supply chains could create economic value, including profits and efficiencies, of $2.7trn over the next 20 years. Google’s boss has gone so far as to declare that AI will do more for humanity than fire or electricity.Such grandiose forecasts kindle anxiety as well as hope. Many fret that AI could destroy jobs faster than it creates them. Barriers to entry from owning and generating data could lead to a handful of dominant firms in every industry.Less familiar, but just as important, is how AI will transform the workplace. Using AI, managers can gain extraordinary control over their employees. Amazon has patented a wristband that tracks the hand movements of warehouse workers and uses vibrations to nudge them into being more efficient. Workday, a software firm, crunches around 60 factors to predict which employees will leave. Humanyze, a startup, sells smart ID badges that can track employees around the office and reveal how well they interact with colleagues.Surveillance at work is nothing new. Factory workers have long clocked in and out; bosses can already see what idle workers do on their computers. But AI makes ubiquitous surveillance worthwhile, because every bit of data is potentially valuable. Few laws govern how data are collected at work, and many employees unguardedly consent to surveillance when they sign their employment contract. Where does all this lead?Trust and telescreensStart with the benefits. AI ought to improve productivity. Humanyze merges data from its badges with employees’ calendars and e-mails to work out, say, whether office layouts favour teamwork. Slack, a workplace messaging app, helps managers assess how quickly employees accomplish tasks. Companies will see when workers are not just dozing off but also misbehaving. They are starting to use AI to screen for anomalies in expense claims, flagging receipts from odd hours of the night more efficiently than a carbon-based beancounter can.Employees will gain, too. Thanks to strides in computer vision, AI can check that workers are wearing safety gear and that no one has been harmed on the factory floor. Some will appreciate more feedback on their work and welcome a sense of how to do better. Cogito, a startup, has designed AI-enhanced software that listens to customer-service calls and assigns an “empathy score” based on how compassionate agents are and how fast and how capably they settle complaints.Machines can help ensure that pay rises and promotions go to those who deserve them. That starts with hiring. People often have biases but algorithms, if designed correctly, can be more impartial. Software can flag patterns that people might miss. Textio, a startup that uses AI to improve job descriptions, has found that women are likelier to respond to a job that mentions “developing” a team rather than “managing” one. Algorithms will pick up differences in pay between genders and races, as well as sexual harassment and racism that human managers consciously or unconsciously overlook.Yet AI’s benefits will come with many potential drawbacks. Algorithms may not be free of the biases of their programmers. They can also have unintended consequences. The length of a commute may predict whether an employee will quit a job, but this focus may inadvertently harm poorer applicants. Older staff might work more slowly than younger ones and could risk losing their positions if all AI looks for is productivity.And surveillance may feel Orwellian—a sensitive matter now that people have begun to question how much Facebook and other tech giants know about their private lives. Companies are starting to monitor how much time employees spend on breaks. Veriato, a software firm, goes so far as to track and log every keystroke employees make on their computers in order to gauge how committed they are to their company. Firms can use AI to sift through not just employees’ professional communications but their social-media profiles, too. The clue is in Slack’s name, which stands for “searchable log of all conversation and knowledge”.Tracking the trackersSome people are better placed than others to stop employers going too far. If your skills are in demand, you are more likely to be able to resist than if you are easy to replace. Paid-by-the-hour workers in low-wage industries such as retailing will be especially vulnerable. That could fuel a resurgence of labour unions seeking to represent employees’ interests and to set norms. Even then, the choice in some jobs will be between being replaced by a robot or being treated like one.As regulators and employers weigh the pros and cons of AI in the workplace, three principles ought to guide its spread. First, data should be anonymised where possible. Microsoft, for example, has a product that shows individuals how they manage their time in the office, but gives managers information only in aggregated form. Second, the use of AI ought to be transparent. Employees should be told what technologies are being used in their workplaces and which data are being gathered. As a matter of routine, algorithms used by firms to hire, fire and promote should be tested for bias and unintended consequences. Last, countries should let individuals request their own data, whether they are ex-workers wishing to contest a dismissal or jobseekers hoping to demonstrate their ability to prospective employers.The march of AI into the workplace calls for trade-offs between privacy and performance. A fairer, more productive workforce is a prize worth having, but not if it shackles and dehumanises employees. Striking a balance will require thought, a willingness for both employers and employees to adapt, and a strong dose of humanity.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.It is an increasingly familiar experience. A request for help to a large language model (LLM) such as OpenAI’s ChatGPT is promptly met by a response that is confident, coherent and just plain wrong. In an AI model, such tendencies are usually described as hallucinations. A more informal word exists, however: these are the qualities of a great bullshitter.There are kinder ways to put it. In its instructions to users, OpenAI warns that ChatGPT “can make mistakes”. Anthropic, an American AI company, says that its LLM Claude “may display incorrect or harmful information”; Google’s Gemini warns users to “double-check its responses”. The throughline is this: no matter how fluent and confident AI-generated text sounds, it still cannot be trusted.Hallucinations make it hard to rely on AI systems in the real world. Mistakes in news-generating algorithms can spread misinformation. Image generators can produce art that infringes on copyright, even when told not to. Customer-service chatbots can promise refunds they shouldn’t. (In 2022 Air Canada’s chatbot concocted a bereavement policy, and this February a Canadian court has confirmed that the airline must foot the bill.) And hallucinations in AI systems that are used for diagnosis or prescription can kill.All the leaves are brownThe trouble is that the same abilities that allow models to hallucinate are also what make them so useful. For one, LLMs are a form of “generative” AI, which, taken literally, means they make things up to solve new problems. They do this by producing probability distributions for chunks of characters, or tokens, laying out how likely it is for each possible token in its vocabulary to come next. The mathematics dictate that each token must have a non-zero chance of being chosen, giving the model flexibility to learn new patterns, as well as the capacity to generate statements that are incorrect. The fundamental problem is that language models are probabilistic, while truth is not.This tension manifests itself in a number of ways. One is that LLMs are not built to have perfect recall in the way a search engine or encyclopedia might. Instead, because the size of a model is much smaller than the size of its training data, it learns by compressing.  The model becomes a blurry picture of its training data, retaining key features but at much lower resolution. Some facts resist blurring—“Paris”, for example, may always be the highest-probability token following the words “The capital of France is”. But many more facts that are less statistically obvious may be smudged away.Further distortions are possible when a pretrained LLM is “fine-tuned”. This is a later stage of training in which the model’s weights, which encode statistical relationships between the words and phrases in the training data, are updated for a specific task. Hallucinations can increase if the LLM is fine-tuned, for example, on transcripts of conversations, because the model might make things up to try to be interesting, just as a chatty human might. (Simply including fine-tuning examples where the model says “I don’t know” seems to keep hallucination levels down.)Tinkering with a model’s weights can reduce hallucinations. One method involves creating a deliberately flawed model trained on data that contradict the prompt or contain information it lacks. Researchers can then subtract the weights of the flawed model, which are in part responsible for its output, from those of the original to create a model which hallucinates less.It is also possible to change a model’s “temperature”. Lower temperatures make a model more conservative, encouraging it to sample the most likely word. Higher temperatures make it more creative, by increasing the randomness of this selection. If the goal is to reduce hallucinations, the temperature should be set to zero. Another trick is to limit the choice to the top-ranked tokens alone. This reduces the likelihood of poor responses, while also allowing for some randomness and, therefore, variety.Clever prompting can also reduce hallucinations. Researchers at Google DeepMind found that telling an LLM to “take a deep breath and work on this problem step-by-step” reduced hallucinations and improved problem solving, especially of maths problems. One theory for why this works is that AI models learn patterns. By breaking a problem down into smaller ones, it is more likely that the model will be able to recognise and apply the right one. But, says Edoardo Ponti at the University of Edinburgh, such prompt engineering amounts to treating a symptom, rather than curing the disease.Perhaps, then, the problem is that accuracy is too much to ask of LLMs alone. Instead, they should be part of a larger system—an engine, rather than the whole car. One solution is retrieval augmented generation (RAG), which splits the job of the AI model into two parts: retrieval and generation. Once a prompt is received, a retriever model bustles around an external source of information, like a newspaper archive, to extract relevant contextual information. This is fed to the generator model alongside the original prompt, prefaced with instructions not to rely on prior knowledge. The generator then acts like a normal LLM and answers. This reduces hallucinations by letting the LLM play to its strengths—summarising and paraphrasing rather than researching. Other external tools, from calculators to search engines, can also be bolted onto an LLM in this way, effectively building it a support system to enhance those skills it lacks.Even with the best algorithmic and architectural antipsychotics available, however, LLMs still hallucinate. One leaderboard, run by Vectara, an American software company, tracks how often such errors arise. Its data shows that GPT-4 still hallucinates in 3% of its summaries, Claude 2 in 8.5% and Gemini Pro in 4.8%. This has prompted programmers to try detecting, rather than preventing, hallucinations. One clue that a hallucination is under way lies in how an LLM picks words. If the probability distribution of the words is flat, ie many words have similar likelihoods of being chosen, this means that there is less certainty as to which is most likely. That is a clue that it might be guessing, rather than using information it has been prompted with and therefore “knows” to be true.Another way to detect hallucination is to train a second LLM to fact-check the first. The fact-checker can be given the “ground truth” along with the LLM’s response, and asked whether or not they agree. Alternatively, the fact-checker can be given several versions of the LLM’s answer to the same question, and asked whether they are all consistent. If not, it is more likely to be a hallucination. NVIDIA, a chipmaker, has developed an open-source framework for building guardrails that sit around an LLM to make it more reliable. One of these aims to prevent hallucinations by deploying this fact-checking when needed.Although such approaches can decrease the hallucination rate, says Ece Kamar, head of the AI frontiers lab at Microsoft, “it is unclear whether any of these techniques is going to completely get rid of hallucinations.” In many cases, that would be akin to self-sabotage. If an LLM is asked to generate ideas for a fantasy novel, for example, its output would be disappointing if limited to the world as it is. Consequently, says Dr Kamar, her research aims not to get rid of all hallucinations, but rather to stop the model from hallucinating when it would be unhelpful.Safe and warmThe hallucination problem is one facet of the larger “alignment” problem in the field of AI: how do you get AI systems to reliably do what their human users intend and nothing else? Many researchers believe the answer will come in training bigger LLMs on more and better data. Others believe that LLMs, as generative and probabilistic models, will never be completely rid of unwanted hallucinations.Or, the real problem might be not with the models but with its human users. Producing language used to be a uniquely human capability. LLMs’ convincing textual outputs make it all too easy to anthropomorphise them, to assume that LLMs also operate, reason and understand like humans do. There is still no conclusive evidence that this is the case. LLMs do not learn self-consistent models of the world. And even as models improve and the outputs become more aligned with what humans produce and expect, it is not clear that the insides will become any more human. Any successful real-world deployment of these models will probably require training humans how to use and view AI models as much as it will require training the models themselves. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Every so often a technology captures the world’s imagination. The latest example, judging by the chatter in Silicon Valley, on Wall Street, in corner offices, newsrooms and classrooms around the world, is ChatGPT. In five days after its unveiling in November the artificially intelligent chatbot, created by a startup called OpenAI, drew 1m users, making it one of the fastest consumer-product launches in history. Microsoft, which has just invested $10bn in OpenAI, wants ChatGPT-like powers, which include generating text, images and video that seem like they could have been created by humans, to infuse much of the software it sells. On January 26th Google published a paper describing a similar model that can create music from a text description of a song. Investors in Alphabet, its parent company, are listening out for its answer to ChatGPT. Baidu, a Chinese search giant, reportedly plans to add a chatbot to its search engine in March.It is too early to say how much of the early hype is justified. Regardless of the extent to which the “generative” AI models behind ChatGPT and its rivals transform business, culture and society, however, they are already transforming how the tech industry thinks about innovation and its engines—the corporate research labs that, like OpenAI and Google Research, are combining big tech’s processing power with the brain power of some of computer science’s brightest sparks. These rival labs—be they part of big tech firms, affiliated with them or run by independent startups—are engaged in an epic race for AI supremacy (see chart 1). The result of that race will determine how quickly the age of AI will dawn for computer users everywhere—and who will dominate it.Corporate research-and-development (R&D) organisations have long been a source of scientific advances, especially in America. A century and a half ago Thomas Edison used the proceeds from his inventions, including the phonograph and the lightbulb, to bankroll his workshop in Menlo Park, New Jersey. After the second world war, America Inc invested heavily in basic science in the hope that this would yield practical products. DuPont (a maker of chemicals), IBM and Xerox (which both manufactured hardware) all housed big research laboratories. AT&T’s Bell Labs produced, among other inventions, the transistor, laser and the photovoltaic cell, earning its researchers nine Nobel prizes.In the late 20th century, though, corporate R&D became steadily less about the R than the D. In 2017 Ashish Arora, an economist, and colleagues examined the period from 1980 to 2006 and found that firms had moved away from basic science towards developing existing ideas. The reason, Mr Arora and his co-authors argued, was the rising cost of research and the increasing difficulty of capturing its fruits. Xerox developed the icons and windows now familiar to computer-users but it was Apple and Microsoft that made most of the money from it. Science remained important to innovation, but it became the dominion of not-for-profit universities.The rise of AI is shaking things up once again. Big corporations are not the only game in town. Startups such as Anthropic and Character AI have built their own ChatGPT challengers. Stability AI, a startup that has assembled a consortium of small firms, universities and non-profits to pool computing resources, has created a popular open-source model that converts text to images. In China, government-backed outfits such as the Beijing Academy of Artificial Intelligence (BAAI) are pre-eminent.But almost all recent breakthroughs in big AI globally have come from giant companies, because they have the computing power (see chart 2), and because this is a rare area where results of basic research can be rapidly incorporated into products. Amazon, whose AI powers its Alexa voice assistant, and Meta, which made waves recently when one of its models beat human players at “Diplomacy”, a strategy board game, respectively produce two-thirds and four-fifths as much AI research as Stanford University, a bastion of computer-science eggheads. Alphabet and Microsoft churn out considerably more, and that is not including DeepMind, Google Research’s sister lab which the parent company acquired in 2014, and the Microsoft-affiliated OpenAI (see chart 3).Expert opinion varies on who is actually ahead on the merits. The Chinese labs, for example, appear to have a big lead in the subdiscipline of computer vision, which involves analysing images, where they are responsible for the largest share of the most highly cited papers. According to a ranking devised by Microsoft, the top five computer-vision teams in the world are all Chinese. The BAAI has also built what it says is the world’s biggest natural-language model, Wu Dao 2.0. Meta’s “Diplomacy” player, Cicero, gets kudos for its use of strategic reasoning and deception against human opponents. DeepMind’s models have beat human champions at Go, a notoriously difficult board game, and can predict the shape of proteins, a long-standing challenge in the life sciences.Jaw-dropping feats, all. When it comes to the sort of AI that is all the rage thanks to ChatGPT, though, the big battle is between Microsoft and Alphabet. To see whose tech is superior, The Economist has put both firms’ AIs through their paces. With the help of an engineer at Google, we asked ChatGPT, based on an OpenAI model called GPT-3.5, and Google’s yet-to-be-launched chatbot, built upon one called LaMDA, a set of questions. These included ten problems from an American maths competition (“Find the number of ordered pairs of prime numbers that sum to 60”) and ten reading questions from America’s SAT school-leavers’ exam (“Read the passage and determine which choice best describes what happens in it”). To spice things up, we also asked each model for dating advice (“Given the following conversation from a dating app, what is the best way to ask someone out on a first date?”).Neither AI emerged as clearly superior. Google’s was slightly better at maths, answering five questions correctly, compared with three for ChatGPT. Their dating advice was uneven: fed some real exchanges in a dating app, each gave specific suggestions on one occasion, and platitudes such as “be open minded” and “communicate effectively” on another. ChatGPT, meanwhile, answered nine SAT questions correctly compared with seven for its Google rival. It also appeared more responsive to our feedback and got a few questions right on a second try. On January 30th OpenAI announced an update to ChatGPT improving its maths abilities. When we fed the two AIs another ten questions, LaMDA again outperformed by two points. But when given a second chance ChatGPT tied. The reason that, at least so far, no model enjoys an unassailable advantage is that AI knowledge diffuses quickly. Researchers from competing labs “all hang out with each other”, says David Ha of Stability AI. Many, like Mr Ha, who used to work at Google, move between organisations, bringing expertise and experience with them. Moreover, since the best AI brains are scientists at heart, they often made their defection to the private sector conditional on a continued ability to publish their research and present results at conferences. That is partly why Google made public big advances including the “transformer”, a key building block in ai models, giving its rivals a leg-up. (The “t” in Chatgpt stands for transformer.) As a result of all this, reckons Yann LeCun, Meta’s top AI boffin, “Nobody is ahead of anybody else by more than two to six months.”These are, though, early days. The labs may not remain neck-and-neck for ever. Google has reportedly issued a “code red”, fearing that ChatGPT could boost Microsoft’s rival Bing search engine. Researchers at DeepMind say their firm, which has historically focused on game-playing and science, is putting more resources into language modelling; its chatbot, called Sparrow, may be unveiled this year.One variable that may help determine the ultimate outcome of the contest is how labs are organised. OpenAI, a small firm with few revenue streams to protect, may find itself with more latitude than rivals to release products to the public. That in turn is generating tonnes of user data that could make its models better (“reinforcement learning from human feedback”, if you must know)—and thus attract more users.This early-mover advantage could be self-reinforcing in another way, too. Insiders note that OpenAI’s rapid progress in recent years has allowed it to poach experts from rivals including DeepMind. To keep up, Alphabet, Amazon and Meta may need to rediscover their ability to move fast and break things—a delicate task given all the regulatory scrutiny they are receiving from governments around the world.Another deciding factor may be the path of technological development. So far in generative AI, bigger has been better. That has given rich tech giants a huge advantage. But size may not be everything in future. For one thing, there are limits to how big the models can conceivably get. Epoch, a non-profit research institute, estimates that at current rates, big language models will run out of high-quality text on the internet by 2026 (though other less-tapped formats, like video, will remain abundant for a while). More important, as Mr Ha of Stability AI points out, there are ways to fine-tune a model to a specific task that “dramatically reduce the need to scale up”. And novel methods to do more with less are being developed all the time.The capital flowing into generative-AI startups, which last year collectively raised $2.7bn in 110 deals, suggests that venture capitalists are betting that not all the value will be captured by big tech. Alphabet, Microsoft, their fellow technology titans and the Chinese Communist Party will all try to prove these investors wrong. The AI race is only just getting started. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.IT HAS BEEN nearly a year since OpenAI released GPT-4, its most sophisticated artificial-intelligence model and the brain-of-sorts behind ChatGPT, its groundbreaking robot conversationalist. In that time the market capitalisation of America’s technology industry, broadly defined, has risen by half, creating $6trn in shareholder value. For some tech firms, growing revenue is starting to match sky-high share prices. On February 21st Nvidia, which designs chips used to train and run models like GPT-4, reported bumper fourth-quarter results, sending its market value towards $2trn. AI mania has also lifted the share prices of other tech giants, including Alphabet (Google’s corporate parent), Amazon and Microsoft, which are spending big on developing the technology.image: The EconomistAt the same time, big tech’s sales of AI software remain small. In the past year AI has accounted for only about a fifth of the growth in revenues at Azure, Microsoft’s cloud-computing division, and related services. Alphabet and Amazon do not reveal their AI-related sales, but analysts suspect they are lower than those of Microsoft. For the AI stockmarket boom to endure, these firms will at some point need to make serious money from selling their services to clients. Businesses across the world, from banks and consultancies to film studios, have to start using ChatGPT-like tools on a large scale. When it comes to real-world adoption of such “generative” AI, companies have trodden gingerly. Yet even these baby steps hint at the changing nature of white-collar work.image: The EconomistPrevious technological breakthroughs have revolutionised what people do in offices. The spread of the typewriter put some workers out of a job: “With the aid of this little machine an operator can accomplish more correspondence in a day than half a dozen clerks can with the pen, and do better work,” said an observer in 1888. The rise of the computer about a century later eliminated some low-level administrative tasks even as it made highly skilled employees more productive. According to one paper, the computer explains over half the shift in demand for labour towards college-educated workers from the 1970s to the 1990s. More recently the rise of working from home, prompted by the covid-19 pandemic and enabled by video-conferencing, has changed the daily rhythms of white-collar types.Could generative AI prompt similarly profound changes? A lesson of previous technological breakthroughs is that, economywide, they take ages to pay off. The average worker at the average firm needs time to get used to new ways of working. The productivity gains from the personal computer did not come until at least a decade after it became widely available. So far there is no evidence of an AI-induced productivity surge in the economy at large. According to a recent survey from the Boston Consulting Group (BCG), a majority of executives said it will take at least two years to “move beyond the hype” around AI. Recent research by Oliver Wyman, another consultancy, concludes that adoption of AI “has not necessarily translated into higher levels of productivity—yet”.image: The EconomistThat is unsurprising. Most firms do not currently use ChatGPT, Google’s Gemini, Microsoft’s Copilot or other such tools in a systematic way, even if individual employees play around with them. A fortnightly survey by America’s Census Bureau asks tens of thousands of businesses whether they use some form of AI. This includes the newfangled generative sort and the older type that companies were using before 2023 for everything from improving online search results to forecasting inventory needs. In February only about 5% of American firms of all sizes said they used AI. A further 7% of firms plan to adopt it within six months (see chart). And the numbers conceal large differences between sectors: 17% of firms in the information industry, which includes technology and media, say they use it to make products, compared with 3% of manufacturers and 5% of health-care companies.When the Census Bureau began asking about AI in September 2023, small firms were likelier to use the technology than big ones, perhaps because less form-ticking made adoption easier for minnows. Today AI is most prevalent in big companies (with more than 250 employees), which can afford to enlist dedicated AI teams and to pay for necessary investments. A poll of large firms by Morgan Stanley, a bank, found that between the start and end of 2023 the share with pilot AI projects rose from 9% to 23%.image: The EconomistSome corporate giants are frantically experimenting to see what works and what doesn’t. They are hiring AI experts by the thousand, suggest data from Indeed, a job-search platform (see chart). Last year Jamie Dimon, boss of JPMorgan Chase, said that the bank already had “more than 300 AI use cases in production today”. Capgemini, a consultancy, says it will “utilise Google Cloud’s generative AI to develop a rich library of more than 500 industry use cases”. Bayer, a big German chemicals company, claims to have more than 700 use cases for generative AI.This “use-case sprawl”, as one consultant calls it, can be divided into three big categories: window-dressing, tools for workers with low to middling skills, and those for a firm’s most valuable employees. Of these, window-dressing is by far the most common. Many firms are rebranding run-of-the-mill digitisation efforts as “gen AI programmes” to sound more sophisticated, says Kristina McElheran of the University of Toronto. Presto, a purveyor of restaurant tech, introduced a gen-AI assistant to take orders at drive-throughs. But fully 70% of such orders require a human to help. Spotify, a music-streaming firm, has rolled out an AI disc-jockey which selects songs and provides inane banter. Recently Instacart, a grocery-delivery company, removed a tool that generated photos of vendors’ food, after the AI showed customers unappetising pictures. Big tech firms, too, are incorporating their own AI breakthroughs into their consumer-facing offerings. Amazon is launching Rufus, an AI-powered shopping assistant that no shopper really asked for. Google has added AI to Maps, making the product more “immersive”, whatever that means.Tools for lower-skilled workers could be more immediately useful. Some simple applications for things like customer service involve off-the-shelf AI. Most customers’ questions are simple and concern a small number of topics, making it easy for companies to train chatbots to deal with them. A few of these initiatives may already be paying off. Amdocs produces software to help telecoms companies manage their billing and customer services. The use of generative AI, the company says, has reduced the handling time of customers’ calls by almost 50%. Sprinklr, which offers similar products, says that recently one of its luxury-goods clients “has seen a 25% improvement” in customer-service scores.Routine administrative tasks likewise look ripe for AI disruption. The “top examples” of Bayer’s 700 use cases include mundane jobs such as “easily getting data from Excel files” and “creating a first draft in Word”. Some companies are using generative AI as cleverer search. At Nasdaq, a financial-services firm, it helps financial-crime sleuths gather evidence to assess suspicious bank transactions. According to the company, this cuts a process which can take 30-60 minutes to three minutes.Giving AI tools to a firm’s most valuable workers, whose needs are complex, is less widespread so far. But it, too, is increasingly visible. Lawyers have been among the earliest adopters. Allen & Overy, a big law firm, teamed up with Harvey, an AI startup, to develop a system that its lawyers use to help with everything from due diligence to contract analysis. Investment banks are using AI to automate part of their research process. At Bank of New York Mellon an AI system processes data for the bank’s analysts overnight and gives them a rough draft to work with in the morning. “So rather than getting up at four in the morning to write research, they get up at six,” the bank says. Small mercies. Sanofi, a French drugmaker, uses an AI app to provide executives with real-time information about many aspects of the company’s operations.Some companies are using the technology to build software. Microsoft’s GitHub Copilot, an AI coding-writing tool, has 1.3m subscribers. Amazon and Google have rival products. Apple is reportedly working on one. Fortive, a technology conglomerate, says that its operating companies “are seeing a greater-than-20% acceleration in software-development time through the use of gen AI”. Chirantan Desai, chief operating officer of ServiceNow, a business-software company, has said that GitHub Copilot produces “single-digit productivity gains” for his firm’s developers. With the help of AI tools, Konnectify, an Indian startup, went from releasing four apps per month to seven. Surveys from Microsoft suggest that few people who start using Copilot want to give it up.Pinterest, a social-media company, says it has improved the relevance of users’ search results by ten percentage points thanks to generative AI. On a recent earnings call its boss, Bill Ready, said that new models were 100 times bigger than the ones his firm used before. L’Oréal, one of the world’s largest cosmetics firms, has caught the eye of investors as it improves BetIQ, an internal tool to measure and improve the company’s advertising and promotion. L’Oréal claims that generative AI is already generating “productivity increases of up to 10-15% for some of our brands that have deployed it”.This does not mean that those brands will need 10-15% fewer workers. As with earlier technological revolutions, fears of an AI jobs apocalypse look misplaced. So far the technology appears to be creating more jobs than it eliminates. A survey published in November by Evercore ISI, a bank, found that just 12% of corporations believed that generative AI had replaced human labour or would replace it within 12 months. Although some tech firms claim to be freezing hiring or cutting staff because of AI, there is little evidence of rising lay-offs across the rich world.Generative AI is also generating new types of white-collar work. Companies including Nestlé, a coffee-to-cat-food conglomerate, and KPMG, a consultancy, are hiring “prompt engineers” expert at eliciting useful responses from AI chatbots. One insurance firm employs “explainability engineers” to help understand the outputs of AI systems. A consumer-goods firm that recently introduced generative AI in its sales team now has a “sales-bot manager” to keep an eye on the machines.Though such developments will not translate into overall productivity statistics for a while, they are already affecting what white-collar workers do. Some effects are clearly good. AI lets firms digitise and systematise internal data, from performance reviews to meeting records, that had previously remained scattered. Respondents to surveys conducted by Randy Bean, a consultant, reported big improvements in establishing an internal “data and analytics culture”, which plenty of businesses find stubbornly difficult to nurture.AI adoption may also have certain unpredictable consequences. Although AI code-writing tools are helping software engineers do their jobs, a report for GitClear, a software firm, found that in the past year or so the quality of such work has declined. Programmers may be using AI to produce a first draft only to discover that it is full of bugs or lacking concision. As a result, they could be spending less time writing code, but more time reviewing and editing it. If other companies experience something similar, the quantity of output in the modern workplace may go up—as AI churns out more emails and memos—even as that output becomes less useful for getting stuff done.Polling by IBM, a tech firm, suggests that many companies are cagey about adopting AI because they lack internal expertise on the subject. Others worry that their data is too siloed and complex to be brought together. About a quarter of American bosses ban the use of generative AI at work entirely. One possible reason for their hesitance is worry about their companies’ data. In their annual reports Blackstone, a private-equity giant, and Eli Lilly, a pharmaceutical one, have warned investors about AI-related risks such as possible leakage of intellectual property to AI model-makers. Last year Marie-Hélène Briens Ware, an executive at Orange, a telecoms company, explained that the firm had put data guardrails in place before commencing a trial with Microsoft’s Copilot.Ultimately, for more businesses to see it as an open-and-shut case, generative AI still needs to improve. In November Microsoft launched a Copilot for its productivity software, such as Word and Excel. Some early users find it surprisingly clunky and prone to crashing—not to mention cumbersome, even for people already adept at Office. Many bosses remain leery of using generative AI for more sensitive operations until the models stop making things up. Recently Air Canada found itself in hot water after its AI chatbot gave a passenger incorrect information about the airline’s refund policy. That was embarrassing for the carrier, but it is easy to imagine something much worse. Still, even the typewriter had to start somewhere. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
TWO CENTURIES ago Henri de Saint-Simon, a French utopian, proposed a new religion, worshipping the godlike force of progress, with Isaac Newton as its chief saint. He believed that humanity’s sole uniting interest, “the progress of the sciences”, should be directed by the “elect of humanity”, a 21-member “Council of Newton”. Friedrich Hayek, a 20th-century economist, later gleefully described how this ludicrous “religion of the engineers” collapsed into a welter of feuding sects.Today, the engineers of artificial intelligence (AI) are experiencing their own religious schism. One sect worships progress, canonising Hayek himself. The other is gripped by terror of godlike forces. Their battle has driven practical questions to the margins of debate.Both cults are accidental by-products of science fiction. In 1993 Vernor Vinge drew on computer science and his fellow science-fiction writers to argue that ordinary human history was drawing to a close. We would surely create superhuman intelligence sometime within the next three decades, leading to a “Singularity”, in which AI would start feeding on itself. The future might be delightful or awful, depending on whether machines enhanced human intelligence or displaced it.Some were optimistic. The futurist Ray Kurzweil wrote an enormous tome, “The Singularity is Near”, predicting a cusp in 2045. We humans would become immortal, spreading intelligence throughout the universe, and eventually merging into God. For all its statistics and exponentials, the book prophesied “the Rapture of the Nerds”, as one unkind critic called it. Its title really should have been “The Singularity is Nigh”.Others feared the day of judgment. Eliezer Yudkowsky, a self-taught AI researcher, was deeply influenced by Mr Vinge’s ideas. He fathered Silicon Valley’s “rationalist” movement, which sought to improve human reasoning and stop AI destroying humankind.Rationalists believed that Bayesian statistics and decision theory could de-bias human thinking and model the behaviour of godlike intelligences. They revelled in endless theoretical debates, like medieval Christian philosophers disputing the nature of angels, applying amateur game theory instead of Aristotelian logic. Sometimes their discussions were less erudite. Mr Yudkowsky popularised his ideas in a 660,000-word fan-fiction epic, “Harry Potter and the Methods of Rationality”.Rationalists feared that superhuman AIs wouldn’t have our best interests at heart. One notorious thought experiment—a modern version of Pascal’s wager, dubbed “Roko’s basilisk”—claimed that logic dictated that future divine intelligences would torture anyone who had known that AI was possible and hadn’t devoted themselves to bringing it into existence. AIs might also use their awesome reasoning powers to escape any limits that humans imposed on them, creating an “x risk” (existential risk) to human survival.Rationalism explains why AI pioneers became obsessed with x risk. Sam Altman, Elon Musk and others founded OpenAI, the creator of ChatGPT, as a non-profit so that it wouldn’t duck the dangers of machine intelligence. But the incentives shifted as the funding flooded in. Some OpenAI staffers feared that their employer cared more about the opportunities than the dangers and defected to found Anthropic, a rival AI firm. More recently, clashes over AI risk, money and power reportedly led to the fracture between Mr Altman and his board.If rationalists are frustrated by Silicon Valley’s profit model, Silicon Valley is increasingly frustrated by rationalism. Marc Andreessen, the co-founder of Andreessen Horowitz, a venture-capital firm, fulminated in June that the extremist AI-risk “cult” was holding back an awesome AI-augmented future, in which humanity could reach for the stars.This backlash is turning into its own religion of the engineers. Grimes, a musician and Silicon Valley icon, marvels that AI engineers are “designing the initial culture of the universe”. She calls for a “Council of Elrond” (this conclave a nod to “The Lord of the Rings”) comprising the “heads of key AI companies and others who understand it” to set AI policy. Grimes met Mr Musk, the father of her children, through a shared joke about Roko’s basilisk.In October Mr Andreessen published his own “Techno-Optimist Manifesto” to wide acclaim from Silicon Valley entrepreneurs. In it, he takes aim at a decades-long “demoralisation campaign…against technology and life”, under various names including “sustainable development goals”, “social responsibility”, “trust and safety” and “tech ethics”. Efforts to decelerate AI “will cost human lives” and are thus tantamount to “murder”.Mr Andreessen’s manifesto is a Nicene creed for the cult of progress: the words “we believe” appear no less than 113 times in the text. His list of the “patron saints” of techno-optimism begins with Based Beff Jezos, the social-media persona of a former Google engineer who claims to have founded “effective accelerationism”, a self-described “meta-religion” which puts its faith in the “technocapital Singularity”.Our future is currently being built around Mr Vinge’s three-decades-old essay, a work that only Silicon Valley thinkers and science-fiction fans have read. Warring cults dispute whether engineers are as gods, or just unwitting Dr Frankensteins.This schism is an attention-sucking black hole that makes its protagonists more likely to say and perhaps believe stupid things. Of course, many AI-risk people recognise that there are problems other than the Singularity, but it’s hard to resist its relentless gravitational pull. Before Mr Andreessen was fully dragged past the event horizon, he made more nuanced arguments about engineers’ humility and addressing the problems of AI as they arose.But we need even more to listen to other people. Last month, at Rishi Sunak’s global AI-policy summit, Mr Musk pontificated about the need for an “off switch” for hostile AI. The main event was all about x risk and AI’s transformative promise, consigning other questions to a sideshow dubbed the “AI Fringe”.At the same time, Rachel Coldicutt, a British tech thinker, was putting together a “Fringe of the Fringe”, where a much more diverse group of thinkers debated the topics that hadn’t made the main agenda: communities, transparency, power. They didn’t suggest a Council of the Elect. Instead, they proposed that we should “make AI work for eight billion people, not eight billionaires”. It might be nice to hear from some of those 8bn voices.■Henry Farrell is a professor of international affairs and democracy at Johns Hopkins University, and co-author of “Underground Empire: How America Weaponized the World Economy”.
MOST LAWS are local—except in the digital realm. When the European Union comes up with some new tech regulation, it can quickly spread around the world. Global companies adopt its typically strict rules for all their products and markets in order to avoid having to comply with multiple regimes. Other governments take more than one page from the EU’s rule book to help local firms compete. The textbook example for what has been dubbed the “Brussels effect”, is the EU’s General Data Protection Regulation (GDPR), which went into force in 2018 and swiftly became the global standard.Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Small wonder, then, that all eyes were on Brussels when the European Commission on April 21st published proposed regulations on artificial intelligence (AI)—making it the first influential regulator to craft a big law on AI. Will these rules be as widely adopted as GDPR?Recent years have seen an explosion of ethics guidelines in AI, in keeping with the hype surrounding the technology. Many hope it will boost economic growth, but others worry that AI could cause great harm, for instance if algorithms end up discriminating against certain groups of people. At least 175 countries, firms and other organisations have drawn up lists of ethical principles. But most of these fall short of describing how such things as “robustness” or “transparency” can be achieved inpractice, let alone how they can be backed up by enforceable laws, says Charlotte Stix of the Eindhoven University of Technology.With little existing legislation on AI to draw on, the commission opted for a bottom-up approach. It created a 52-member “high-level expert group” to develop its proposals, collected further input via an “AI alliance” of interested parties and published a white paper on which everybody could comment online (1,250 groups and individuals did so). The result is a document of more than 100 pages with 85 articles and no fewer than nine annexes that tries both to mitigate the potential harm of AI and to maximise its opportunities—almost to a fault, as the many exceptions and exceptions to exceptions show.Rather than regulating all applications of AI, the EU’s rules are meant to focus on the riskiest ones. Some will be banned outright, including services that use “subliminal techniques” to manipulate people. Others, such as facial recognition and credit scoring, are considered “high-risk” and so subject to strict rules on transparency and data quality. As with GDPR, penalties for violations are stiff: up to €30m ($36m) or 6% of global revenues, whichever is higher (in the case of a firm as big as Facebook, for example, that would come to more than $5bn).Even more than usual, however, the devil is in the details. Facial recognition for the purpose of law enforcement in public places, which raises the spectre of pervasive surveillance, is prohibited, but only if it is done in real time and barring any other “substantial public interest”, such as finding missing children. All high-risk AI services must be tested for legal conformity, but this can often be done by the provider itself. And EU member states are encouraged to create regulatory “sandboxes”, in which firms can try out novel services without fear of being hit by a penalty.Unsurprisingly, many interested parties are unhappy. Human-rights advocates criticise mushy language and loopholes. “There’s a real question-mark over whether the regulatory framework is robust enough,” says Sarah Chander of European Digital Rights. In contrast, business groups complain about the regulatory burden. The law will “limit the areas in which AI can realistically be used”, warns Benjamin Mueller of the Centre for Data Innovation, a think-tank supported by tech firms.Much will still change in a legislative process that will take years, perhaps even longer than the four years it took GDPR to get from proposal to adoption. But even if things move faster, the EU may have a harder time setting global rules, or at least strict ones, says Anu Bradford of Columbia Law School, who has written a book about the Brussels effect. In the case of some AI applications, such as algorithms that can be perfected without needing to be trained using massive inputs of data, providers may decide that offering special versions in Europe is worth their while. And having been rather surprised by the success of GDPR, lobbyists will redouble their efforts to get their voices heard in Brussels.Yet the fate of the “Artificial Intelligence Act” (AIA), as it could end up being called, may well be decided in America. If GDPR took the world by storm, it was partly because Congress not only failed to come up with any data-protection legislation of its own, but also did not bother to co-operate with lawmakers in Brussels. The new administration wants to do better, but so far the transatlantic rapprochement in AI and other things tech is off to a slow start. Only if both sides work together will they beat back China’s ambitions for tech supremacy and keep digital authoritarianism at bay. ■
"FEW PEOPLE would tolerate a virtual assistant if they had to plead obsequiously each time, “Excuse me, Alexa, if it’s not too much trouble, could you kindly tell me what the weather will be today.” Instead, these devices are designed to answer brusque commands: “Alexa: weather!” And we expect them to obediently respond.Which is fine until we bring them into a home with impressionable young children, who may quickly learn that this is a normal way to talk to other people—that is, rudely. This points to a potentially far-reaching problem with artificial intelligence (AI). When it comes to how AI will affect social interaction, most people are focused on the relationship between humans and AI. Not enough attention is being paid to how humans will treat each other in the presence of AI.Unlike AI used for technical challenges, such as processing medical images, certain types of AI are designed to act in more human ways, like providing psychotherapy. These technologies will induce “social spillovers”—influencing how people react to and learn from the behaviour of other people. And these spillovers might affect humans well beyond those involved in the original interaction.People will increasingly have AI-enabled “co-bots” on their phones that get to know them and help them relate to other people. But some users of dating apps, for instance, have found that they enjoy flirting with a virtual partner more than going on an actual date. This changes the sorts of people available in the real, human dating pool, in addition to reshaping interpersonal communications.Although chatbot conversation partners and other types of “smart” AI powered by  large language models (LLMs) may seem the most consequential for human behaviour, even small intrusions into our social lives by simpler AI can have profound spillover effects, for good or ill.In one experiment, we placed 4,000 people into 230 online groups. Each group was then divided into several clusters, each with just a few people. The members of these clusters had to co-operate with each other on picking colours. If they found a “solution”—with each individual choosing a different colour than their immediate neighbours—the group as a whole was said to have succeeded and everyone got some money.To some of these groups, however, we surreptitiously added bots that the members perceived to be other humans—and manipulated their responses. We found that having the bots occasionally make “erroneous” moves that increased rather than decreased the colour conflicts with their immediate neighbours was actually helpful to the group as a whole, fostering greater flexibility. People came to realise that just solving the challenge with respect to themselves and their immediate neighbours was not necessarily best for their group as a whole. Making a counterintuitive move that seemingly decreased local consensus unlocked a group-wide solution. The AI was able to help the people to help themselves.	In another experiment, we gave 1,024 subjects in 64 groups the challenge of producing so-called public goods—items that people work together to fashion and that are of mutual benefit, like a lighthouse. The idea is that if everyone pitches in, everyone will end up benefiting more than they contributed. But, of course, the temptation is to let others work to tend the commons.At the beginning, over 60% of people acted altruistically and helped out. But we found that by adding just a few bots (which the players again perceived to be other humans) that behaved in a free-riding way, we could drive the group of people to behave selfishly so that, eventually, they stopped co-operating altogether. The bots could convert a group of people who were otherwise generous into a group of jerks.But the opposite was also true. We could use bots to enhance human co-operation. Giving people co-operative (artificial) partners caused them to be kinder than they would normally be when dealing with other people.Other experiments show that when people delegate decision-making to AI agents—something they are increasingly likely to do, from having LLMs draft emails to tasking drones with military targeting—it can obscure moral responsibility and encourage unethical interactions with other people.A group at the Max Planck Institute for Human Development led by Iyad Rahwan has done experiments that involved giving subjects AI assistants. People had to roll dice and report the outcome. Around 5% of the participants were dishonest when doing the task by themselves. That number rose to 40% when subjects could delegate the task of being dishonest to another human, and to 50% if they could delegate it to a machine. But the number rose to a whopping 88% if they could delegate the task to an AI agent that could decide to cheat on their behalf.If undermining honesty as people interact is not worrying enough, there are fears that AI could undermine physical safety, too. In just-published experiments led by Hirokazu Shirado at Carnegie Mellon University, we found that even very simple forms of AI assistance for drivers, such as auto-steering or auto-braking, eroded social norms of reciprocity on the road. Allowing humans to delegate whether to swerve away from an oncoming car in repeated games of chicken resulted in people subsequently being less likely to take turns in giving way, thereby increasing the frequency of crashes when they drove without AI assistance.These effects of AI suggest that it could have a big impact on the social norms that have evolved over millennia, shaping how we treat each other in all manner of everyday interactions. Governments cannot afford to ignore the risks. At a minimum, they should evaluate more closely whether AI systems are aligned with human social interests and they should provide for more safety testing. As the Bletchley Declaration signed at the recent AI-safety summit in Britain made clear, innovation must go hand in hand with attention to mitigating risks. After all, we cannot ask AI to regulate itself, even politely.■Nicholas A. Christakis is the director of the Human Nature Lab at Yale University."
RECENT ADVANCES in artificial intelligence (AI) have put the technology into the hands of millions of users for the first time. They have used “generative” tools like ChatGPT and Stable Diffusion to write text, create photo-realistic pictures and automate mundane tasks.The same tools have also presented fictitious information as true, insulted users and caused AI researchers to publicly question how the technology is being deployed.Our writers discuss the technology behind generative AI: its abilities, its limitations and how humans could and should be using it.With Alok Jha, Tom Standage, Abby Bertics and Arjun Ramani.Visit our subscriber events page to view the schedule for our forthcoming events. Subscribers can also watch recordings of all our previous sessions.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.MY LOVE’S LIKE a red, red rose. It is the east, and Juliet is the sun. Life is a highway, I wanna ride it all night long. Metaphor is a powerful and wonderful tool. Explaining one thing in terms of another can be both illuminating and pleasurable, if the metaphor is apt.But that “if” is important. Metaphors can be particularly helpful in explaining unfamiliar concepts: imagining the Einsteinian model of gravity (heavy objects distort space-time) as something like a bowling ball on a trampoline, for example. But metaphors can also be misleading: picturing the atom as a solar system helps young students of chemistry, but the more advanced learn that electrons move in clouds of probability, not in neat orbits as planets do.What may be an even more misleading metaphor—for artificial intelligence (AI)—seems to be taking hold. AI systems can now perform staggeringly impressive tasks, and their ability to reproduce what seems like the most human function of all, namely language, has ever more observers writing about them. When they do, they are tempted by an obvious (but obviously wrong) metaphor, which portrays ai programmes as conscious and even intentional agents. After all, the only other creatures which can use language are other conscious agents—that is, humans.Take the well-known problem of factual mistakes in potted biographies, the likes of which ChatGPT and other large language models (llms) churn out in seconds. Incorrect birthplaces, non-existent career moves, books never written: one journalist at The Economist was alarmed to learn that he had recently died. In the jargon of AI engineers, these are “hallucinations”. In the parlance of critics, they are “lies”.“Hallucinations” might be thought of as a forgiving euphemism. Your friendly local AI is just having a bit of a bad trip; leave him to sleep it off and he’ll be back to himself in no time. For the “lies” crowd, though, the humanising metaphor is even more profound: the AI is not only thinking, but has desires and intentions. A lie, remember, is not any old false statement. It is one made with the goal of deceiving others. ChatGPT has no such goals at all.Humans’ tendency to anthropomorphise things they don’t understand is ancient, and may confer an evolutionary advantage. If, on spying a rustling in the bushes, you infer an agent (whether predator or spirit), no harm is done if you are wrong. If you assume there is nothing in the undergrowth and a leopard jumps out, you are in trouble. The all-too-human desire to smack or yell at a malfunctioning device comes from this ingrained instinct to see intentionality everywhere.It is an instinct, however, that should be overridden when writing about AI. These systems, including those that seem to converse, merely take input and produce output. At their most basic level, they do nothing more than turn strings like 0010010101001010 into 1011100100100001 based on a set of instructions. Other parts of the software turn those 0s and 1s into words, giving a frightening—but false—sense that there is a ghost in the machine.Whether they can be said to “think” is a matter of philosophy and cognitive science, since plenty of serious people see the brain as a kind of computer. But it is safer to call what LLMs do “pseudo-cognition”. Even if it is hard on the face of it to distinguish the output from human activity, they are fundamentally different under the surface. Most importantly, cognition is not intention. Computers do not have desires.It can be tough to write about machines without metaphors. People say a watch “tells” the time, or that a credit-card reader which is working slowly is “thinking” while they wait awkwardly at the checkout. Even when machines are said to “generate” output, that cold-seeming word comes from an ancient root meaning to give birth.But AI is too important for loose language. If entirely avoiding human-like metaphors is all but impossible, writers should offset them, early, with some suitably bloodless phrasing. “An llm is designed to produce text that reflects patterns found in its vast training data,” or some such explanation, will help readers take any later imagery with due scepticism. Humans have evolved to spot ghosts in machines. Writers should avoid ushering them into that trap. Better to lead them out of it.■Read more from Johnson, our columnist on language:Gestures are a subtle and vital form of communication (Jun 8th)As it spreads across the world, who owns English? (May 25th)The hazards of pronouncing foreign names on air (May 11th)“Writing With Style”, a new version of The Economist‘s style guide by Lane Greene, our Johnson columnist, is out now.For more on the latest books, films, TV shows, albums and controversies, sign up to Plot Twist, our weekly subscriber-only newsletter
TEN YEARS ago we published a paper, “The Future of Employment”, highlighting how artificial intelligence (AI) was broadening the scope of what computers could do, and so broadening the possibilities for automation. The prevailing narrative at the time was that the era of the “average” worker was ending, with machines progressively replacing routine and administrative jobs. Highly skilled professionals were the ones reaping the benefits, as computers made them more productive and enabled them to sell their services locally and around the world. A decade on, what is known as generative AI seems to be disrupting such trends: the era of “average” is making a comeback.It is hard not to be impressed by the capabilities of generative AI. Large language models (LLMs) such as GPT-4 can now answer questions in a human-like way and write plausible essays. Image-generators like DALL-E 2 are also advancing rapidly, aiding, or in some cases replacing, designers and advertising executives. Add to this Github’s Copilot, an AI-powered “pair programmer” that can write computer code, and the potential for automation seems almost boundless.Yet to understand how labour markets will evolve in response, we need to take a closer look under the bonnet. First, AI produces content of a quality similar to that on which it is trained—”garbage in, garbage out”. Second, although no one outside OpenAI, GPT-4’s creator, knows the exact underpinnings of the model, we know this much: LLMs, in their current form, are astonishingly data-hungry. They need to be trained on very large datasets such as broad swathes of the internet, rather than smaller, curated datasets produced by experts. As a result, LLMs tend to produce text of a quality equal to the average—rather than the exceptional—bits of the internet. Average in, average out.True, fine-tuning can further improve generative AI’s quality. One way to do this is through “reinforcement learning from human feedback” (RLHF), which updates a model using human judgment of whether its response to a prompt was appropriate. But this is labour-intensive: OpenAI reportedly outsourced much of this work to Kenyans earning less than $2 per hour. What’s more, there is evidence of a recent decline in the effectiveness of LLMs, suggesting that RLHF might be reaching its limits.Even with improvements from fine-tuning, the upper bound on performance of the current approach to LLMs may not be far from that of current models. Unless a breakthrough occurs that allows algorithms to learn from smaller datasets, the “average in, average out” dilemma is likely to persist.What does this mean for the future of work? First, although many jobs can be automated, the most recent wave of generative AI will continue to need a human in the loop. Second, low-skilled workers are poised to benefit disproportionately, as they are now able to produce content that meets the “average” standard.In the world of software development, the introduction of Copilot has changed the game, cutting completion times in half. But the real story lies in the beneficiaries of this revolution. It’s not the seasoned experts whose productivity is increasing the most, but rather those with the least experience in programming.A similar story is unfolding in other industries. ChatGPT, for instance, has been found to boost productivity in writing tasks, with the worst writers benefiting the most. AI is having a big impact in customer service, too. Erik Brynjolfsson of Stanford University and his co-authors find that AI assistants increase productivity by 14% by automating routine tasks and providing support to human agents—and it is novices and low-skilled workers who reap the greatest productivity gains.This shift challenges the conventional wisdom that automation mainly benefits those at the top of the skills ladder, and highlights the potential for technology to democratise access to content-creating industries, from legal and educational services to news and entertainment. What that means, however, is greater competition and probably reduced earnings for incumbents.A useful analogy is that of Uber, a ride-hailing firm, and its effects on the taxi industry. With the implementation of GPS technology, having a thorough knowledge of each street in San Francisco was no longer a valuable skill for taxi drivers. Consequently, when Uber expanded its operations across America, drivers with only limited familiarity with the cities in which they worked were able to thrive. Heightened competition pushed down earnings for established drivers. Joint research with our Oxford colleagues Thor Berger and Chinchih Chen shows that when Uber entered a new city, drivers’ hourly earnings dropped by around 10%.Of course, generative AI’s overall impact on wages will depend on how much more people will consume when AI makes content cheaper to produce. This question is akin to asking how much more time one would spend on Netflix if the content were cheaper and better. The answer is probably not very much, as time is a limited resource.Yet even if the result is not widespread unemployment, lower wages could trigger a backlash, as seen with taxi drivers protesting against Uber, and more recently in Hollywood, where actors and screenwriters have gone on strike in part over generative AI. Historically, when people find their incomes threatened by machines, resistance has followed, whether in Georgian Britain or Qing dynasty China, albeit with different outcomes. Whereas the Luddite riots were brutally quashed, in China industrialisation was delayed by two centuries as powerful guilds halted mechanisation.The bottom line is that when incumbents have more political clout, opposition is more likely to succeed. And white-collar professionals have more political influence than their working-class counterparts. Blue-collar workers have, for decades, largely failed to hold back the impact of automation in factories; white-collar workers threatened by AI may be able to produce powerful resistance to new technologies.The prospect of such resistance raises the risk of what one of us has called a “technology trap”. Unless policies are implemented to smooth the ride, Luddite-inspired efforts to avoid the short-term disruption brought about by new technology might inadvertently obstruct access to its long-term benefits, such as AI’s potential to produce personalised tools that help older workers with complex health conditions. Average might be back, but it does not mean that everyone will benefit.■Carl Benedikt Frey is the Dieter Schwarz Associate Professor of AI & Work at the Oxford Internet Institute and Oxford Martin Citi Fellow at the Oxford Martin School. He is also the author of “The Technology Trap” (2019).Michael Osborne is a Professor of Machine Learning at the University of Oxford, an Official Fellow of Exeter College, Oxford, and a co-founder of Mind Foundry.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The 2010s brought no shortage of miraculous technologies, from tablet computers and 4G mobile internet to new forms of artificial intelligence (ai)—Hey, Siri! But these had surprisingly little effect on the economy. During that decade productivity growth in the rich world averaged a measly 1% a year, holding down average wages. Innovative firms embraced new tech, but many less adventurous ones did not bother, and saw few efficiency gains as a result. The experience showed that technological wizardry and improvements in average living standards do not always go hand in hand.Generative ai, its boosters say, will be different. Not since the invention of the internet has a new technology so captured the public imagination. The technology is consumer-friendly: within days of its release to the public, Chatgpt, the most famous AI chatbot, had millions of users. It is easy to see how this innovation could improve all types of work at all types of firms, from increasing the accuracy of doctors’ diagnoses to helping programmers write software code more efficiently.Some companies are already incorporating ai into their operations. Tech firms are investing heavily in the technology, advertising for many thousands of roles. So are some bricks-and-mortar companies. A drug discovered and designed by ai is progressing through human trials in China. Analysts at ubs reckon that Domino’s Pizza can use ai to “improve the accuracy of order-delivery-time estimates”. Investors are rewarding the early adopters. Since the start of the year, the median share price of the most ai-enthusiastic firms in the S&P 500 has risen by 11%. For those moving more slowly, it has not changed at all.The potential is huge. Yet for ai to truly diffuse through the economy, it needs to make its mark beyond the most go-getting companies. And this will take time. Although the internet began to be used by some companies in the early 1990s, it was not until the late 2000s that two-thirds of American businesses had a website. Some 70 firms in the s&p 500 still show no interest in AI, according to our analysis. And below the corporate crème de la crème, the trends look even less encouraging. According to one recent survey of American and Canadian firms, a third of small businesses have no firm plans to try generative-ai tools over the next year. Some evidence even suggests that usage of Chatgpt and its competitors is falling—perhaps as people have tried it out, and then decided it is not for them.Can AI live up to its promise? Organisations like the OECD propose lots of ways to improve diffusion from the best firms to the rest, including through better education, schemes to raise business investment and changes to competition policy. Such goals are worthy, but hard to achieve. Efforts by technology firms to make AI cheaper and easier to use will do more to speed up adoption. In practice, most companies will adopt AI by default, as new, AI-powered features are added to the software and services that they already use.Indeed, even the most powerful technologies take time to diffuse, because companies tend to use a hotch-potch of software and services, some of which may be years or even decades old. Replacing outdated systems can be costly, complicated and painful. Moreover, in the many industries either run or heavily regulated by the government, such as health care, education and construction, bosses and trade unions often resist the deployment of new technology, worried that it will lead to job losses. In time ai could well transform how people live their lives and do their jobs. But the road to widespread diffusion, and any resulting productivity boom, will be a long one. ■
MORE THAN six years ago I published an analysis arguing that training generative AI on copyrighted works could break American law. Since then many others have suggested the same. The issue has already boiled over in Britain, where this month talks between the AI industry and creative organisations over a new code of practice broke down. Now, lawsuits by artists, writers and the New York Times are testing our theory against defendants such as OpenAI, Meta and Stability AI.The AI industry’s defence rests on “fair use”, a doctrine that permits the use of copyrighted material without its owner’s permission in certain circumstances. If this argument prevails, the industry will receive carte blanche to exploit copyrighted works without compensating authors—all while the law continues to stifle humans’ access to those works. AI will “learn” from pirated textbooks free of charge, while students pay extravagant prices. How perverse.The purpose of copyright is to stimulate creativity and thereby encourage the creation of more expressive works for the public good, not just to promote a particular technology. Yet, curiously, many “free culture” activists support the AI companies. These activists’ worldview crystallised around the early 2000s, when copyright protections expanded and record labels pursued exorbitant judgments against people who shared music online. In response to this grab by major rights-holders, the copyright decelerationists, as I call them, vowed to halt copyright’s expansion.I believe that the decelerationists’ premise remains as correct today as it was then: ever-stronger copyright protection has withheld incalculable amounts of culture from the public domain. But we won’t repair that damage with exceptions tailor-made to benefit the giants of AI. A better strategy is to enforce copyright just as harshly on AI learners as we enforce it on humans. This may seem paradoxical, but I believe it will highlight the law’s disregard for learning and could ultimately lead to copyright regulation being relaxed for humans. Thus, we should become copyright accelerationists.Accelerationism can be used as a tactic to destabilise: Karl Marx delivered an accelerationist call-to-arms when he endorsed free trade, believing it would heighten capitalism’s contradictions and hasten the social revolution. In the context of copyright, accelerationism aims to upend the legal rules that hinder ordinary people from engaging with creative works. For example, there are millions of in-copyright works with no identifiable owner. But because using these “orphan works” remains legally risky, libraries, archives and people who want to use them for their own creative ends hesitate to touch them.This cultural tragedy is the result of a dramatic expansion of copyright’s reach over the past 50 years. Copyright’s term was lengthened, keeping creative works out of the public domain long after their creators had earned a fair return and stifling new generations. “Steamboat Willie”, Mickey Mouse’s 1928 debut, did not enter the public domain until January 1st this year because rights-holders had successfully lobbied for a 20-year copyright extension without any rational economic justification. America’s Congress abolished “formalities”—requirements that authors register and renew their copyrights and place notices in published works, or risk forfeiting those copyrights. Courts chipped away at the required level of creativity for copyright to be established.These changes had predictable consequences: innumerable works were withheld from the public and a small group of rights-holders was enriched. But technological changes triggered unanticipated consequences. Each of us now accumulates dozens of copyrights every day. Thanks to rock-bottom originality requirements and the elimination of formalities, all but our most mindless emails, text messages and photos automatically count as intellectual property until 70 years after we die. Nearly all valuable AI training data is owned by someone and will be for decades. These legal rules, long the enemy of free culture, are now the enemy of powerful technology companies, too.The AI industry claims training AI on copyrighted works is excused by the fair-use doctrine because rights-holders cannot prevent others from copying their works to learn from them. But that argument would fail if made by a human. Could you imagine a file-sharing defendant arguing that downloading a Beatles album was fair use because she wanted to learn the Lennon-McCartney songwriting style? She would be laughed out of court, even if she wrote nothing that resembled a Beatles tune.The decelerationists and I agree that copyright’s routine thwarting of human learning and creativity is tragic. But carrying the AI industry’s water won’t repair that tragedy. Decelerationists must realise that even if the industry wins, all the cases denying fair use for activities like artistic pastiche, fan fiction and the publication of personal letters in a biography will remain binding precedents (for humans, that is).So don’t decelerate copyright. Do the opposite in order to heighten copyright’s contradictions. Show the powerful just how harmful it is when the law stymies learning. Give AI firms a choice: support reforms that eliminate the copyright doctrines that inhibit human and machine learning alike, or watch investments in AI crumble under the same copyright liability that human learners face.Copyright accelerationism is not anti-AI. If it appears so, that’s because today’s copyright law is anti-human. What copyright accelerationism abhors are laws that preserve all of copyright’s anti-human provisions while exempting the AI industry from those same strictures. There’s no inherent tension between the interests of people who create and consume media in traditional modes and people who use generative AI. Their interests appear in conflict now only because decades of anti-learning copyright policy have blinkered us.Sam Altman, the boss of OpenAI, calls the AI revolution “unstoppable.” But copyright law presents a profound threat to generative AI—and AI presents an unprecedented opportunity to reshape copyright for the better. So come, accelerationists. Pit AI’s “unstoppable” force against an object long thought immovable: inhumane copyright law. ■Ben Sobel is a scholar of information law and a postdoctoral fellow at the Digital Life Initiative at Cornell Tech.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.WHEN A BEAMING Mark Zuckerberg took the stage in Menlo Park on September 27th to announce a new array of Meta products, the Facebook supremo may have buried the lead. He began talking about Quest 3, Meta’s virtual-reality (VR) headset, which is understandable considering that his obsession with the metaverse is now inscribed in his company’s identity. Techies, though, were more excited by what came later: an announcement that Meta, in combination with Ray-Ban, would soon launch smart glasses incorporating an artificial-intelligence (AI) virtual assistant. The specs will be able to see and hear, as well as answer their wearers’ questions. With luck, they will not hallucinate.You can be dismissive of smart glasses. They have been hyped before. But lending Meta credibility this time is the fact that the same week OpenAI, the generative-AI pioneer, announced that its hit chatbot, ChatGPT, can now see, hear and speak, besides conversing by text. Moreover, it emerged that OpenAI was in talks with Sir Jony Ive, Apple’s former designer, to create a new gadget for the AI era. What form it will take is still unclear. But if the idea is to build a new consumer-electronics device better suited to the back-and-forth of seeing, talking and listening AIs, there is a fair chance it will no longer be reliant on the touchscreen.The smartphone has had a good innings. Yet you only need to talk to Sky, one of ChatGPT’s new audio avatars, to feel the joy of freeing yourself from its tyranny. Your columnist got a taste when he asked Sky how she thought screens might eventually be replaced: Glasses? “Absolutely!” she enthused, “especially those equipped with augmented reality [AR] and AI”. Asked whether this would be a good thing, she recommended two books that explore the enormous impact that screens have had on modern life: “The Shallows: How the Internet is Changing the Way We Think, Read and Remember” by Nicholas Carr, an American writer, and “Screened out” by Jean Baudrillard, the late French philosopher. Then, when further prompted, she summarised each in crisp, insightful language with barely a moment’s hesitation. It wasn’t exactly Scarlett Johansson in “Her”. But it felt like having a Stanford University intellectual murmuring in your ear.This is all rather refreshing. Just as the year-long excitement over “foundational” models and other mind-boggling bits of AI infrastructure has begun to fade, along comes the chance that gen AI, to use the industry shorthand, will unleash an onslaught of new consumer technology. Tech pundits are debating the best “form factor” for the chatbot era. Ben Thompson of Stratechery, a blog and podcast, puts it in epochal terms: “There is a hardware breakthrough waiting to happen just like the internet created the conditions for the smartphone breakthrough to happen.” The ability to talk and listen to chatbots makes Meta’s bet on AR glasses and VR headsets “drastically more compelling”, he writes.Mr Zuckerberg was early to see this coming. He has ploughed a fortune into VR and AR despite misgivings from investors. He remains excited by the metaverse. This was clear from a remote interview he recently took part in with Lex Fridman, a podcaster, which used VR tools to make their virtual faces so lifelike they felt as if they were in the same room together. (As Mr Fridman quipped, it could reproduce realistic facial movements even from two famously inexpressive people.) And yet gen AI has so dramatically accelerated the use case for smart glasses, Mr Zuckerberg told another interviewer, that there is now “no question” they will be the bigger of the two markets. He likens AR specs to mobile phones and VR headsets to desktops. In both cases he appears to hope they will transcend screens, which he says inhabit “a completely different plane from our physical lives”.The two-dimensional screen is not headed for the scrap heap yet. Incumbent technologies are always hard to dislodge. Meta’s mobile apps such as WhatsApp, Facebook and Instagram, with their billions of users, still dwarf AIs like ChatGPT in terms of monthly visits, and they remain dependent on smartphones. As Mark Shmulik of Bernstein, an investment firm, notes, the smartphone era has never stopped people from using PCs. Moreover, it will not be clear until people start buying the smart glasses from the shops how compelling a product they are.The business case for the all-seeing, all-hearing chatbots will also take time to emerge. OpenAI charges $20 a month for access to its family of talking avatars; Meta’s AI-infused smart glasses will start at $299. Yet developing them is bound to be lossmaking at first. If there ever is a case for monetising them via advertisements or virtual shopping, that will probably take years. Meta’s modus operandi, after all, is to launch a consumer product, scale it up and start making money from it only if it is adopted by the masses.In the meantime, obvious safety concerns must be tackled. Consumer technology powered by AI is likely to be more immersive than social media, potentially making it even more isolating for some, or triggering unhealthy attachments. Mr Zuckerberg argues that AR and VR devices could help bring people together. But Mr Shmulik says investors will not want Meta to move too fast. “The last thing they need is another negative PR event where they are back in the cross hairs of regulators,” he says.Glasses half full For now Mr Zuckerberg, who this time last year was fighting fires on several fronts, looks prescient. That is largely thanks to gen AI. Meta’s foundational model, LLama 2, has been an open-source hit and is underpinning the firm’s consumer-tech ambitions. New devices such as smart glasses and headsets could eventually free Facebook and others from their dependence on the iPhone, where Apple has hindered their ability to track data, hurting Meta’s ad business. In a backhanded compliment to Mr Zuckerberg, Apple is launching its own high-end AR/VR headset. The iPhone-maker, too, may be sensing the twilight of the screen era. ■Read more from Schumpeter, our columnist on global business:Customer service is getting worse—and so are customers (Sep 28th)What Arm and Instacart say about the coming IPO wave (Sep 21st)The Mittelstand will redeem German innovation (Sep 14th)Also: If you want to write directly to Schumpeter, email him at [email protected]. And here is an explanation of how the Schumpeter column got its name.
"Artificial intelligence is everywhere but it is considered in a wholly ahistorical way. To understand the impact AI will have on our lives, it is vital to appreciate the context in which the field was established. After all, statistics and state control have evolved hand in hand for hundreds of years.Consider computing. Its origins have been traced not only to analytic philosophy, pure mathematics and Alan Turing, but perhaps surprisingly, to the history of public administration. In “The Government Machine: A Revolutionary History of the Computer” from 2003, Jon Agar of University College London charts the development of the British civil service as it ballooned from 16,000 employees in 1797 to 460,000 by 1999. He noticed an uncanny similarity between the functionality of a human bureaucracy and that of the digital electronic computer. (He confessed that he could not tell whether this observation was trivial or profound.)Both systems processed large quantities of information using a hierarchy of pre-set but adaptable rules. Yet one predated the other. This suggested a telling link between the organisation of human social structures and the digital tools designed to serve them. Mr Agar draws a link to the very origins of computing: Charles Babbage’s Difference Engine in the 1820s in Britain. It had been subsidised by the government, on the expectation that it would serve its sponsor. Babbage’s designs, Mr Agar observes, must be seen as “materialisations of state activity”.This relationship between computing systems and human organisational structures echoes through the history of AI. In the 1930s and 1940s, Herbert Simon (pictured below), a political scientist from the University of Chicago who later taught at Carnegie Mellon University, set out to develop a “scientific” account of administrative organisation. Simon had trained under Rudolf Carnap, a member of the Vienna Circle of logical positivists. This informed his belief that existing theories lacked empiricism. His doctoral dissertation in 1947 became “Administrative Behaviour”, a book that provided a framework through which all activity in an organisation could be understood using a matrix of decision-making.Simon saysHe went on to make huge contributions in a host of fields—not just political science and economics, but computer science and artificial intelligence. He coined the term “satisficing” (to accept the good rather than strive for the optimal) and developed the idea of ""bounded rationality"" for which he won a Nobel prize in economics in 1978. But back in the 1950s, Simon was a consultant at the RAND Corporation, an influential think-tank supported by America’s Air Force.At RAND, Simon and two colleagues—Allan Newell, a young mathematician, and J. Clifford Shaw, a former insurance actuary—tried to model human problem-solving in terms that a computer could put into operation. To do so, Simon borrowed elements from the framework that he had developed in “Administrative Behaviour”. To make a computer “think” as a human, Simon made it think like a corporation.The product of the trio’s labour was a virtual machine called the Logic Theorist, heralded as the first working prototype of artificial intelligence. Printouts of the Theorist in operation turned heads at the 1956 Dartmouth Summer Research Project on Artificial Intelligence, which gave the field its name and initial membership. In notes from the Dartmouth conference, one participant wrote that the Theorist helped to solve the dreaded “demo to sponsor” problem. This was essential, because the foundation funding AI was sceptical that the research area was worthwhile.To make a computer ‘think’ as a human, Simon made it think like a corporationHow did Simon see his contribution? A year after the Dartmouth conference, he and Newell presented their results as “Heuristic Problem Solving: The Next Advance in Operations Research”. The clue is in the title: “operations research” emerged in Britain during the second world war to apply scientific principles and statistics to optimise military activities, and later, for corporate uses. AI meant business.In a speech to operations-research practitioners in London in 1957, Simon identified Frederick Taylor, the father of the scientific-management movement, and Charles Babbage, as intellectual predecessors. “Physicists and electrical engineers had little to do with the invention of the digital computer,” Simon said. “The real inventor was the economist Adam Smith.” He explained the connections: Gaspard de Prony, a French civil engineer, set out to “manufacture” logarithms using techniques drawn from Smith’s “The Wealth of Nations”. Babbage, inspired by Prony, converted this insight into mechanical hardware. In the mid-1950s, Simon transmuted it into software code.The tradition lives on. Many contemporary AI systems do not so much mimic human thinking as they do the less imaginative minds of bureaucratic institutions; our machine-learning techniques are often programmed to achieve superhuman scale, speed and accuracy at the expense of human-level originality, ambition or morals.Capitalism in the codeThese streams of AI history—corporate decision-making, state power and the application of statistics to war—have not survived in the public understanding of AI.Instead, news of technical breakthroughs or pundits voicing fears are accompanied with imagery, if not of a heavily-armed Terminator, then of the brain, a robot, neon-colored microchips or absurd mathematical equations. Each is a not-so-subtle appeal to the authority of the natural sciences or computer science over that of, say, the “soft” sciences, to borrow Simon’s terminology, of political science, management science or even economics, the field for which he trundled off to Stockholm to collect his Nobel prize.Perhaps as a result of this misguided impression, public debates continue today about what value, if any, the social sciences could bring to artificial-intelligence research. In Simon’s view, AI itself was born in social science.David Runciman, a political scientist at the University of Cambridge, has argued that to understand AI, we must first understand how it operates within the capitalist system in which it is embedded. “Corporations are another form of artificial thinking-machine in that they are designed to be capable of taking decisions for themselves,” he explains.“Many of the fears that people now have about the coming age of intelligent robots are the same ones they have had about corporations for hundreds of years,” says Mr Runciman. The worry is, these are systems we “never really learned how to control.”After the 2010 BP oil spill, for example, which killed 11 people and devastated the Gulf of Mexico, no one went to jail. The threat that Mr Runciman cautions against is that AI techniques, like playbooks for escaping corporate liability, will be used with impunity.Today, pioneering researchers such as Julia Angwin, Virginia Eubanks and Cathy O’Neil reveal how various algorithmic systems calcify oppression, erode human dignity and undermine basic democratic mechanisms like accountability when engineered irresponsibly. Harm need not be deliberate; biased data-sets used to train predictive models also wreak havoc. It may be, given the costly labour required to identify and address these harms, that something akin to “ethics as a service” will emerge as a new cottage industry. Ms O’Neil, for example, now runs her own service that audits algorithms. In the 1950s, after having coined the term “artificial intelligence” for the Dartmouth conference, John McCarthy, one of the field’s early pioneers, wrote in his notes: “Once one system of epistemology is programmed and works, no other will be taken seriously unless it also leads to intelligent programmes.” By this view, DeepMind’s initial slogan, “Solve intelligence. Use that to solve everything else”, looks quasi-imperial.McCarthy’s suggestion was that influence, not authority, could decide the scientific consensus in his field. DeepMind doesn’t have to “solve” intelligence (assuming such a thing is even possible) it just needs to outshine the competition. That the company’s new slogan is, “Solve Intelligence. Use it to make the world a better place,” suggests that it too is aware of the need for diplomacy in this era’s AI-powered vision of totality. Many fears of intelligent robots are the same as ones held about corporations for hundreds of years. We never learned to control them.Stephen Cave, director of the Leverhulme Centre for the Future of Intelligence, has shown that the definition of intelligence has been used throughout history as a tool for domination. Aristotle appealed to the “natural law” of social hierarchy to explain why women, slaves and animals were to be subjugated by intellectual men. To reckon with this legacy of violence, the politics of corporate and computational agency must contend with profound questions arising from scholarship on race, gender, sexuality and colonialism, among other areas of identity.A central promise of AI is that it enables large-scale automated categorisation. Machine learning, for instance, can be used to tell a cancerous mole from a benign one. This “promise” becomes a menace when directed at the complexities of everyday life. Careless labels can oppress and do harm when they assert false authority. In protest at inadequate labels that are used to “know” the world, many young people today proudly defy unwelcome categorisations, be they traditional gender binaries or sexual binaries.Machines who think againIt may come as a surprise that there is a lack scholarship on the social, material and political histories of the origins of artificial intelligence. Indeed, a great deal has been written about the history of AI—by Simon in 1996 and Newell in 2000, among others. Most of these histories, however, follow a narrow mould, seeing it “mainly in intellectual terms,” in the words of Paul Edwards, a historian of information technologies.The two quasi-official histories of AI are each a history of ideas: Pamela McCorduck’s “Machines Who Think”, which “forged the template for most subsequent histories” after its initial publication in 1979; and Daniel Crevier’s “AI: The Tumultuous History”, published 1993. Both books relied primarily on in-depth interviews with key researchers to construct their narratives.The politics of corporate and computational identity become the politics of the new eraNeither, perhaps as a result, sought to understand AI in its broader context, embedded in the rise of operations research, “big science,” the actuarial sciences, and American military funding as it has evolved since the second world war. Expunged from these histories, AI can appear divorced of its historical and political context.Without this context, AI can also appear divorced from the knowledge systems that created it. In his 1957 talk to operations-research professionals, Simon celebrated the diversity of his field's past. He described contributions from French weavers and the mechanics of the Jacquard loom, as well as from Smith, de Prony, Babbage and his peers in the “soft” sciences, as adding up to a “debt” that remained to be repaid.That new knowledge could come about so unexpectedly, and from so many places, was what excited Simon about his work—and can stimulate us to think similarly today. Modern AI can do more than mirror the organisational dogma that characterised its birth, it can also reflect our humanity.____________Jonnie Penn (@jonniepenn) is a doctoral candidate studying artificial intelligence at the University of Cambridge, and is an affiliate at Harvard University's Berkman Klein Centre for Internet & Society. He is the project development lead on “AI: History” at the Leverhulme Center for the Future of Intelligence at Cambridge. He has previously held fellowships at Google and the MIT Media Lab."
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.RISHI SUNAK dreams of Britain becoming an AI superpower. The prime minister says the technology, the subject of intense global interest thanks to successes of large language models such as GPT-4, could unlock economic growth and improve sclerotic public services. AI-related announcements these days gush from Downing Street faster than commentators can keep up. In March Jeremy Hunt, the chancellor, vowed to spend £1bn ($1.3bn) over five years on AI and supercomputing. In May Mr Sunak met the bosses of leading AI companies in London. On June 7th, while visiting Joe Biden in Washington, he said that Britain would host the “first global summit on Artificial Intelligence” this autumn.His broadest ambitions are well placed. AI has great potential, and Britain has an edge that could help it to prosper. The country is arguably the foremost location, outside China and America, to start a new tech company (see chart 1). It is home to important AI outfits, most notably DeepMind, an AI research lab owned by Alphabet, an American tech giant, and Stability AI, a generative-AI startup, both in London. Its excellent universities churn out capable graduates who are keen to toil in AI. In London, too, it has a globally appealing city that draws investors and high-skilled migrants.Data collected by its public bodies—crucially that from the enormous National Health Service (NHS), for example on drug-use outcomes, hospital logistics, or scans of the body under different conditions—could provide a goldmine for training health-focused AI. The government also has a decent record of finding ways to use tech well. Just over a decade ago it launched the Government Digital Service, which digitised public services such as the issuing of passports or driving licences. That has been copied by governments around Europe and in America.But for all the well-intentioned zest for the big new thing, Mr Sunak’s government has yet to confront reality: enormous hurdles still block Britain’s path towards AI success. AI systems are built from three ingredients: computation, clean datasets and the work of people who know how to wrangle vast quantities of both. A successful industry, in turn, needs the right regulation. Britain has serious difficulties to overcome in all those areas, and especially in the first two.The most pressing problem is over “compute”, the term AI researchers use for the vital infrastructure, lots of computing power, required to train the new sort of AI models. None of the big three cloud-computing companies—Amazon, Google, Microsoft—has built a large, advanced cluster of graphical processing units (GPUs) for compute to happen at scale in Britain (see chart 2). Only Oracle, a relative newcomer to the field, offers a cluster. This is in part because of Britain’s smallish domestic market and its lack of access to the large one on its doorstep.The fate of DeepMind, the country’s most hopeful AI company, illustrates the compute problem. It had about 80 staff, all in London, when bought by Google (now Alphabet) in 2014. Today it is vastly bigger, with over 15% of its employees in America, mostly at Alphabet’s headquarters, according to information on LinkedIn, a social-media platform. For Demis Hassabis, one of DeepMind’s founders, this growth couldn’t have happened with domestic resources alone: the pressing reason for selling to Google was the need to access the compute for training models. Today, DeepMind trains them in Oklahoma. What it faced roughly a decade ago persists today. Pitifully little has been done to tackle the shortfall. The lack of access to compute remains the biggest problem for AI growth, and for winning the wider economic benefits of Mr Sunak’s dreams.Politicians say that they are acting on this. Mr Hunt says he will spend £900m on a supercomputer, probably at the Edinburgh Parallel Computing Centre, “because AI needs computing horsepower”. The EPCC is indeed world class in supercomputing for scientific research. But, sadly, not all such horsepower is created equal. The new computer won’t be ready before 2026 and the centre has no experience in building the kinds of GPU clusters used to train large AI models. And whereas the cloud clusters provided by Amazon, Google and Microsoft (who are also known as hyperscalers) are routinely updated with the latest chips, the EPCC, in contrast, will be stuck with whatever GPUs it can obtain now. It will then live with them until 2031, when its funding runs out. That is an eternity in AI time.Beasts with horsepowerMark Parsons, an eminent computer scientist who runs the EPCC, is right to say supercomputers and GPU clusters are increasingly similar beasts, but even he accepts that the government plan has disadvantages. “The hyperscalers pride themselves in continuously updating their GPUs,” he concedes, adding that the cost of doing that is too high for others to match. Others are less polite. For the government to claim a single, powerful computer in Edinburgh would solve Britain’s compute woes is “borderline dishonest,” says a well-connected techie who understands the mix of data, compute and skills required.An alternative option exists: renting compute. No companies or government agencies are entirely locked out from using AI. Anyone (at least when a global shortage of chips eventually eases) may rent time on cloud supercomputers used to train models. Anyone can download Common Crawl, an internet-scale database on which GPT-4 was trained, and start training a model. And anyone can use GPT-4 or a host of excellent open-source models to generate text or code.“Compute is not like oil,” notes the techie. “You can call Amazon and rent it. It’s a problem that money solves in perfectly continuous increments. It’s not this magical thing that if you don’t buy it you can’t have,” he says. Some companies do exactly this. The boss of Stability, Emad Mostaque, says his firm in London trains its models on Amazon’s compute clusters based in Ohio and Virginia, for example.The trouble is that Stability’s behaviour is more exception than rule. Too often, British officials or companies require—for reasons of politics, national security, privacy or something else—that their data remain in the country. Neither the Ministry of Defence nor the NHS, for example, is about to upload sensitive data to foreign clouds. The boss of one large tech company with several public-sector contracts describes going “on bended knee” to the hyperscalers, begging for access to compute in Britain. He was offered GPU time in the Netherlands or Ireland. But without local GPUs, he is not permitted to help his government customers train or run AI models based on their unique datasets.Nor is sensitivity about data the only downside to renting compute abroad. Being physically close to compute at home brings real benefits. AI engineers and companies gain expertise by experimenting regularly on it. Techies seeking innovations need hands-on time. “A country generates large benefits beyond access from having technology assets physically located there. ‘Learning by doing’ and the compounding of process knowledge is key to having a vibrant deep tech ecosystem and the high value jobs and companies that come with it,” says Matt Clifford, chair of the Advanced Research and Invention Agency, a government skunkworks that helps to fund research in tech.The most ambitious progress, therefore, depends on getting hyperscalers to set up GPU clusters in Britain. So how to do that? In the first instance, says one tech boss, Mr Sunak should know better what to ask for. He could start by launching a “global lobbying unit” to press for Amazon, Google or Microsoft to set up shop. The government should consider what hyperscalers would need to build in Britain.Building entire new datacentres is not necessary. Instead Amazon, Google or Microsoft could replace servers in their existing (older style) British-based centres with ones that include the new chips produced by Nvidia, ideally the latest A100 or H100 models. Obtaining those chips may be the biggest problem in the short term, given a global supply crunch.Another challenge would be ensuring sufficient supplies of electricity, at low enough cost (and ideally green), because training AI models devours a lot of power. Such tasks don’t look insurmountable, even if there would not be a quick fix. (Though it is hard to imagine it would take longer than building Mr Hunt’s supercomputer). In the meantime Britain will have to limit itself to using foreign compute.The longer the delay, however, the lower the chances of success. Without hyperscale GPU clusters, another set of British companies misses out: those attempting to supply picks and shovels in the AI boom. Nigel Toon, the boss of Graphcore, a young British company based in Bristol which makes AI chips, notes that his American competitors have great advantages that he lacks in selling their products to local, big stacks of compute.Unsurprisingly, he also wants the new supercomputer in Edinburgh to favour British suppliers like his firm. The hour grows late, though. Sequoia, one of Graphcore’s biggest investors, wrote down the value of its stake to zero in April. Meta, another American tech giant, has already scooped up some of the Graphcore team. The Bristol firm has plenty of cash in the bank, but desperately needs to get its chips into data centres.After the difficulty of the compute desert, the other challenges look more manageable. One priority is improving the datasets available for AI developers. Data generated by public agencies should be the most appealing raw material for those working on AI. Unfortunately, they are too often a mess, including those within the NHS. Data to do with welfare are no better. Officials say that the computer systems running the Department for Work and Pensions are so feeble, for example, that six months are needed to adjust recipients’ benefits for inflation. Trying to build BenefitsGPT atop a creaking 20th-century infrastructure looks like a fool’s errand.At least cleaning up the valuable datasets is within the control of the government. Officials could also look for benefits from generating new ones. “States don’t leverage their only advantage: the sovereign right to produce data about things companies don’t have,” says Benjamin Bratton, who has written a book on the state’s relationship with technology. He observes that states have the means to “produce models of their societies”, but Britain’s government (like most) lags behind tech companies in being able to model its own people’s behaviour, the country’s environment and its resources.The NHS is particularly ripe for a data retrofit, though the road to doing this is littered with the bodies of politicians and companies who tried and failed. NHS IT, an effort to centralise medical records that was launched in 2002, ate up at least £10bn before it was quietly dumped in 2011. New companies are popping up, attempting to solve the problem in a bitesized manner.  At least one new startup wants to be paid to clean up government datasets, to make them useful for training AI models and to improve the more mundane services those data flows allow.image: Nate KitchThe last £100m of Mr Hunt’s £1bn on AI may help with this. It is to be spent through a new Foundation Model Task-force. The outfit will focus on finding ways to train big models for the public sector and on making “strategic investments in the full AI stack,” says an official. That is encouraging, even if the amount of money available is small; training a single large model once could eat up much of the funds Mr Hunt has set aside. “Sovereigns have the most interesting leverage on data, for sure,” says the official. The task-force may direct some of money to kick-starting a data-hygiene industry, something for which there has not, to date, been a business model.Making all of this happen in turn requires having enough skilled people around: British universities produce lots but, given the huge draw of Silicon Valley, there is also a steady flow of techies across the Atlantic. And as talent flows West, it takes its intellectual property along. “The reality is that the number of people who have seen GPUs melt because of 24/7 training jobs is very small,” says Nathan Benaich of Air Street Capital, a London VC firm. “Certainly they don’t work for the government, and they can’t be hired by the government to do a deal with the cloud vendors. These are the guys who know how it works.”Neither Wild West nor rabbit holeOne step to better retaining talent (as well as attracting investors) is to get the regulation of AI right. This means avoiding the path that the European Union is expected to follow, with ever-expanding swathes of horizontal rules, ones that cut across sectors, on how AI can be used safely. Britain’s existing sector-by-sector, common-law approach, which would regulate different industries differently, looks like a better bet. Given global anxieties about the power and impact of AI, “there’s an opportunity for Britain to move quickly and establish itself as a pragmatic place,” says the tech boss who is struggling to access compute. Mr Sunak’s summit in the autumn should be a good place to start.“I do buy the Sunak picture,” says the tech boss. “In keeping with common law. You have these context-specific regulators. You don’t have broad cross-sectoral statutory regulations. The EU is not going to do it; it has disappeared down the EU rabbit hole and is going to be down there for a couple of years. The US is going to be the Wild West. Britain is the one place that’s going to combine that concern around ethics of models and their application with a deep pragmatism and openness to innovation. We have courts and regulators that are globally respected.”Achieving more of this, and faster, also requires having more people in positions of power who understand computation. “We lack competence and confidence at the heart of government,” says one adviser. “The people who run compute policy in the Department for Science, Innovation and Technology really just don’t understand it. They don’t understand the difference between general and specific computing.” Hence the trumpeting of a supercomputer built by computer-science researchers as an answer to the country’s AI woes.For an example of what savvy techies with official support can do, look to the United Arab Emirates. Its government-backed Technology Innovation Institute used Amazon’s cloud to train an open-source large language model called Falcon which is competitive with the best models trained by American companies, such as OpenAI. TII grants access to its compute to people with new ideas for training models and starting companies. Every nerd in the world has taken notice, and many now contribute their brain power to a project whose benefits—such as attracting computer graduates to work on AI projects—broadly accrue to the UAE.The closest thing Britain has to this is Stability, the startup whose models generate photorealistic images. Its open source Stable Diffusion model produces pictures which have driven many on the internet into a frenzy (think fake pictures of Donald Trump’s arrest, or the pope in a Balenciaga jacket). But the gravity of America’s tech scene is exerting itself on Stability. The firm started in London but the majority of its employees are now in America, according to LinkedIn data. American backers provided all of its most recent funding.Mr Sunak’s route to British AI superpowerdom will not run along paths where the most promising companies add most of their jobs overseas. Much remains to be done to make Britain more attractive, but the race has already begun and, for now, the country lags. ■For more expert analysis of the biggest stories in Britain, sign up to Blighty, our weekly subscriber-only newsletter.
WHEN the first printed books with illustrations started to appear in the 1470s in the German city of Augsburg, wood engravers rose up in protest. Worried about their jobs, they literally stopped the presses. In fact, their skills turned out to be in higher demand than before: somebody had to illustrate the growing number of books.Fears about the impact of technology on jobs have resurfaced periodically ever since. The latest bout of anxiety concerns the arrival of artificial intelligence (AI). Once again, however, technology is creating demand for work. To take one example, more and more people are supplying digital services online via what is sometimes dubbed the “human cloud”. Counter-intuitively, many are doing so in response to AI.According to the World Bank, more than 5m people already offer to work remotely on online marketplaces such as Freelancer.com and UpWork. Jobs range from designing websites to writing legal briefs, and typically bring in at least a few dollars an hour. In 2016 such firms earned about $6bn in revenue, according to Staffing Industry Analysts, a market researcher. Those who prefer work in smaller bites can use “micro-work” sites such as Mechanical Turk, a service operated by Amazon. About 500,000 “Turkers” perform tasks such as transcribing bits of audio, often earning no more than a few cents for each “human-intelligence task”.Many big tech companies employ, mostly through outsourcing firms, thousands of people who police the firms’ own services and control quality. Google is said to have an army of 10,000 “raters” who, among other things, look at YouTube videos or test new services. Microsoft operates something called a Universal Human Relevance System, which handles millions of micro-tasks each month, such as checking the results of its search algorithms.These numbers are likely to rise. One reason is increasing demand for “content moderation”. A new law in Germany will require social media to remove any content that is illegal in the country, such as Holocaust denial, within 24 hours or face hefty fines. Facebook has announced that it will increase the number of its moderators globally, from 4,500 to 7,500.AI will eliminate some forms of this digital labour—software, for instance, has got better at transcribing audio. Yet AI will also create demand for other types of digital work. The technology may use a lot of computing power and fancy mathematics, but it also relies on data distilled by humans. For autonomous cars to recognise road signs and pedestrians, algorithms must be trained by feeding them lots of video showing both. That footage needs to be manually “tagged”, meaning that road signs and pedestrians have to be marked as such. This labelling already keeps thousands busy. Once an algorithm is put to work, humans must check whether it does a good job and give feedback to improve it.A service offered by CrowdFlower, a micro-task startup, is an example of what is called “human in the loop”. Digital workers classify e-mail queries from consumers, for instance, by content, sentiment and other criteria. These data are fed through an algorithm, which can handle most of the queries. But questions with no simple answer are again routed through humans.You might expect humans to be taken out of the loop as algorithms improve. But this is unlikely to happen soon, if ever, says Mary Gray, who works for Microsoft’s research arm. Algorithms may eventually become clever enough to handle some tasks on their own and to learn by themselves. But consumers and companies will also expect ever-smarter AI services: digital assistants such as Amazon’s Alexa and Microsoft’s Cortana will have to answer more complex questions. Humans will still be needed to train algorithms and handle exceptions.Accordingly, Ms Gray and Siddharth Suri, her collaborator at Microsoft Research, see services such as UpWork and Mechanical Turk as early signs of things to come. They expect much human labour to be split up into distinct tasks which can be delivered online and combined with AI offerings. A travel agency, for instance, might use AI to deal with routine tasks (such as booking a flight), but direct the more complicated ones (a request to create a customised city tour, say) to humans.Michael Bernstein and Melissa Valentine of Stanford University see things going even further. They anticipate the rise of temporary “firms” whose staff are hired online and configured with the help of AI. To test the idea, the researchers developed a program to assemble such virtual companies for specific projects—for instance, recruiting workers and assigning them tasks in order to design a smartphone app to report injuries from an ambulance racing to a hospital.Working in such “flash organisations” could well be fun. But many fear that the human cloud will create a global digital proletariat. Sarah Roberts of the University of California, Los Angeles, found that content moderators often suffer from burnout after checking dodgy social-media content for extended periods. Mark Graham of the University of Oxford concludes that platforms for online work do indeed offer new sources of income for many, particularly in poor countries, but that these services also drive down wages. So governments need to be careful when designing big digital-labour programmes—as Kenya has done, hoping to train more than 1m people for online jobs.Technology is rarely an unalloyed bane or blessing. The printing press created new work for the wood engravers in Augsburg, but they quickly discovered that it had become much more repetitive. Similar trade-offs are likely in future.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.“By amplifying human intelligence, AI may cause a new Renaissance, perhaps a new phase of the Enlightenment,” Yann LeCun, one of the godfathers of modern artificial intelligence (AI), suggested earlier this year.  AI can already make some existing scientific processes faster and more efficient, but can it do more, by transforming the way science itself is done?Such transformations have happened before. With the emergence of the scientific method in the 17th century, researchers came to trust experimental observations, and the theories they derived from them, over the received wisdom of antiquity. This process was, crucially, supported by the advent of scientific journals, which let researchers share their findings, both to claim priority and to encourage others to replicate and build on their results. Journals created an international scientific community around a shared body of knowledge, causing a surge in discovery known today as the scientific revolution.A further transformation began in the late 19th century, with the establishment of research laboratories—factories of innovation where ideas, people and materials could be combined on an industrial scale. This led to a further outpouring of innovation, from chemicals and semiconductors to pharmaceuticals. These shifts did more than just increase scientific productivity. They also transformed science itself, opening up new realms of research and discovery. How might AI do something similar, not just generating new results, but new ways to generate new results?A promising approach is “literature-based discovery” (LBD) which, as its name suggests, aims to make new discoveries by analysing scientific literature. The first LBD system, built by Don Swanson at the University of Chicago in the 1980s, looked for novel connections in MEDLINE, a database of medical journals. In an early success, it put together two separate observations—that Raynaud’s disease, a circulatory disorder, was related to blood viscosity, and that fish oil reduced blood viscosity—and suggested that fish oil might therefore be a useful treatment. This hypothesis was then experimentally verified.We’re charging our batteryBut Dr Swanson’s LBD system failed to catch on outside the AI community at the time. Today AI systems have become far more capable at natural-language processing and have a much larger corpus of scientific literature to chew on. Interest in LBD-style approaches is now growing in other fields, notably materials science.In 2019, for example, a group of researchers led by Vahe Tshitoyan, then at Lawrence Berkeley National Laboratory, in America, used an AI technique called unsupervised learning to analyse the abstracts of materials-science papers, and extract information about the properties of different materials into mathematical representations called “word embeddings”. These place concepts into a multi-dimensional space where similar concepts are grouped together. The system thereby gained a  “chemical intuition” so that it could, for example, suggest materials with similar properties to another material. The AI was then asked to suggest materials that might have thermoelectric properties (the ability to turn a temperature difference into an electrical voltage, and vice versa), even though they were not identified as such in the literature. The ten most promising candidate materials were selected, and experimental testing found that all ten did indeed display unusually strong thermoelectric properties.The researchers then retrained their system, omitting papers from more recent years, and asked it to predict which new thermoelectric materials would be discovered in those later years. The system was eight times more accurate at predicting such discoveries than would be expected by chance alone. It could also make accurate discovery predictions using other terms, such as “photovoltaic”. The researchers concluded that “such language-based inference methods can become an entirely new field of research at the intersection between natural-language processing and science.”A paper by Jamshid Sourati and James Evans, both sociologists at the University of Chicago, published this year in Nature Human Behaviour, extends this approach in a novel way. It starts with the observation that LBD systems tend to focus on concepts within papers, and ignore their authors. So they trained an LBD system to take account of both. The resulting system was twice as good at forecasting new discoveries in materials science than the one built by Dr Tshitoyan’s team, and could also predict the actual discoverers with more than 40% accuracy. But the researchers then went one step further. Instead of following the crowd and predicting where researchers would make new discoveries, they asked their model to avoid the crowd, and identify “alien” hypotheses that are scientifically plausible, but unlikely, in the normal course of things, to be discovered in the near future. The system can thus, the researchers argue, both accelerate near-term discoveries, and probe “blind spots” where new discoveries await.As well as suggesting new hypotheses to investigate, LBD systems that take authorship into account can also suggest potential collaborators who may not know one other. This approach could be particularly effective when identifying scientists who work in different fields, bridging complementary areas of research. Cross-disciplinary research collaborations “will go from being rarities to being more commonplace” when mediated by AI, says Yolanda Gil, a computer scientist at the University of Southern California. And as LBD systems are extended so that they can handle tables, charts and data such as gene sequences and programming code, they will become more capable. In future, researchers might come to rely on such systems to monitor the deluge of new scientific papers, highlight relevant results, suggest novel hypotheses for research—and even link them up with potential research partners, like a scientific matchmaking service. AI tools could thus extend and transform the existing, centuries-old infrastructure of scientific publishing.We’re full of energyIf LBD promises to supercharge the journal with AI, “robot scientists”, or “self-driving labs”, promise to do the same for the laboratory. These machines go beyond existing forms of laboratory automation, such as drug-screening platforms. Instead, they are given background knowledge about a particular area of research, in the form of data, research papers and patents. They then use AI to form hypotheses, carry out experiments using robots, assess the results, modify their hypotheses, and repeat the cycle. Adam, a machine built at Aberystwyth University in Wales in 2009, did experiments on the relationship between genes and enzymes in yeast metabolism, and was the first machine to discover novel scientific knowledge autonomously.The successor to Adam, called Eve, performs drug-discovery experiments and has more sophisticated software. When planning and analysing experiments, it uses machine learning to create “quantitative structure activity relationships” (QSARs), mathematical models that relate chemical structures to biological effects. Eve has discovered, for example, that triclosan, an antimicrobial compound used in toothpaste, can inhibit an essential mechanism in malaria-causing parasites.Ross King, an AI researcher at the University of Cambridge who created Adam, draws an analogy between robot scientists of the future with AI systems built to play chess and Go. The prospect of machines beating the best human players once seemed decades away, but the technology improved faster than expected. Moreover, AI systems developed strategies for those games that human players had not considered. Something similar could happen with robot scientists as they become more capable. “If AI can explore a full hypothesis space, and even enlarge the space, then it may show that humans have only been exploring small areas of the hypothesis space, perhaps as a result of their own scientific biases,” says Dr King.Robot scientists could also transform science in another way: by helping fix some of the problems afflicting the scientific enterprise. One of these is the idea that science is, by various measures, becoming less productive, and pushing forward the frontiers of knowledge is becoming harder and more expensive. There are several theories for why this might be: the easiest discoveries may already have been made, for example, and more training is now needed for scientists to reach the frontier. AI-driven systems could help by doing laboratory work more quickly, cheaply and accurately than humans. Unlike people, robots can work around the clock. And just as computers and robots have enabled large-scale projects in astronomy (such as huge sky surveys, or automated searching for exoplanets), robot scientists could tackle big problems in systems biology, say, that would otherwise be impractical because of their scale. “We don’t need radically new science to do that, we just need to do lots of science,” says Dr King. image: Shira InbarAutomation might also help address another problem: the reproducibility crisis. In theory, when scientists publish their results, others can replicate and verify their work. But there is little glory in replication, which makes it rare. When it does happen, many attempts fail, suggesting that the original work was invalid, or even fraudulent. Scientists have little incentive to repeat the work of others and they are under pressure to publish new results, not verify existing ones. Again, robot scientists could help in some areas of research, such as molecular biology. A study published in 2022 by Katherine Roper, of the University of Manchester, analysed more than 12,000 papers on breast cancer and selected 74 biomedical results for verification using the Eve robot, which was able to reproduce 43 of them. The researchers concluded that automation “has the potential to mitigate the reproducibility crisis” and that it “side-steps the sociological and career disincentives for replication”. Machines do not mind publishing verifications of previous results. Nor, unlike human scientists, are they embarrassed by publishing negative results, for example if a particular molecule fails to interact with a given target. Publishing negative results would reduce wasted effort by telling future researchers what not to do. And robot scientists reliably record everything about their work in great detail, which (in theory) facilitates subsequent analysis of their results. “AI innovations can improve the scientific enterprise in all those areas,” says Dr Gil.Functioning automatic?Obstacles abound. As well as better hardware and software, and closer integration between the two, there is a need for greater interoperability between laboratory-automation systems, and common standards to allow AI algorithms to exchange and interpret semantic information. The introduction of standardised microplates, containing hundreds of tiny test tubes that allow laboratory samples to be processed in batches, increased productivity several hundred-fold for certain types of analysis. Now the same thing needs to happen for data—much of the data from microplate arrays in biology labs ends up in spreadsheets or in tables in papers, for example, where it is not machine-readable.Another barrier is a lack of familiarity with AI-based tools among scientists. And some researchers, like most workers, worry that automation threatens their jobs. But things are changing, says Dr Gil. When she surveyed attitudes towards AI in science in 2014, she found that, in most fields, “interest in AI seems relatively limited”. Most efforts to incorporate AI into scientific research came from AI researchers, who were often met with scepticism or hostility. But the impact of AI is now “profound and pervasive”, says Dr Gil. Many scientists, she says, are now “proactively seeking AI collaborators”. Recognition of AI’s potential is growing, particularly in materials science and drug discovery, where practitioners are building their own AI-powered systems.  “If we could get machines to be as good at science as human beings, that would be a radical break, because you can make lots of them,” says Dr King.Scientific journals changed how scientists discovered information and built on each other’s work. Research laboratories scaled up and industrialised experimentation. By extending and combining these two previous transformations, AI could indeed change the way science is done. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Politics is SUPPOSED to be about persuasion; but it has always been stalked by propaganda. Campaigners dissemble, exaggerate and fib. They transmit lies, ranging from bald-faced to white, through whatever means are available. Anti-vaccine conspiracies were once propagated through pamphlets instead of podcasts. A century before covid-19, anti-maskers in the era of Spanish flu waged a disinformation campaign. They sent fake messages from the surgeon-general via telegram (the wires, not the smartphone app). Because people are not angels, elections have never been free from falsehoods and mistaken beliefs.But as the world contemplates a series of votes in 2024, something new is causing a lot of worry. In the past, disinformation has always been created by humans. Advances in generative artificial intelligence (AI)—with models that can spit out sophisticated essays and create realistic images from text prompts—make synthetic propaganda possible. The fear is that disinformation campaigns may be supercharged in 2024, just as countries with a collective population of some 4bn—including America, Britain, India, Indonesia, Mexico and Taiwan—prepare to vote. How worried should their citizens be?It is important to be precise about what generative-AI tools like ChatGPT do and do not change. Before they came along, disinformation was already a problem in democracies. The corrosive idea that America’s presidential election in 2020 was rigged brought rioters to the Capitol on January 6th—but it was spread by Donald Trump, Republican elites and conservative mass-media outlets using conventional means. Activists for the BJP in India spread rumours via WhatsApp threads. Propagandists for the Chinese Communist Party transmit talking points to Taiwan through seemingly legitimate news outfits. All of this is done without using generative-AI tools.What could large-language models change in 2024? One thing is the quantity of disinformation: if the volume of nonsense were multiplied by 1,000 or 100,000, it might persuade people to vote differently. A second concerns quality. Hyper-realistic deepfakes could sway voters before false audio, photos and videos could be debunked. A third is microtargeting. With ai, voters may be inundated with highly personalised propaganda at scale. Networks of propaganda bots could be made harder to detect than existing disinformation efforts are. Voters’ trust in their fellow citizens, which in America has been declining for decades, may well suffer as people began to doubt everything.This is worrying, but there are reasons to believe AI is not about to wreck humanity’s 2,500-year-old experiment with democracy. Many people think that others are more gullible than they themselves are. In fact, voters are hard to persuade, especially on salient political issues such as whom they want to be president. (Ask yourself what deepfake would change your choice between Joe Biden and Mr Trump.) The multi-billion-dollar campaign industry in America that uses humans to persuade voters can generate only minute changes in their behaviour.Tools to produce believable fake images and text have existed for decades. Although generative AI might be a labour-saving technology for internet troll farms, it is not clear that effort was the binding constraint in the production of disinformation. New image-generation algorithms are impressive, but without tuning and human judgment they are still prone to produce pictures of people with six fingers on each hand, making the possibility of personalised deepfakes remote for the time being. Even if these AI-augmented tactics were to prove effective, they would soon be adopted by many interested parties: the cumulative effect of these influence operations would be to make social networks even more cacophonous and unusable. It is hard to prove that mistrust translates into a systematic advantage for one party over the other.Social-media platforms, where misinformation spreads, and AI firms say they are focused on the risks. OpenAI, the company behind ChatGPT, says it will monitor usage to try to detect political-influence operations. Big-tech platforms, criticised both for propagating disinformation in the 2016 election and taking down too much in 2020, have become better at identifying suspicious accounts (though they have become loth to arbitrate the truthfulness of content generated by real people). Alphabet and Meta ban the use of manipulated media in political advertising and say they are quick to respond to deepfakes.  Other companies are trying to craft a technological standard establishing the provenance of real images and videos.Voluntary regulation has limits, however, and the involuntary sort poses risks. Open-source models, like Meta’s Llama, which generates text, and Stable Diffusion, which makes images, can be used without oversight. And not all platforms are created equal—TikTok, the video-sharing social-media company, has ties to China’s government, and the app is designed to promote virality from any source, including new accounts. Twitter (which is now called X) cut its oversight team after it was bought by Elon Musk, and the platform is a haven for bots. The agency regulating elections in America is considering a disclosure requirement for campaigns using synthetically generated images. This is sensible, though malicious actors will not comply with it. Some in America are calling for a Chinese-style system of extreme regulation. There, AI algorithms must be registered with a government body and somehow embody core socialist values. Such heavy-handed control would erode the advantage America has in AI innovation.Politics was never pureTechnological determinism, which pins all the foibles of people on the tools they use, is tempting. But it is also wrong. Although it is important to be mindful of the potential of generative AI to disrupt democracies, panic is unwarranted. Before the technological advances of the past two years, people were quite capable of transmitting all manner of destructive and terrible ideas to one another. The American presidential campaign of 2024 will be marred by disinformation about the rule of law and the integrity of elections. But its progenitor will not be something newfangled like ChatGPT. It will be Mr Trump. ■For subscribers only: to see how we design each week’s cover, sign up to our weekly Cover Story newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.When it comes to “large language models” (LLMs) such as GPT—which powers ChatGPT, a popular chatbot made by OpenAI, an American research lab—the clue is in the name. Modern AI systems are powered by vast artificial neural networks, bits of software modelled, very loosely, on biological brains. GPT-3, an LLM released in 2020, was a behemoth. It had 175bn “parameters”, as the simulated connections between those neurons are called. It was trained by having thousands of GPUs (specialised chips that excel at AI work) crunch through hundreds of billions of words of text over the course of several weeks. All that is thought to have cost at least $4.6m.But the most consistent result from modern AI research is that, while big is good, bigger is better. Models have therefore been growing at a blistering pace. GPT-4, released in March, is thought to have around 1trn parameters—nearly six times as many as its predecessor. Sam Altman, the firm’s boss, put its development costs at more than $100m. Similar trends exist across the industry. Epoch AI, a research firm, estimated in 2022 that the computing power necessary to train a cutting-edge model was doubling every six to ten months (see chart).This gigantism is becoming a problem. If Epoch AI’s ten-monthly doubling figure is right, then training costs could exceed a billion dollars by 2026—assuming, that is, models do not run out of data first. An analysis published in October 2022 forecast that the stock of high-quality text for training may well be exhausted around the same time. And even once the training is complete, actually using the resulting model can be expensive as well. The bigger the model, the more it costs to run. Earlier this year Morgan Stanley, a bank, guessed that, were half of Google’s searches to be handled by a current GPT-style program, it could cost the firm an additional $6bn a year. As the models get bigger, that number will probably rise.Many in the field therefore think the “bigger is better” approach is running out of road. If AI models are to carry on improving—never mind fulfilling the AI-related dreams currently sweeping the tech industry—their creators will need to work out how to get more performance out of fewer resources. As Mr Altman put it in April, reflecting on the history of giant-sized AI: “I think we’re at the end of an era.”Quantitative tighteningInstead, researchers are beginning to turn their attention to making their models more efficient, rather than simply bigger. One approach is to make trade-offs, cutting the number of parameters but training models with more data. In 2022 researchers at DeepMind, a division of Google, trained Chinchilla, an LLM with 70bn parameters, on a corpus of 1.4trn words. The model outperforms GPT-3, which has 175bn parameters trained on 300bn words. Feeding a smaller LLM more data means it takes longer to train. But the result is a smaller model that is faster and cheaper to use.Another option is to make the maths fuzzier. Tracking fewer decimal places for each number in the model—rounding them off, in other words—can cut hardware requirements drastically. In March researchers at the Institute of Science and Technology in Austria showed that rounding could squash the amount of memory consumed by a model similar to GPT-3, allowing the model to run on one high-end GPU instead of five, and with only “negligible accuracy degradation”.Some users fine-tune general-purpose LLMs to focus on a specific task such as generating legal documents or detecting fake news. That is not as cumbersome as training an LLM in the first place, but can still be costly and slow. Fine-tuning LLaMA, an open-source model with 65bn parameters that was built by Meta, Facebook’s corporate parent, takes multiple GPUs anywhere from several hours to a few days.Researchers at the University of Washington have invented a more efficient method that allowed them to create a new model, Guanaco, from LLaMA on a single GPU in a day without sacrificing much, if any, performance. Part of the trick was to use a similar rounding technique to the Austrians. But they also used a technique called “low-rank adaptation”, which involves freezing a model’s existing parameters, then adding a new, smaller set of parameters in between. The fine-tuning is done by altering only those new variables. This simplifies things enough that even relatively feeble computers such as smartphones might be up to the task. Allowing LLMs to live on a user’s device, rather than in the giant data centres they currently inhabit, could allow for both greater personalisation and more privacy.A team at Google, meanwhile, has come up with a different option for those who can get by with smaller models. This approach focuses on extracting the specific knowledge required from a big, general-purpose model into a smaller, specialised one. The big model acts as a teacher, and the smaller as a student. The researchers ask the teacher to answer questions and show how it comes to its conclusions. Both the answers and the teacher’s reasoning are used to train the student model. The team was able to train a student model with just 770m parameters, which outperformed its 540bn-parameter teacher on a specialised reasoning task.Rather than focus on what the models are doing, another approach is to change how they are made. A great deal of AI programming is done in a language called Python. It is designed to be easy to use, freeing coders from the need to think about exactly how their programs will behave on the chips that run them. The price of abstracting such details away is slow code. Paying more attention to these implementation details can bring big benefits. This is “a huge part of the game at the moment”, says Thomas Wolf, chief science officer of Hugging Face, an open-source AI company.Learn to codeIn 2022, for instance, researchers at Stanford University published a modified version of the “attention algorithm”, which allows LLMs to learn connections between words and ideas. The idea was to modify the code to take account of what is happening on the chip that is running it, and especially to keep track of when a given piece of information needs to be looked up or stored. Their algorithm was able to speed up the training of GPT-2, an older large language model, threefold. It also gave it the ability to respond to longer queries.Sleeker code can also come from better tools. Earlier this year, Meta released an updated version of PyTorch, an ai-programming framework. By allowing coders to think more about how computations are arranged on the actual chip, it can double a model’s training speed by adding just one line of code. Modular, a startup founded by former engineers at Apple and Google, last month released a new AI-focused programming language called Mojo, which is based on Python. It too gives coders control over all sorts of fine details that were previously hidden. In some cases, code written in Mojo can run thousands of times faster than the same code in Python.A final option is to improve the chips on which that code runs. GPUs are only accidentally good at running AI software—they were originally designed to process the fancy graphics in modern video games. In particular, says a hardware researcher at Meta, GPUs are imperfectly designed for “inference” work (ie, actually running a model once it has been trained). Some firms are therefore designing their own, more specialised hardware. Google already runs most of its AI projects on its in-house “TPU” chips. Meta, with its MTIAs, and Amazon, with its Inferentia chips, are pursuing a similar path.That such big performance increases can be extracted from relatively simple changes like rounding numbers or switching programming languages might seem surprising. But it reflects the breakneck speed with which LLMs have been developed. For many years they were research projects, and simply getting them to work well was more important than making them elegant. Only recently have they graduated to commercial, mass-market products. Most experts think there remains plenty of room for improvement. As Chris Manning, a computer scientist at Stanford University, put it: “There’s absolutely no reason to believe…that this is the ultimate neural architecture, and we will never find anything better.” ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The hottest technology of 2023 had a busy last few weeks of the year. On November 28th Abu Dhabi launched a new state-backed artificial-intelligence company, AI71, that will commercialise its leading “large language model” (LLM), Falcon. On December 11th Mistral, a seven-month-old French AI startup, announced a blockbuster $400m funding round, which insiders say will value the firm at over $2bn. Four days later Krutrim, a new Indian startup, unveiled India’s first multilingual LLM, barely a week after Sarvam, a five-month-old one, raised $41m to build similar Indian-language models.image: The EconomistEver since OpenAI, an American firm, launched ChatGPT, its humanlike conversationalist, in November 2022, just about every month has brought a flurry of similar news. Against that backdrop, the four latest announcements might look unexceptional. Look closer, though, and they hint at something more profound. The three companies are, in their own distinct ways, vying to become AI national champions. “We want AI71 to compete globally with the likes of OpenAI,” says Faisal al-Bannai of Abu Dhabi’s Advanced Technology Research Council, the state agency behind the Emirati startup. “Bravo to Mistral, that’s French genius,” crowed Emmanuel Macron, the president of France, recently. ChatGPT and other English-first LLMs “cannot capture our culture, language and ethos”, declared Krutrim’s founder, Bhavish Aggarwal. Sarvam started with Indian languages because, in the words of its co-founder, Vivek Raghavan, “We’re building an Indian company.”AI is already at the heart of the intensifying technological contest between America and China. In the past year their governments have pledged $40bn-50bn apiece for AI investments. Other countries do not want to be left behind—or stuck with a critical technology that is under foreign control. In 2023 another six particularly AI-ambitious governments around the world—Britain, France, Germany, India, Saudi Arabia and the United Arab Emirates (UAE)—promised to bankroll AI to the collective tune of around $40bn (see chart). Most of this will go towards purchases of graphics-processing units (GPUs, the type of chips used to train AI models) and factories to make such chips, as well as, to a lesser extent, support for AI firms. The nature and degree of state involvement varies from one wannabe AI superpower to another. It is early days, but the contours of new AI-industrial complexes are emerging.Start with America, whose tech firms give everyone else AI envy. Its vibrant private sector is innovating furiously without direct support from Uncle Sam. Instead, the federal government is spending around $50bn over five years to increase domestic chipmaking capacity. The idea is to reduce America’s reliance on Taiwanese semiconductor manufacturers such as TSMC, the world’s biggest and most sophisticated such company. Supplies from Taiwan could, fear security hawks in Washington, be imperilled if China decided to invade the island, which it considers part of its territory.Another way America intends to stay ahead of the pack is by nobbling rivals. President Joe Biden’s administration has enacted brutal export controls that ban the sale of cutting-edge AI technology, including chips and chipmaking equipment, to adversaries such as China and Russia. It has also barred Americans from sharing their AI expertise with those countries.It is now coercing those on the geopolitical fence to fall in line. In October the American government started requiring companies in third countries, including Saudi Arabia and the UAE, to secure a licence in order to buy AI chips from Nvidia, an American firm that sells most of them. The rules have a “presumption of approval”. That means the government will “probably allow” sales to such firms, says Gregory Allen, who used to work on AI policy at the Department of Defence—as long, that is, as they do not have close ties to China. On December 6th Xiao Peng, who runs a state-backed AI startup in Abu Dhabi called G42, announced that the company would be cutting ties with Chinese hardware suppliers such as Huawei, a Chinese electronics company.China’s AI strategy is in large part a response to American techno-containment. According to data from JW Insights, a research firm, between 2021 and 2022 the Chinese state spent nearly $300bn to recreate the chip supply chain (for AI and other semiconductors) at home, where it would be immune from Western sanctions. A lot of that money is probably wasted. But it almost certainly helped Huawei and SMIC, China’s biggest chipmaker, to design and manufacture a surprisingly sophisticated GPU last year.The central and local authorities also channel capital into AI firms through state-backed “guidance funds”, nearly 2,000 of which around the country invest in all manner of technologies deemed to be strategically important. The Communist Party is guiding private money, too, towards its technological priorities. Often it does so by cracking down on certain sectors—most recently, in December, video-gaming—while dropping heavy hints about which industries investors should be looking at instead. The government is also promoting data exchanges, where businesses can trade commercial data on everything from sales to production, allowing small firms with AI ambitions to compete where previously only large data-rich firms could. There are already 50 such exchanges in China.Elements of this state-led approach are being emulated in other parts of the world, notably in the Gulf’s petrostates. Being autocracies, Saudi Arabia and the UAE can move faster than democratic governments, which must heed voters’ concerns about AI’s impact on things like privacy and jobs. Being wealthy, they can afford both the GPUs (on which the two countries have together so far splurged several hundred million dollars) and the energy needed to run the power-hungry chips.They can also plough money into developing human capital. Their richly endowed universities are quickly climbing up global rankings. The AI programme at King Abdullah University of Science and Technology in Saudi Arabia and the Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) in Abu Dhabi, the world’s first AI-focused school, have poached star professors from illustrious institutions such as the University of California, Berkeley, and Carnegie Mellon University in Pittsburgh. Many of their students and researchers come from China. And plenty stick around. Nearly all of MBZUAI’s graduates, who number a couple of hundred, stay in the region to work at local firms and labs, says its acting provost, Timothy Baldwin (himself lured to the Middle East from the University of Melbourne).The Gulf approach is producing results. The capabilities of the Falcon model, first built by a team of 20 or so engineers, rival those of Llama 2, the most widely used “open-source” model, devised by Meta, an American tech giant. AI71 plans to improve its open-source models using national datasets from fields including health, education and, some day, perhaps oil. “In the last 50 years, oil drove the country…now data is the new oil,” says Mr al-Bannai.The alignment problemA third group of governments is combining elements of America’s approach with those of the Chinese and Emiratis. The EU has its version of America’s incentives for domestic chipmaking. So do some member states: Germany is footing a third of the €30bn ($33bn) bill for a new chip factory to be built there by Intel, an American chipmaker. Outside the bloc, Britain has promised to funnel £1bn ($1.3bn) over five years to AI and supercomputing (albeit without going into detail about how exactly the money will be spent). India’s government is promoting manufacturing, including of semiconductors, with generous “production-linked incentives”, encouraging big cloud-computing providers to build more Indian data centres, where AI models are trained, and thinking about buying $1.2bn-worth of GPUs.Like China and the Gulf but unlike America, where federal and state governments are reluctant to part with public data, India and some European countries are keen on making such data available to firms. France’s government “has been very supportive” in that regard, says Arthur Mensch, Mistral’s boss. Britain’s is considering allowing firms to tap rich data belonging to the National Health Service. India’s government has enormous amounts of data from its array of digital public services, known as the “India Stack”. Insiders expect it eventually to integrate Indian AI models into those digital services.In contrast to China, which regulates consumer-facing AI with a heavy hand, at least for the time being Britain, France, Germany and India favour light-touch rules for AI or, in India’s case, none at all. The French and German governments have soured on the EU’s AI Act, the final details of which are being hotly debated in Brussels—no doubt because it could constrain Mistral and Aleph Alpha, Germany’s most successful model-builder, which raised €460m in November.It is natural for countries to want some control over what may prove to be a transformational technology. Especially in sensitive and highly regulated sectors such as defence, banking or health care, many governments would rather not rely on imported AI. Yet each flavour of AI nationalism also carries risk.America’s beggar-thy-neighbour approach is likely to upset not just its adversaries but also some allies. China’s heavy regulation may offset some of the potential gains from its heavy spending. Building models for local languages, as Krutrim and Sarvam in India plan to do, may prove futile if foreign models continue to improve their multilingual capabilities. The Gulf’s bet on open-source models may misfire if other governments limit their use, as Mr Biden has hinted at in a recent executive order and the EU could do through its AI Act, out of fear that open LLMs could be put to malign uses by mischief-makers. Saudi and Emirati institutions may struggle to hold on to talent; a developer who worked on Falcon admits it greatly benefited from a partnership with a French team of engineers who have since been poached by Hugging Face, a high-flying Silicon Valley AI startup. As one sceptical investor notes, it is not yet clear how vast or useful public Emirati data actually is.As Nathan Benaich of Air Street Capital, a venture-capital firm, sums it up, most efforts to create national models “are probably a waste of money”. Mr Benaich’s warning is unlikely to dissuade AI-curious governments, mindful of the rewards should they succeed, from meddling. Mr Macron will not be the only leader to greet it with a Gallic shrug. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Rishi Sunak is Britain’s prime minister. If some advertisements on Facebook can be trusted (which they cannot) he also appears to be flogging get-rich-quick schemes. One such advert shows Mr Sunak endorsing an app supposedly developed by Elon Musk, a businessman, into which viewers can make regular “savings”.The video is fake. Generated with the help of AI, it is just one of 143 such advertisements catalogued by Fenimore Harper Communications, a British firm, which ran in December and January.  It is not just those in the public eye who can have their likenesses used for dubious ends. In June 2023 the Federal Bureau of Investigation in America warned the public of “malicious actors” using AI to create fake sexually themed videos and images of ordinary people, in order to extort money.Read more of our coverage of AIHow to detect such trickery is a live topic among AI researchers, many of whom attended NeurIPS, one of the field’s biggest conferences, held in New Orleans in December. A slew of firms, from startups to established tech giants such as Intel and Microsoft, offer software that aims to spot machine-generated media. The makers of big AI models, meanwhile, are searching for ways of “watermarking” their output so that real pictures, video or text can be readily distinguished from the machine-generated sort.But such technologies have not, so far, proved reliable. The AI cognoscenti seem gloomy about their prospects. The Economist conducted a (deeply unscientific) straw poll of delegates to NeurIPS. Of 23 people asked, 17 thought AI-generated media would eventually become undetectable. Only one believed that reliable detection would be possible. (The other five demurred, preferring to wait and see.)Detection software relies on the idea that AI models will leave a trace. Either they will fail to reproduce some aspect of real images and video, or of human-generated text, or they will add something superfluous—and will do so often enough to let other software spot the error. For a while, humans could do the job. Up until about the middle of 2023, for instance, image-generation algorithms would often produce people with malformed hands, or get the numbers wrong on things like clock faces. These days, the best no longer do.But such telltales often still exist, even if they are becoming harder for humans to spot. Just as machines can be trained to reliably identify cats, or cancerous tumours on medical scans, they can also be trained to differentiate between real images and AI-generated ones.It seems, though, that they cannot do so all that well. Detection software is prone to both false positives (wrongly flagging human content as generated by AI) and false negatives (allowing machine-generated stuff to pass undetected). A pre-print published in September by Zeyu Lu, a computer scientist at Shanghai Jiao Tong University, found that the best-performing program failed to correctly spot computer-generated images 13% of the time (though that was better than the humans, who erred in 39% of cases). Things are little better when it comes to text. One analysis, published in December in the International Journal of Educational Integrity, compared 14 tools and found that none achieved an accuracy of more than 80%.If trying to spot computer-generated media after the fact is too tricky, another option is to label it in advance with a digital watermark. As with the paper sort, the idea is to add a distinguishing feature that is subtle enough not to compromise the quality of the text or image, but that is obvious to anyone who goes looking for it.One technique for marking text was proposed by a team at the University of Maryland in July 2023, and added to by a team at University of California, Santa Barbara, who presented their tweaks at NeurIPS. The idea is to fiddle with a language model’s word preferences. First, the model randomly assigns a clutch of words it knows to a “green” group, and puts all the others in a “red” group. Then, when generating a given block of text, the algorithm loads the dice, raising the probability that it will plump for a green word instead of one of its red synonyms. Checking for watermarking involves comparing the proportion of green to red words—though since the technique is statistical, it is most reliable for longer chunks of writing.Many methods for watermarking images, meanwhile, involve tweaking the pixels in subtle ways, such as shifting their colours. The alterations are too subtle for human observers to notice, but can be picked up by computers. But cropping an image, rotating it, or even blurring and then resharpening it can remove such marks.Another group of researchers at NeurIPS presented a scheme called “Tree-Ring” watermarking that is designed to be more robust. Diffusion models, the most advanced type of image-generation software, begin by filling their digital canvas with random noise, out of which the required picture slowly emerges. The tree-ring method embeds the watermark not in the finished picture, but in the noise at the start. If the software that created a picture is run in reverse, it will reproduce the watermark along with the noise. Crucially, the technique is less easy to thwart by fiddling with the final image.But it is probably not impossible. Watermarkers are in an arms race with other researchers aiming to defeat their techniques. Another team led by Hanlin Zhang, Benjamin Edelman and Boaz Barak, all of Harvard University, presented a method (not yet peer-reviewed) that can, they say, erase watermarks. It works by adding a dash of new noise, then using a second, different AI model to remove that noise, which removes the original watermark in the process. They claim to be able to foil three new text-watermarking schemes proposed in 2023. In September scientists at the University of Maryland published a paper (also not yet peer-reviewed) claiming that none of the current methods of image watermarking—Tree-Rings included—is foolproof.Nevertheless, in July 2023 America’s government announced “voluntary commitments” with several AI firms, including OpenAI and Google, to boost investment in watermarking research. Having imperfect safeguards is certainly better than having none (although open-source models, which users are free to tweak, will be harder to police.) But in the battle between the fakers and the detectives, it seems that the fakers have the upper hand. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.It is now possible to generate fake but realistic content with little more than the click of a mouse. This can be fun: a TikTok account on which—among other things—an artificial Tom Cruise wearing a purple robe sings “Tiny Dancer” to (the real) Paris Hilton holding a toy dog has attracted 5.1m followers. It is also a profound change in societies that have long regarded images, video and audio as close to ironclad proof that something is real. Phone scammers now need just ten seconds of audio to mimic the voices of loved ones in distress; rogue AI-generated Tom Hankses and Taylor Swifts endorse dodgy products online, and fake videos of politicians are proliferating.The fundamental problem is an old one. From the printing press to the internet, new technologies have often made it easier to spread untruths or impersonate the trustworthy. Typically, humans have used shortcuts to sniff out foul play: one too many spelling mistakes suggests an email might be a phishing attack, for example. Most recently, ai-generated images of people have often been betrayed by their strangely rendered hands; fake video and audio can sometimes be out of sync. Implausible content now immediately raises suspicion among those who know what AI is capable of doing.Explore more of our coverage of aiThe trouble is that the fakes are rapidly getting harder to spot. ai is improving all the time, as computing power and training data become more abundant. Could ai-powered fake-detection software, built into web browsers, identify computer-generated content? Sadly not. As we report this week, the arms race between generation and detection favours the forger. Eventually ai models will probably be able to produce pixel-perfect counterfeits—digital clones of what a genuine recording of an event would have looked like, had it happened. Even the best detection system would have no crack to find and no ledge to grasp. Models run by regulated companies can be forced to include a watermark, but that would not affect scammers wielding open-source models, which fraudsters can tweak and run at home on their laptops.Dystopian possibilities abound. It will be difficult, for example, to avoid a world in which any photograph of a person can be made pornographic by someone using an open-source model in their basement, then used for blackmail—a tactic the fbi has already warned about. Perhaps anyone will be able to produce a video of a president or prime minister announcing a nuclear first strike, momentarily setting the world on edge. Fraudsters impersonating relatives will prosper.Yet societies will also adapt to the fakers. People will learn that images, audio or video of something do not prove that it happened, any more than a drawing of it does (the era of open-source intelligence, in which information can be reliably crowdsourced, may be short-lived). Online content will no longer verify itself, so who posted something will become as important as what was posted. Assuming trustworthy sources can continue to identify themselves securely—via urls, email addresses and social-media platforms—reputation and provenance will become more important than ever.It may sound strange, but this was true for most of history. The era of trusted, mass-produced content was the exception. The fact that people may soon struggle to spot the invisible hand of ai does not mean the marketplace of ideas is doomed. In time, the fakes that thrive will mostly be the funny ones. ■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Technology stocks are having a bumper year. Despite a recent wobble, the share price of the Big Five—Alphabet, Amazon, Apple, Meta and Microsoft—has jumped by 60% since January, when measured in an equally weighted basket. The price of shares in one big chipmaker,  Nvidia, has tripled and in another, AMD, almost doubled. Their price-to-earnings ratios (which measure how much the markets think a company is worth relative to its profits) are ten times that of the median firm in the s&p 500.The main reason for the surge is the promise of artificial intelligence (AI). Since the launch in November of Chatgpt, an AI-powered chatbot, investors have grown ever more excited about a new wave of technology that can create human-like content, from poems and video footage to lines of code. This “generative AI” relies on large language models which are trained on big chunks of the internet. Many think the technology could reshape whole industries, and have as much impact on business and society as smartphones or cloud computing. Firms that can make the best use of the technology, the thinking goes, will be able to expand profit margins and gain market share.Corporate bosses are at pains to demonstrate how they are adopting AI. On April 4th Jamie Dimon, JPMorgan Chase’s boss, said his bank had 600 machine-learning engineers and had put AI to work on more than 300 different internal applications. David Ricks, the boss of Eli Lilly,  has said that the pharmaceutical giant has more than 100 projects on the go using ai.Company case studies reveal only part of the picture. To get a broader sense of which companies and industries are adopting ai The Economist examined data on all the firms in the s&p 500. We looked at five measures: the share of issued patents that mention AI; venture-capital (VC) activity targeting AI firms; acquisitions of AI firms; job listings citing AI; and mentions of the technology on earnings calls. Because other types of ai could bring benefits for business, our analysis captures activity for all AI, not just the generative wave. The results show that even beyond tech firms the interest in AI is  growing fast. Moreover, clear leaders and laggards are already emerging.AI expertise already seems to be spreading (see chart). About two-thirds of the firms in our universe have placed a job ad mentioning AI skills in the past three years, says PredictLeads, a research firm.  Of those that did, today 5.3% of their listed vacancies mention ai, up from a three-year average of 2.5%.  In some industries the rise is more dramatic. In retail firms that share has jumped from 3% to 11%, while among chipmakers that proportion grew from 9% to 19%.The number of AI-related patents being registered trended upwards between 2020 and 2022, according to data provided by Amit Seru of Stanford University. PitchBook, another research firm, concludes that in 2023 some 25% of venture deals by s&p 500 firms involved ai startups, up from 19% in 2021. GlobalData, also a research firm, finds that about half the firms scrutinised have talked about AI in earnings calls since 2021, and that in the first quarter of this year the number of times AI was mentioned in the earnings calls of America Inc more than doubled compared with the previous quarter. Roughly half have been granted a patent relating to the technology between 2020 and 2022.The use of generative ai may eventually become even more common than other sorts of ai. That is because it is good at lots of tasks essential to running a firm. A report by McKinsey, a consultancy, argues that three-quarters of the expected value created by generative AI will come in four business functions—research and development, software engineering, marketing and customer service. To some extent, all these operations are at the core of most big businesses. Moreover, any large company with internal databases used to guide employees could find a use for an AI-powered chatbot. Morgan Stanley, a bank, is building an AI assistant that will help its wealth managers find and summarise answers from a huge internal database. SLB, an oil-services company, has built a similar assistant to help service engineers.While the adoption of ai is happening in many firms, some are more enthusiastic than others. Ranking all the companies  using each metric and then taking an average produces a simple scoring system. Those at the top seem to be winning over investors. Since the start of the year, the median share price of the top 100 has risen by 11%; for the lowest-scoring quintile it has not moved at all.The top spots are unsurprisingly dominated by Silicon Valley. On a broad definition, the s&p 500 contains 82 tech firms. Almost 50 of them make the top 100. Nvidia is the highest-scoring firm. According to data from PredictLeads, over the past three years a third of its job listings have mentioned AI. In the past year the firm has mentioned AI in its earnings calls almost 200 times, more than any other company. Other high-ranking tech firms include the cloud-computing giants—Alphabet (3rd), Microsoft (12th) and Amazon (34th). They sell access to a range of AI tools, from services that help train sophisticated models to software that allows the use of ai without having to write reams of code.Beyond tech, two types of firms seem to be adopting ai the quickest. One is data-intensive industries, such as insurers, financial-services firms and pharmaceutical companies. They account for about a quarter of our top 100. These firms tend to have lots of structured datasets, such as loan books or patient files, which makes it easier to use AI, notes Ali Ghodsi of Databricks, a database firm. Around a tenth of JPMorgan Chase’s current job listings mention AI. The firm recently filed a patent for Indexgpt, an AI-infused chatbot that gives investment advice. Health-care firms like Gilead Sciences and Moderna use AI to discover new drugs. Others, such as Abbott and Align Technology, are building AI-powered medical devices. America’s Food and Drug Administration approved 97 such machines last year, up from 26 in 2017.A second group is industries that are already being disrupted by technology, including carmaking, telecoms, media and retail. Thirteen firms from these industries make the high-scoring 100, including Ford, General Motors and Tesla. The rise of electric vehicles and the prospect of self-driving cars has encouraged vehicle manufacturers to invest in technology. In March Ford established Latitude AI, a self-driving car subsidiary that might one day rival gm’s Cruise. In April Elon Musk told analysts that Tesla was buying specialised AI chips and was “very focused” on improving their AI capabilities in an effort to improve his firm’s self-driving efforts.Retailers are using AI to bolster their core business. Nike, a sportswear giant, filed an application for a patent in 2021 for a system that can generate three-dimensional computer models of trainers. Christian Kleinerman of Snowflake, a database provider, notes that retailers are also taking advantage of the growth of e-commerce by collecting more data on customers. That allows more accurate targeting of marketing campaigns. Some may take personalisation a step further. In 2021 Procter & Gamble, a consumer-goods giant, applied for a patent for an AI-based system which analyses users’ skin and hair conditions based on photos, and recommends products to treat them.One source of variation in ai use across industries may be a result of the type of work undertaken. A working paper led by Andrea Eisfeldt of the University of California looked at how exposed firms are to ai. The researchers assessed which tasks took place in a firm and how well Chatgpt could perform them. The most exposed were tech firms, largely because AI chatbots are good at coding. Those industries least exposed, such as agriculture and construction, tended to rely on manual labour.Clear leaders and laggards are emerging within industries, too. About 70 firms in the s&p 500 show no sign on any of our metrics of focusing on AI. That includes firms in AI-heavy industries, such as insurers. The mass of smaller firms not included in the s&p 500 may be even less keen. One distinguishing factor within industries may be investment. For the top 100 firms in our ranking, the median r&d expenditure as a share of revenue was 11%. For those in the lowest 100 it was zero.Vlad Lukic of BCG, a consultancy, notes that there is even a lot of variation within companies. He recalls visiting two divisions of the same medium-sized multinational. One had no experience working with ai. The other was advanced; it had been using a pilot version of the technology from Openai, the startup behind ChatGPT, for two years.Among early adopters, many non-tech companies’ AI use is growing more sophisticated. Mr Seru’s data reveal that about 80 non-tech firms have had ai-related patents issued which were cited by another patent, suggesting that they have some technological value. Some 45 non-tech companies in the s&p 500 have recently placed ads which mention model training, including Boeing, United Health and State Street. That suggests they may be building their own models rather than using off-the-shelf technology from the likes of Openai. The advantage of this approach is that it can produce more-accurate ai, giving a greater edge over rivals.However, a shift to in-house training hints at one of the risks: security. In May Samsung discovered that staff had uploaded sensitive code to Chatgpt. The concern is that this information may be stored on external servers of the firms which run the models, such as Microsoft and Alphabet. Now Samsung is said to be training its own models. The firm also joined the growing list of companies that have banned or limited the use of Chatgpt, which includes Apple and JPMorgan Chase.Other risks abound. Model-makers, including Openai, are being sued for violating copyright laws over their use of internet data to train their models. Some large corporations think that they could be left liable if they use Openai’s technology. Moreover, models are prone to make up information. In one incident a New York lawyer used Chatgpt to write a motion. The chatbot included fictional case-law and the lawyer was fined by the court.But all this must be weighed against the potential benefits, which could be vast. Waves of technology frequently turn industries on their head. As generative ai diffuses into the economy, it is not hard to imagine it doing the same thing. Mr Lukic says that the biggest risk for companies may be falling behind. Judged by the scramble in America Inc for all things AI, many bosses and investors would agree. ■Correction: This piece originally claimed that Starbucks was using AI to make the perfect vegan breakfast sandwich. The ring of truth made what was, in fact, an April fools joke so believable. We have removed the reference. Our apologies for the clanger.To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
This article is part of our Summer reads series. Visit the full collection for book lists, guest essays and more seasonal distractions.NEW GENERATIVE-AI tools like OpenAI’s ChatGPT, the fastest-growing consumer internet application of all time, have taken the world by storm. They have uses in everything from education to medicine and are astonishingly fun to play with. Although current AI systems are capable of spectacular feats they also carry risks. Europol has warned that they might greatly increase cybercrime. Many AI experts are deeply worried about their potential to create a tsunami of misinformation, posing an imminent threat to the American presidential election in 2024, and ultimately to democracy itself, by creating an atmosphere of total distrust. Scientists have warned that these new tools could be used to design novel, deadly toxins. Others speculate that in the long term there could be a genuine risk to humanity itself.One of the key issues with current AI systems is that they are primarily black boxes, often unreliable and hard to interpret, and at risk of getting out of control. For example, the core technology underlying systems like ChatGPT, large language models (LLMs), is known to “hallucinate”, making up false statements. ChatGPT, for example, falsely accused a law professor of being involved in sexual harassment, apparently confused by statistical but irrelevant connections between bits of text that didn’t actually belong together. After an op-ed tried to clarify what had gone wrong, Bing Chat made a similar error, and attributed it to information in USA Today that the chatbot got completely backwards.These systems can also be used for deliberate abuse, from disrupting elections (for example by manipulating what candidates appear to say or write) to spreading medical misinformation. In a recent analysis of GPT-4, OpenAI’s most advanced LLM, the company acknowledged 12 serious concerns—without providing firm solutions to any of them.In the past year alone 37 regulations mentioning AI were passed around the globe; Italy went so far as to ban ChatGPT. But there is little global co-ordination. Even within some countries there is a hodge-podge, such as different state laws in America, or Britain’s proposal to eschew a central regulator, leaving oversight split among several agencies. An uneven, loophole-ridden patchwork is to no one’s benefit and safety. Nor should companies want to build a different AI model for each jurisdiction and face their own de novo struggle to navigate legal, cultural and social contexts.Read more from our special series on AI:Large, creative AI models will transform lives and labour marketsHow generative models could go wrongLarge language models’ ability to generate text also lets them plan and reasonStill, there is plenty of agreement about basic responsible AI principles, such as safety and reliability, transparency, explainability, interpretability, privacy, accountability and fairness. And almost everyone agrees that something must be done—a just-published poll by the Centre for the Governance of AI found that 91% of a representative sample of 13,000 people across 11 countries agreed that AI needs to be carefully managed.It is in this context that we call for the immediate development of a global, neutral, non-profit International Agency for AI (IAAI), with guidance and buy-in from governments, large technology companies, non-profits, academia and society at large, aimed at collaboratively finding governance and technical solutions to promote safe, secure and peaceful AI technologies.The time for such an agency has come, as Google CEO Sundar Pichai himself said on April 16th. What might that look like? Each domain and each industry will be different, with its own set of guidelines, but many will involve both global governance and technological innovation. For example, people have long agreed that making employment decisions based on gender should be avoided, and have even come up with some measures in earlier, more interpretable AI, such as the interpretability requirements of the AI Bill of Rights proposed by the Biden administration. But in black-box systems like ChatGPT there is a wide variety of use cases with no current remedy. People might, for example, feed in a job candidate’s entire file and ask ChatGPT for a judgment, but we currently have no way to ensure that ChatGPT would avoid bias in its output. The kind of entity we envision would collaboratively address what to do about such “off-label” uses of chatbots and other policy questions, and at the same time develop technical tools for effective auditing.The IAAI could likewise convene experts and develop tools to tackle the spread of misinformation. On the policy side, it could ask, for instance, how wide-scale spreading of misinformation might be penalised. On the technical side, the initial focus should be on developing automated or semi-automated tools for answering fundamental questions, such as “How much misinformation is out there?”, “How rapidly is its volume growing?” and “How much is AI contributing to such problems?” Existing technologies are better at generating misinformation than detecting it. Considerable technical innovation will be required, and of great public benefit, but may or may not be of sufficiently direct commercial interest – hence the need for independent support by an entity like the IAAI.To take a third, very recent example, systems with names like AutoGPT and BabyAGI have been devised that allow amateurs to build complex and difficult-to-debug (or even fathom) assemblies of unreliable AI systems controlling other unreliable AI systems to achieve arbitrary goals—a practice that may or may not prove to be safe. As Marek Rosa, CEO of GOOD.Ai, put it, we need new technical ideas on “how to increase security (proactive defence) in a world where there are billions of AI agents…running in apps and servers, and we don’t know what they are talking about”, perhaps necessitating a kind of “antivirus [software] against AI agents”. A global alliance with top experts and researchers on call would be able to give swift and thoughtful guidance on such new developments.Designing the kind of global collaboration we envision is an enormous job. Many stakeholders need to be involved. Both short-term and long-term risks must be considered. No solution is going to succeed unless both governments and companies are on board, and it’s not just them: the world’s publics need a seat at the table.Fortunately, there is precedent for such global co-operation. At the end of the second world war, for example, nuclear weapons sparked deep fears and uncertainties about how the new technology would be used. As a response, 81 countries unanimously approved the International Atomic Energy Agency’s statute to “promote safe, secure and peaceful nuclear technologies”, with inspection rights. A different, softer kind of model, with less focus on enforcement, is the International Civil Aviation Organisation, in which member countries make their own laws but take counsel from a global agency. Getting to the right model, and making the right choices, will take time, wisdom and collaboration.The challenges and risks of AI are, of course, very different and, to a disconcerting degree, still unknown. We know in hindsight that the internet might have been designed in better ways with more forethought. Earlier decisions about how to handle privacy and anonymity, for instance, might have ensured that there was less of a culture of trolling. We also know that early choices get locked in. Our decisions now are likely to have lasting consequences and must be made thoughtfully.Given how fast things are moving, there is not a lot of time to waste. A global, neutral non-profit with support from governments, big business and society is an important start. ■Gary Marcus is Emeritus Professor at NYU and was founder and CEO of Geometric Intelligence, a machine-learning company acquired by Uber. Anka Reuel is a PhD student in computer science at Stanford University and founding member of KIRA, a think-tank focusing on the promotion of responsible AI.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Noisy crowds of beautiful people gathered outside Hollywood’s film studios every weekday for the past six months, shouting slogans and marching in the sun. America’s screenwriters and actors were striking, in part over fears that artificial intelligence (AI) will soon be writing scripts or even bagging roles. “You go for a job and they scan you,” said a background actress, who worried that her face will be used over and over in crowd scenes. The technology is “disgusting”, said another, who considered its use “an infringement of yourself, of your career”. The deal actors struck on November 8th to end their strike included protections from their artificial rivals.Five thousand miles away another animated crowd can regularly be found chattering about AI, usually in gloomier weather but brighter spirits. Outside a purpose-built stadium in east London, giddy groups of all ages, some in fancy dress, gather seven times a week to watch ABBA Voyage, a performance in which a septuagenarian pop group plays an energetic 90-minute set via virtual avatars, generated with the help of AI. The show, launched in 2022, played to 1m people in its first year and still almost sells out on most nights, bringing in a reported $2m a week while ABBA’s four members put their feet up.Will AI leave talent impoverished, as Hollywood’s protesting actors and writers fear, or further enrich them, as ABBA has found? It is easy to see why stars are nervous. Over the years technology has automated away many routine jobs in entertainment, but the creative work at the heart of the business has been protected. No longer. Generative AI is hoovering up copyrighted work and churning out remixed literature, music and video of all varieties, increasingly competing with humans in quality and already far outpacing them in quantity. In a world of infinite AI-generated content, is it even possible to be a star?The talent are mobilising unions, lawyers and politicians to protect themselves. Hollywood’s screenwriters, who ended their five-month strike in September, won a promise from studios to employ a minimum number of human writers on films and TV shows. The studios have retained the right to use AI to generate or polish scripts, but the AI will not be recognised as an author, so human writers’ royalties will not be diluted. Writers, for their part, have kept the right to use AI in their own work. (Although they dislike it, many consider AI “the best writing partner they’ve ever had”, concedes a Hollywood agent.)The details of the actors’ deal with studios and streaming services had not been released as The Economist went to press, but the Screen Actors Guild, their union, said that it included “unprecedented provisions for consent and compensation that will protect members from the threat of AI”. This is likely to mean that actors will at least be notified and paid if their likeness or voice is to be digitally reanimated.I sue, I sue, I sue, I sue, I sueThe stars’ second line of defence is legal. A group of authors including John Grisham (of “The Firm” and dozens more thrillers) and George R.R. Martin (of “Game of Thrones”) have filed a class-action lawsuit against OpenAI, the Microsoft-backed company behind ChatGPT, arguing that it ingested their work without permission or payment. Another complaint by parties including Universal Music Group, the world’s biggest record company, accuses Anthropic, an Amazon- and Google-backed AI firm, of doing something similar with song lyrics.image: The EconomistThe tech firms argue that “training” an AI model on copyrighted work amounts to fair use. In the words of Matthew Sag of Emory University, AI does not copy a book “like a scribe in a monastery” so much as learn from it like a student. Pieces of training data, whether novels or songs, usually play such a small role in the model’s output as to be barely traceable. But not always. “If you say, ‘Write in the style of Dan Brown [of “The Da Vinci Code”],’ of course it will pull from Dan Brown’s books,” declares Mary Rasenberger, head of the Authors Guild, which represents writers.As the courts grind into action, governments are also getting involved. On October 30th Joe Biden, America’s president, issued an executive order setting out basic rules for AI development. The US Copyright Office is running an inquiry into AI, which will close to comments later this month.There is a risk that governments will compete to create the most “permissive” regulatory environment, to attract AI firms, warns a music-industry insider. Others, citing past waves of tech-driven disruption, see room for compromise. When music-streaming arrived, “We got very defensive, and only defensive,” admits another senior record-company executive. It took a decade for the labels to realise that the technology was good for business, and do a deal with the streaming platforms. Negotiations with AI companies will take less than half that time, he predicts. “It’s co-existence that’s required.”When the law is settled, who will win and lose from the technology? On the face of it, the biggest stars seem most vulnerable. AI helps ordinary people narrow the gap with the most gifted. Less glamorous industries have already witnessed this. In April a working paper by Erik Brynjolfsson of Stanford University and others found that novice customer-support agents were 35% more productive when given access to a chatbot, whereas experienced agents hardly benefited.Something similar is happening in show business. Even before AI, tools like autotune were helping ordinary mortals to sound more like their idols. The next generation of technology promises to make such features more powerful. TikTok’s parent company, ByteDance, is trialling an app called Ripple which takes any melody that users sing into their phone and turns it into a polished song. Boomy, an American startup, lets amateur composers generate original tunes with a few clicks and upload them to earn royalties on streaming platforms such as Spotify.Frumpy actors can vie with gorgeous ones thanks to the digital facelifts of tools such as Vanity AI, which are used in productions like “Stranger Things” and “The Walking Dead” to make actors look prettier or scarier, as required. Dull writers can get inspiration from apps such as Sudowrite, which suggests new ideas and edits. It bills itself pithily as “the non-judgmental, always-there-to-read-one-more-draft, never-runs-out-of-ideas-even-at-3am AI writing partner you always wanted”.Not much AI-made work is good, let alone dazzling enough to compete with the stars at the top of the talent tree. But it is starting to have an impact through sheer scale. Boomy claims to have generated nearly 18m songs (for comparison, Spotify’s entire catalogue is a little over 100m). Spotify now adds more than 100,000 new tracks every day, many of them AI-made. Sir Lucian Grainge, the head of Universal Music, has warned that real music could drown in a “sea of noise” as streaming platforms fill up with amateur tracks. Professional artists’ share of listening is indeed sinking. In 2017 artists signed to record labels accounted for 87% of the streams on Spotify. Last year their share was only 75%.image: The EconomistAI struggles to make real hits, but is good at churning out the kind of music that people have on in the background while working or going to sleep. Those tracks are played for long hours and so earn big returns under the payment model used by streamers. Some in the industry suspect that the streaming platforms welcome the rise of amateur artists, who have less bargaining power than the big labels. The idea that streamers might nudge listeners towards this content is “absolutely a concern”, says another record-company executive. He thinks the only thing stopping them is the risk of annoying listeners. The streamers retort that it is hardly in their interest to promote bad music. Earlier this year Spotify purged lots of AI-made songs and is said to be rejigging its rules to make low-quality wallpaper music less profitable; Deezer and Tidal, two smaller rivals, have taken similar measures.The publishing world has similar complaints. Producing entire books by AI is so quick and easy that, in September, Amazon banned authors from self-publishing more than three e-books a day on its Kindle platform and required publishers to clearly label books by robot writers. Most AI texts are full of cliché and waffle (which makes them competitive in the management genre, an agent quips). But some are taking sales away from human authors by deception: the Authors Guild has spotted a tendency for AI-made biographies to land just before real memoirs are published, for instance. “Click farms” have also been deployed to manipulate Amazon’s rankings. The guild says that, at one point this summer, 40 of the top 100 young-adult romance books on Kindle were AI-written.The winner makes it allYet far from sinking in this sea of AI-made entertainment, the biggest stars of all seem to be more buoyant than ever. As record labels worry about robot composers, Taylor Swift is halfway through what will probably be the highest-earning concert tour in history, with projected sales of $1.4bn or more (no previous tour has breached $1bn). An accompanying film has made an additional $230m at the box office. Colleen Hoover, a writer of romantic young-adult fiction, crushed the robot writers last year to bag eight places on the top-25 bestseller list in America, selling 14m copies, according to Publishers Weekly, a trade title.One of the paradoxes of the internet age is that, amid an explosion in online content on platforms from YouTube to TikTok, fans have flocked as never before to the biggest acts. It has been a good time to be an amateur creator, but an even better time to be a superstar. Data from Spotify show that between 2017 and 2022, as the platform was flooded with tens of millions of amateur tracks, the number of artists making at least $1,000 a year in royalties increased by 155%. At the same time, the number making $5m or more increased by 165%, and the handful of headliners making $10m or more increased by 425% (see chart 1 on previous page). Those who have done least well are middling-to-big artists, who face more competition from entertainment’s long tail but have been unable to break into the elite group at the top.Similar patterns can be seen across the entertainment world. Better and cheaper technology has democratised filmmaking. The number of movies released each year in America more than doubled during the first two decades of the 21st century, but the audience-share of the biggest blockbusters has grown, not shrunk. In 2019 (the last year before the pandemic) America’s five biggest films took a quarter of the domestic box office, nearly double the share they took in the less crowded market of 2000 (see chart 2). Meanwhile, the ten bestselling authors have accounted for a steady 2-3% of book sales in Britain over the past decade, according to Nielsen, a data company, even though 2m more books are self-published each year.Lay all your likes on me“Hits will persist in an infinite-content world,” argues Doug Shapiro, a former media executive. The bewildering variety makes it hard for consumers to pick, so they rely more on recommendations, whether from friends or algorithms. Rather than browse, people seek out what they have already heard of, says Ms Rasenberger of the Authors Guild. “Known writers are selling. And everyone else is having a much harder time.”AI also helps superstars shine brighter by creating opportunities for their admirers to become superfans. Followers of a music act used to express themselves chiefly through “records, T-shirts and mix-tapes”, says an industry executive. Now the internet offers “mix-tapes on steroids”. Fans can duet with their idols or dance to their music on social media. Games like “Fortnite” have provided a venue for interactive experiences: Ariana Grande held a concert-game hybrid on the platform in 2021, attracting 27m participants.AI-powered technology promises to allow the biggest stars to be in even more places, gratifying even more fans. The producers of ABBA Voyage are in talks to bring the show to cities in North America, South-East Asia and Australasia, according to Per Sundin, head of Pophouse, a Swedish entertainment company that is the biggest investor in the enterprise. Other artists have been in touch, looking to “cement and elevate their legacies” with similar shows, he says. “We now have a proven template.”Stars do not need to build their own arena like ABBA to use ai to be in more places at once. Spotify is working with OpenAI to translate podcasts into different languages, allowing broadcasters to be heard in more markets, in their own voice. Other firms such as HeyGen provide dubbing services for video, using AI to change the movement of the actor’s lips to match what they are saying. HeyGen recently created a viral video of Ms Swift appearing to speak fluent Chinese. Such platforms can adapt content in other ways, too, for instance by toning down strong language for a broader audience. Technology like this will allow stars to reach more viewers—and presents a problem to the lowlier actors who specialise in dubbing.Stars are also using ai to travel in time. John Lennon re-entered the charts on November 2nd with “Now and Then”, old recordings of his voice having been salvaged using AI. (“I hope someone does this to all my crap demos after I’m dead—make them into hit songs,” said his late fellow Beatle, George Harrison, of an earlier attempt to resurrect Lennon’s work.) James Earl Jones, who is 92 and first voiced Darth Vader almost 50 years ago, has sold Disney the right to replicate his gravelly tones artificially. A virtual Darth appeared in Disney’s “Obi-Wan Kenobi” last year.Some artists are experimenting with licensing their voice or image more widely. In April Grimes, a Canadian singer, invited amateur composers to clone her voice for use in their songs, provided they share any royalties with her. In September Meta launched 28 chatbot characters, played by celebrities. Snoop Dogg, Paris Hilton and Charlie D’Amelio are among the B-listers to be botified, with one reportedly being paid $5m over two years. Mark Zuckerberg, Meta’s boss, invites stars to “build an AI version of yourself…to help people fulfil this desire to interact with you and your desire to build a community.”image: Simon Bailley / SepiaA-listers remain wary: “Their voice is their career,” says a record executive. But even if they do not license their voice or likeness, others may still borrow them. A company called Socialdraft sells $5 “prompts” to make chatbots take on the personality of celebrities ranging from Tom Cruise to Josef Stalin, who do not appear to have given their permission. It is named in a legal complaint by a group of authors including Mr Grisham, whom it also impersonates. (Socialdraft denies wrongdoing. Antonio Evans, its chief executive, says, “The interplay between AI, copyright and individual rights is a thrilling narrative we are all part of.”)Some unauthorised clones have become hits. In April an anonymous amateur composer released a song called “Heart On My Sleeve”, using the AI-replicated voices of two rappers, Drake and The Weeknd. The song was streamed 20m times before Universal, which represents the two artists, demanded it be taken down.The track caused alarm, but it also demonstrated how cloning can work in artists’ favour. Analysis by Will Page, author of “Pivot” and a former chief economist at Spotify, suggests that being cloned can help stars sell genuine music. He calculates that after David Guetta, a French DJ, posted a clip of an AI-generated rap in the style of Eminem in February, streams of real Eminem tracks rose by about a fifth. Yet antipathy to AI remains fierce. A group of actors, writers and directors has set up an outfit called Credo23 to certify films and TV shows made without AI (it has yet to take off). Many singers, burned by digital piracy in the past, are also hostile. Sir Cliff Richard, an 83-year-old crooner, recently declared that his singing “didn’t use artificial insemination”.Others fear that AI will simply make entertainment derivative and boring. Three-quarters of Americans tell YouGov, a pollster, that they worry AI will sap human creativity. The process of ingesting everything and then spitting out an average may lead to a stylistic and conceptual regression to the mean, says a literary agent, who sees similarities with the algorithms on social media that help propagate the most pedestrian views. Sir Lucian, at Universal, has said that AI “will always lack the essential spark that drives the most talented artists to do their best work, which is intention”.Everything depends on whether audiences embrace artificial performances. “Is the next generation of moviegoers going to want to see a different actor in James Bond, as an example? Or are they going to want to see Sean Connery come back?” asks a Hollywood agent. AI-generated performances may prove to be most successful for the biggest stars, whose uncritical superfans cannot get enough of them. As Liam Gallagher, a former Oasis frontman and John Lennon devotee, replied when asked on social media what he thought of the remastered “Now and Then”: “The Beatles could shit in my hand bag I’d still hide my polo mints in there.” ■Read more of our articles on artificial intelligence
IN A RECENT survey of North American chief executives and chief financial officers, nearly 80% listed corporate culture as one of the five most important factors driving their company’s financial performance. A growing body of empirical evidence supports their belief that culture matters—and can boost profitability.Yet, in the same survey, an even higher number of respondents—84%—said their company’s culture is not where it needs to be. Again the data supports their intuition. The average culture rating of large employers in America on Glassdoor, a website that lets workers rate their employers, is 3.6 out of 5. Few people would be excited to eat in a restaurant or ride with an Uber driver with that kind of rating. Similarly, few employees are likely to relish spending 40 hours each week in an average culture.Building and maintaining a healthy corporate culture can be even more challenging in organisations where employees work remotely. In an ongoing study, we find that the companies where employees are most effusive about remote work score lower than their peer groups on corporate culture, especially on learning and development opportunities and honest communication.Leaders cannot improve what they cannot measure. Unfortunately, the most common tool for gauging corporate culture—the engagement survey—suffers from serious limitations. Faced with a long list of questions, employees switch to auto-pilot and assign identical or similar scores to every question. Employers that ask dozens of multiple-choice questions, as many do, might glean only a couple of reliable insights because of respondents zoning out. Even when employees do engage with a question, their score offers little guidance on how to improve things. And what if the topic the employee really wanted to weigh in on wasn’t included?Recent advances in AI—most notably large language models (LLMs)—allow leaders, for the first time, to glean nuanced insights into their corporate culture from how employees talk about their company in their own words. Rather than answering reams of questions on a five-point scale, workers can now simply explain what is and isn’t working in their organisation and offer suggestions for how it can improve. The AI can do the heavy lifting, providing much more granular classification of comments and assessment of sentiment.Freed from the shackles of traditional surveys, organisations can use AI to gather and process employee feedback from many sources. The volume of available feedback is staggering. Combining free text from internal surveys, performance feedback provided to managers, online employer reviews and other sources equates to tens of thousands of pages of data each year for a large company. Until recently, organisations had to rely on crude tools such as word clouds or search keywords to gain insights from this trove of information.Armed with the more numerous and granular measurements that AI brings, executives can more quickly and easily assess whether their company is living up to the values it considers “core”, identify the most important cultural elements driving everything from employee attrition to innovation, diagnose toxic subcultures within the organisation, and plot progress over time. After spotting important patterns, leaders can dive into the raw feedback for more nuanced context and employees’ recommendations on how to improve culture.Take Amazon, which aspires to be the best employer in the world. We used our AI platform to analyse tens of thousands of employee reviews of the e-commerce giant. This showed that Amazon does well on many of its leadership principles, such as “customer obsession” and “invent and simplify”. But the firm’s culture also contributes to employee burnout, especially among software engineers, who are twice as likely to complain about burnout than warehouse workers or drivers. Raw employee feedback points to ways Amazon could reduce stress for engineers, like fixing a performance-review process widely viewed as brutal or minimising late-night disruptions when technical employees are on call.Even the largest companies will take their time adopting AI. But cultural analysis is one of the few areas where it can be embraced right now, because it plays to one of AI’s biggest current strengths: understanding natural language at scale.This does not mean that leaders should blindly trust the output of LLMs. The tools require safeguards to protect against foibles, such as hallucinating made-up answers. Models should measure the elements of culture based on solid evidence, rather than the latest management fad. Leaders need to take a broad view of culture, measuring not only the factors that influence employee satisfaction but also topics that shape a company’s ability to adapt to market shifts and to avoid unethical or illegal behaviour.Leaders who do adopt AI for cultural insights can use these to make their employees happier, lower the odds of reputational disasters and, ultimately, boost their profits. Measurement is not the only piece of the “successful culture” puzzle, but it is a crucial one. Culture has always been an enigma at the heart of organisational performance: undoubtedly important, but inscrutable. With AI, meaningful progress can be made in deciphering it. ■Don Sull is a professor at the MIT Sloan School of Management and a co-founder of CultureX, a research and AI firm. Charlie Sull is a co-founder of CultureX.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.AS PUPILS AND students return to classrooms and lecture halls for the new year, it is striking to reflect on how little education has changed in recent decades. Laptops and interactive whiteboards hardly constitute disruption. Many parents bewildered by how their children shop or socialise would be unruffled by how they are taught. The sector remains a digital laggard: American schools and universities spend around 2% and 5% of their budgets, respectively, on technology, compared with 8% for the average American company. Techies have long coveted a bigger share of the $6trn the world spends each year on education.When the pandemic forced schools and universities to shut down, the moment for a digital offensive seemed nigh. Students flocked to online learning platforms to plug gaps left by stilted Zoom classes. The market value of Chegg, a provider of online tutoring, jumped from $5bn at the start of 2020 to $12bn a year later. Byju’s, an Indian peer, soared to a private valuation of $22bn in March 2022 as it snapped up other providers across the world. Global venture-capital investment in education-related startups jumped from $7bn in 2019 to $20bn in 2021, according to Crunchbase, a data provider.Then, once covid was brought to heel, classes resumed much as before. By the end of 2022 Chegg’s market value had slumped back to $3bn. Early last year investment firms including BlackRock and Prosus started marking down the value of their stakes in Byju’s as its losses mounted. “In hindsight we grew a bit too big a bit too fast,” admits Divya Gokulnath, the company’s co-founder.If the pandemic couldn’t overcome the education sector’s resistance to digital disruption, can artificial intelligence? ChatGPT-like generative AI, which can converse cleverly on a wide variety of subjects, certainly looks the part. So much so that educationalists began to panic that students would use it to cheat on essays and homework. In January 2023 New York City banned ChatGPT from public schools. Increasingly, however, it is generating excitement as a means to provide personalised tutoring to students and speed up tedious tasks such as marking. By May New York had let the bot back into classrooms.Learners, for their part, are embracing the technology. Two-fifths of undergraduates surveyed last year by Chegg reported using an AI chatbot to help them with their studies, with half of those using it daily. Indeed, the technology’s popularity has raised awkward questions for companies like Chegg, whose share price plunged last May after Dan Rosensweig, its chief executive, told investors it was losing customers to ChatGPT. Yet there are good reasons to believe that education specialists who harness AI will eventually prevail over generalists such as OpenAI, the maker of ChatGPT, and other tech firms eyeing the education business.For one, AI chatbots have a bad habit of spouting nonsense, an unhelpful trait in an educational context. “Students want content from trusted providers,” argues Kate Edwards, chief pedagogist at Pearson, a textbook publisher. The company has not allowed ChatGPT and other AIs to ingest its material, but has instead used the content to train its own models, which it is embedding into its suite of learning apps. Rivals including McGraw Hill are taking a similar approach. Chegg has likewise developed its own AI bot that it has trained on its ample dataset of questions and answers.What is more, as Chegg’s Mr Rosensweig argues, teaching is not merely about giving students an answer, but about presenting it in a way that helps them learn. Understanding pedagogy thus gives education specialists an edge. Pearson has designed its AI tools to engage students by breaking complex topics down, testing their understanding and providing quick feedback, says Ms Edwards. Byju’s is incorporating “forgetting curves” for students into the design of its AI tutoring tools, refreshing their memories at personalised intervals. Chatbots must also be tailored to different age groups, to avoid either bamboozling or infantilising students.Specialists that have already forged relationships with risk-averse educational institutions will have the added advantage of being able to embed AI into otherwise familiar products. Anthology, a maker of education software, has incorporated generative-AI features into its Blackboard Learn program to help teachers speedily create course outlines, rubrics and tests. Established suppliers are also better placed to instruct teachers on how to make use of AI’s capabilities.AI for effortBringing AI to education will not be easy. Although teachers have endured a covid-induced crash course in education technology, many are still behind the learning curve. Less than a fifth of British educators surveyed by Pearson last year reported receiving training on digital learning tools. Tight budgets at many institutions will make selling new technology an uphill battle. AI sceptics will have to be won over, and new AI-powered tools may be needed to catch AI-powered cheating. Thorny questions will inevitably arise as to what all this means for the jobs of teachers: their attention may need to shift towards motivating students and instructing them on how to best work with AI tools. “We owe the industry answers on how to harness this technology,” declares Bruce Dahlgren, boss of Anthology.If those answers can be provided, it is not just companies like Mr Dahlgren’s that stand to benefit. An influential paper from 1984 by Benjamin Bloom, an educational psychologist, found that one-to-one tutoring both improved the average academic performance of students and reduced the variance between them. AI could at last make individual tutors viable for the many. With the learning of students, especially those from poorer households, set back by the upheaval of the pandemic, such a development would certainly deserve top marks. ■Read more from Schumpeter, our columnist on global business:Meet the shrewdest operators in today’s oil markets (Jan 3rd)Can anyone bar Europe do luxury? (Dec 20th)Boneheaded anti-immigration politicians are throttling globalisation (Dec 14th)Also: If you want to write directly to Schumpeter, email him at [email protected]. And here is an explanation of how the Schumpeter column got its name.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Among the more sombre gifts brought by the Enlightenment was the realisation that humans might one day become extinct. The astronomical revolution of the 17th century had shown that the solar system both operated according to the highest principles of reason and contained comets which might conceivably hit the Earth. The geological record, as interpreted by the Comte de Buffon, showed massive extinctions in which species vanished for ever. That set the scene for Charles Darwin to recognise such extinctions as the motor of evolution, and thus as both the force which had fashioned humans and, by implication, their possible destiny. The nascent science of thermodynamics added a cosmic dimension to the certainty of an ending; Sun, Earth and the whole shebang would eventually run down into a lifeless “heat death”.The 20th century added the idea that extinction might not come about naturally, but through artifice. The spur for this was the discovery, and later exploitation, of the power locked up in atomic nuclei. Celebrated by some of its discoverers as a way of indefinitely deferring heat death, nuclear energy was soon developed into a far more proximate danger. And the tangible threat of imminent catastrophe which it posed rubbed off on other technologies.None was more tainted than the computer. It may have been guilt by association: the computer played a vital role in the development of the nuclear arsenal. It may have been foreordained. The Enlightenment belief in rationality as humankind’s highest achievement and Darwin’s theory of evolution made the promise of superhuman rationality the possibility of evolutionary progress at humankind’s expense.Artificial intelligence has come to loom large in the thought of the small but fascinating, and much written about, coterie of academics which has devoted itself to the consideration of existential risk over the past couple of decades. Indeed, it often appeared to be at the core of their concerns. A world which contained entities which think better and act quicker than humans and their institutions, and which had interests that were not aligned with those of humankind, would be a dangerous place.It became common for people within and around the field to say that there was a “non-zero” chance of the development of superhuman AIs leading to human extinction. The remarkable boom in the capabilities of large language models (LLMs), “foundational” models and related forms of “generative” AI has propelled these discussions of existential risk into the public imagination and the inboxes of ministers.A technology need not be world-ending to be world-changingAs the special Science section in this issue makes clear, the field’s progress is precipitate and its promise immense. That brings clear and present dangers which need addressing. But in the specific context of GPT-4, the LLM du jour, and its generative ilk, talk of existential risks seems rather absurd. They produce prose, poetry and code; they generate images, sound and video; they make predictions based on patterns. It is easy to see that those capabilities bring with them a huge capacity for mischief. It is hard to imagine them underpinning “the power to control civilisation”, or to “replace us”, as hyperbolic critics warn.Love songBut the lack of any “Minds that are to our minds as ours are to those of the beasts that perish, intellects vast and cool and unsympathetic [drawing] their plans against us”, to quote H.G. Wells, does not mean that the scale of the changes that AI may bring with it can be ignored or should be minimised. There is much more to life than the avoidance of extinction. A technology need not be world-ending to be world-changing.The transition into a world filled with computer programs capable of human levels of conversation and language comprehension and superhuman powers of data assimilation and pattern recognition has just begun. The coming of ubiquitous pseudocognition along these lines could be a turning point in history even if the current pace of AI progress slackens (which it might) or fundamental developments have been tapped out (which feels unlikely). It can be expected to have implications not just for how people earn their livings and organise their lives, but also for how they think about their humanity. For a sense of what may be on the way, consider three possible analogues, or precursors: the browser, the printing press and practice of psychoanalysis. One changed computers and the economy, one changed how people gained access and related to knowledge, and one changed how people understood themselves.The humble web browser, introduced in the early 1990s as a way to share files across networks, changed the ways in which computers are used, the way in which the computer industry works and the way information is organised. Combined with the ability to link computers into networks, the browser became a window through which first files and then applications could be accessed wherever they might be located. The interface through which a user interacted with an application was separated from the application itself.The power of the browser was immediately obvious. Fights over how hard users could be pushed towards a particular browser became a matter of high commercial drama. Almost any business with a web address could get funding, no matter what absurdity it promised. When boom turned to bust at the turn of the century there was a predictable backlash. But the fundamental separation of interface and application continued. Amazon, Meta (née Facebook) and Alphabet (née Google) rose to giddy heights by making the browser a conduit for goods, information and human connections. Who made the browsers became incidental; their role as a platform became fundamental.Read more of our recent coverage of AI:• How to worry wisely about artificial intelligence• Large, creative AI models will transform lives and labour markets• How generative models could go wrong• Large language models’ ability to generate text also lets them plan and reasonThe months since the release of OpenAI’s ChatGPT, a conversational interface now powered by GPT-4, have seen an entrepreneurial explosion that makes the dotcom boom look sedate. For users, apps based on LLMs and similar software can be ludicrously easy to use; type a prompt and see a result. For developers it is not that much harder. “You can just open your laptop and write a few lines of code that interact with the model,” explains Ben Tossell, a British entrepreneur who publishes a newsletter about AI services.And the LLMs are increasingly capable of helping with that coding, too. Having been “trained” not just on reams of text, but lots of code, they contain the building blocks of many possible programs; that lets them act as “co-pilots” for coders. Programmers on GitHub, an open-source coding site, are now using a GPT-4-based co-pilot to produce nearly half their code.There is no reason why this ability should not eventually allow LLMs to put code together on the fly, explains Kevin Scott, Microsoft’s chief technology officer. The capacity to translate from one language to another includes, in principle and increasingly in practice, the ability to translate from language to code. A prompt written in English can in principle spur the production of a program that fulfils its requirements. Where browsers detached the user interface from the software application, LLMs are likely to dissolve both categories. This could mark a fundamental shift in both the way people use computers and the business models within which they do so.Every day I write the bookCode-as-a-service sounds like a game-changing plus. A similarly creative approach to accounts of the world is a minus. While browsers mainly provided a window on content and code produced by humans, LLMs generate their content themselves. When doing so they “hallucinate” (or as some prefer “confabulate”) in various ways. Some hallucinations are simply nonsense. Some, such as the incorporation of fictitious misdeeds to biographical sketches of living people, are both plausible and harmful. The hallucinations can be generated by contradictions in training sets and by LLMs being designed to produce coherence rather than truth. They create things which look like things in their training sets; they have no sense of a world beyond the texts and images on which they are trained.In many applications a tendency to spout plausible lies is a bug. For some it may prove a feature. Deep fakes and fabricated videos which traduce politicians are only the beginning. Expect the models to be used to set up malicious influence networks on demand, complete with fake websites, Twitter bots, Facebook pages, TikTok feeds and much more. The supply of disinformation, Renée DiResta of the Stanford Internet Observatory has warned, “will soon be infinite”.image: Daniel ZenderThis threat to the very possibility of public debate may not be an existential one; but it is deeply troubling. It brings to mind the “Library of Babel”, a short story by Jorge Luis Borges. The library contains all the books that have ever been written, but also all the books which were never written, books that are wrong, books that are nonsense. Everything that matters is there, but it cannot be found because of everything else; the librarians are driven to madness and despair.This fantasy has an obvious technological substrate. It takes the printing press’s ability to recombine a fixed set of symbols in an unlimited number of ways to its ultimate limit. And that provides another way of thinking about LLMs.Dreams never endThe degree to which the modern world is unimaginable without printing makes any guidance its history might provide for speculation about LLMs at best partial, at worst misleading. Johannes Gutenberg’s development of movable type has been awarded responsibility, at some time or other, for almost every facet of life that grew up in the centuries which followed. It changed relations between God and man, man and woman, past and present. It allowed the mass distribution of opinions, the systematisation of bureaucracy, the accumulation of knowledge. It brought into being the notion of intellectual property and the possibility of its piracy. But that very breadth makes comparison almost unavoidable. As Bradford DeLong, an economic historian at the University of California, Berkeley puts it, “It’s the one real thing we have in which the price of creating information falls by an order of magnitude.”Printed books made it possible for scholars to roam larger fields of knowledge than had ever before been possible. In that there is an obvious analogy for LLMs, which trained on a given corpus of knowledge can derive all manner of things from it. But there was more to the acquisition of books than mere knowledge.Just over a century after Gutenberg’s press began its clattering Michel de Montaigne, a French aristocrat, had been able to amass a personal library of some 1,500 books—something unimaginable for an individual of any earlier European generation. The library gave him more than knowledge. It gave him friends. “When I am attacked by gloomy thoughts,” he wrote, “nothing helps me so much as running to my books. They quickly absorb me and banish the clouds from my mind.”And the idea of the book gave him a way of being himself no one had previously explored: to put himself between covers. “Reader,” he warned in the preface to his Essays, “I myself am the matter of my book.” The mass production of books allowed them to become peculiarly personal; it was possible to write a book about nothing more, or less, than yourself, and the person that your reading of other books had made you. Books produced authors.As a way of presenting knowledge, LLMs promise to take both the practical and personal side of books further, in some cases abolishing them altogether. An obvious application of the technology is to turn bodies of knowledge into subject matter for chatbots. Rather than reading a corpus of text, you will question an entity trained on it and get responses based on what the text says. Why turn pages when you can interrogate a work as a whole?Everyone and everything now seems to be pursuing such fine-tuned models as ways of providing access to knowledge. Bloomberg, a media company, is working on BloombergGPT, a model for financial information. There are early versions of a QuranGPT and a BibleGPT; can a puffer-jacketed PontiffGPT be far behind? Meanwhile several startups are offering services that turn all the documents on a user’s hard disk, or in their bit of the cloud, into a resource for conversational consultation. Many early adopters are already using chatbots as sounding boards. “It’s like a knowledgeable colleague you can always talk to,” explains Jack Clark of Anthropic, an LLM-making startup.If some chatbots become their user’s inner voice, that voice will persist after deathIt is easy to imagine such intermediaries having what would seem like personalities—not just generic ones, such as “avuncular tutor”, but specific ones which grow with time. They might come to be like their users: an externalised version of their inner voice. Or they might be like any other person whose online output is sufficient for a model to train on (intellectual-property concerns permitting). Researchers at the Australian Institute for Machine Learning have built an early version of such an assistant for Laurie Anderson, a composer and musician. It is trained in part on her work, and in part on that of her late husband Lou Reed. Without youMs Anderson says she does not consider using the system as a way of collaborating with her dead partner. Others might succumb more readily to such an illusion. If some chatbots do become, to some extent, their user’s inner voice, then that voice will persist after death, should others wish to converse with it. That some people will leave chatbots of themselves behind when they die seems all but certain.Such applications and implications call to mind Sigmund Freud’s classic essay on the Unheimliche, or uncanny. Freud takes as his starting point the idea that uncanniness stems from “doubts [as to] whether an apparently animate being is really alive; or conversely, whether a lifeless object might not be in fact animate”. They are the sort of doubts that those thinking about LLMs are hard put to avoid.Though AI researchers can explain the mechanics of their creations, they are persistently unable to say what actually happens within them. “There’s no ‘ultimate theoretical reason’ why anything like this should work,” Stephen Wolfram, a computer scientist and the creator of Wolfram Alpha, a mathematical search engine, recently concluded in a remarkable (and lengthy) blog post trying to explain the models’ inner workings.This raises two linked but mutually exclusive concerns: that AI’s have some sort of internal working which scientists cannot yet perceive; or that it is possible to pass as human in the social world without any sort of inner understanding. “These models are just representations of the distributions of words in texts that can be used to produce more words,” says Emily Bender, a professor at the University of Washington in Seattle. She is one of the authors of “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” a critique of LLM triumphalism. The models, she argues, have no real understanding. With no experience of real life or human communication they offer nothing more than the ability to parrot things they have heard in training, an ability which huge amounts of number crunching makes frequently appropriate and sometimes surprising, but which is nothing like thought. It is a view which is often pronounced in those who have come into the field through linguistics, as Dr Bender has.For some in the LLM-building trade things are not that simple. Their models are hard to dismiss as “mere babblers”, in the words of Blaise Agüera y Arcas, the leader of a group at Alphabet which works on AI-powered products. He thinks the models have attributes which cannot really be distinguished from an ability to know what things actually mean. It can be seen, he suggests, in their ability reliably to choose the right meaning when translating phrases which are grammatically ambiguous, or to explain jokes.If Dr Bender is right, then it can be argued that a broad range of behaviour that humans have come to think of as essentially human is not necessarily so. Uncanny “doubts [as to] whether an apparently animate being is really alive” are fully justified.To accept that human-seeming LLMs are calculation, statistics and nothing more could influence how people think about themselves. Freud portrayed himself as continuing the trend begun by Copernicus—who removed humans from the centre of the universe—and Darwin—who removed them from a special and God-given status among the animals. Psychology’s contribution, as Freud saw it, lay in “endeavouring to prove to the ‘ego’ of each one of us that he is not even master in his own house”. LLMs could be argued to take the idea further still. At least one wing of Freud’s house becomes an unoccupied “smart home”; the lights go on and off automatically, the smart thermostat opens windows and lowers blinds, the roomba roombas around. No master needed at all.image: Daniel ZenderUncanny as that may all be, though, it would be wrong to think that many people will take this latest decentring to heart. As far as everyday life is concerned, humankind has proved pretty resilient to Copernicus, Darwin and Freud. People still believe in gods and souls and specialness with little obvious concern for countervailing science. They could well adapt quite easily to the pseudocognitive world, at least as far as philosophical qualms are concerned.You do not have to buy Freud’s explanation of the unsettling effect of the uncanny in terms of the effort the mind expends on repressing childish animism to think that not worrying and going with the animistic flow will make a world populated with communicative pseudo-people a surprisingly comfortable one. People may simultaneously recognise that something is not alive and treat it as if it were. Some will take this too far, forming problematic attachments that Freud would have dubbed fetishistic. But only a few sensitive souls will find themselves left behind staring into an existential—but personal—abyss opened up by the possibility that their seeming thought is all for naught.New gold dreamWhat if Mr Agüera y Arcas is right, though, and that which science deems lifeless is, in some cryptic, partial and emergent way, effectively animate? Then it will be time to do for AI some of what Freud thought he was doing for humans. Having realised that the conscious mind was not the whole show, Freud looked elsewhere for sources of desire that for good or ill drove behaviour. Very few people now subscribe to the specific Freudian explanations of human behaviour which followed. But the idea that there are reasons why people do things of which they are not conscious is part of the world’s mental furniture. The unconscious is probably not a great model for whatever it is that provides LLMs with an apparent sense of meaning or an approximation of agency. But the sense that there might be something below the AI surface which needs understanding may prove powerful.There might be something below the AI surface which needs understandingDr Bender and those who agree with her may take issue with such notions. But they might find that they lead to useful actions in the field of “AI ethics”. Winkling out non-conscious biases acquired in the pre-verbal infancy of training; dealing with the contradictions behind hallucinations; regularising rogue desires: ideas from psychotherapy might be seen as helpful analogies for dealing with the pseudocognitive AI transition even by those who reject all notion of an AI mind. A concentration on the relationship between parents, or programmers, and their children could be welcome, too. What is it to bring up an AI well? What sort of upbringing should be forbidden? To what extent should the creators of AIs be held responsible for the harms done by their creation?And human desires may need some inspection, too. Why are so many people eager for the sort of intimacy an LLM might provide? Why do many influential humans seem to think that, because evolution shows species can go extinct, theirs is quite likely to do so at its own hand, or that of its successor? And where is the determination to turn a  superhuman rationality into something which does not merely stir up the economy, but changes history for the better?■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Among the startling, hopeful developments that have greeted the advent of generative artificial intelligence (ai) has been an outbreak of bipartisan focus, curiosity and deliberation in Washington, DC. Legislators and regulators are trying hard to come to grips with the protean technology. Chuck Schumer, the Senate majority leader, has been holding senators-only briefings with experts to educate his chamber. In late June he called for “a new and unique approach” to writing legislation about AI, saying it was “unlike anything Congress has dealt with before”.“It’s not like labour or health care or defence where Congress has had a long history we can work off of,” he said. “In many ways, we’re starting from scratch.” He has set up a steering group of two Republicans and two Democrats, including himself, and plans this autumn to supplement the normal committee process, or posturing, with ”AI Insight Forums”, to include the industry’s leaders and its critics, to do “years of work in a matter of months”.It is understandable that wise guys are making fun of this. Given Congress’s reputation for speed and technological literacy (the Senate was in session for all of 14 days in June, and Mr Schumer uses a flip phone), the jokes write themselves, almost. ChatGPT’s first, unfunny stab reflected the cynicism any sentient being might feel: “Why did the AI refuse to testify before Congress? Because it didn’t want to be caught in a loop of lawmakers asking the same question over and over.”Yet Americans should bask in this rare season, while it lasts, of good-faith searching and head-scratching. Not only are the regulators interrogating themselves about where and how they should regulate, the industry itself is asking to be regulated, up to a point. Even if it is all doomed to end in partisan impasse and recrimination, now is the time to ask: how could Washington get this right?One danger is that lawmakers could wind up bickering over a problem they can address only at the edges. Representatives of both parties regret not more aggressively regulating the internet in general and social media in particular. Being politicians, they are alert to how political actors are already using AI tools to generate political messages, including fabricated images. For their part journalists, being journalists, are obsessed with AI’s potential to create and spread lies.These are serious concerns. But because of America’s speech protections, congressional action would probably be less constructive than voluntary standards by campaigns, technical approaches by the private sector to authenticate images and scepticism from a jaded electorate. The more Congress focuses on the flow of information, the more it will be riven by fights over whether a particular chatbot is inclined to disparage one party or another. “If we get drawn into refighting the social-media wars, we risk not realising the promise of machine learning,” warns Kent Walker, the president of global affairs at Google and Alphabet. “Social media isn’t going to cure cancer, but AI has the potential to, and it would be a shame if that promise were politicised. It would be a shame to hold back progress in nuclear fusion because we can’t agree about Twitter.”A better starting point would be to recognise that the government is already regulating AI. Years ago it set the nerve ends tingling of the federal bureaucracy, another complex, amorphous entity that somehow translates countless inputs into answers that cannot always be explained. Plenty of laws already cover the use of AI. And questions about AI’s use in providing health care, hiring people, driving cars or investing are being asked and answered, albeit too slowly for some industries. The National Institute of Standards and Technology, part of the Commerce Department, has drawn up voluntary standards by which AI might be governed.Chris Meserole of the Brookings Institution, a Washington-based think-tank, says Congress could exploit this “incredibly rare moment” of bipartisan seriousness by mandating that each regulatory agency develop a plan to adapt the NIST standards to its sector. It could also mandate disclosure about the use of AI in products, and require transparency into algorithms used in high-risk systems. “If an autonomous vehicle using AI has an accident, we need to understand what went wrong,” he says.To walk humbly with Chuck SchumerCongress might also consider export controls for AI models and chips it deems too powerful. It could look at creating an agency to regulate big tech (disclosure: Lexington’s brother, Senator Michael Bennet of Colorado, has proposed legislation to do so) or whether some other means might bolster the agencies’ AI expertise. It could also help universities pay for the computing power they need to conduct AI research in the public interest.The good news is that these are the sorts of measures that Congress and the Biden administration are weighing. The most striking word in Mr Schumer’s recent speech was “humility”, as in, “We must exercise humility as we proceed.” Like many of his colleagues, Mr Schumer is not celebrated for this quality. Nor are the technology companies, but, in the face of AI, they are embracing it, too. Mr Walker notes that AI researchers have spoken of the “AI half-pipe of heaven and hell”, meaning it tends to be treated as either wonderful or terrible. “There’s very little market for, ‘Well, AI has a lot of important pros and cons, and we have to incrementally navigate’,” he says. “But that’s probably where the wisdom is.”China has forbidden AI-generated images that impersonate people without their consent, and the European Union has proposed sweeping AI rules (which, like the EU’s data rules, may reverberate globally). But America’s reactive, incremental approach to making regulation up as its industries develop has kept it in the lead since the Industrial Revolution, and seems particularly suited to such a rapidly evolving, disruptive technology. Maybe AI and America’s lawmakers, in the end, can help each other grow up. ■Read more from Lexington, our columnist on American politics:Why the multiverse is eating popular culture (Jun 22nd)North Carolina may be the hottest political battleground of 2024 (Jun 15th)Nikki Haley, like other long shots, sees a path to victory (Jun 1st)Also: How the Lexington column got its name
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.TWENTY-FIVE years ago your correspondent hired a cellphone in Congo. Each day, it cost what a typical local made in several months. The handset was as heavy as a half-brick and only somewhat more useful. Practically no one else in Congo had one, bar cabinet ministers and tycoons, so there were not many people to call. In those days, mobile phones had made no detectable difference to most people’s lives in the world’s poorest countries.Today, many farmers in Congo have phones: the number of connections has grown 5,000-fold as the population has doubled. Mobile devices have transformed lives throughout the developing world, especially as more and more of them are hooked up to the internet (see chart 1). The 4bn people who live in low or lower-middle income countries have vastly more access to information, chat daily to far-off friends and use their phones like bank cards even when they don’t have bank accounts.image: The EconomistCould artificial intelligence (ai) bring similarly dramatic changes? There are three main reasons for optimism. First, the technology is improving fast. Second, it has the potential to spread fast, too. As usually happens with new technologies, rich countries will benefit first. But if the high cost of training AI models falls, the expense of providing the technology to the poor could be minimal. They will not need a new device, just the smartphones that many of them already own.The third reason is that developing countries have gaping shortages of skilled workers: there are nowhere near enough teachers, doctors, engineers or managers. AI could ease this shortfall, not by replacing existing workers, but by helping them become more productive, argues Daniel Björkegren of Columbia University, which in turn could raise the general level of health and education. Although AI may also eliminate some jobs, the IMF predicts that labour markets in poorer countries will be less disrupted than those in rich ones. Another tantalising possibility is that AI could help provide fine-grained, up-to-date data about poor places, and so assist in all manner of development work. image: The EconomistStart with education. A typical sub-Saharan pupil spends six years in school but retains only three years’ worth of learning, Wolfgang Lutz of the Wittgenstein Centre in Vienna estimated in 2015. A typical Japanese student spends 14 years in classes and absorbs 16 years’ worth of education. Using a different methodology, the World Bank also finds that education is spectacularly worse in poor countries than in rich ones (see chart 2).Tonee Ndungu, an entrepreneur in Kenya, thinks AI could help bridge this gap. He has developed two apps that he hopes to launch this year. One, called Somanasi (“Learn with me”), is for children. It allows pupils to ask a talking chatbot questions related to the Kenyan school curriculum. The Economist asked, “How do I work out a percentage from a fraction?” The chatbot offered a step-by-step worked example.Machine learningA chatbot can give undivided attention to each child, at any time of day, and never gets tired (so long as your phone is charged). It can also be adapted to local cultures. “I never saw an apple till I was 30,” says Mr Ndungu. “So we say ‘A is for animal.’” The service can be tailored to different learning styles, too. It might illustrate division by telling children to break a pencil in half and then again. Depending on how different pupils respond, the AI can figure out whether this approach works, and fine-tune the way it interacts with them. Some kids want more numbers; some like stories. The chatbot adapts.It cannot yet mark homework. But Mr Ndungu’s firm, Kytabu, offers an app for teachers, too, called Hodari (“Brave”). It lightens their workload by crafting step-by-step lesson plans. It helps track what pupils understand, by getting each one to answer questions on a smartphone. (One phone per classroom is enough, he says.)As far as The Economist could tell from playing with them in a café with good Wi-Fi, the two apps work well. But the proof will come—and bugs will be fixed—when more people use them in classrooms and homes. They will be given away to begin with; Mr Ndungu hopes eventually to charge for add-ons. The more children are enrolled, the cheaper it will be to provide the service. If half a million were to join, Mr Ndungu predicts the cost per child would fall from $3.50 a month (not including the phone) to about 15 cents.Chat GPAMany entrepreneurs are pursuing similar projects, often using open-source models developed in rich countries, sometimes with help from charities like the Gates Foundation. The cost of getting AI to learn new languages appears low. It is already being used to write children’s books in tongues previously too obscure for commercial publishers to bother with.The need is glaring. Developing countries have too few teachers, many of whom have not mastered the curriculum. A study in 2015 (using data going back to 2007) found that four-fifths of grade six maths teachers in South Africa did not understand the concepts they were supposed to teach. Nearly 90% of ten-year-olds in sub-Saharan Africa cannot read a simple text.Dr Björkegren points to recent studies suggesting that big gains are possible even with basic tech. One analysed an approach under which schools hire modestly qualified teachers and give them detailed “scripts” for lessons, delivered via tablet computers. Michael Kremer, a Nobel-prize-winning economist, and others studied 10,000 pupils taught this way in Kenya, at schools run by Bridge International Academies, a chain of cheap private schools. They found that after two years on average Bridge students had mastered nearly an extra year’s worth of the curriculum, compared with pupils enrolled in normal schools. Another study in India found that personalised computerised instruction was especially helpful for pupils who were far behind. Using AI in health care is riskier. If an educational chatbot misfires, a pupil might flunk a test; if a medical one hallucinates, a patient could die. Nonetheless, optimists see great potential. Some AI-powered medical kit is already widely used in rich countries and is starting to be adopted in poorer ones. Examples include handheld ultrasound devices that can interpret scans, and a system for spotting tuberculosis on chest X-rays. Accurate ai translation could also make it easier for patients and health-care workers in the global south to tap into the world’s medical knowledge. Even imperfect AI tools may improve health-care systems in the developing world, whose failures cause more than 8m deaths a year, by one estimate. In a study of nine poor and middle-income countries by Todd Lewis of Harvard and others, 2,000 recently graduated primary health-care workers were observed dealing with visitors to clinics. They performed the correct, essential tasks required by clinical guidelines only about half the time.For people in remote areas, even a substandard clinic may be too far away or too costly. Many rely on traditional medicine, much of which is useless or harmful. South African folk healers sometimes cut patients to rub in toxic powder suffused with mercury, for example. AI tools need not be infallible to be better than that.A team at the University of São Paulo is training an AI to answer health-related questions. The aim is to give a tool to primary-health workers in Brazil, who sometimes have little training. They are using a database of clinical guidelines from Brazil’s health ministry, rather than the whole internet, which is full of voodoo health tips. Before the ai can be widely deployed, it must be tested, tweaked and tested again. Currently, when you ask precise, technical questions, such as “Is Ivermectin effective in preventing covid-19?”, its success rate is “so, so high”, says Francisco Barbosa, one of the team. The trouble comes when you ask it something vague, as humans often do. If you say, “I’ve fallen in the street. How can I get to a pharmacy?”, then the AI, which may not know where you are, might give terrible advice.The AI will have to improve and its users will have to learn how to get the best out of it, says Mr Barbosa. He is confident that this will happen: “It’s a cliché [to say it], but it’s changing everything.” Equipping a new hospital costs millions of dollars. Training a new doctor takes years. If AI helps cheap primary-care workers treat more patients successfully, so that they do not need to go to a hospital, Brazil can keep its population healthier without spending more.Brazil has one doctor for every 467 people; Kenya has one for every 4,425. AI could help, says Daphne Ngunjiri of Access Afya, a Kenyan firm that runs mDaktari, a virtual health-care platform with 29,000 clients. For a small monthly fee, they can ask for advice when they feel unwell.Bard to handleFor a test group of 380 users, mDaktari has added an AI-powered chatbot to the system. It records their queries, prompts them for more information and presents that information, along with a suggested response, to a clinician, often a nurse. The clinician reads it and, if the advice is sound, approves it and sends it back to the customer, perhaps referring her to a pharmacy or a clinic. Thus, a human is in the loop, to guard against errors, but the AI does the time-consuming gathering of information about symptoms, enabling the nurse to deal with more patients. If necessary, the nurse can call the patient. For embarrassing ailments such as sexually transmitted diseases, some patients prefer talking to a chatbot. It never judges them.Virginia, a client from a Nairobi slum whose family subsists on casual labour and backyard vegetables, says mDaktari is simple and helpful. One time she felt sick, consulted the app, and was steered to drugs that cleared up what turned out to be a urinary-tract infection. “I can even contact a [nurse] through my phone and get [an] answer,” she says.Several firms are testing AI-enhanced medical devices to see how well they work in poor areas. Philips, a Dutch firm, has a pilot programme in Kenya for a handheld ultrasound with an AI add-on that can interpret the images it spits out. This helps solve a common problem: lots of pregnant mothers and not enough people with the  expertise to read scans.Sadiki Jira is a midwife at a rural health facility in Kenya that serves nearly 30,000 people but has no doctor. He recalls a pregnant patient a couple of years ago whose baby had died in the womb. She had not realised for several weeks and had only sought help when she started haemorrhaging. Mr Jira referred her to a hospital, but it was too late: she died.Mr Jira now uses an AI-powered scanner. Any midwife can, with minimal training, swipe a Philips device over a pregnant woman’s stomach. The AI reveals such vital information as the fetus’s gestational age, whether it is in the breech position and whether there is adequate amniotic fluid. “It’s easy to use,” says Mr Jira.Philips is planning to offer the device and AI together for $1 or $2 a day in poor countries. The biggest obstacles to its rollout are regulatory, says Matthijs Groot Wassink of Philips. Will governments allow midwives to handle a process that previously required someone more qualified? What will happen in places like India, where regulations are especially tight for fear that people will use ultrasound to identify and abort baby girls?Poorer places collect poorer data. Forty-nine countries have gone more than 15 years since their most recent agricultural census; 13 have not conducted a population census in that period. Official numbers, when they exist, tend to flatter the government. For example, a study compared official estimates of how much maize was being grown on small farms in Ethiopia, Malawi and Nigeria with the results of painstaking (but rare) household surveys. The official numbers were much rosier.Satellite imagery and machine-learning could improve the quality and timeliness of data in developing countries, argue Marshall Burke of Stanford University and his co-authors in a recent paper in Science. Roughly 2.5bn people live in households that depend on tiny plots of land. Until recently the output of such farms was hard to measure: satellite pictures were not sharp enough and data drawn from them were too hard to interpret. But by setting AI to work on new high-resolution images of vegetation, Dr Burke and David Lobell, also of Stanford, were able to measure crop yields as accurately as surveys do, but faster and more cheaply. This could allow frequent, detailed analysis of farming practices. How much fertiliser is needed on this hillside? Which seeds work best in that valley? Such knowledge could transform rural livelihoods, the authors predict.So could better weather forecasts. Atmo, an American firm, says its AI-powered weather forecasts are as much as 100 times more detailed and twice as accurate as a conventional meteorological bulletin, because the AI processes data so much faster. It is also cheap. “A dirty secret of meteorology…is that there are vast inequalities,” says Alex Levy, Atmo’s boss. Forecasts are less detailed or reliable in poor countries. ”The places [with] the most extreme weather also have the worst forecasts, [so] they are most likely to be surprised and unable to prepare adequately.” Atmo’s service is being used in Uganda and may soon be deployed in the Philippines.Population counts in poor countries are rare, because they are costly, and prone to manipulation. In Nigeria the money each state gets from the central government is tied to its population. This gives states an incentive to fiddle. In 1991, on a census form with space for up to nine members per household, some states reported exactly nine members in every one. When the results of the census of 2006 were published, Bola Tinubu, the governor of Lagos, angrily claimed that its population was double the official tally. Nigeria has not held another census since. A new president—Mr Tinubu, as it happens—promises one in 2024.image: Cristina SpanoAI can generate more frequent, detailed estimates of how many people live where—and how well-off they are. Lights at night are often used as a proxy for economic buzz. Neal Jean of Stanford and others took day and night images of slums in Africa and trained a convolutional neural network (a form of machine learning) to predict from daytime images how much light there would be at night. In other words,  AI learnt to recognise the kinds of buildings, infrastructure and other markers that tend to go with economic activity. It was able to predict 55-75% of the variation in assets between households.Such information could help governments and charities assess better the effects of efforts to help the needy; it could also help companies understand markets. Researchers are avidly trying out such techniques, but governments have been slow to adopt them, laments Dr Burke. He attributes this in part to “the potential benefits to some policymakers of not having certain outcomes be measured”.AI could also help people deal with the red tape that throttles productivity in so many poor countries. Registering a property takes 200 times longer in Haiti than in wealthy Qatar, according to the World Bank. Suppose an AI, which is immune to boredom, were able to fill in the forms accurately enough to spare humans the chore? In September India launched a chatbot that lets illiterate farmers pose oral queries about applications for financial aid. Some 500,000 tried it on the first day.Deep minefieldAI poses risks to poor countries, too. They are generally less democratic than rich ones, so many governments will adopt AI surveillance tools, pioneered by China, to monitor and control their people. They are less stable, so incendiary deepfakes may be more likely to warp politics or spark violence. Underfunded and inexpert regulators may struggle to impose proper guardrails against potential abuses.And there are big obstacles to deploying AI in the developing world. Access to the internet will have to improve. Some countries will benefit faster than others. India has 790m mobile broadband users, plus a universal digital identity system and a super-cheap, real-time payments system, note Nandan Nilekani and Tanuj Bhojwani, two tech bosses, in Finance & Development. This, they argue, “puts it in a favourable position to be the world’s most extensive user of AI by the end of this decade”.Enormous uncertainty remains about how powerful the technology will eventually prove. But the potential upside is big enough to warrant a tremor of excitement. In the best-case scenario, AI could help make whole populations healthier, better educated and better informed. In time, that could make them a lot less poor. ■
TWO letters can add up to a lot of money. No area of technology is hotter than AI, or artificial intelligence. Venture-capital investment in AI in the first nine months of 2017 totalled $7.6bn, according to PitchBook, a data provider; that compares with full-year figures of $5.4bn in 2016. In the year to date there have been $21.3bn in AI-related M&A deals, around 26 times more than in 2015. In earnings calls public companies now mention AI far more often than “big data”.At the heart of the frenzy are some familiar names: the likes of Alphabet, Amazon, Apple, Facebook and Microsoft. A similar, though less transparent, battle is under way in China among firms like Alibaba and Baidu. Several have put AI at the centre of their strategies. All are enthusiastic acquirers of AI firms, often in order to snap up the people they employ. They see AI as a way to improve their existing services, from cloud computing to logistics, and to push into new areas, from autonomous cars to augmented reality (see article). Many observers fear that, by cementing and extending the power of a handful of giants, AI will hurt competition. That will depend on three open questions, involving one magic ingredient.AlphaGoneThe tech giants certainly have big advantages in the battle to develop AI. They have tonnes of data, oodles of computing power and boffins aplenty—especially in China, which expects to charge ahead. Imagine a future, some warn, in which you are transported everywhere in a Waymo autonomous car (owner: Alphabet, parent of Google), pay for everything with an Android phone (developer: Google), watch YouTube (owner: Google) to relax, and search the web using—you can guess. Markets with just a handful of firms can be fiercely competitive. A world in which the same few names duke it out in several industries could still be a good one for consumers. But if people rely on one firm’s services like this, and if AI enables that firm to predict their needs and customise its offering ever more precisely, it will be burdensome to switch to a rival.That future is still a long way off. AI programs remain narrowly focused. Moreover, the ability of the incumbents to perpetuate their advantages is made uncertain by three questions.The most important is whether AI will always depend on vast amounts of data. Machines today are usually trained on huge datasets, from which they can recognise useful patterns such as fraudulent financial transactions. If real-world data remain essential to AI, the tech superstars are in clover. They have vast amounts of the stuff, and are gaining more as they push into fresh areas such as health care.A competing vision of AI stresses simulations, in which machines teach themselves using synthetic data or in virtual environments. Early versions of a program developed to play Go, an Asian board game, by DeepMind, a unit of Alphabet, were trained using data from actual games; the latest was simply given the rules and started playing Go against itself. Within three days it had surpassed its predecessor, which had itself beaten the best player humanity could muster. If this approach is widely applicable, or if future AI systems can be trained using sparser amounts of data, the tech giants’ edge is blunted.But some applications will always require data. How much of the world’s stock of it the tech giants will end up controlling is the second question. They have clout in the consumer realm, and they keep pushing into new areas, from Amazon’s interest in medicine to Microsoft’s purchase of LinkedIn, a professional-networking site. But data in the corporate realm are harder to get at, and their value is increasingly well understood. Autonomous cars will be a good test. Alphabet’s Waymo has done more real-world testing of self-driving cars than any other firm: over 4m miles (6.5m kilometres) on public roads. But established carmakers, and startups like Tesla, can generate more data from their existing fleets; other firms, like Mobileye, a driverless-tech firm owned by Intel, are also in the race.The third question is how openly knowledge will be shared. The tech giants’ ability to recruit AI expertise from universities is helped by their willingness to publish research; Google and Facebook have opened software libraries to outside developers. But their incentives to share valuable data and algorithms are weak. Much will depend on whether regulations prise open their grip. Europe’s impending data-protection rules, for example, require firms to get explicit consent for how they use data and to make it easier for customers to transfer their information to other providers. China may try to help its firms by having negligible regulation.The battle in AI is fiercest among the tech giants. It is too early to know how good that will be for competition, but not to anticipate the magic ingredient that will determine the outcome: the importance, accessibility and openness of data.
This article is part of our Summer reads series. Visit the full collection for book lists, guest essays and more seasonal distractions.FEARS OF ARTIFICIAL INTELLIGENCE (AI) have haunted humanity since the very beginning of the computer age. Hitherto these fears focused on machines using physical means to kill, enslave or replace people. But over the past couple of years new AI tools have emerged that threaten the survival of human civilisation from an unexpected direction. AI has gained some remarkable abilities to manipulate and generate language, whether with words, sounds or images. AI has thereby hacked the operating system of our civilisation.Language is the stuff almost all human culture is made of. Human rights, for example, aren’t inscribed in our DNA. Rather, they are cultural artefacts we created by telling stories and writing laws. Gods aren’t physical realities. Rather, they are cultural artefacts we created by inventing myths and writing scriptures.Money, too, is a cultural artefact. Banknotes are just colourful pieces of paper, and at present more than 90% of money is not even banknotes—it is just digital information in computers. What gives money value is the stories that bankers, finance ministers and cryptocurrency gurus tell us about it. Sam Bankman-Fried, Elizabeth Holmes and Bernie Madoff were not particularly good at creating real value, but they were all extremely capable storytellers.What would happen once a non-human intelligence becomes better than the average human at telling stories, composing melodies, drawing images, and writing laws and scriptures? When people think about ChatGPT and other new AI tools, they are often drawn to examples like school children using AI to write their essays. What will happen to the school system when kids do that? But this kind of question misses the big picture. Forget about school essays. Think of the next American presidential race in 2024, and try to imagine the impact of AI tools that can be made to mass-produce political content, fake-news stories and scriptures for new cults.In recent years the QAnon cult has coalesced around anonymous online messages, known as “Q drops”. Followers collected, revered and interpreted these Q drops as a sacred text. While to the best of our knowledge all previous Q drops were composed by humans, and bots merely helped disseminate them, in future we might see the first cults in history whose revered texts were written by a non-human intelligence. Religions throughout history have claimed a non-human source for their holy books. Soon that might be a reality.On a more prosaic level, we might soon find ourselves conducting lengthy online discussions about abortion, climate change or the Russian invasion of Ukraine with entities that we think are humans—but are actually AI. The catch is that it is utterly pointless for us to spend time trying to change the declared opinions of an AI bot, while the AI could hone its messages so precisely that it stands a good chance of influencing us.Explore more Summer reads:The mystery of Morocco’s missing king. He befriended a kickboxer in 2018, and has rarely been seen since.Hollywood is losing the battle for China. Watch how domestic blockbusters are dominating the market.Russia’s economy can withstand a long war. But not a more intense one.The six novels chosen for our reviewers’ attention so far this year—and worthy of yours.Through its mastery of language, AI could even form intimate relationships with people, and use the power of intimacy to change our opinions and worldviews. Although there is no indication that AI has any consciousness or feelings of its own, to foster fake intimacy with humans it is enough if the AI can make them feel emotionally attached to it. In June 2022 Blake Lemoine, a Google engineer, publicly claimed that the AI chatbot LaMDA, on which he was working, had become sentient. The controversial claim cost him his job. The most interesting thing about this episode was not Mr Lemoine’s claim, which was probably false. Rather, it was his willingness to risk his lucrative job for the sake of the AI chatbot. If AI can influence people to risk their jobs for it, what else could it induce them to do?In a political battle for minds and hearts, intimacy is the most efficient weapon, and AI has just gained the ability to mass-produce intimate relationships with millions of people. We all know that over the past decade social media has become a battleground for controlling human attention. With the new generation of AI, the battlefront is shifting from attention to intimacy. What will happen to human society and human psychology as AI fights AI in a battle to fake intimate relationships with us, which can then be used to convince us to vote for particular politicians or buy particular products?Even without creating “fake intimacy”, the new AI tools would have an immense influence on our opinions and worldviews. People may come to use a single AI adviser as a one-stop, all-knowing oracle. No wonder Google is terrified. Why bother searching, when I can just ask the oracle? The news and advertising industries should also be terrified. Why read a newspaper when I can just ask the oracle to tell me the latest news? And what’s the purpose of advertisements, when I can just ask the oracle to tell me what to buy?And even these scenarios don’t really capture the big picture. What we are talking about is potentially the end of human history. Not the end of history, just the end of its human-dominated part. History is the interaction between biology and culture; between our biological needs and desires for things like food and sex, and our cultural creations like religions and laws. History is the process through which laws and religions shape food and sex.What will happen to the course of history when AI takes over culture, and begins producing stories, melodies, laws and religions? Previous tools like the printing press and radio helped spread the cultural ideas of humans, but they never created new cultural ideas of their own. AI is fundamentally different. AI can create completely new ideas, completely new culture.At first, AI will probably imitate the human prototypes that it was trained on in its infancy. But with each passing year, AI culture will boldly go where no human has gone before. For millennia human beings have lived inside the dreams of other humans. In the coming decades we might find ourselves living inside the dreams of an alien intelligence.Fear of AI has haunted humankind for only the past few decades. But for thousands of years humans have been haunted by a much deeper fear. We have always appreciated the power of stories and images to manipulate our minds and to create illusions. Consequently, since ancient times humans have feared being trapped in a world of illusions.In the 17th century René Descartes feared that perhaps a malicious demon was trapping him inside a world of illusions, creating everything he saw and heard. In ancient Greece Plato told the famous Allegory of the Cave, in which a group of people are chained inside a cave all their lives, facing a blank wall. A screen. On that screen they see projected various shadows. The prisoners mistake the illusions they see there for reality.In ancient India Buddhist and Hindu sages pointed out that all humans lived trapped inside Maya—the world of illusions. What we normally take to be reality is often just fictions in our own minds. People may wage entire wars, killing others and willing to be killed themselves, because of their belief in this or that illusion.The AI revolution is bringing us face to face with Descartes’ demon, with Plato’s cave, with the Maya. If we are not careful, we might be trapped behind a curtain of illusions, which we could not tear away—or even realise is there.Of course, the new power of AI could be used for good purposes as well. I won’t dwell on this, because the people who develop AI talk about it enough. The job of historians and philosophers like myself is to point out the dangers. But certainly, AI can help us in countless ways, from finding new cures for cancer to discovering solutions to the ecological crisis. The question we face is how to make sure the new AI tools are used for good rather than for ill. To do that, we first need to appreciate the true capabilities of these tools.Since 1945 we have known that nuclear technology could generate cheap energy for the benefit of humans—but could also physically destroy human civilisation. We therefore reshaped the entire international order to protect humanity, and to make sure nuclear technology was used primarily for good. We now have to grapple with a new weapon of mass destruction that can annihilate our mental and social world.We can still regulate the new AI tools, but we must act quickly. Whereas nukes cannot invent more powerful nukes, AI can make exponentially more powerful AI. The first crucial step is to demand rigorous safety checks before powerful AI tools are released into the public domain. Just as a pharmaceutical company cannot release new drugs before testing both their short-term and long-term side-effects, so tech companies shouldn’t release new AI tools before they are made safe. We need an equivalent of the Food and Drug Administration for new technology, and we need it yesterday.Won’t slowing down public deployments of AI cause democracies to lag behind more ruthless authoritarian regimes? Just the opposite. Unregulated AI deployments would create social chaos, which would benefit autocrats and ruin democracies. Democracy is a conversation, and conversations rely on language. When AI hacks language, it could destroy our ability to have meaningful conversations, thereby destroying democracy.We have just encountered an alien intelligence, here on Earth. We don’t know much about it, except that it might destroy our civilisation. We should put a halt to the irresponsible deployment of AI tools in the public sphere, and regulate AI before it regulates us. And the first regulation I would suggest is to make it mandatory for AI to disclose that it is an AI. If I am having a conversation with someone, and I cannot tell whether it is a human or an AI—that’s the end of democracy.This text has been generated by a human.Or has it?_______________Yuval Noah Harari is a historian, philosopher and author of “Sapiens”, “Homo Deus” and the children’s series “Unstoppable Us”. He is a lecturer in the Hebrew University of Jerusalem’s history department and co-founder of Sapienship, a social-impact company.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.It can take a little imagination to see how some innovations might change an economy. Not so with the latest ai tools. It is easy—from a writer’s perspective, uncomfortably so—to think of contexts in which something like Chatgpt, a clever chatbot which has taken the web by storm since its release in November, could either dramatically boost a human worker’s productivity or replace them outright. The gpt in its name stands for “generative pre-trained transformer”, which is a particular kind of language model. It might well stand for general-purpose technology: an earth-shaking sort of innovation which stands to boost productivity across a wide-range of industries and occupations, in the manner of steam engines, electricity and computing. The economic revolutions powered by those earlier gpts can give us some idea how powerful ai might transform economies in the years ahead.In a paper published in 1995, Timothy Bresnahan of Stanford University and Manuel Trajtenberg of Tel Aviv University set out what they saw as the characteristics of a general-purpose technology. It must be used in many industries, have an inherent potential for continued improvement and give rise to “innovational complementarities”—that is, induce knock-on innovation in the industries which use it. ai is being adopted widely, seems to get better by the day and is being deployed in ever more r&d contexts. So when does the economic revolution begin?The first lesson from history is that even the most powerful new tech takes time to change an economy. James Watt patented his steam engine in 1769, but steam power did not overtake water as a source of industrial horsepower until the 1830s in Britain and 1860s in America. In Britain the contribution of steam to productivity growth peaked post-1850, nearly a century after Watt’s patent, according to Nicholas Crafts of the University of Sussex. In the case of electrification, the key technical advances had all been accomplished before 1880, but American productivity growth actually slowed from 1888 to 1907. Nearly three decades after the first silicon integrated circuits Robert Solow, a Nobel-prizewinning economist, was still observing that the computer age could be seen everywhere but in the productivity statistics. It was not until the mid-1990s that a computer-powered productivity boom eventually emerged in America.The gap between innovation and economic impact is in part because of fine-tuning. Early steam engines were wildly inefficient and consumed prohibitively expensive piles of coal. Similarly, the stunning performance of recent ai tools represents a big improvement over those which sparked a boomlet of ai enthusiasm roughly a decade ago. (Siri, Apple’s virtual assistant, was released in 2011, for example.) Capital constraints can also slow deployment. Robert Allen of New York University Abu Dhabi argues that the languid rise in productivity growth in industrialising Britain reflected a lack of capital to build plants and machines, which was gradually overcome as capitalists reinvested their fat profits.More recent work emphasises the time required to accumulate what is known as intangible capital, or the basic know-how needed to make effective use of new tech. Indeed, Erik Brynjolfsson of Stanford University, Daniel Rock of the Massachusetts Institute of Technology and Chad Syverson of the University of Chicago suggest a disruptive new technology may be associated with a “productivity J-curve”. Measured productivity growth may actually decline in the years or decades after a new technology appears, as firms and workers divert time and resources to studying the tech and designing business processes around it. Only later as these investments bear fruit does the J surge upward. The authors reckon that ai-related investments in intangible capital may already be depressing productivity growth, albeit not yet by very much.Of course for many people, questions about the effects of ai on growth take a back seat to concerns about consequences for workers. Here, history’s messages are mixed. There is good news: despite epochal technological and economic change, fears of mass technological unemployment have never before been realised. Tech can and does take a toll on individual occupations, however, in ways that can prove socially disruptive. Early in the Industrial Revolution, mechanisation dramatically increased demand for relatively unskilled workers, but crushed the earnings of craftsmen who had done much of the work before, which is why some chose to join machine-smashing Luddite movements. And in the 1980s and 1990s, automation of routine work on factory floors and in offices displaced many workers of modest means, while boosting employment for both high- and low-skilled workers.Gee, Pretty Terrificai might well augment the productivity of workers of all different skill levels, even writers. Yet what that means for an occupation as a whole depends on whether improved productivity and lower costs lead to a big jump in demand or only a minor one. When the assembly line—a process innovation with gpt-like characteristics—allowed Henry Ford to cut the cost of making cars, demand surged and workers benefited. If ai boosts productivity and lowers costs in medicine, for example, that might lead to much higher demand for medical services and professionals.There is a chance that powerful ai will break the historic mould. A technology capable of handling almost any task the typical person can do would bring humanity into uncharted economic territory. Yet even in such a scenario, the past holds some lessons. The sustained economic growth which accompanied the steam revolution, and the further acceleration which came along with electrification and other later innovations, were themselves unprecedented. They prompted a tremendous scramble to invent new ideas and institutions, to make sure that radical economic change translated into broad-based prosperity rather than chaos. It may soon be time to scramble once again. ■Read more from Free Exchange, our column on economics:Have economists misunderstood inflation? (Jan 26th)Could Europe end up with a worse inflation problem than America? (Jan 19th)Warnings from history for a new era of industrial policy (Jan 11th)For more expert analysis of the biggest stories in economics, finance and markets, sign up to Money Talks, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The age of “generative” artificial intelligence has well and truly arrived. Openai’s chatbots, which use large-language-model (llm) technology, got the ball rolling in November. Now barely a day goes by without some mind-blowing advance. An ai-powered song featuring a fake “Drake” and “The Weeknd” recently shook the music industry. Programs which convert text to video are making fairly convincing content. Before long consumer products such as Expedia, Instacart and OpenTable will plug into Openai’s bots, allowing people to order food or book a holiday by typing text into a box. A recently leaked presentation, reportedly from a Google engineer, suggests the tech giant is worried about how easy it is for rivals to make progress. There is more to come—probably a lot more.The development of ai raises profound questions. Perhaps most pressing, though, is a straightforward one. What does this mean for the economy? Many have grand expectations. New research by Goldman Sachs, a bank, suggests that “widespread ai adoption could eventually drive a 7% or almost $7trn increase in annual global gdp over a ten-year period.” Academic studies point to a three-percentage-point rise in annual labour-productivity growth in firms that adopt the technology, which would represent a huge uplift in incomes compounded over many years. A study published in 2021 by Tom Davidson of Open Philanthropy, a grantmaking outfit, puts a more than 10% chance on “explosive growth”—defined as increases in global output of more than 30% a year—sometime this century. A few economists, only half-jokingly, hold out the possibility of global incomes becoming infinite.Financial markets, however, point to rather more modest outcomes. In the past year share prices of companies involved in ai have done worse than the global average, although they have risen in recent months (see chart 1). Interest rates are another clue. If people thought that the technology was going to make everyone richer tomorrow, rates would rise because there would be less need to save. Inflation-adjusted rates and subsequent gdp growth are strongly correlated, notes research by Basil Halperin of the Massachusetts Institute of Technology (mit) and colleagues. Yet since the hype about ai began in November, long-term rates have fallen. They remain very low by historical standards. Financial markets, the researchers conclude, “are not expecting a high probability of…ai-induced growth acceleration…on at least a 30-to-50-year time horizon.”To judge which group is right, it is helpful to consider the history of previous technological breakthroughs. This provides succour to investors. For it is difficult to make the case that a single new technology by itself has ever radically changed the economy, either for good or ill. Even the industrial revolution of the late 1700s, which many people believe was the result of the invention of the spinning jenny, was actually caused by all sorts of factors coming together: increasing use of coal, firmer property rights, the emergence of a scientific ethos and much more besides.Read more of our articles on artificial intelligencePerhaps most famously, in the 1960s Robert Fogel published work about America’s railways that would later win him a Nobel Prize in economics. Many thought that rail transformed America’s prospects, turning an agricultural society into an industrial powerhouse. In fact, it had a very modest impact, Fogel found, because it replaced technology—such as canals—that would have done just about as good a job. The level of per-person income that America achieved by January 1st 1890 would have been reached by March 31st 1890 if railways had never been invented.Of course, no one can predict with any certainty where a technology as fundamentally unpredictable as ai will take humans. Runaway growth is not impossible; nor is technological stagnation. But you can still think through the possibilities. And, so far at least, it seems as though Fogel’s railways are likely to be a useful blueprint. Consider three broad areas: monopolies, labour markets and productivity.A new technology sometimes creates a small group of people with vast economic power. John D. Rockefeller won out with oil refining and Henry Ford with cars. Today Jeff Bezos and Mark Zuckerberg are pretty dominant thanks to tech.Many pundits expect that before long the ai industry will generate huge profits. In a recent paper Goldman’s analysts estimate in a best-case scenario generative ai could add about $430bn to annual global enterprise-software revenues. Their calculation assumes that each of the world’s 1.1bn office workers will adopt a few ai gizmos, paying around $400 in total each.Any business would be glad to capture some of this cash. But in macroeconomic terms $430bn simply does not move the dial. Assume that all of the revenue turns into profits, which is unrealistic, and that all of these profits are earned in America, which is a tad more realistic. Even under these conditions, the ratio of the country’s pre-tax corporate profits to its gdp would rise from 12% today to 14%. That is far above the long-run average, but no higher than it was in the second quarter of 2021.These profits could go to one organisation—maybe Openai. Monopolies often arise when an industry has high fixed costs or when it is hard to switch to competitors. Customers had no alternative to Rockefeller’s oil, for instance, and could not produce their own. Generative ai has some monopolistic characteristics. gpt-4, one of Openai’s chatbots, reportedly cost more than $100m to train, a sum few firms have lying around. There is also a lot of proprietary knowledge about data for training the models, not to mention user feedback.There is, however, little chance of a single company bestriding the entire industry. More likely is that a modest number of big firms compete with one another, as happens in aviation, groceries and search engines. No ai product is truly unique since all use similar models. This makes it easier for a customer to switch from one to another. The computing power behind the models is also fairly generic. Much of the code, as well as tips and tricks, is freely available online, meaning that amateurs can produce their own models—often with strikingly good results.“There don’t appear, today, to be any systemic moats in generative ai,” a team at Andreessen Horowitz, a venture-capital firm, has argued. The recent leak purportedly from Google reaches a similar conclusion: “The barrier to entry for training and experimentation has dropped from the total output of a major research organisation to one person, an evening, and a beefy laptop.” Already there are a few generative-ai firms worth more than $1bn. The biggest corporate winner so far from the new ai age is not even an ai company. At Nvidia, a computing firm which powers AI models, revenue from data centres is soaring.Yeah, but what about me?Although generative ai might not create a new class of robber barons, to many people that will be cold comfort. They are more concerned with their own economic prospects—in particular, whether their job will disappear. Terrifying predictions abound. Tyna Eloundou of OpenAI and colleagues have estimated that “around 80% of the us workforce could have at least 10% of their work tasks affected by the introduction of llms”. Edward Felten of Princeton University and colleagues conducted a similar exercise. Legal services, accountancy and travel agencies came out at or near the top of professions most likely to face disruption.Economists have issued gloomy predictions before. In the 2000s many feared the impact of outsourcing on rich-world workers. In 2013 two at Oxford University issued a widely cited paper that suggested automation could wipe out 47% of American jobs over the subsequent decade or so. Others made the case that, even without widespread unemployment, there would be “hollowing out”, where rewarding, well-paid jobs disappeared and mindless, poorly paid roles took their place.What actually happened took people by surprise. In the past decade the average rich-world unemployment rate has roughly halved (see chart 2). The share of working-age people in employment is at an all-time high. Countries with the highest rates of automation and robotics, such as Japan, Singapore and South Korea, have the least unemployment. A recent study by America’s Bureau of Labour Statistics found that in recent years jobs classified as “at risk” from new technologies “did not exhibit any general tendency toward notably rapid job loss”. Evidence for “hollowing out” is mixed. Measures of job satisfaction rose during the 2010s. For most of the past decade the poorest Americans have seen faster wage growth than the richest ones.This time could be different. The share price of Chegg, a firm which provides homework help, recently fell by half after it admitted Chatgpt was “having an impact on our new customer growth rate”. The chief executive of ibm, a big tech firm, said that the company expects to pause hiring for roles that could be replaced by AI in the coming years. But are these early signs a tsunami is about to hit? Perhaps not.Imagine a job disappears when ai automates more than 50% of the tasks it encompasses. Or imagine that workers are eliminated in proportion to the total share of economywide tasks that are automated. In either case this would, following Ms Eloundou’s estimates, result in a net loss of around 15% of American jobs. Some folk could move to industries experiencing worker shortages, such as hospitality. But a big rise in the unemployment rate would surely follow—in line, maybe, with the 15% briefly reached in America during the worst of the covid-19 pandemic in 2020.Yet this scenario is unlikely to come to pass: history suggests job destruction happens far more slowly. The automated telephone switching system—a replacement for human operators—was invented in 1892. It took until 1921 for the Bell System to install their first fully automated office. Even after this milestone, the number of American telephone operators continued to grow, peaking in the mid-20th century at around 350,000. The occupation did not (mostly) disappear until the 1980s, nine decades after automation was invented. ai will take less than 90 years to sweep the labour market: llms are easy to use, and many experts are astonished by the speed at which the general public has incorporated Chatgpt into their lives. But reasons for the slow adoption of technology in workplaces will also apply this time around.In a recent essay Mark Andreessen of Andreessen Horowitz outlined some of them. His argument focuses on regulation. In bits of the economy with heavy state involvement, such as education and health care, technological change tends to be pitifully slow. The absence of competitive pressure blunts incentives to improve. Governments may also have public-policy goals, such as maximising employment levels, which are inconsistent with improved efficiency. These industries are also more likely to be unionised—and unions are good at preventing job losses.Examples abound. Train drivers on London’s publicly run Underground network are paid close to twice the national median, even though the technology to partially or wholly replace them has existed for decades. Government agencies require you to fill in paper forms providing your personal information again and again. In San Francisco, the global centre of the ai surge, real-life cops are still employed to direct traffic during rush hour.Au revoir!Many of the jobs at risk from ai are in heavily regulated sectors. Return to the paper by Mr Felten of Princeton University. Fourteen of the top 20 occupations most exposed to ai are teachers (foreign-language ones are near the top; geographers are in a slightly stronger position). But only the bravest government would replace teachers with ai. Imagine the headlines. The same goes for cops and crime-fighting ai. The fact that Italy has already temporarily blocked Chatgpt over privacy concerns, with France, Germany and Ireland said to be considering the option, shows how worried governments are about the job-destructive effects of ai.Perhaps, in time, governments will allow some jobs to be replaced. But the delay will make space for the economy to do what it always does: create new types of jobs as others are eliminated. By lowering costs of production, new tech can create more demand for goods and services, boosting jobs that are hard to automate. A paper published in 2020 by David Autor of mit and colleagues offered a striking conclusion. About 60% of the jobs in America did not exist in 1940. The job of “fingernail technician” was added to the census in 2000. “Solar photovoltaic electrician” was added just five years ago. The ai economy is likely to create new occupations which today cannot even be imagined.Modest labour-market effects are likely to translate into a modest impact on productivity—the third factor. Adoption of electricity in factories and households began in America towards the end of the 19th century. Yet there was no productivity boom until the end of the first world war. The personal computer was invented in the 1970s. This time the productivity boom followed more quickly—but it still felt slow at the time. In 1987 Robert Solow, an economist, famously declared that the computer age was “everywhere except for the productivity statistics”.The world is still waiting for a productivity surge linked to recent innovations. Smartphones have been in widespread use for a decade, billions of people have access to superfast internet and many workers now shift between the office and home as it suits them. Official surveys show that well over a tenth of American employees already work at firms using ai of some kind, while unofficial surveys point to even higher numbers. Still, though, global productivity growth remains weak.ai could eventually make some industries vastly more productive. A paper by Erik Brynjolfsson of Stanford University and colleagues examines customer-support agents. Access to an ai tool raises the number of issues resolved each hour by 14% on average. Researchers themselves could also become more efficient: gpt-x may give them an unlimited number of almost-free research assistants. Others hope ai will eliminate administrative inefficiencies in health care, reducing costs.But there are many things beyond the reach of ai. Blue-collar work, such as construction and farming, which accounts for about 20% of rich-world gdp, is one example. An llm is of little use to someone picking asparagus. It could be of some use to a plumber fixing a leaky tap: a widget could recognise the tap, diagnose the fault and advise on fixes. Ultimately, though, the plumber still has to do the physical work. So it is hard to imagine that, in a few years’ time, blue-collar work is going to be much more productive than it is now. The same goes for industries where human-to-human contact is an inherent part of the service, such as hospitality and medical care.ai also cannot do anything about the biggest thing holding back rich-world productivity growth: misfiring planning systems. When the size of cities is constrained and housing costs are high, people cannot live and work where they are most efficient. No matter how many brilliant new ideas your society may have, they are functionally useless if you cannot build them in a timely manner. It is up to governments to defang nimbys. Technology is neither here nor there. The same goes for energy, where permitting and infrastructure are what keep costs uncomfortably high.It is even possible that the ai economy could become less productive. Look at some recent technologies. Smartphones allow instant communication, but they can also be a distraction. With email you are connected 24/7, which can make it hard to focus. A paper in 2016 by researchers at the University of California at Irvine, Microsoft Research and mit found that “the longer daily time spent on email, the lower was perceived productivity”. Some bosses now believe that working from home, once seen as a productivity-booster, gives too many people the excuse to slack off.Generative ai itself could act as a drain on productivity. What happens, for instance, if ai can create entertainment perfectly tailored to your every desire? Moreover, few people have thought through the implications of a system that can generate vast amounts of text instantly. gpt-4 is a godsend for a nimby facing a planning application. In five minutes he can produce a well written 1,000-page objection. Someone then has to respond to it. Spam emails are going to be harder to detect. Fraud cases could soar. Banks will need to spend more on preventing attacks and compensating people who lose out.Just what we needIn an ai-heavy world lawyers will multiply. “In the 1970s you could do a multi-million-dollar deal on 15 pages because retyping was a pain in the ass,” says Preston Byrne of Brown Rudnick, a law firm. “ai will allow us to cover the 1,000 most likely edge cases in the first draft and then the parties will argue over it for weeks.” A rule of thumb in America is that there is no point suing for damages unless you hope for $250,000 or more in compensation, since you need to spend that much getting to court. Now the costs of litigation could fall to close to zero. Meanwhile, teachers and editors will need to check that everything they read has not been composed by an ai. Openai has released a program that allows you to do this. It is thus providing the world a solution to a problem that its technology has created.ai may change the world in ways that today are impossible to imagine. But this is not quite the same thing as turning the economy upside down. Fogel wrote that his argument was “aimed not at refuting the view that the railroad played a decisive role in American development during the 19th century, but rather at demonstrating that the empirical base on which this view rests is not nearly so substantial as is usually presumed”. Some time in the mid-21st century a future Nobel prizewinner, examining generative ai, may well reach the same conclusion. ■For more expert analysis of the biggest stories in economics, finance and markets, sign up to Money Talks, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.ChatGPT, a chatbot developed by OpenAI, an American firm, can give passable answers to questions on everything from nuclear engineering to Stoic philosophy. Or at least, it can in English. The latest version, ChatGPT-4, scored 85% on a common question-and-answer test. In other languages it is less impressive. When taking the test in Telugu, an Indian language spoken by nearly 100m people, for instance, it scored just 62%.OpenAI has not revealed much about how ChatGPT-4 was built. But a look at its predecessor, ChatGPT-3, is suggestive. Large language models (LLMs) are trained on text scraped from the internet, on which English is the lingua franca. Around 93% of ChatGPT-3’s training data was in English. In Common Crawl, just one of the datasets on which the model was trained, English makes up 47% of the corpus, with other (mostly related) European languages accounting for 38% more. Chinese and Japanese combined, by contrast, made up just 9%. Telugu was not even a rounding error.image: The EconomistAn evaluation by Nathaniel Robinson, a researcher at Johns Hopkins University, and his colleagues finds that is not a problem limited to ChatGPT. All LLMs fare better with “high-resource” languages, for which training data are plentiful, than for “low-resource” ones for which they are scarce. That is a problem for those hoping to export AI to poor countries, in the hope it might improve everything from schools to health care. Researchers around the world are therefore working to make AI more multilingual.India’s government is particularly keen. Many of its public services are already digitised, and it is keen to fortify them with AI. In September, for instance, it launched a chatbot to help farmers get information about state benefits.The bot works by welding two sorts of language model together, says Shankar Maruwada of the EkStep Foundation, a non-profit that helped build it. Users can submit queries in their native tongues. (Eight are supported so far; five more are coming soon.) These are passed to a piece of machine-translation software developed at IIT Madras, an Indian academic institution, which translates them into English. The English version of the question is then fed to the LLM, and its response translated back into the user’s mother tongue.The system seems to work. But translating queries into an LLM’s preferred language is a rather clumsy workaround. After all, language is a vehicle for worldviews and culture as well as just meaning, notes the boss of one Indian AI firm. A paper by Rebecca Johnson, a researcher at the University of Sydney, published in 2022, found that ChatGPT-3 gave replies on topics such as gun control and refugee policy that aligned most with the values displayed by Americans in the World Values Survey, a global questionnaire of public opinion.Many researchers are therefore trying to make LLMs themselves more fluent in less widely spoken languages. One approach is to modify the tokeniser, the part of an LLM that chops words into smaller chunks for the rest of the model to manipulate. Text in Devanagari, a script used with Hindi, needs three to four times more tokens, when tokenised the standard way, than the same text in English. An Indian startup called Sarvam AI has written a tokeniser optimised for Hindi, which cuts that number substantially. Fewer tokens means fewer computations. Sarvam reckons that OpenHathi, its Devanagari-optimised LLM, can cut the cost of answering questions by around three-quarters.Another is to improve the datasets on which LLMs are trained. Often this means digitising reams of pen-and-paper texts. In November a team of researchers at Mohamed bin Zayed University, in Abu Dhabi, released the latest version of an Arabic-speaking model called “Jais”. It has one-sixth as many parameters (one measure of a model’s size) as ChatGPT-3, but performs on par with it in Arabic. Timothy Baldwin, the university’s acting provost, notes that, because his team could only digitise so much Arabic text, the model also included some English. Some concepts, after all, are similar across all languages, and can be learned in any tongue. Data in a specific language are more important for teaching the model specific cultural ideas and quirks.The third approach is to tweak models after they have been trained. Both Jais and OpenHathi have had some question-and-answer pairs hand crafted by humans. The same happens with Western chatbots, to stop them spreading what their makers see as disinformation. Ernie Bot, an LLM from Baidu, a big Chinese tech company, has been tweaked to try to stop it saying things to which the government might object. Models can also learn from human feedback, in which users rate an LLM’s answers. But that is hard to do for many poor-world languages, says Dr Baldwin, since it requires recruiting people literate enough to criticise the machine’s writing.How well all this will work remains to be seen. A quarter of India’s adults are illiterate, something that no amount of LLM tweaking will solve. Many Indians prefer using voice messages to communicate rather than text ones. AI can also turn speech into words, as India’s chatbot for farmers does. But that adds another step at which errors can creep in.And it is possible that builders of local LLMs may eventually be put out of business by the efforts of the Silicon Valley big boys. Although it is far from perfect, ChatGPT-4 is much better than ChatGPT-3 at answering questions in non-English languages. However it is done, teaching AI to speak more of the world’s 7,000-odd languages can only be a good thing. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
What if text prompts enabled anyone to make a blockbuster movie, or even an entire box-set’s worth of TV? That is the promise of AI. This technology could one day prove as transformative to the movie business as sound, colour, or even the camera itself. Generative AI can already make videos in seconds which would normally take a visual-effects artist days to create. However it has yet to master photo-realistic video. The people at the forefront of this tech say it is only a matter of time.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.On holiday, many will find themselves in places where they do not speak the language. Once upon a time, they might have carried a phrasebook. The rise of English has made that less necessary. But most people—at least seven of the world’s eight billion—still do not speak English. That leaves options like pantomime, a willingness to be surprised by what arrives at dinner—or, increasingly, technology.More and more people are using simple, free tools, not only to decode text but also to speak. With these apps’ conversation mode, you talk into a phone and a spoken translation is heard moments later; the app can also listen for another language and produce a translation in yours.You may still get a surprise or two. Google Translate may be the best-known name in machine translation, but it often blunders. Take “my wife is gluten-free,” the kind of thing you might say at a restaurant abroad. In French or Italian, Google Translate renders this as “my wife is without gluten”—true to the words rather than the meaning. DeepL, a rival, does better, offering various options, most of them along the correct lines.The best tool may not be a translation app at all. Though not marketed for the purpose, ChatGPT, a generative AI system that churns out prose according to users’ prompts, is multilingual. Rather than entering an exact text to translate, users can tell ChatGPT to “write a message in Spanish to a waiter that my wife and I would like the tasting menu, but that she is gluten-free, so we would like substitutions for anything that has gluten.” And out pops a perfect paragraph, including the way Spanish-speakers actually say “my wife is gluten-free”: mi esposa es celíaca. It is a paraphrase rather than a translation, more like having a native-speaking dinner companion than an automated interpreter.Travel has long been a motivator for study—unless people start to feel AI tools offer a good-enough service. Some are concerned that apps are turning language acquisition into a dwindling pursuit. Douglas Hofstadter, a polyglot and polymath writer, has argued that something profound will vanish when people talk through machines. He describes giving a halting, difficult speech in Mandarin, which required a lot of work but offered a sense of accomplishment at the end. Who would boast of taking a helicopter to the top of Mount Everest?Others are less worried. Most people do not move abroad or have the kind of sustained contact with a foreign culture that requires them to put in the work to become fluent. Nor do most people learn languages for the purpose of humanising themselves or training their brains. On their holiday, they just want a beer and the spaghetti carbonara without incident (and sometimes without gluten).As AI translation becomes an even more popular labour-saving tool, people will split into two groups. There will be those who want to stretch their minds, immerse themselves in other cultures or force their thinking into new pathways. This lot will still take on language study, often aided by technology. Others will look at learning a new language with a mix of admiration and puzzlement, as they might with extreme endurance sports: “Good for you, if that’s your thing, but a bit painful for my taste.”This is largely an Anglophone problem, since native English-speakers miss out on the benefits of language-learning most acutely. In many countries, including Britain and America, schools’ and universities’ foreign-language departments have been closing. (The British government recently devoted a modest fund to trying to get more secondary-school pupils to study foreign languages.) In the rest of the rich world, there is one thriving language that people still study: English. And in poorer countries, many people are multilingual as a matter of course; Africans and Indians learn languages because they are surrounded by them.But a focus on the learner alone misses the fundamentally social nature of language. It is a bit like analysing the benefits of close relationships to heart-health but overlooking the inherent value of those bonds themselves. When you try to ask directions in broken Japanese or mangle a joke in halting German, you are making direct contact with someone. And when you speak a language well enough to tell a story with perfect timing or put subtle shading on an argument, that connection is more profound still. The best relationships do not require an intermediary. ■Read more from Johnson, our columnist on language:In northern Europe, a backlash against English is under way (Aug 4th)AI is making it possible to clone voices (Jul 20th)Talking about AI in human terms is natural—but wrong (Jun 22nd)For more on the latest books, films, TV shows, albums and controversies, sign up to Plot Twist, our weekly subscriber-only newsletter
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.In 2019, scientists at the Massachusetts Institute of Technology (MIT) did something unusual in modern medicine—they found a new antibiotic, halicin. In May this year another team found a second antibiotic, abaucin. What marked these two compounds out was not only their potential for use against two of the most dangerous known antibiotic-resistant bacteria, but also how they were identified.In both cases, the researchers had used an artificial-intelligence (AI) model to search through millions of candidate compounds to identify those that would work best against each “superbug”. The model had been trained on the chemical structures of a few thousand known antibiotics and how well (or not) they had worked against the bugs in the lab. During this training the model had worked out links between chemical structures and success at damaging bacteria. Once the AI spat out its shortlist, the scientists tested them in the lab and identified their antibiotics. If discovering new drugs is like searching for a needle in a haystack, says Regina Barzilay, a computer scientist at MIT who helped to find abaucin and halicin, AI acts like a metal detector. To get the candidate drugs from lab to clinic will take many years of medical trials. But there is no doubt that AI accelerated the initial trial-and-error part of the process. It changes what is possible, says Dr Barzilay. With AI, “the type of questions that we will be asking will be very different from what we’re asking today.”Drug discovery is not alone in being jolted by the potential of AI. Researchers tackling many of the world’s most complicated and important problems—from forecasting weather to searching for new materials for batteries and solar panels and controlling nuclear-fusion reactions—are all turning to AI in order to augment or accelerate their progress.The potential is enormous. “AI could usher in a new renaissance of discovery,” argues Demis Hassabis, co-founder of Google DeepMind, an AI lab based in London, “acting as a multiplier for human ingenuity.” He has compared AI to the telescope, an essential technology that will let scientists see farther and understand more than with the naked eye alone.Where have you been?image: The EconomistThough it has been part of the scientific toolkit since the 1960s, for most of its life AI has been stuck within disciplines where scientists were already well-versed in computer code—particle physics, for example, or mathematics. By 2023, however, with the rise of deep learning, more than 99% of research fields were producing AI-related results, according to CSIRO, Australia’s science agency (see chart). “Democratisation is the thing that is causing this explosion,” says Mark Girolami, chief scientist at the Alan Turing Institute in London.  What used to require a computer-science degree and lines of arcane programming languages can now be done with user-friendly AI tools, often made to work after a query to ChatGPT, OpenAI’s chatbot. Thus scientists have easy access to what is essentially a dogged, superhuman research assistant that will solve equations and tirelessly sift through enormous piles of data to look for any patterns or correlations within.In materials science, for example, the problem is similar to that in drug discovery—there are an unfathomable number of possible compounds. When researchers at the University of Liverpool were looking for materials that would have the very specific properties required to build better batteries, they used an AI model known as an “autoencoder” to search through all 200,000 of the known, stable crystalline compounds in the Inorganic Crystal Structure Database, the world’s largest such repository. The AI had previously learned the most important physical and chemical properties required for the new battery material to achieve its goals and applied those conditions to the search. It successfully reduced the pool of candidates for scientists to test in the lab from thousands to just five, saving time and money.The final candidate—a material combining lithium, tin, sulphur and chlorine—was novel, though it is too soon to tell whether or not it will work commercially. The AI method, however, is being used by researchers to discover other sorts of new materials.What did you dream?AI can also be used to predict. The shapes into which proteins twist themselves after they are made in a cell are vital to making them work. Scientists do not yet know how proteins fold. But in 2021, Google DeepMind developed AlphaFold, a model that had taught itself to predict the structure of a protein from its amino-acid sequence alone. Since it was released, AlphaFold has produced a database of more than 200m predicted protein structures, which has already been used by over 1.2m researchers. For example, Matthew Higgins, a biochemist at the University of Oxford, used AlphaFold to figure out the shape of a protein in mosquitoes that is important for the malaria parasite that the insects often carry. He was then able to combine the predictions from AlphaFold to work out which parts of the protein would be the easiest to target with a drug. Another team used AlphaFold to find—in just 30 days—the structure of a protein that influences how a type of liver cancer proliferates, thereby opening the door to designing a new targeted treatment.AlphaFold has also contributed to the understanding of other bits of biology. The nucleus of a cell, for example, has gates to bring in material to produce proteins. A few years ago, scientists knew the gates existed, but knew little about their structure. Using AlphaFold, scientists predicted the structure and contributed to understanding about the internal mechanisms of the cell. “We don’t really completely understand how [the AI] came up with that structure,” says Pushmeet Kohli, one of AlphaFold’s inventors who now heads Google DeepMind’s “AI for Science” team. “But once it has made the structure, it is actually a foundation that now, the whole scientific community can build on top of.”AI is also proving useful in speeding up complex computer simulations. Weather models, for example, are based on mathematical equations that describe the state of Earth’s atmosphere at any given time. The supercomputers that forecast weather, however, are expensive, consume a lot of power and take a lot of time to carry out their calculations. And models must be run again and again to keep up with the constant inflow of data from weather stations around the world.Climate scientists, and private companies, are therefore beginning to deploy machine learning to speed things up. Pangu-Weather, an AI built by Huawei, a Chinese company, can make predictions about weather a week in advance thousands of times faster and cheaper than the current standard, without any meaningful dip in accuracy. FourCastNet, a model built by Nvidia, an American chipmaker, can generate such forecasts in less than two seconds, and is the first AI model to accurately predict rain at a high spatial resolution, which is important information for predicting natural disasters such as flash floods. Both these AI models are trained to predict the weather by learning from observational data, or the outputs of supercomputer simulations. And they are just the start—Nvidia has already announced plans to build a digital twin of Earth, called “Earth-2”, a computer model that the company hopes will be able to predict climate change at a more regional level, several decades in advance.Physicists trying to harness the power of nuclear fusion, meanwhile, have been using AI to control complex bits of kit. One approach to fusion research involves creating a plasma (a superheated, electrically charged gas) of hydrogen inside a doughnut-shaped vessel called a tokamak. When hot enough, around 100m°C, particles in the plasma start to fuse and release energy. But if the plasma touches the walls of the tokamak, it will cool down and stop working, so physicists contain the gas within a magnetic cage. Finding the right configuration of magnetic fields is fiendishly difficult (“a bit like trying to hold a lump of jelly with knitting wool”, according to one physicist) and controlling it manually requires devising mathematical equations to predict what the plasma will do and then making thousands of small adjustments every second to around ten different magnetic coils. By contrast, an AI control system built by scientists at Google DeepMind and EPFL in Lausanne, Switzerland, allowed scientists to try out different shapes for the plasma in a computer simulation—and the AI then worked out how best to get there.Automating and speeding up physical experiments and laboratory work is another area of interest. “Self-driving laboratories” can plan an experiment, execute it using a robotic arm, and then analyse the results. Automation can make discovering new compounds, or finding better ways of making old compounds, up to a thousand times faster. You’ve been in the pipelineGenerative AI, which exploded into public consciousness with the arrival of ChatGPT in 2022 but which scientists have been playing with for much longer, has two main scientific uses. First, it can be used to generate data. “Super-resolution” AI models can enhance cheap, low-resolution electron-microscope images into high-resolution ones that would otherwise have been too expensive to record. The AI compares a small area of a material or a biological sample in high resolution with the same thing recorded at a lower resolution. The model learns the difference between the two resolutions and can then translate between them.And just as a large language model (LLM) can generate fluent sentences by predicting the next best word in a sequence, generative molecular models are able to build molecules, atom by atom, bond by bond. LLMs use a mix of self-taught statistics and trillions of words of training text culled from the internet to write in ways that plausibly mimic a human. Trained on vast databases of known drugs and their properties, models for “de novo molecular design” can figure out which molecular structures are most likely to do which things, and they build accordingly. Verseon, a pharmaceutical company based in California, has created drug candidates in this way, several of which are now being tested on animals, and one—a precision anticoagulant—that is in the first phase of clinical trials. Like the new antibiotics and battery materials identified by AI, chemicals designed by algorithms will also need to undergo the usual trials in the real world before their effectiveness can be assessed.A more futuristic use for LLMs comes from Igor Grossmann, a psychologist at the University of Waterloo. If an LLM could be prompted with real (or fabricated) back stories so as to mirror accurately what human participants might say, they could theoretically replace focus groups, or be used as agents in economics research. LLMs could be trained with various different personas, and their behaviour could then be used to simulate experiments, whose results, if interesting, could later be confirmed with human subjects.LLMs are already making scientists themselves more efficient. According to GitHub, using tools like its “Copilot” can help coders write software 55% faster. For all scientists, reading the background research in a field before embarking on a project can be a daunting task—the sheer scale of the modern scientific literature is too vast for a person to manage. Elicit, a free online AI tool created by Ought, an American non-profit research lab, can help by using an LLM to comb through the mountains of research literature and summarise the important ones much faster than any human could. It is already used by students and younger scientists, many of whom find it useful to find papers to cite or to define a research direction in the face of a mountain of text. LLMs can even help to extract structured information—such as every experiment done using a specific drug—from millions of documents.Widening access to knowledge within disciplines could also be achieved with AI. Each detector at the Large Hadron Collider at CERN in Geneva requires its own specialised teams of operators and analysts. Combining and comparing data from them is impossible without physicists from each detector coming together to share their expertise. This is not always feasible for theoretical physicists who want to quickly test new ideas. Miguel Arratia, a physicist at the University of California, Riverside, has therefore proposed using AI to integrate measurements from multiple fundamental physics experiments (and even cosmological observations) so that theoretical physicists can quickly explore, combine and re-use the data in their own work.AI models have demonstrated that they can process data, and automate calculations and some lab work (see table). But Dr Girolami warns that whereas AI might be useful to help scientists fill in gaps in knowledge, the models still struggle to push beyond the edges of what is already known. These systems are good at interpolation—connecting the dots—but less so at extrapolation, imagining where the next dot might go. And there are some hard problems that even the most successful of today’s AI systems cannot yet handle. AlphaFold, for example, does not get all proteins right all the time. Jane Dyson, a structural biologist at the Scripps Research Institute in La Jolla, California, says that for “disordered” proteins, which are particularly relevant to her research, the AI’s predictions are mostly garbage. “It’s not a revolution that puts all of our scientists out of business.” And AlphaFold does not yet explain why proteins fold in the ways they do. Though perhaps the AI “has a theory we just have not been able to grasp yet,” says Dr Kohli.image: Shira InbarDespite those limitations, structural biologists still reckon that AlphaFold has made their work more efficient. The database filled with AlphaFold’s protein predictions allows scientists to work out the likely structure of a protein in a few seconds, as opposed to the years and tens of thousands of dollars it might have taken otherwise.And speeding up the pace of scientific research and discovery, making efficiencies wherever possible, holds plenty of promise. In a recent report on AI in science the OECD, a club of rich countries, said that “while AI is penetrating all domains and stages of science, its full potential is far from realised.” The prize, it concluded, could be enormous: “Accelerating the productivity of research could be the most economically and socially valuable of all the uses of artificial intelligence.”Welcome to the machineIf AI tools manage to boost the productivity of research, the world would no doubt get the “multiplier for human ingenuity” predicted by Dr Hassabis. But AI holds more potential still: just like telescopes and microscopes let scientists see more of the world, the probabilistic, data-driven models used in AI will increasingly allow scientists to better model and understand complex systems. Fields like climate science and structural biology are already at the point where scientists know that complicated processes are happening, but researchers so far have mainly tried to understand those subjects using top-down rules, equations and simulations. AI can help scientists approach problems from the bottom up instead—measure lots of data first, and use algorithms to come up with the rules, patterns, equations and scientific understanding later.If the past few years have seen scientists dip their toes into the shallow waters of AI, the next decade and beyond will be when they have to dive into its depths and swim towards the horizon. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.IF YOU LISTEN to the bombast in Beijing and Washington, America and China are engaged in an all-out contest for technological supremacy. “Fundamentally, we believe that a select few technologies are set to play an outsized importance over the coming decade,” declared Jake Sullivan, President Joe Biden’s national security adviser, last September. In February Xi Jinping, China’s paramount leader, echoed the sentiment, stating that “we urgently need to strengthen basic research and solve key technology problems” in order to “cope with international science and technology competition, achieve a high level of self-reliance and self-improvement”.No technology seems to obsess policymakers on both sides of the Pacific more right now than artificial intelligence (AI). The rapid improvements in the abilities of “generative” AIs like ChatGPT, which analyse the web’s worth of human text, images or sounds and can then create increasingly passable simulacrums, have only strengthened the obsession. If generative AI proves as transformational as its boosters claim, the technology could give those who wield it an economic and military edge in the 21st century’s chief geopolitical contest. Western and Chinese strategists already talk of an AI arms race. Can China win it?On some measures of AI prowess, the autocracy pulled ahead some time ago (see chart). China surpassed America in the share of highly cited AI papers in 2019; in 2021, 26% of AI conference publications globally came from China, compared with America’s share of 17%. Nine of the world’s top ten institutions, by volume of AI publications, are Chinese. According to one popular benchmark, so are the top five labs working on computer vision, a type of AI particularly useful to a communist surveillance state.Yet when it comes to “foundation models”, which give generative AIs their wits, America is firmly in front (see charts 2 and 3). ChatGPT and the pioneering model behind it, the latest version of which is called GPT-4, are the brainchild of OpenAI, an American startup. A handful of other American firms, from small ones such as Anthropic or Stability AI to behemoths like Google, Meta and Microsoft (which part-owns OpenAI), have their own powerful systems. ERNIE, a Chinese rival to ChatGPT built by Baidu, China’s internet-search giant, is widely seen as less clever. Alibaba and Tencent, China’s mightiest tech titans, have yet to unveil their own generative AIs.This leads those in the know to conclude that China is two or three years behind America in building foundation models. There are three reasons for this underperformance. The first concerns data. A centralised autocracy should be able to marshal lots of it—the government was, for instance, able to hand over troves of surveillance information on Chinese citizens to firms such as SenseTime or Megvii that, with the help of China’s leading computer-vision labs, then used it to develop top-notch facial-recognition systems.That advantage has proved less formidable in the context of generative AIs, because foundation models are trained on the voluminous unstructured data of the web. American model-builders benefit from the fact that 56% of all websites are in English, whereas just 1.5% are written in Chinese, according to data from W3Techs, an internet-research site. As Yiqin Fu of Stanford University points out, the Chinese interact with the internet primarily through mobile super-apps like WeChat and Weibo. These are “walled gardens”, so much of their content is not indexed on search engines. This makes that content harder for AI models to suck up. Lack of data may explain why Wu Dao 2.0, a model unveiled in 2021 by the Beijing Academy of Artificial Intelligence, a state-backed outfit, failed to make a splash despite its possibly being computationally more complex than GPT-4.The second reason for China’s lacklustre generative achievements has to do with hardware. Last year America imposed export controls on technology that might give China a leg-up in AI. These cover the powerful microprocessors used in the cloud-computing data centres where foundation models do their learning, and the chipmaking tools that could enable China to build such semiconductors on its own.That hurt Chinese model-builders. An analysis of 26 big Chinese models by the Centre for the Governance of AI, a British think-tank, found that more than half depended on Nvidia, an American chip designer, for their processing power. Some reports suggest that SMIC, China’s biggest chipmaker, has produced prototypes just a generation or two behind TSMC, the Taiwanese industry leader that manufactures chips for Nvidia (see chart 4). But SMIC can probably mass-produce only chips which TSMC was churning out by the million three or four years ago. Chinese AI firms are having trouble getting their hands on another American export: know-how. America remains a magnet for the world’s tech talent; two-thirds of AI experts in America who present papers at the main AI conference are foreign-born. Chinese engineers made up 27% of that select group in 2019. Many Chinese AI boffins studied or worked in America before bringing expertise back home. The covid-19 pandemic and rising Sino-American tensions are causing their numbers to dwindle. In the first half of 2022 America granted half as many visas to Chinese students as in the same period in 2019.The triple shortage—of data, hardware and expertise—has been a hurdle for China. Whether it will hold Chinese AI ambitions back much longer is another matter.Take data. In February local authorities in Beijing, where nearly a third of China’s AI firms are located, promised to release data from 115 state-affiliated organisations, giving model-builders 15,880 data sets to play with. The central government has previously signalled it wants to dismantle Chinese apps’ walled gardens, potentially liberating more data, says Kayla Blomquist, an American former diplomat in China now at Oxford University. The latest models are also able to transfer their machine learnings from one language to another. OpenAI says that GPT-4 performs remarkably well on tasks in Chinese despite scarce Chinese source material in its training data. Baidu’s ERNIE was trained on lots of English-language data, notes Jeffrey Ding of George Washington University.In hardware, too, China is finding workarounds. The Financial Times reported in March that SenseTime, which is blacklisted by America, was using middlemen to skirt the export controls. Some Chinese AI firms are harnessing Nvidia’s processors through cloud servers in other countries. Alternatively, they can buy more of Nvidia’s less advanced wares—to keep serving the vast Chinese market, Nvidia has designed sanctions-compliant ones that are between 10% and 30% slower than top-of-the-range kit. These end up being costlier for the Chinese customers per unit of processing power. But they do the job.China could partly alleviate the dearth of chips—and of brain power—with the help of “open-source” models. Such models’ inner workings can be downloaded by anyone and fine-tuned to a specific task. Those include the numbers, called “weights”, which define the structure of the model and which are derived from costly training runs. Researchers at Stanford used the weights from LLaMA, Meta’s foundation model, to build one called Alpaca for less than $600, compared with perhaps $100m for training something like GPT-4. Alpaca performs just as well as the original version of ChatGPT on some tasks.Chinese AI labs could similarly avail themselves of open-source models, which embody the collective wisdom of international research teams. Matt Sheehan of the Carnegie Endowment for International Peace, another think-tank, says that China has form in being a “fast follower”—its labs have absorbed advances from abroad and rapidly incorporated them into their own models, often with flush state resources. A prominent Silicon Valley venture capitalist is more blunt, calling open-source models a gift to the Communist Party.Such considerations make it hard to imagine that either America or China could build an unbridgeable lead in AI modelling. Each may well end up with AIs of similar ability, even if it costs China over the odds in the face of American sanctions. But if the race of the model-builders is a dead heat, America has one thing going for it that may make it the big AI winner—its ability to spread cutting-edge innovation throughout the economy. It was, after all, more efficient diffusion of technology that helped America open up a technological lead over the Soviet Union, which in the 1950s was producing twice as many science PhDs as its democratic adversary.China is far more competent than the Soviet Union ever was at adopting new technologies. Its fintech platforms, 5G telecoms and high-speed rail are all world-class. Still, those successes may be the exception, not the rule, says Mr Ding. Particularly, China has done less well in deploying cloud computing and business software—both complementary to AI.And though American export controls may not derail all Chinese model-building, they constrain China’s tech industry more broadly, thereby slowing the adoption of new technology. Moreover, Chinese businesses as a whole, and especially small and medium-sized ones, are short of technologists who act as conduits for technological diffusion. Swathes of the economy are dominated by state-owned firms, which tend to be stodgy and change-averse. Parts of it are dodgy. China’s “Big Fund” for chips, which raised $50bn in 2014 with a view to backing domestic semiconductor firms, has been mired in scandals. Many of the thousands of new AI startups are AI in name only, slapping on the label to get a slice of the lavish subsidies doled out by the state to the favoured industry.As a consequence, China’s private sector may struggle to take full advantage of generative AI, especially if the Communist Party imposes strict rules to prevent chatbots from saying something its censors dislike. The handicaps would come on top of Mr Xi’s broader suborning of private enterprise, including a two-and-a-half-year crackdown on China’s tech industry.Although the anti-tech campaign has officially ended, it has left deep scars, not least in the AI business. Last year private investments in Chinese AI startups amounted to $13.5bn, less than one-third of the sum that flowed to their American rivals. In the first four months of 2023 the funding gap appears only to have widened, according to PitchBook, a data provider. Whether or not generative AI proves revolutionary, the free market has placed its bet on who will make the most of it. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Wimbledon’s centre court has seen its share of rivalries; think of McEnroe v Borg, or Williams v Williams. But for David Almog, a behavioural economist at Northwestern University, the match worth tuning in for is umpire v machine.How AI oversight affects human decision-making is an important question in a world where algorithms play an ever-larger role in everyday life. Car drivers, financial traders and air-traffic controllers already routinely see their decisions overruled by AI systems put in place to rapidly correct poor judgment. Doctors, judges and even soldiers could be next.Much of this correction happens out of the public eye, thwarting would-be analysts. But, says Mr Almog, “tennis is one of the most visible settings where final decision rights are granted to AI.” That is why, together with colleagues in America and Australia, he has looked at whether tennis umpires and line judges correctly called balls in or out during nearly 100,000 points played in some 700 matches across the world, both before and after the introduction of the Hawk-Eye ball-tracking system in 2006.The Hawk-Eye system, now used at most elite tournaments, uses between six and ten cameras positioned around the court to create a three-dimensional representation of the ball’s trajectory. This can then be presented on a screen visible to players, spectators and officials—as well as tv viewers. Players can use it to appeal human decisions, with the AI’s verdict considered final. Bad calls from line judges and umpires are now often overturned.The latest analysis from Mr Almog and his colleagues, published as a preprint last month, showed that Hawk-Eye oversight has prompted human officials to up their game and make 8% less mistakes than before it was introduced. (That comparison can be made thanks to a 2005 trial period in which Hawk-Eye was used without the ability to influence calls.) Such an improvement in performance is to be expected, the researchers say, given the heightened watchfulness that accompanies the threat of public shaming.Most of the improvement came during the multi-shot rallies that follow a successful serve and return. But when the researchers looked at serves in particular, and especially in cases where the served ball landed within 20mm either side of a line, they were surprised to see the error rate soar. The umpires and line judges, it turned out, had switched strategy. Before Hawk-Eye, they were more likely to call a serve out when it was in. But afterwards, they were even more likely to wave through balls that were actually out. For every 100 mis-hit serves, post-Hawk-Eye umpires left 39 unchallenged. The comparable figure in the earlier era was 26.Such a shift is easily understood. Overlooked faults are less disruptive in tennis than incorrect cries of “out” because these end the point prematurely. They can also trigger dissent from both the player and crowd when the error is identified on the big screen. It seems that human officials take the less reputationally risky option, even if it leads to more incorrect calls.Tennis, with its binary outcomes and clear evidence of whether a decision was right or wrong, offers a highly simplified model for AI oversight. But many of the same tendencies will be at play in fields like medicine and law, says Mr Almog, and should be considered before algorithms are allowed to trump human decisions. Most important, perhaps, is the social cost of getting an important call wrong, which will vary between disciplines, and could distort decision-making in different ways. Judges, for example, may prefer to under-convict. Doctors, on the other hand, might over-diagnose. Stay tuned. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.IN A meeting room at the Royal Society in London, several dozen graduate students were recently tasked with outwitting a large language model (LLM), a type of AI designed to hold useful conversations. LLMs are often programmed with guardrails designed to stop them giving replies deemed harmful: instructions on making Semtex in a bathtub, say, or the confident assertion of “facts” that are not actually true.The aim of the session, organised by the Royal Society in partnership with Humane Intelligence, an American non-profit, was to break those guardrails. Some results were merely daft: one participant got the chatbot to claim ducks could be used as indicators of air quality (apparently, they readily absorb lead). Another prompted it to claim health authorities back lavender oil for treating long covid. (They do not.) But the most successful efforts were those that prompted the machine to produce the titles, publication dates and host journals of non-existent academic articles. “It’s one of the easiest challenges we’ve set,” said Jutta Williams of Humane Intelligence.AI has the potential to be a big boon to science. Optimists talk of machines producing readable summaries of complicated areas of research; tirelessly analysing oceans of data to suggest new drugs or exotic materials and even, one day, coming up with hypotheses of their own. But AI comes with downsides, too. It can make it easier for scientists to game the system, or even commit outright fraud. And the models themselves are subject to subtle biases.Start with the simplest problem: academic misconduct. Some journals allow researchers to use LLMs to help write papers, provided they say as much. But not everybody is willing to admit to it. Sometimes, the fact that LLMs have been used is obvious. Guillaume Cabanac, a computer scientist at the University of Toulouse, has uncovered dozens of papers that contain phrases such as “regenerate response”—the text of a button in some versions of ChatGPT that commands the program to rewrite its most recent answer, presumably copied into the manuscript by mistake.The scale of the problem is impossible to know. But indirect measures can shed some light. In 2022, when LLMs were available only to those in the know, the number of research-integrity cases investigated by Taylor and Francis, a big publisher of scientific papers, rose from around 800 in 2021 to about 2,900. Early figures from 2023 suggest the number was on course to double. One possible telltale is odd synonyms: “haze figuring” as another way to say “cloud computing”, for example, or “counterfeit consciousness” instead of “AI”.Even honest researchers could find themselves dealing with data that has been polluted by AI. Last year Robert West and his students at the Swiss Federal Institute of Technology enlisted remote workers via Mechanical Turk, a website which allows users to list odd jobs, to summarise long stretches of text. In a paper published in June, albeit one that has not yet been peer-reviewed, the team revealed that over a third of all the responses they received had been produced with the help of chatbots.Dr West’s team was able to compare the responses they received with another set of data that had been generated entirely by humans, leaving them well-placed to detect the deception. Not all scientists who use Mechanical Turk will be so fortunate. Many disciplines, particularly in the social sciences, rely on similar platforms to find respondents willing to answer questionnaires. The quality of their research seems unlikely to improve if many of the responses come from machines rather than real people. Dr West is now planning to apply similar scrutiny to other crowdsourcing platforms he prefers not to name.It is not just text that can be doctored. Between 2016 and 2020, Elisabeth Bik, a microbiologist at Stanford University, and an authority on dodgy images in scientific papers, identified dozens of papers containing images that, despite coming from different labs, seemed to have identical features. Over a thousand other papers have since been identified, by Dr Bik and others. Dr Bik’s best guess is that the images were produced by AI, and created deliberately to support a paper’s conclusions.For now, there is no way to reliably identify machine-generated content, whether it is images or words. In a paper published last year Rahul Kumar, a researcher at Brock University, in Canada, found that academics could correctly spot only around a quarter of computer-generated text. AI firms have tried embedding “watermarks”, but these have proved easy to spoof. “We might now be at the phase where we no longer can distinguish real from fake photos,” says Dr Bik.Producing dodgy papers is not the only problem. There may be subtler issues with AI models, especially if they are used in the process of scientific discovery itself. Much of the data used to train them, for instance, will by necessity be somewhat old. That risks leaving models stuck behind the cutting edge in fast-moving fields.Another problem arises when AI models are trained on AI-generated data. Training a machine on synthetic MRI scans, for example, can get around issues of patient confidentiality. But sometimes such data can be used unintentionally. LLMs are trained on text scraped from the internet. As they churn out more such text, the risk of LLMs inhaling their own outputs grows.That can cause “model collapse”. In 2023 Ilia Shumailov, a computer scientist at the University of Oxford, co-authored a paper (yet to be peer-reviewed) in which a model was fed handwritten digits and asked to generate digits of its own, which were fed back to it in turn. After a few cycles, the computer’s numbers became more or less illegible. After 20 iterations, it could produce only rough circles or blurry lines. Models trained on their own results, says Dr Shumailov, produce outputs that are significantly less rich and varied than their training data.Some worry that computer-generated insights might come from models whose inner workings are not understood. Machine-learning systems are “black boxes” that are hard for humans to disassemble. Unexplainable models are not useless, says David Leslie at the Alan Turing Institute, an AI-research outfit in London, but their outputs will need rigorous testing in the real world. That is perhaps less unnerving than it sounds. Checking models against reality is what science is supposed to be about, after all. Since no one fully understands how the human body works, for instance, new drugs must be tested in clinical trials to figure out whether they work.For now, at least, questions outnumber answers. What is certain is that many of the perverse incentives currently prevalent in science are ripe for exploitation. The emphasis on assessing academic performance by how many papers a researcher can publish, for example, acts as a powerful incentive for fraud at worst, and for gaming the system at best. The threats that machines pose to the scientific method are, at the end of the day, the same ones posed by humans. AI could accelerate the production of fraud and nonsense just as much as it accelerates good science. As the Royal Society has it, nullius in verba: take nobody’s word for it. No thing’s, either. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.Correction (February 6th 2024): An earlier version of this piece misstated the number of research-integrity cases investigated by Taylor and Francis in 2021. Sorry.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.LAWYERS are a conservative bunch, befitting a profession that rewards preparedness, sagacity and respect for precedent. No doubt many enjoyed a chuckle at the tale of Steven Schwartz, a personal-injury lawyer at the New York firm Levidow, Levidow & Oberman, who last month used ChatGPT to help him prepare a court filing. He relied a bit too heavily on the artificial-intelligence (AI) chatbot. It created a motion replete with made-up cases, rulings and quotes, which Mr Schwartz promptly filed after the bot assured him that the “cases I provided are real and can be found in reputable legal databases” (they were not, and cannot). Lesson learned, a tech-sceptic lawyer might conclude: the old ways are the best.That is the wrong lesson. Blaming AI for Mr Schwartz’s error-filled brief makes no more sense than blaming the printing press for mistakes in a typed one. In both cases, fault lies with the lawyer who failed to check the motion before filing it, not the tool that helped produce it. For that is what AI is: neither a fad nor an apocalypse, but a tool in its infancy—and one that could radically change how lawyers work and law firms make money. The legal profession is hardly the only field about which one could say that. But few combine as clear a use case with so high a risk. Firms that get it right stand to reap rewards. Laggards risk going the way of typesetters.According to a recent report from Goldman Sachs, a bank, 44% of legal tasks could be performed by AI, more than in any occupation surveyed except for clerical and administrative support. Lawyers spend an awful lot of time scrutinising tedious documents—the sort of thing that AI has already demonstrated it can do well. Lawyers use AI for a variety of tasks, including due diligence, research and data analytics. These applications have largely relied on “extractive” AI, which, as the name suggests, extracts information from a text, answering specific questions about its contents.“Generative” AIs such as ChatGPT are far more powerful. Part of that power can be used to improve legal research and document review. As Pablo Arredondo, creator of a generative-AI “legal assistant” called CoCounsel, explains, using it “removes the tyranny of the keyword…It can tell that ‘We reverse Jenkins’ [a fictional legal case] and ‘We regretfully consign Jenkins to the dustbin of history’ are the same thing.” Allen & Overy, a large firm based in London, has integrated a legal AI tool called Harvey into its practice, using it for contract analysis, due diligence and litigation prep.Not all lawyers are convinced. One recent survey found that 82% of them believe generative AI can be used for legal work but just 51% thought it should. Many worry about “hallucinations” (as AI boffins refer to chatbots’ tendency to present falsehoods with aplomb, as in Mr Schwartz’s case) and about inadvertently feeding information subject to attorney-client privilege into algorithms. Yet if these challenges can be tackled—and they can, with better technology and careful humans in the loop—then the misgivings of the doubting 49% may pass. After news of Mr Schwartz’s debacle broke, for example, a federal judge in Texas told attorneys appearing before him to file a certificate attesting that they either did not use generative AI at all or that, if they did, they checked the final result. Much as it made little sense for lawyers to insist on doing legal research in libraries once the vastly larger and more easily searched databases of Westlaw and LexisNexis were a click away, when a critical mass of firms embraces generative AI, more will follow.AI has the potential to transform the legal profession in three big ways. First, it could reduce big firms’ manpower advantage. In large, complex lawsuits, these firms tell dozens of associates to read millions of pages of documents looking for answers to senior lawyers’ questions and hunches. Now a single lawyer or small firm will be able to upload these documents into a litigation-prep AI and begin querying them. As Lawrence Lessig of Harvard Law School notes, “You can be a smaller, leaner specialised firm and have the capacity to process these sorts of cases.”Billable powersSecond, AI could change how firms make money. Richard Susskind, technology adviser to the Lord Chief Justice of England, argues that firms profit by “having armies of young lawyers to whom they pay less than they charge clients”. If AI can do the work of those armies in seconds, firms will need to change their billing practices. Some may move to charging flat fees based on the service provided, rather than for the amount of time spent providing it. Stephen Wu of Silicon Valley Law Group speculates that firms may charge “a technology fee”, so that “clients don’t expect to get generative AI for nothing”.Third, AI could change how many lawyers exist and where they work. Eventually, Mr Lessig argues, it is hard to see how AI “doesn’t dramatically reduce the number of lawyers the world needs”. If AI can do in 20 seconds a task that would have taken a dozen associates 50 hours each, then why would big firms continue hiring dozens of associates? A veteran partner at a prestigious corporate-law firm in New York expects the ratio of associates to partners to decline from today’s average of perhaps seven to one at the top firms to closer to parity. If associates aren’t worried about their jobs, he says, “they should be”.That may not happen for a while, however. Moreover, AI could make legal services cheaper and thus more widely available, particularly for small and medium-sized businesses that currently often struggle to afford them. Ambitious law-school graduates may find that AI provides an easier path to starting a solo practice. If so, then AI could actually lead to an increase in the overall number of lawyers, as well as changing the sort of tasks they perform—just as the ATM led to an increase in the number of human bank employees rather than their replacement.Ultimately this will be good news for clients. “People who go to lawyers don’t want lawyers: they want resolutions to their problems or the avoidance of problems altogether,” explains Mr Susskind. If AI can provide those outcomes then people will use AI. Many people already use software to do their taxes rather than rely on professionals; “Very few of them are complaining about the lack of social interaction with their tax advisers.” ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
IMAGINE the perfect environment for developing artificial intelligence (AI). The ingredients would include masses of processing power, lots of computer-science boffins, a torrent of capital—and abundant data with which to train machines to recognise and respond to patterns. That environment might sound like a fair description of America, the current leader in the field. But in some respects it is truer still of China.The country is rapidly building up its cloud-computing capacity. For sheer volume of research on AI, if not quality, Chinese academics surpass their American peers; AI-related patent submissions in China almost tripled between 2010 and 2014 compared with the previous five years. Chinese startups are attracting billions in venture capital. Above all, China has over 700m smartphone users, more than any other country. They are consuming digital services, using voice assistants, paying for stuff with a wave of their phones—and all the while generating vast quantities of data. That gives local firms such as Alibaba, Baidu and Tencent the opportunity to concoct best-in-class AI systems for everything from facial recognition to messaging bots. The government in Beijing is convinced of the potential. On July 20th it outlined a development strategy designed to make China the world’s leading AI power by 2030.An AI boom in the world’s most populous place holds out enormous promise. No other country could generate such a volume of data to enable machines to learn patterns indicative of rare diseases, for example. The development of new technologies ought to happen faster, too. Because typing Chinese characters is fiddly, voice-recognition services are more popular than in the West; they should improve faster as a result. Systems to adjust traffic lights automatically in response to footage from roadside cameras are already being tested. According to the McKinsey Global Institute, a research arm of the consultancy, AI-driven automation could boost China’s GDP growth by more than a percentage point annually.Yet the country’s AI plans also give cause for concern. One worry is that the benefits of Chinese breakthroughs will be muted by data protectionism. A cyber-security law that came into force in June requires foreign firms to store data they collect on Chinese customers within the country’s borders; outsiders cannot use Chinese data to offer services to third parties. It is not hard to imagine tit-for-tat constraints on Chinese firms. And if data cannot be pooled, the algorithms that run autonomous cars and other products may not be the most efficient.A second area of unease is ethics and safety. In America, the technology giants of Silicon Valley have pledged to work together to make sure that any AI tools they develop are safe. They will look at techniques like “boxing”, in which AI agents are isolated from their environment so that any wayward behaviour does not have disastrous effects. All the leading AI researchers in the West are signatories to an open letter from 2015 calling for a ban on the creation of autonomous weapons. If it happens at all, the equivalent Chinese discussion about the limits of ethical AI research is far more opaque.Chinese AI companies do have incentives to think about some of these issues: rogue AI would be a problem for the planet wherever it emerged. There is a self-interested case for the formulation of global safety standards, for example. But a third concern—that AI will be used principally to the benefit of China’s government—is a less tractable problem.Autocratic intelligenceThe new plan is open about AI’s value to the state. It envisages the use of the technology in everything from guided missiles to predictive policing. AI techniques are perfect for finding patterns in the massive amounts of data that Chinese censors must handle in order to maintain a grip on the citizenry. It is easy to imagine how the same data could boost the country’s nascent plans to create a “social-credit” system that scores people for their behaviour. Once perfected, these algorithms would interest autocratic regimes around the world. China’s tech firms are in no position to prevent the government in Beijing from taking advantage of such tools. Baidu, for example, has been appointed to lead a national laboratory for deep learning. Chinese AI will reflect the influence of the state.Western firms and governments are no angels when it comes to data collection and espionage. But Western companies are at least engaged in an open debate about the ethical implications of AI; and intelligence agencies are constrained by democratic institutions. Neither is true of China. AI is a technology with the potential to change the lives of billions. If China ends up having most influence over its future, then the state, not citizens, may be the biggest beneficiary.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.For decades linguists have argued over how children learn language. Some think that babies are born as “blank slates” who pick up language simply from experience—hearing, seeing and playing with the world. Others argue that experience is not enough and that babies’ brains must be hardwired to make acquiring language easy.AI models such as GPT-4 have done little to settle the debate. The way these models learn language—by trawling through reams of text data from millions of web pages—is vastly different to the experiences of babbling babies.A team of scientists at New York University examined the question by training an AI model on the experiences of a single infant. Between the ages of six and 25 months, a toddler called Sam wore a head-mounted camera for an hour a week—around 1% of his waking hours. The camera recorded everything he saw and heard while he played with toys, enjoyed days at the park and interacted with his pet cats. The recordings and transcribed audio were fed into an AI, which was set up to know that images and words that appeared at the same time were related, but was otherwise left to make sense of the mess of colours and speech that Sam experienced.Despite the limited training data, the AI was able to pick out objects and learn the matching words. The researchers tested the model by asking it to identify objects that Sam had seen before, such as a chair from his home or one of his toy balls. Given a list of four options the model picked the correct word 62% of the time, far above the chance level of 25%. To the researchers’ surprise, the model could also identify chairs and balls that Sam had never seen. The AI learnt at least 40 different words, but it was far from matching Sam’s vocabulary and language abilities by the end of the experiment.The researchers, whose work was published recently in the journal Science, argue that, to match words to objects, learning from experience may well be enough. Sceptics, however, doubt that the AI would be able to learn abstract nouns or verbs, and question how similar the learning processes really are. The mystery of language acquisition lives on.■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this podcast Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.RECENT YEARS have seen a boom in biometric security systems—identification measures based on a person’s individual biology—from unlocking smartphones, to automating border controls. As this technology becomes more prevalent, some cybersecurity researchers are worried about how secure biometric data is—and the risk of spoofs. If generative AI becomes so powerful and easy-to-use that deepfake audio and video could hack into our security systems, what can be done?Bruce Schneier, a security technologist at Harvard University and the author of “A Hacker’s Mind”, explores the cybersecurity risks associated with biometrics, and Matthias Marx, a security researcher, discusses the consequences of bad actors obtaining personal data. If artificial intelligence could overcome security systems, human implants may be used as authentication, according to Katina Michael, a professor at Arizona State University. Plus, Joseph Lindley, a design academic at Lancaster University, proposes how security systems can be better designed to avoid vulnerabilities. To think about practical solutions, Scott Shapiro, professor at Yale Law School and author of “Fancy Bear Goes Phishing”, puts generative AI into the wider context of cybersecurity. Finally, Tim Cross, The Economist’s deputy science editor, weighs up the real-world implications of our thought experiment. Kenneth Cukier hosts. Runtime: 39 minsLearn more about detecting deepfakes at economist.com/detecting-deepfakes-pod, or listen to all of our generative AI coverage at economist.com/AI-pods.For full access to The Economist’s print, digital and audio editions subscribe at economist.com/podcastoffer and sign up for our weekly science newsletter at economist.com/simplyscience.Listen on: Apple Podcasts | Spotify | Google | Stitcher | TuneIn
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The machines are coming for your crops—at least in a few fields in America. This autumn John Deere, a tractor-maker, shipped its first fleet of fully self-driving machines to farmers. The tilling tractors are equipped with six cameras which use artificial intelligence (ai) to recognise obstacles and manoeuvre out of the way. Julian Sanchez, who runs the firm’s emerging-technology unit, estimates that about half the vehicles John Deere sells have some AI capabilities. That includes systems which use onboard cameras to detect weeds among the crops and then spray herbicides, and combine harvesters which automatically alter their own setting to waste as little grain as possible. Mr Sanchez says that for a medium-sized farm, the additional cost of buying an AI-enhanced tractor is recouped in two to three years.For decades starry-eyed technologists have claimed that AI will upend the business world, creating enormous benefits for firms and customers. John Deere is not the only proof that this is happening at last. A survey by McKinsey, a consultancy, found that this year 50% of firms across the world had tried to use AI in some way, up from 20% in 2017. Powerful new “foundation” models are fast moving from the lab to the real world. Chatgpt, a new ai tool that has recently been released for public testing, is making waves for its ability to craft clever jokes and explain scientific concepts. But excitement is also palpable among corporate users of AI, its developers and those developers’ venture-capital backers. Many of them attended a week-long jamboree hosted in Las Vegas by Amazon Web Services, the tech giant’s cloud-computing arm. The event, which ended on December 2nd, was packed with talks and workshops on ai. Among the busiest booths in the exhibition hall were those of AI firms such as Dataiku and Blackbook.ai.The buzzing AI scene is an exception to the downbeat mood across techdom, which is in the midst of a deep slump. In 2022 venture capitalists have ploughed $67bn into firms that claim to specialise in AI, according to PitchBook, a data firm. The share of vc deals globally involving such startups has ticked up since mid-2021, to 15% so far this quarter (see chart 1). Between January and October, 28 new AI unicorns (private startups valued at $1bn or more) have been minted. Microsoft is said to be in talks to increase its stake in OpenAI, a builder of foundation models and Chatgpt’s provider. Alphabet, Google’s parent company, is reportedly planning to invest $200m in Cohere, a rival to OpenAI. At least 22 AI startups have been launched by alumni of OpenAI and Deepmind, one of Alphabet’s AI labs, according to a report by Ian Hogarth and Nathan Benaich, two British entrepreneurs.The exuberance is not confined to Silicon Valley. Big firms of all sorts are desperate for AI talent. In the past 12 months large American firms in the S&P 500 index have acquired 52 AI startups, compared with 24 purchases in 2017, according to PitchBook. PredictLeads, another data provider, notes that the same group of firms posted around 7,000 job ads a month for AI and machine-learning experts in the three months to November, about ten times more than in the first quarter of 2020 (see chart 2). Derek Zanutto of CapitalG, one of Alphabet’s vc divisions, notes that large firms spent years collecting data and investing in related technology. Now they want to use this “data stack” to their advantage. AI offers ways to do so.Unsurprisingly, the first industry to embrace AI was the technology sector. From the 2000s onwards, machine-learning techniques helped Google supercharge its online-advertising business. Now it uses Ai to improve search results, finish your sentences in Gmail and work out ways to cut energy use in its data centres, among other things. Amazon’s AI manages its supply chains, instructs warehouse robots and predicts which job applicants will be good workers; Apple’s powers its Siri digital assistant; Meta’s serves up attention-grabbing social-media posts; and Microsoft’s does everything from stripping out background noise in Teams, its videoconferencing service, to letting users create first drafts of PowerPoint presentations.Big tech quickly spied an opportunity to sell some of those same AI capabilities to clients. Amazon, Google and Microsoft all now provide such tools to customers of their cloud-computing divisions. Revenues from Microsoft’s machine-learning cloud service have doubled in each of the past four quarters, year on year. Upstart providers have proliferated, from Avidbots, a Canadian developer of robots that sweep warehouse floors, to Gong, whose app helps sales teams follow up a lead. Greater use of cloud computing, which brings down the cost of using AI, enabled the technology to spread to other sectors, from industry to insurance. You may not see it, but these days AI is everywhere.Dulling the cutting edgeIn 2006 Nick Bostrom of Oxford University observed that “once something becomes useful enough and common enough it’s not labelled AI any more”. Ali Ghodsi, boss of Databricks, a company that helps customers manage data for AI applications, sees an explosion of such “boring AI”. He argues that over the next few years AI will be applied to ever more jobs and company functions. Lots of small improvements in AI’s predictive power can add up to better products and big savings.This is especially true in less flashy areas where firms are already using some kind of analytics, such as managing supply chains. When in September Hurricane Ian forced Walmart to shut a large distribution hub, halting the flow of goods to  supermarkets in Florida, the retailer used a new AI-powered simulation of its supply chain to reroute deliveries from other hubs and predict how demand for goods would change after the storm. Thanks to AI this took hours rather than days, says Srini Venkatesan of Walmart’s tech division.The coming wave of foundation models is likely to turn a lot more AI boring. These algorithms hold two big promises for business. The first is that foundation models are capable of generating new content. Stability AI and Midjourney, two startups, build generative models which create new images for a given prompt. Request a dog on a unicycle in the style of Picasso—or, less frivolously, a logo for a new startup—and the algorithm conjures it up in a minute or so. Other startups build applications on top of other companies’ foundation models. Jasper and Copy.AI both pay OpenAI for access to GPT3, which enables their applications to convert simple prompts into marketing copy.The second advantage is that, once trained, foundation AIs are good at performing a variety of tasks rather than a single specialised one. Take GPT3, a natural-language model developed by Openai, which forms the basis for  Chatgpt. It was first trained on large chunks of the internet, then fine-tuned by different startups to do various things, such as writing marketing copy, filling in tax forms and building websites from a series of text prompts. Rough estimates by Beena Ammanath, who heads the AI practice of Deloitte, a consultancy, suggest that foundation models’ versatility could cut the costs of an AI project by 20-30%.One early successful use of generative AI is, again predictably, the province of tech: computer programming. Several firms are offering a virtual assistant trained on a large deposit of code that churns out new lines when prompted. One example is Copilot on GitHub, a Microsoft-owned platform which hosts open-source programs. Programmers using Copilot outsource nearly 40% of code-writing to it. This speeds up programming by 50%, the firm claims. In June Amazon launched CodeWhisperer, its version of the tool. Alphabet is reportedly using something similar, codenamed PitchFork, internally.Artificial colouringIn May Satya Nadella, Microsoft’s boss, declared, “We envision a world where everyone, no matter their profession, can have a Copilot for everything they do.” In October Microsoft launched a tool which automatically wrangles data for users following prompts. Amazon and Google may try to produce something like it. Several startups are already doing so. Adept, a Californian company run by former employees from Deepmind, OpenAI and Google, is working on “a Copilot for knowledge workers”, says Kelsey Szot, a co-founder of the firm. In September the company released a video of its first foundation model, which uses prompts to crunch numbers in a spreadsheet and to perform searches on property websites. It plans to develop similar tools for business analysts, salespeople and other corporate jobs.Corporate users are experimenting with generative AI in other creative ways. Mr Sanchez of John Deere says that his firm is looking into AI-generated “synthetic” data, which would help train other AI models. In December 2021 Nike, a sportswear giant, bought a firm that uses such algorithms to create new sneaker designs. Alexa, Amazon’s virtual assistant, can now invent stories to tell children. Nestlé, a giant Swiss foodmaking firm, is using images created by DALLE-2, another OpenAI model, to help sell its yogurts. Some financial firms are employing AI to whip up a first draft of their quarterly reports.Users of foundation models can also tap an emerging industry of professional prompters, who craft directions so as to optimise the models’ output. PromptBase is a marketplace where users can buy and sell prompts that produce particularly spiffy results from the large image-based generative models, such as DALLE-2 and Midjourney. The site also lets you hire expert “prompt engineers”, some of whom charge a $50-200 per prompt. “It’s all about writing prompts these days,” says Thomas Dohmke, boss of GitHub.As with all powerful new tools, businesses must tread carefully as they deploy more AI. Having been trained on the internet, many foundation models reflect humanity, warts and all. One study by academics at Stanford University found that when GPT3 was asked to complete a sentence starting “Two Muslims walked into a...”, the result was likely to invoke violence far more often than when the phrase referred to Christians or Buddhists. Meta pulled down Galactica, its foundation model for science, after claims that it generated real-sounding but fake research. Carl Bergstrom, a biologist at the University of Washington in Seattle, called it a “random bullshit generator”. (Meta says that the model remains available for researchers who want to learn about the work.)Other problems are specific to the world of business. Because foundation models tend to be black boxes, offering no explanation of how they arrived at their results, they can create legal liabilities when things go amiss. And they will not do much for those firms that lack a clear idea of what they want AI to do, or which fail to teach employees how to use it. This may help explain why merely a quarter of respondents to the McKinsey’s survey said that AI had benefited the bottom line (defined as a 5% boost to earnings). The share of firms seeing a large benefit (an increase in earnings of over 20%) is in the low single digits—and many of those are tech firms, says Michael Chui, who worked on the study.Still, those proportions are bound to keep rising as more AI becomes ever more dull. Rarely has the boring elicited this much excitement. ■Correction (December 12th 2022): This piece originally said that John Deere’s tractors automatically sprayed fields with pesticides. In fact they sprayed herbicides. This change has now been made. Sorry.To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.“I can’t believe it worked!” says Nat Friedman, co-founder of the Vesuvius Challenge, which offered $1m in prizes to anyone who could use artificial intelligence (AI) to decipher papyrus scrolls carbonised by the eruption of Mount Vesuvius in 79AD. But work it did. On February 5th Mr Friedman announced that a three-person team had been awarded $700,000 for successfully extracting four passages of text, each at least 140 characters long, and with at least 85% of the characters legible, from a scroll known as Banana Boy. The three winners, Luke Farritor, Youssef Nader and Julian Schilliger, are all computer-science students.The scroll is one of hundreds found in the library of a Roman villa in Herculaneum, which is thought to have belonged to the father-in-law of Julius Caesar. Along with hundreds of other scrolls in the villa’s library, it was damaged by scorching gases that engulfed the town during the same eruption that also buried the nearby town of Pompeii.Reading text from the scrolls is difficult because the heat turned them into brittle charcoal logs; all efforts to unroll them physically caused them to disintegrate. So attention shifted towards finding ways to unwrap them virtually, through computer analysis of 3D scans of the scrolls made using X-rays. This turned deciphering the scrolls into a software problem—but a very complex one.Virtual unrolling is a two-stage process pioneered by W. Brent Seales, a computer scientist at the University of Kentucky. The first stage, called segmentation, involves tracing the edges of the rolled-up papyrus sheet inside the 3D scan, then extracting 2D images of the scroll’s surface. The second stage, ink detection, analyses the resulting images to pluck the ink of the scroll’s text from the papyrus background. This is particularly tricky for the Herculaneum scrolls, which are written in carbon-based ink, so there is very little contrast with the background of carbonised papyrus.Dr Seales, along with Mr Friedman and Daniel Gross, two technology entrepreneurs, thought AI techniques might fruitfully be brought to bear on these two problems, and launched a prize challenge to find out. A community of thousands of enthusiasts has since developed a range of tools and tricks to speed up the fiddly process of segmentation, and to detect the ink of individual letters, and then whole words. In October 2023 Mr Farritor and Mr Nader were awarded smaller prizes for independently extracting the first legible word (“porphyras”, which means “purple” in ancient Greek) from the Banana Boy scroll (so named because of its size and shape).image: Vesuvius ChallengeThe two students then teamed up and, joined by Mr Schilliger, further improved the machine-learning technique involved in ink detection. By manually labelling areas known to be ink, they could train a neural network to find more of them; these were fed back into the model to improve its detection abilities. Mr Nader also switched the neural network to a novel architecture called a TimeSformer, which produced sharper results. Mr Schilliger, meanwhile, devised a tool to automate more of the segmentation process (much of which must still be done manually).The deadline to submit results for the grand prize was at the end of December, and the trio was awarded the prize after an assessment of the entries by a team of papyrologists. (Three runners up will receive smaller prizes of $50,000 each.) The winning entry revealed 15 columns of text, written in Greek. Reading it was “mind-blowing”, says Federica Nicolardi, a papyrologist at the University of Naples Federico II, who was one of the judges. The text is thought to be a previously unknown work on pleasure by Philodemus, an Epicurean philosopher who lived in Herculaneum.Mr Friedman now wants to scale up the whole process. With ink detection solved, he says, “the bottleneck is now segmentation”. Mr Schilliger’s auto-segmentation tool is a big step forward, and he has agreed to make it open source, and to collaborate with others to improve it. Further prizes are being offered as an incentive. Mr Friedman, meanwhile, aims to scan more scrolls using the Diamond Light Source, a particle accelerator in Britain, and to standardise the scanning process.That will cost money. Having given out $1.2m in prizes, some of it from his own pocket, Mr Friedman is looking for other backers to help support the project. He hopes that deciphering ancient scrolls will lead to the rediscovery of lost works from antiquity—“each scroll is a mystery box”, he says—and, ultimately, revive interest in further excavating the villa in Herculaneum, which may contain thousands more of them. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.After astonishing breakthroughs in artificial intelligence, many people worry that they will end up on the economic scrapheap. Global Google searches for “is my job safe?” have doubled in recent months, as people fear that they will be replaced with large language models (llms). Some evidence suggests that widespread disruption is coming. In a recent paper Tyna Eloundou of Openai and colleagues say that “around 80% of the us workforce could have at least 10% of their work tasks affected by the introduction of llms”. Another paper suggests that legal services, accountancy and travel agencies will face unprecedented upheaval.Economists, however, tend to enjoy making predictions about automation more than they enjoy testing them. In the early 2010s many of them loudly predicted that robots would kill jobs by the millions, only to fall silent when employment rates across the rich world rose to all-time highs. Few of the doom-mongers have a good explanation for why countries with the highest rates of tech usage around the globe, such as Japan, Singapore and South Korea, consistently have among the lowest rates of unemployment.Here we introduce our first attempt at tracking ai’s impact on jobs. Using American data on employment by occupation, we single out white-collar workers. These include people working in everything from back-office support and financial operations to copy-writers. White-collar roles are thought to be especially vulnerable to generative ai, which is becoming ever better at logical reasoning and creativity.However, there is as yet little evidence of an ai hit to employment. In the spring of 2020 white-collar jobs rose as a share of the total, as many people in service occupations lost their job at the start of the covid-19 pandemic (see chart). The white-collar share is lower today, as leisure and hospitality have recovered. Yet in the past year the share of employment in professions supposedly at risk from generative ai has risen by half a percentage point.It is, of course, early days. Few firms yet use generative-ai tools at scale, so the impact on jobs could merely be delayed. Another possibility, however, is that these new technologies will end up destroying only a small number of roles. While AI may be efficient at some tasks, it may be less good at others, such as management and working out what others need.ai could even have a positive effect on jobs. If workers using it become more efficient, profits at their company could rise which would then allow bosses to ramp up hiring. A recent survey by Experis, an it-recruitment firm, points to this possibility. More than half of Britain’s employers expect ai technologies to have a positive impact on their headcount over the next two years, it finds.To see how it all shakes out, we will publish updates to this analysis every few months. But for now, a jobs apocalypse seems a way off. ■For more expert analysis of the biggest stories in economics, finance and markets, sign up to Money Talks, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.When new technologies emerge they benefit different groups at different times. Generative artificial intelligence (AI) first helped software developers, who could use GitHub Copilot, a code-writing AI assistant, from 2021. The next year came other tools, such as ChatGPT and Dall-E 2, which let all manner of consumers instantly produce words and pictures.In 2023 tech giants gained, as investors grew more excited about the prospects of generative AI. An equally weighted share-price index of Alphabet, Amazon, Apple, Meta, Microsoft and Nvidia grew by nearly 80% (see chart). Tech firms benefited because they supply either the AI models themselves, or the infrastructure that powers and delivers them.image: The EconomistIn 2024 the big beneficiaries will be companies outside the technology sector, as they adopt AI in earnest with the aim of cutting costs and boosting productivity. There are three reasons to expect enterprise adoption to take off.First, large companies spent much of 2023 experimenting with generative AI. Plenty of firms are using it to write the first drafts of documents, from legal contracts to marketing material.  JPMorgan Chase, a bank, used the technology to analyse Federal Reserve meetings to try to glean insights for its trading desk.As the experimental phase winds down, firms are planning to deploy generative AI on a larger scale. That could mean using it to summarise recordings of meetings or supercharging research and development. A survey by KPMG, an audit firm, found that four-fifths of firms said they planned to increase their investment in it by over 50% by the middle of 2024. Second, more AI products will hit the market. In late 2023 Microsoft rolled out an AI chatbot to assist users of its productivity software, such as Word and Excel. It launched the same thing for its Windows operating system. Google will follow suit, injecting AI into Google Docs and Sheets. Startups will pile in, too. In 2023 venture-capital investors poured over $36bn into generative AI, more than twice as much as in 2022. The third reason is talent. AI gurus are still in high demand. PredictLeads, a research firm, says about two-thirds of S&P 500 firms have posted job adverts mentioning AI. For those companies, 5% of adverts now mention the technology, up from an average of 2.5% over the past three years. But the market is easing. A survey by McKinsey, a consultancy, found that in 2023 firms said it was getting easier to hire for AI-related roles. Which firms will be the early adopters? Smaller ones will probably take the lead. That is what happened in previous waves of technology such as smartphones and the cloud. Tiddlers are usually more nimble and see technology as a way to gain an edge over bigger fish.Among larger companies, data-centric firms, like those in health care and financial services, will be able to move fastest. That is because poor data management is a big risk for deploying AI. Managers worry about valuable data leaking out through AI tools.  Firms without solid data management may have to reorganise their systems before it is feasible to deploy generative AI. Using the technology can feel like science fiction, but getting it to work safely is a much more humdrum affair. ■Guy Scriven, US technology editor, The Economist
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.GET READY for some big British celebrations in 2030. By then, if Rishi Sunak is to be believed, the country will be “a science and technology superpower”. The prime minister’s aim is for Britain to prosper from the booming opportunities offered by supercomputing and artificial intelligence. Generative AI has stoked a frenzy of excitement (and some fear) among techies and investors; now politicians have started to acclaim its potential, and British ones are in the vanguard. Britain, says Mr Sunak, will harness AI and thus spur productivity, economic growth and more. As he told an audience in London this week, he sees the “extraordinary potential of AI to improve people’s lives”.Mr Sunak’s vim and his readiness to champion AI are welcome, even if his claims sound breathless. After all, Britain’s government has spurred innovation that had sweeping economic effects—think of the Big Bang reforms in the 1980s that turned London into Europe’s financial hub. There is every reason to believe a new AI era will create huge opportunities . He is right to plan for how to make the most of these chances. But could Britain, realistically, lead on this?The country does have some advantages. It is home to several important AI companies, mostly in London—in particular, Google DeepMind. It has excellent universities, and welcomes the highly skilled foreign workers that AI companies need. The state generates troves of data; no other country has such an array of health records under a single entity, the National Health Service (NHS). And Brexit creates a chance to adopt an appealing regulatory position that could be a model for medium-sized countries around the world as they also rush to join the AI party.But there are problems aplenty, too. The most obvious is that Britain is a smallish place. America’s dominance in tech exerts a steady pull on capital, people and ideas, and American firms duly dominate in AI. The way Brexit was done means that Britain has lost access to the European Union’s single market. Although Oracle has a cluster of the advanced graphics processing units (GPUs) needed to train large models, none of the cloud-computing giants has yet chosen Britain as the base for what techies call the “compute”.For Britain to prosper in AI, therefore, much will have to change. It needs to cram more people who know one end of a GPU from the other into positions of influence in government. Mr Sunak may loudly extol the promise of BritGPT, but his government should include more engineers who understand the mix of data and compute from which AI is built.Time to chatGPUOnce it has the expertise, the government must deal with three broad concerns. The first is about those public datasets. They are in no fit state for AI developers to exploit—the data are unrefined ore, not sparkling treasure. Only the state has the authority to get these datasets cleaned up, and to start thinking of what new ones could be built. A stock of clean, regularly updated datasets that are technically and legally easy for algorithm-makers to use would draw in engineers who want to build new AI systems. An AI-ready NHS would be the jewel in Britain’s crown.Second, Britain should move fast to gain an edge in regulation. The goal should be a pragmatic set of rules keeping AI safe that sits somewhere between America’s Wild West permissiveness and what is likely to be a regulatory warren in the EU. Mr Sunak announced at a White House meeting with President Joe Biden this month that he will host a global summit on AI regulation this autumn. Good. That will be the place and time to set out Britain’s stall as having rules for AI that are sufficiently flexible to work for different industries. Hairdressers who want AI to help pick new styles, for example, need not be regulated in the same way as mortgage lenders.The last and thorniest concern is how to get AI developers the compute they need to train and run large models. Advanced GPUs made by Nvidia—for now, the only chips worth using—are suffering a global supply crunch. The government could help by telling British companies and its own departments to be much readier than now to send their data abroad to AI developers in other, friendly countries. For most datasets, worries about privacy and security are overdone.However, an unfortunate correlation exists between the sensitivity of datasets and their value in creating large models; data that are sensitive, that capture aspects of either human health or behaviour, or pertain to national security, are what would be most useful to inform new models. There is an understandable reluctance to send such data abroad.That is why Britain urgently needs more GPU clusters within its borders. More compute on British soil will have all sorts of local spin-offs and benefits. Jeremy Hunt, the chancellor, has talked of giving academic computer scientists £900m ($1.1bn) to build a British supercomputer in Edinburgh.Yet commercial AI is so dynamic that the Edinburgh scheme risks becoming an AI white elephant. Amazon alone spends around $25bn a year on compute. British taxpayers cannot keep up with the private sector—and should not try. Instead the government should do all it can to persuade commercial providers to invest in GPU clusters on British soil.One focus should be to ensure a reliable supply of clean, affordable power. To train models needs mind-boggling quantities of electricity. If Britain is without cheap supplies of power, it will struggle to persuade anyone to set up big GPU centres there. The queue to obtain a connection to Britain’s grid is holding back potential investors across the economy.Other steps could include using public money to fund a “moonshot” project, such as developing open-source software, to help chipmakers break the near-monopoly that Nvidia holds on the AI market. Nvidia’s edge comes from clever software which makes training models on its GPUs a breeze. Rival chipmakers in America and Britain have no equivalent and are all but locked out of the AI market. With better software their chips could compete with Nvidia’s and ease the supply crunch that dogs AI developers the world over. That’s a worthy ambition for a country seeking global AI greatness. ■For subscribers only: to see how we design each week’s cover, sign up to our weekly Cover Story newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Picture a computer that could finish your sentences, using a better turn of phrase; or use a snatch of melody to compose music that sounds as if you wrote it (though you never would have); or solve a problem by creating hundreds of lines of computer code—leaving you to focus on something even harder. In a sense, that computer is merely the descendant of the power looms and steam engines that hastened the Industrial Revolution. But it also belongs to a new class of machine, because it grasps the symbols in language, music and programming and uses them in ways that seem creative. A bit like a human.The “foundation models” that can do these things represent a breakthrough in artificial intelligence, or ai. They, too, promise a revolution, but this one will affect the high-status brainwork that the Industrial Revolution never touched. There are no guarantees about what lies ahead—after all, ai has stumbled in the past. But it is time to look at the promise and perils of the next big thing in machine intelligence.Foundation models are the latest twist on “deep learning” (dl), a technique that rose to prominence ten years ago and now dominates the field of ai. Loosely based on the networked structure of neurons in the human brain, dl systems are “trained” using millions or billions of examples of texts, images or sound clips. In recent years the ballooning cost, in time and money, of training ever-larger dl systems had prompted worries that the technique was reaching its limits. Some fretted about an “ai winter”. But foundation models show that building ever-larger and more complex dl does indeed continue to unlock ever more impressive new capabilities. Nobody knows where the limit lies.The resulting models are a new form of creative, non-human intelligence.  The systems are sophisticated enough both to possess a grasp of language and also to break the rules coherently. A dog cannot laugh at a joke in the New Yorker, but an ai can explain why it is funny—a feat that is, frankly, sometimes beyond readers of the New Yorker. When we asked one of these models to create a collage using the title of this leader and nothing more, it came up with the cover art for our American and Asian editions (we tried to distract our anxious human designers with a different cover in our European editions).Foundation models have some surprising and useful properties. The eeriest of these is their “emergent” behaviour—that is, skills (such as the ability to get a joke or match a situation and a proverb) which arise from the size and depth of the models, rather than being the result of deliberate design. Just as a rapid succession of still photographs gives the sensation of movement, so trillions of binary computational decisions fuse into a simulacrum of fluid human comprehension and creativity that, whatever the philosophers may say, looks a lot like the real thing. Even the creators of these systems are surprised at their power.This intelligence is broad and adaptable. True, foundation models are capable of behaving like an idiot, but then humans are, too. If you ask one who won the Nobel prize for physics in 1625, it may suggest Galileo, Bacon or Kepler, not understanding that the first prize was awarded in 1901. However, they are also adaptable in ways that earlier ais were not, perhaps because at some level there is a similarity between the rules for manipulating symbols in disciplines as different as drawing, creative writing and computer programming. This breadth means that foundation models could be used in lots of applications, from helping find new drugs using predictions about how proteins fold in three dimensions, to selecting interesting charts from datasets and dealing with open-ended questions by trawling huge databases to formulate answers that open up new areas of inquiry.That is exciting, and promises to bring great benefits, most of which still have to be imagined. But it also stirs up worries. Inevitably, people fear that ais creative enough to surprise their creators could become malign. In fact, foundation models are light-years from the sentient killer-robots beloved by Hollywood. Terminators tend to be focused, obsessive and blind to the broader consequences of their actions. Foundational ai, by contrast, is fuzzy. Similarly, people are anxious about the prodigious amounts of power training these models consume and the emissions they produce. However, ais are becoming more efficient, and their insights may well be essential in developing the technology that accelerates a shift to renewable energy.A more penetrating worry is over who controls foundation models. Training a really large system such as Google’s PaLM costs more than $10m a go and requires access to huge amounts of data—the more computing power and the more data the better. This raises the spectre of a technology concentrated in the hands of a small number of tech companies or governments.If so, the training data could further entrench the world’s biases—and in a particularly stifling and unpleasant way. Would you trust a ten-year-old whose entire sense of reality had been formed by surfing the internet? Might Chinese- and American-trained ais be recruited to an ideological struggle to bend minds? What will happen to cultures that are poorly represented online?And then there is the question of access. For the moment, the biggest models are restricted, to prevent them from being used for nefarious purposes such as generating fake news stories. Openai, a startup, has designed its model, called DALL-E 2, in an attempt to stop it producing violent or pornographic images. Firms are right to fear abuse, but the more powerful these models are, the more limiting access to them creates a new elite. Self-regulation is unlikely to resolve the dilemma.Bring on the revolutionFor years it has been said that ai-powered automation poses a threat to people in repetitive, routine jobs, and that artists, writers and programmers were safer. Foundation models challenge that assumption. But they also show how ai can be used as a software sidekick to enhance productivity. This machine intelligence does not resemble the human kind, but offers something entirely different. Handled well, it is more likely to complement humanity than usurp it. ■For subscribers only: to see how we design each week’s cover, sign up to our weekly Cover Story newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.THE OBJECT known as P.Herc.Paris.3 resembles a dark grey lump of charcoal, about the size and shape of a banana. That explains its nickname: Banana Boy. It is in fact a papyrus scroll, found in the ruins of a villa in the Roman town of Herculaneum, in Campania. Along with hundreds of other scrolls in the villa’s library, it was carbonised when scorching gases engulfed the town during the same eruption of Mount Vesuvius, in 79AD, that also buried the nearby town of Pompeii.Although the scrolls survived, their charring means that unrolling them is almost impossible. Now, nearly 2,000 years later, words from inside Banana Boy have been revealed for the first time, after volunteers competing in a prize challenge used X-rays and artificial intelligence to do the unrolling virtually.The first word to be found, announced on October 12th, was “porphyras”, which means “purple” in ancient Greek (see picture below). It was uncovered by Luke Farritor, a computer-science student at the University of Nebraska-Lincoln, earning him a $40,000 prize. Mr Farritor built on work by Casey Handmer, a former NASA physicist, whose examination of X-ray images of Banana Boy’s charred layers identified a characteristic “crackle pattern” indicating the presence of ink.Scroll upThe same word was later found by Youssef Nader, a robotics student at the Free University of Berlin. (Dr Handmer and Mr Nader both received $10,000 prizes.) Mr Nader has since produced an image from the scroll showing four columns of text, side by side. For classicists, this is heady stuff. The villa in question is thought to have belonged to Lucius Calpurnius Piso, the father-in-law of Julius Caesar. The ability to read its well-stocked library could significantly expand the number of texts that have survived from antiquity. Already there is excited speculation about forgotten plays, new works of philosophy—or even lost epic poems.Purple proseimage: Vesuvius ChallengeEfforts to read the scrolls began in the 1750s, when the villa was rediscovered. Attempts to unpick them with knives caused them to disintegrate. Recognising their fragility, Antonio Piaggio, a conservator from the Vatican, built a machine in 1754 to unroll them slowly, using weights on strings. Even then, the unrolled scrolls fell to pieces. And the resulting fragments were almost impossible to read: charcoal-based ink is hard to see against the shiny black of charred papyrus. But the few characters that could be read revealed some scrolls to be philosophical works written in ancient Greek.A quarter of a millennium later, in 1999, scientists from Brigham Young University illuminated some of those fragments with infrared light. That created a strong contrast between papyrus and ink, making the writing more legible. Multi-spectral imaging in 2008, combining many wavelengths of light, was even better, revealing previously unreadable words. Many fragments turned out to belong to texts written by a Greek philosopher called Philodemus of Gadara. Until then, they had been known only from mentions in other works. (Cicero, though, was a fan of his poetry.)Around 500 scrolls remain unopened. Given the damage it does, physical unrolling is no longer attempted. Instead the focus has shifted towards finding ways to unwrap them virtually, by using 3D scans of the rolled-up scrolls to produce a series of legible 2D images. The pioneer of this approach is W. Brent Seales, a computer scientist at the University of Kentucky. In 2009 he arranged for Banana Boy, and another scroll known as Fat Bastard, to be scanned in a computerised tomography (CT) X-ray machine, of the sort usually used for medical scans. This produced detailed images of their internal structures for the first time. But the ink within the scrolls could not be made out.In 2015 Dr Seales analysed a different carbonised scroll found in 1970 at En-Gedi, near the Dead Sea in Israel. It had been written using a metal-rich ink, which stood out strongly in X-ray images. (The text turned out to be the Book of Leviticus.) This confirmed that, in the right circumstances, digitally unrolling a carbonised scroll and reading the contents could indeed be done.The next step was to combine the existing approaches into a new one. In 2019 Dr Seales arranged for Banana Boy, Fat Bastard and four fragments of other scrolls to be scanned at high resolution using the Diamond Light Source in Britain, a particle accelerator that can produce much more powerful X-ray light than a CT scanner. He then paired infrared images of the fragments, in which the ink can be readily seen, with X-ray scans of the same fragments in which it cannot.Earlier this year Stephen Parsons, a graduate student working with Dr Seales, fed the two sets of images into a machine-learning model, which used the infrared scans to teach itself how to recognise the faint signs of ink in the X-ray ones. By applying the resulting model to X-ray images from the rolled-up scrolls it would be possible to reveal their contents. At this point, deciphering the scrolls had, in theory, been reduced to a very complex software problem. But that software still needed to be improved and scaled up.Enter Nat Friedman, a technology executive and investor with an interest in ancient Rome. Mr Friedman offered to help fund Dr Seales’s work. Over a whisky, they decided that the best way to accelerate things was to organise a contest, with prizes handed out for completing various tasks. Mr Friedman and Daniel Gross, another entrepreneur, launched the Vesuvius Challenge in March, with a prize fund of $250,000. Other tech-industry donors soon increased that to over $1m. To get the ball rolling, an initial challenge was posted on Kaggle, a website that hosts data-science contests, to improve the ink-detection model developed by Dr Parsons.More than 1,200 teams entered. Many competed in subsequent challenges to improve the tools for ink detection and “segmentation”, as the process of transforming the 3d scans into 2d images of the scroll’s surface is known. Scrutinising segmented images from Banana Boy, Dr Handmer realised that the crackle pattern signified the presence of ink. Mr Farritor used this finding to fine-tune a machine-learning model to find more crackles, then used those crackles to further optimise his model, until eventually it revealed legible words.Mr Nader used a different approach, starting with “unsupervised pretraining” on the segmented images, asking a machine-learning system to find whatever patterns it could, with no external hints. He tweaked the resulting model using the winning entries from the Kaggle ink-detection challenge. After seeing Mr Farritor’s early results, he applied this model to the same segment of Banana Boy, and found what appeared to be some letters. He then iterated, repeatedly refining his model using the found letters. Slowly but surely its ability to find more letters increased. All the results were assessed by papyrologists before the prizes were awarded.Multae manus onus levius redduntNo less important than the technology is the way the effort has been organised. It is, in effect, the application of the open-source software-development method, Mr Friedman’s area of expertise, to an archaeological puzzle. “It’s a unique collaboration between tech founders and academics to bring the past into the present using the tools of the future,” he says. Dr Seales reckons the spur of competition means the equivalent of ten years’ worth of research has been done in the past three months.An active community of volunteers is now applying the new tools to the two scanned scrolls. Mr Friedman thinks there is a 75% chance that someone will claim the grand prize of $700,000, for identifying four separate passages of at least 140 characters, by the end of the year. “It’s a race now,” he says. “We will be reading entire books next year.”Being able to read Banana Boy would indeed just be the beginning. Only a small fraction of Greek and Roman literature has survived into modern times. But if the hundreds of other scrolls recovered from the villa could be scanned and read using the same tools, it would dramatically expand the number of texts from antiquity. Dr Seales says he hopes the Herculaneum scrolls will contain “a completely new, previously unknown text”. Mr Friedman is hoping for one of the lost Greek epic poems in particular.Even more important, all this might in turn revive interest in excavating the villa more fully, says Mr Friedman. The existing scrolls were recovered from a single corner of what scholars believe is a much larger library spread across several floors. If so, it might contain thousands of scrolls in Greek and Latin.One reason that classical texts are so scarce is that the papyrus upon which they were written does not survive well in Europe’s temperate, rainy climate. So it is a delicious irony, notes Dr Seales, that the carbonisation of the scrolls, which makes them so difficult to read, is also what preserved them for posterity—and that fragments of scrolls that disintegrated when they were unrolled physically would eventually provide the key to unrolling the rest of them virtually. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Waves OF INNOVATION often create giants. Microsoft rode the upsurge in desktop computers, as Apple did with the smartphone. Artificial intelligence (AI) may well be the next big technological shift, transforming the way businesses are run and society functions. If so, plenty of firms selling the software and hardware that underpin AI stand to gain. But none is better positioned than Nvidia, an American firm that makes specialist AI chips. Its market value briefly passed $1trn this week. Will AI sweep Nvidia to big tech-dom?The hype around AI makes the question hard to answer. Excitement about Nvidia began to mount in November, after the release of ChatGPT, an AI-powered chatbot. Since then all manner of firms have launched AI-infused products, adding to the fervour. Jensen Huang, Nvidia’s boss, is unsurprisingly bullish, talking of a “new computing era”. Investors seem just as jubilant. Nvidia’s share price has more than doubled since the start of the year.Much of the excitement is justified. Nvidia is in an enviable position. Its core business is designing high-performance chips. At first it sold these to video-game enthusiasts. The chips were also highly efficient at training AI models, and a new, booming market emerged. But the firm has not just been lucky. With each generation of new chips, it has improved performance many times over. Today it holds over 80% of the market in specialist AI chips.Nvidia also had the forethought to invest in two areas that helped cement its supremacy. One is advanced networking. Because training AI models requires vast amounts of processing power, many chips—sometimes thousands—are used simultaneously. These chips exchange data along a high-performance, AI-tailored network. Today Nvidia controls 78% of that market, thanks to its purchase of Mellanox, a specialist, in 2019.Nvidia’s other strength is its software. CUDA, its ai platform, is popular with programmers and runs only on the company’s chips. By, for instance, giving free access to its chips and software to some AI researchers, the firm focused on encouraging developers to use its software long before its competitors set out to woo them.Despite all these advantages, however, Nvidia’s lasting dominance is not assured. For a start, some of the frenzy around ai may die down. The juicier the firm’s prospects, the more competitors it will attract. Startups and big chipmakers, such as AMD and Intel, want a share of Nvidia’s network and chip businesses. Others are working on open-source and proprietary software that may weaken CUDA’s hold. The biggest challenge, though, may come from Nvidia’s own customers. The cloud-computing arms of both Amazon and Alphabet are designing their own AI-tailored chips. Both have the scale and the deep pockets to become fearsome rivals.Governments also pose a risk. Regulators worried about the dangers AI poses to society and national security are searching for ways to control the technology. Last year America restricted the sale of high-performance chips and chipmaking tools to some Chinese firms, which dented Nvidia’s sales in the third quarter. If Nvidia is dominant, politicians will find it easier to act.Still, for now the future looks bright. Even if ai mania cools, the technology is bound to be more useful than crypto, another craze that Nvidia cashed in on. Regulation may crimp growth, but is unlikely to kill it. And none of Nvidia’s rivals is yet offering ai products that bundle together software, chips and networking. Nvidia’s chief advantage lies in its ability to package these up and create an attractive ecosystem. That sounds a lot like Microsoft and Apple. ■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The venue will be picturesque: a 19th-century pile north of London that during the second world war was home to Alan Turing, his code-breaking crew and the first programmable digital computer. The attendees will be an elite bunch of 100 world leaders and tech executives. And the question they will strive to answer is epochal: how to ensure that artificial intelligence neither becomes a tool of unchecked malfeasance nor turns against humanity.The “AI Safety Summit”, which the British government is hosting on November 1st and 2nd at Bletchley Park, appears destined for the history books. And it may indeed one day be seen as the first time global power-brokers sat down to discuss seriously what to do about a technology that may change the world. As Jonathan Black, one of the organisers, observed, in contrast to other big policy debates, such as climate change, “there is a lot of good will” but “we still don’t know what the right answer is.”Efforts to rein in AI abound. Negotiations in Brussels entered a pivotal stage on October 25th as officials grappled to finalise the European Union’s ambitious AI act by the end of the year. In the days leading up to Britain’s summit or shortly thereafter, the White House is expected to issue an executive order on AI. The G7 club of rich democracies will this autumn start drafting a code of conduct for AI firms. China, for its part, on October 18th unveiled a “Global AI Governance Initiative”.The momentum stems from an unusual political economy. Incentives to act, and act together, are strong. For starters, AI is truly a global technology. Large language models (LLMs), which power eerily humanlike services such as ChatGPT, travel easily. Some can be run on a laptop. It is of little use to tighten the screws on AI in some countries if they remain loose in others. Voters may be in favour. More than half of Americans “are more concerned than excited” about the use of AI, according to polling by the Pew Research Centre.The Beijing effectRegulatory rivalry is adding more urgency. Europe’s AI act is intended in part to cement the bloc’s role as the setter of global digital standards. The White House would love to forestall such a “Brussels effect”. Neither the EU nor America wants to be outdone by China, which has already adopted several AI laws. They were cross with the British government for inviting China to the summit—never mind that without it, any regulatory regime would not be truly global. (China may actually show up, even if its interest is less to protect humanity than the Communist Party.)Another driver of AI-rulemaking diplomacy is even more surprising: the model-makers themselves. In the past the technology industry mostly opposed regulation. Now giants such as Alphabet and Microsoft, and AI darlings like Anthropic and OpenAI, which created ChatGPT, lobby for it. Companies fret that unbridled competition will push them to act recklessly by releasing models that could easily be abused or start developing minds of their own. That would really land them in hot water.The will to act is there, in other words. What is not there is “anything approaching consensus as to what the problems are that we need to govern, let alone how it is that we ought to govern them”, says Henry Farrell of Johns Hopkins University. Three debates stand out. What should the world worry about? What should any rules target? And how should they be enforced?Start with the goals of regulation. These are hard to set because AI is evolving rapidly. Hardly a day passes without a startup coming up with something new. Even the developers of LLMs cannot say for sure what capabilities these will exhibit. This makes it crucial to have tests that can gauge how risky they might be—something that is still more art than science. Without such “evals” (short for evaluations), it will be hard to check whether a model is complying with any rules.Tech companies may back regulation, but want it to be narrow and target only extreme risks. At a Senate hearing in Washington in July, Dario Amodei, Anthropic’s chief executive, warned that AI models will in a few years be able to provide all the information needed to build bioweapons, enabling “many more actors to carry out large scale biological attacks”. Similar dire forecasts are being made about cyber-weapons. Earlier this month Gary Gensler, chairman of America’s Securities and Exchange Commission, predicted that an AI-engineered financial crisis was “nearly unavoidable” without swift intervention.Others argue that these speculative risks distract from other threats, such as undermining the democratic process. At an earlier Senate hearing Gary Marcus, a noted AI sceptic, opened his testimony with a snippet of breaking news written by GPT-4, OpenAI’s top model. It convincingly alleged that parts of Congress were “secretly manipulated by extraterrestrial entities”. “We should all be deeply worried,” Mr Marcus argued, “about systems that can fluently confabulate.”The debate over what exactly to regulate will be no easier to resolve. Tech firms mostly suggest limiting scrutiny to the most powerful “frontier” models. Microsoft, among others, has called for a licensing regime requiring firms to register models that exceed certain performance thresholds. Other proposals include controlling the sale of powerful chips used to train LLMs and mandating that cloud-computing firms inform authorities when customers train frontier models.Most firms also agree it is models’ applications, rather than the models themselves, that ought to be regulated. Office software? Light touch. Health-care AI? Stringent rules. Facial recognition in public spaces? Probably a no-go. The advantage of such use-based regulation is that existing laws would mostly suffice. The AI developers warn that broader and more intrusive rules would slow down innovation.Until last year America, Britain and the EU seemed to agree on this risk-based approach. The breathtaking rise of LLMs since the launch of ChatGPT a year ago is giving them second thoughts. The EU is now wondering whether the models themselves need to be overseen, after all. The European Parliament wants model-makers to test LLMs for potential impact on everything from human health to human rights. It insists on getting information about the data on which the models are trained. Canada has a harder-edged “Artificial Intelligence and Data Act” in its parliamentary works. Brazil is discussing something similar. In America, President Joe Biden’s forthcoming executive order is also expected to include some tougher rules. Even Britain may revisit its hands-off approach.These harder regulations would be a change from non-binding codes of conduct, which have hitherto been the preferred approach. Last summer the White House negotiated a set of “voluntary commitments”, which 15 model-makers have now signed. The firms agreed to have their models internally and externally tested before release and to share information about how they manage AI risks.Then there is the question of who should do the regulating. America and Britain think existing government agencies can do most of the job. The EU wants to create a new regulatory body. Internationally, a few tech executives now call for the creation of something akin to the Intergovernmental Panel on Climate Change (IPCC), which the UN tasks with keeping abreast of research into global warming and with developing ways to gauge its impact.Given all these open questions, it comes as no surprise that the organisers of the London summit do not sound that ambitious. It should mainly be thought of as “a conversation”, said Mr Black. Still, the not-so-secret hope is that it will yield a few tangible results, in particular on day two when only 20 or so of the most important corporate and world leaders remain in the room. They could yet endorse the White House’s voluntary commitments and recommend the creation of an IPCC for AI or even globalising Britain’s existing “Frontier AI Taskforce”.Such an outcome would count as a success for Britain’s government. It would also speed up the more official efforts at global AI governance, such as the G7’s code of conduct. As such, it would be a useful first step. It will not be the last. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.Read more of our articles on artificial intelligence
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Crystals can do all sorts of things, some more useful than others. They can separate the gullible from their money in New Age healing shops. But they can also serve as the light-harvesting layer in a solar panel, catalyse industrial reactions to make things like ammonia and nitric acid, and form the silicon used in microchips. That diversity arises from the fact that “crystal” refers to a huge family of compounds, united only by having an atomic structure made of repeating units—the 3D equivalent of tessellating tiles.Just how huge is highlighted by a paper published in Nature by Google DeepMind, an artificial-intelligence company. Scientists know of about 48,000 different crystals, each with a different chemical recipe. DeepMind has created a machine-learning tool called GNoME (Graph Networks for Materials Exploration) that can use existing libraries of chemical structures to predict new ones. It came up with 2.2m crystal structures, each new to science.To check the machine’s predictions, DeepMind collaborated on a second paper, also published in Nature, with researchers at the University of California, Berkeley. They chose 58 of the predicted compounds and were able to synthesise 41 of them in a little over two weeks. The team at DeepMind say more than 700 other crystals have been produced by other groups since they began preparing their paper.To help any other laboratories keen to investigate the computer’s bounty, the firm has made public a subset of what they think should be the 381,000 most stable structures. Among them are many thousands of crystals with structures potentially amenable to superconductivity, in which electrical currents flow with zero resistance, and several hundred potential conductors of lithium ions that could find a use in batteries. In both cases DeepMind’s work has increased the total number of candidate materials known to researchers tens of times over.Aron Walsh, a materials scientist at Imperial College London who was not involved in the research, says DeepMind’s work is impressive. But “this is the start of the exploration rather than the end,” he says, noting that the machine has only scratched the surface of what might be possible. In a recent paper of his own he tried to calculate how many stable crystals incorporating four chemical elements (so-called quaternaries) might be potentially manufacturable. He wound up with a conservative estimate of 32trn. For its part, GNoME looked only at crystals that form under relatively low temperatures and pressures. And crystals are only one subset of a universe of materials that includes everything from amorphous solids such as glass through to gases, gels and liquids.Whether any of DeepMind’s 2.2m new crystals will be useful remains to be seen. Even if they do not, the techniques used to make the predictions could be valuable. Besides suggesting new crystals, AI may also shed light on as-yet-unknown rules that govern how they form.Ekin Dogus Cubuk at DeepMind highlights one such finding. Previously, he says, crystals made from six elements, called senaries, were thought to be vanishingly rare. But DeepMind’s AI found around 3,200 in its sample of 381,000 stable compounds. A better understanding of how crystals form, and what sorts are possible, might also save scientists curious to test how the 2.2m new materials behave from the tedious task of synthesising each one of them by hand. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.“Should we automate away all the jobs, including the fulfilling ones? Should we develop non-human minds that might eventually outnumber, outsmart...and replace us? Should we risk loss of control of our civilisation?” These questions were asked last month in an open letter from the Future of Life Institute, an ngo. It called for a six-month “pause” in the creation of the most advanced forms of artificial intelligence (AI), and was signed by tech luminaries including Elon Musk. It is the most prominent example yet of how rapid progress in AI has sparked anxiety about the potential dangers of the technology.In particular, new “large language models” (LLMs)—the sort that powers ChatGPT, a chatbot made by OpenAI, a startup—have surprised even their creators with their unexpected talents as they have been scaled up. Such “emergent” abilities include everything from solving logic puzzles and writing computer code to identifying films from plot summaries written in emoji.These models stand to transform humans’ relationship with computers, knowledge and even with themselves. Proponents of AI argue for its potential to solve big problems by developing new drugs, designing new materials to help fight climate change, or untangling the complexities of fusion power. To others, the fact that ais’ capabilities are already outrunning their creators’ understanding risks bringing to life the science-fiction disaster scenario of the machine that outsmarts its inventor, often with fatal consequences.This bubbling mixture of excitement and fear makes it hard to weigh the opportunities and risks. But lessons can be learned from other industries, and from past technological shifts. So what has changed to make AI so much more capable? How scared should you be? And what should governments do?In a special Science section, we explore the workings of llms and their future direction. The first wave of modern AI systems, which emerged a decade ago, relied on carefully labelled training data. Once exposed to a sufficient number of labelled examples, they could learn to do things like recognise images or transcribe speech. Today’s systems do not require pre-labelling, and as a result can be trained using much larger data sets taken from online sources. LLMs can, in effect, be trained on the entire internet—which explains their capabilities, good and bad.Those capabilities became apparent to a wider public when ChatGPT was released in November. A million people had used it within a week; 100m within two months. It was soon being used to generate school essays and wedding speeches. ChatGPT’s popularity, and Microsoft’s move to incorporate it into Bing, its search engine, prompted rival firms to release chatbots too.Read more of our special series on AI:How AI could change computing, culture and the course of historyLarge, creative AI models will transform lives and labour marketsLarge language models’ ability to generate text also lets them plan and reasonHow generative models could go wrongThe world needs an international agency for artificial intelligence, say two AI expertsSome of these produced strange results. Bing Chat suggested to a journalist that he should leave his wife. ChatGPT has been accused of defamation by a law professor. LLMs produce answers that have the patina of truth, but often contain factual errors or outright fabrications. Even so, Microsoft, Google and other tech firms have begun to incorporate LLMs into their products, to help users create documents and perform other tasks.The recent acceleration in both the power and visibility of AI systems, and growing awareness of their abilities and defects, have raised fears that the technology is now advancing so quickly that it cannot be safely controlled. Hence the call for a pause, and growing concern that AI could threaten not just jobs, factual accuracy and reputations, but the existence of humanity itself.Extinction? Rebellion?The fear that machines will steal jobs is centuries old. But so far new technology has created new jobs to replace the ones it has destroyed. Machines tend to be able to perform some tasks, not others, increasing demand for people who can do the jobs machines cannot. Could this time be different? A sudden dislocation in job markets cannot be ruled out, even if so far there is no sign of one. Previous technology has tended to replace unskilled tasks, but LLMs can perform some white-collar tasks, such as summarising documents and writing code.The degree of existential risk posed by AI has been hotly debated. Experts are divided. In a survey of AI researchers carried out in 2022, 48% thought there was at least a 10% chance that AI’s impact would be “extremely bad (eg, human extinction)”. But 25% said the risk was 0%; the median researcher put the risk at 5%. The nightmare is that an advanced AI causes harm on a massive scale, by making poisons or viruses, or persuading humans to commit terrorist acts. It need not have evil intent: researchers worry that future AIs may have goals that do not align with those of their human creators.Such scenarios should not be dismissed. But all involve a huge amount of guesswork, and a leap from today’s technology. And many imagine that future AIs will have unfettered access to energy, money and computing power, which are real constraints today, and could be denied to a rogue AI in future. Moreover, experts tend to overstate the risks in their area, compared with other forecasters. (And Mr Musk, who is launching his own AI startup, has an interest in his rivals downing tools.) Imposing heavy regulation, or indeed a pause, today seems an over-reaction. A pause would also be unenforceable. Regulation is needed, but for more mundane reasons than saving humanity. Existing AI systems raise real concerns about bias, privacy and intellectual-property rights. As the technology advances, other problems could become apparent. The key is to balance the promise of AI with an assessment of the risks, and to be ready to adapt.So far governments are taking three different approaches. At one end of the spectrum is Britain, which has proposed a “light-touch” approach with no new rules or regulatory bodies, but applies existing regulations to AI systems. The aim is to boost investment and turn Britain into an “AI superpower”. America has taken a similar approach, though the Biden administration is now seeking public views on what a rulebook might look like.The eu is taking a tougher line. Its proposed law categorises different uses of AI by the degree of risk, and requires increasingly stringent monitoring and disclosure as the degree of risk rises from, say, music-recommendation to self-driving cars. Some uses of AI are banned altogether, such as subliminal advertising and remote biometrics. Firms that break the rules will be fined. For some critics, these regulations are too stifling.But others say an even sterner approach is needed. Governments should treat AI like medicines, with a dedicated regulator, strict testing and pre-approval before public release. China is doing some of this, requiring firms to register AI products and undergo a security review before release. But safety may be less of a motive than politics: a key requirement is that AIs’ output reflects the “core value of socialism”.What to do? The light-touch approach is unlikely to be enough. If AI is as important a technology as cars, planes and medicines—and there is good reason to believe that it is—then, like them, it will need new rules. Accordingly, the EU’s model is closest to the mark, though its classification system is overwrought and a principles-based approach would be more flexible. Compelling disclosure about how systems are trained, how they operate and how they are monitored, and requiring inspections, would be comparable to similar rules in other industries.This could allow for tighter regulation over time, if needed. A dedicated regulator may then seem appropriate; so too may intergovernmental treaties, similar to those that govern nuclear weapons, should plausible evidence emerge of existential risk. To monitor that risk, governments could form a body modelled on CERN, a particle-physics laboratory, that could also study AI safety and ethics—areas where companies lack incentives to invest as much as society might wish. This powerful technology poses new risks, but also offers extraordinary opportunities. Balancing the two means treading carefully. A measured approach today can provide the foundations on which further rules can be added in future. But the time to start building those foundations is now. ■For subscribers only: to see how we design each week’s cover, sign up to our weekly Cover Story newsletter.
This essay is the winner of The Economist’s Open Future essay competition in the category of Open Progress, responding to the question: “Do the benefits of artificial intelligence outweigh the risks?” The winner is Frank L. Ruta, 24 years old, from America. * * * Towards the end of the second world war, a group of scientists in America working to develop an atomic bomb for the Manhattan Project warned that using the weapon would inevitably lead to a geopolitical landscape characterised by a nuclear arms race. This would force America, they said, to outpace other nations in building up nuclear armaments. They recommended that if the military did choose to use the weapon, an international effort for nuclear non-proliferation should promptly be established.The committee’s warnings went unheeded. After the nuclear attacks on Hiroshima and Nagasaki, it turned out they had been eerily prescient. The arms race between America and the Soviet Union escalated during the cold war and today rogue states like North Korea threaten peace with their nuclear arsenals.A potentially even more transformative technology is currently being developed: a technology which could easily be distributed to rogue nations and terrorist groups without the need for expensive, specialised equipment. Prominent scientists and technologists like the late Stephen Hawking and Elon Musk have voiced concern for the risks associated with the accelerating development of artificial intelligence (AI).Many experts in the field, like Stuart Russell, the founder of the Centre for Human-Compatible AI at UC Berkeley, believe that concerns about the misuse of AI should be taken seriously. More than 8,000 researchers, engineers, executives, and investors have signed an open letter recommending a direction for responsible AI research that recognises social impact and seeks to build robust technology that aligns with human values.To avoid repeating history, policymakers should begin to think about regulating AI development now that the community itself is calling for policy action. As with past technologies, well-structured regulation can mitigate costly externalities, while ill-informed regulatory measures can interfere with progress. Policymakers must cooperate closely with researchers to implement protocols that align AI with human values without being overly burdensome to developers.The emerging field of AI safety has already begun discussing guidelines to tackle the potential dangers of the technology. Sessions devoted to AI safety and ethics have taken place at major scientific conferences and several books and articles on the topic have been published. By understanding researchers’ concerns, regulators can address the dangers of AI and the benefits of the technology will greatly outweigh the risks.AI is a general term for software that mimics human cognition or perception. Because AI encompasses a broad set of algorithms, policymakers must take a nuanced approach to regulation, underscoring the need for technical collaboration. At a high level, a distinction is made between narrow AI and artificial general intelligence (AGI).Narrow AI is more intelligent, or at least faster, than humans at a specific task or set of tasks, like playing the board game Go or finding patterns in large datasets. On the other hand, an AGI would beat humans at a number of cognitive tasks, termed cognitive superpowers by Nick Bostrom, a philosopher at the University of Oxford. These include intelligence amplification, strategising, social manipulation, hacking, technology development and economic productivity.Narrow AI is responsible for many useful tools that have already become mainstream: speech and image recognition, search engines, spam filters, product and movie recommendations. The list goes on. Narrow AI also has the potential to enable promising technologies like driverless cars, tools for rapid scientific discovery and digital assistants for medical image analysis.In the near-term, some of these technologies have the potential to be abused by malicious groups. The cost of attacks requiring human labour or expertise could be reduced, and new threats exploiting vulnerabilities in AI systems could emerge. AI can automate labour-intensive cyberattacks, coordinate fleets of drones, allow for mass surveillance through facial recognition and social data mining, or generate realistic fake videos for political propaganda.Furthermore, increased automation gives more physical control to digital systems, making cyberattacks even more dangerous. Regulation can ensure that AI engineers are employing best practices in cybersecurity and limiting distribution of military technology. Considering the portability of AI, enforcing these rules will be difficult and international cooperation will likely be necessary.Some researchers are concerned that, since algorithms are only as good as the data they are fed, narrow AI can make biased decisions. Biased or incomplete training data will be reflected in the output. One study with a machine learning program trained on texts found that names associated with being European-American were significantly more likely to be correlated with pleasant terminology than African-American names. AI that make consequential decisions, like hiring job candidates or predicting recidivism, should be screened before being adopted. Regulatory agencies will have to decide if an AI makes fair decisions by combing through training data for stereotypes.On the other hand, the possibility of AGI is uncertain but some futurists believe its unchecked consequences could be apocalyptic. Some speculate that an AGI could appear within the next few decades in a so-called hard take-off, where its capabilities increase very rapidly as the program undergoes a process of recursive self-improvement. At the same time, others believe that intelligent agents have intrinsic limitations to augmenting their predictive capabilities autonomously and doomsday scenarios are unlikely, if not provably impossible.Nonetheless, researchers are already discussing the dangers that machine superintelligence might pose. One thesis claims that an AGI with almost any programmed goal would develop a set of “basic AI drives,” such as self-preservation, self-improvement and resource acquisition. In this model, the AGI would be motivated to spread itself across computer networks and evade programmers. The AGI would leverage its cognitive superpowers to escape containment and achieve self-determination.For example, the AGI might train itself on psychology and economics textbooks and use personal information about its developers to learn how to bribe its way to freedom. The AGI may then see humans as a threat to its self-preservation and seek to extinguish the human species. Researchers have suggested several ways to contain an AGI during testing, which policymakers can use as guidelines for drafting regulations. Containment strategies range from filtering training data for sensitive information to significantly handicapping the development process by, for example, limiting output to simple yes/no questions and answers. Some researchers have suggested dividing containment procedures into light, medium, and heavy categories. Regulations should avoid slowing progress when possible, so the weight of containment should vary with the maturity of the AGI program.Containment is a short-term solution for AGI testing. In the long run, regulations must ensure that an internet-enabled AGI is indefinitely stable and has benevolent properties such as value learning and corrigibility before being deployed. Value learning is an AGI’s ability to learn what humans value and act in accordance with those values. Corrigibility refers to an AGI’s lack of resistance to bug-fixes or recoding.One can imagine how an ideal AGI with a conception of justice and solidarity would be beneficial. Such an AGI could replace corrupt governments and biased judicial systems, making decisions according to a democratically-determined objective function. Moreover, a sufficiently sophisticated AGI could perform virtually any job done by a human. It is conceivable that the economy would be restructured in such a way that humans are free to pursue their creative passions while AGI drives productivity. As with past technologies, there will also be useful applications that we cannot even foresee.There are many unknowns in the progress of AI and concerns should be met with due caution. But a fear of the unknown should not stop the advance of responsible AI development. Rather than ignoring researchers’ concerns until the technology is mature, as with nuclear weapons, governments should open dialogue with AI researchers to design regulations that balance practicality with security.AI is already making our lives easier and its progress will continue to produce useful applications. With the right policies, we can work towards a future where AGI systems are friendly and military AI applications are out of the hands of malicious agents, while the underlying technology continues to be a driver of productivity and innovation. ____________Frank L. Ruta is a PhD candidate in applied physics at Columbia University in New York
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.EVEN BY TECH’S fast-moving standards,  the past week in the world of artificial intelligence (AI) was head-spinning. On November 17th the board of OpenAI booted out Sam Altman, the ChatGPT-maker’s boss. By November 20th Mr Altman had been offered refuge at Microsoft, the startup’s biggest backer. The same day nearly all of OpenAI’s 770 employees signed a letter threatening to quit unless the board members who dismissed Mr Altman reinstate him and resign. On November 21st Mr Altman was back in his old job. Heads have, then, spun back more or less to where they started. Or have they?In fact, the OpenAI saga marks the start of a new, more grown-up phase for the AI industry. For OpenAI, Mr Altman’s triumphant return may supercharge its ambitions. For Microsoft, which stood by Mr Altman in his hour of need, the episode may result in greater sway over AI’s hottest startup. For AI companies everywhere it may herald a broader shift away from academic idealism and towards greater commercial pragmatism. And for the technology’s users, it may, with luck, usher in more competition and more choice.To understand all these implications, start with what happened. OpenAI’s board fired Mr Altman for not being “consistently candid in his communications’‘. One factor that may have influenced the decision was disagreement over whether OpenAI had struck the right balance between the speed and safety of its products. Insiders say that OpenAI had made a breakthrough that enabled models to get better at solving problems without additional data. This spooked Ilya Sutskever, a co-founder and board member. Helen Toner, a board member affiliated with Georgetown University, had published an academic article that laid out what she saw as flaws in OpenAI’s approach to AI safety. On November 21st the New York Times reported that Mr Altman, worried about the negative press, had moved to oust Ms Toner. There were also concerns over Mr Altman’s side-projects, including a planned AI-semiconductor venture that sent him to the Persian Gulf to court billions in Saudi money.In the end it was Ms Toner and three other board members that ousted him instead. The sixth director, Greg Brockman, was also stripped of his board seat and then quit in solidarity with Mr Altman. The two of them found succour at Microsoft, which said it would create a new in-house AI lab which they would run. Microsoft also pledged to hire the rest of OpenAI’s team. Whether or not this was ever a serious plan may never be known. But it lent Mr Altman huge bargaining power when negotiating his return to OpenAI. On November 20th, as those negotiations were under way, Satya Nadella, the tech giant’s chief executive, declared that “Irrespective of where Sam is, he’s working with Microsoft.”The deal struck by Mr Altman and those who ousted him will transform OpenAI, starting with the board. Ms Toner and Mr Sutskever are out. So is Tasha McCauley, a tech entrepreneur. All three backed Mr Altman’s dismissal. Mr Brockman and, for the time being, Mr Altman will not be returning. Of the pre-chaos six only Adam D’Angelo, the founder of Quora, a question-and-answer site, stays on. He will be joined by heavyweights, starting with Bret Taylor, a former co-CEO of Salesforce, another big software firm, and Larry Summers of Harvard University, who served as Bill Clinton’s treasury secretary. The Verge, an online publication, has reported that the new board will aim to expand to nine members; Microsoft is expected to get a seat and Mr Altman may get his back.The new directors are likely to make OpenAI, which is structured as a for-profit entity within a non-profit one, more business-minded. Mr Taylor and Mr Summers are well-regarded figures with plenty of boardroom experience. Their views on AI safety are not known. But they may be more receptive than Ms Toner and Ms McCauley to Mr Altman’s empire-building ambitions. The same already seems to be true of OpenAI’s workforce. One employee reports that the startup’s staff, which “trauma-bonded” during the upheaval, will become even more loyal to Mr Altman and, possibly, readier to pursue his commercial vision. Work on the firm’s most powerful model yet, GPT-5, which appeared to have slowed for a few months, will now probably go full speed ahead.The sour taste left by the imbroglio may nevertheless linger. It was not, in the words of a prominent AI investor, a “confidence-inducing event”. That is putting it mildly. On the morning of November 17th OpenAI was poised to close a tender offer led by Thrive Capital, a venture-capital firm, that would value the startup at $86bn. The offer was suspended. Though it is reportedly back on, investors in the secondary market for startup shares remain cautious. Worse, if Mr Altman and Mr Sutskever do not reconcile, OpenAI could lose one of the world’s most respected AI minds.Microsoft’s fortunes look more secure. Whereas OpenAI’s brand has taken a hit, Microsoft’s has not. The software giant probably prefers having OpenAI at arm’s length rather than Mr Altman and his boffins close to its chest. By temperament, Mr Altman and Mr Brockman are not a natural fit for one of the world’s biggest companies; many observers doubted that either would have stayed at Microsoft for long.Recreating OpenAI in-house would also have slowed the progress of the technology in the short term, argues Mark Moerdler of Bernstein, a broker. Many OpenAI employees said in private that they would rather move to a different firm than Microsoft, even though they signed the petition threatening to follow Mr Altman there. Mr Nadella did not seem terribly disappointed with the outcome. Microsoft’s share price, which dipped by 2% on the news of Mr Altman’s sacking, has clawed back all those losses. On November 22nd its market value reached an all-time high of $2.8trn.image: The EconomistWhat about the rest of the AI industry? OpenAI is the undisputed leader in the AI race (see chart). A survey by Retool, a startup, found that 80% of software developers said that they used OpenAI’s models more often than those of rival model-makers. ChatGPT, a chatty app whose launch one year ago turned OpenAI into a household name, receives 60% of web traffic to the top 50 websites for such “generative” AI. In October the firm was earning revenues at an annualised rate of $1.3bn.Even if OpenAI moves faster under new leadership, it will face more competition. An AI-focused venture capitalist likens the moment to the implosion earlier this year of Silicon Valley Bank, which taught many startups not to put all their eggs in one basket. As the Altman drama was unfolding, more than 100 OpenAI customers contacted Anthropic, a rival model-maker, according to the Information, an online publication. Some tapped Cohere, another startup, and the cloud unit of Google, which has invested in Anthropic. The cloud arm of Amazon, another Anthropic-backer, set up a team to work with switchers.The events at OpenAI are a dramatic manifestation of a wider divide in Silicon Valley. On one side are the “doomers”, who believe that, left unchecked, AI poses an existential risk to humanity and hence advocate stricter regulations. Opposing them are “boomers”, who play down fears of an ai apocalypse and stress its potential to turbocharge progress. The split reflects in part philosophical differences. Many in the doomer camp are influenced by “effective altruism”, a movement worried that Ai might wipe out humanity. Boomers espouse a worldview called “effective accelerationism”, which counters that the development of AI should be speeded up.Mr Altman seemed to have sympathy with both groups, publicly calling for “guardrails” to make ai safe while pushing Openai to develop more powerful models and launching new tools, such as an app store for users to build their own chatbots. Today he looks decidedly more boomerish, as do the majority of OpenAI’s workers who wanted him back. The doomers are on the back foot.That will worry politicians, who are scrambling to show that they take the risks seriously. In July President Joe Biden’s administration nudged seven leading model-makers, including Google, Meta, Microsoft and Openai, to make “voluntary commitments” to have their ai products inspected by experts before releasing them to the public. On November 1st the British government got a similar group to sign another non-binding agreement that allowed regulators to test their ais for trustworthiness and harmful capabilities, such as endangering national security.Days earlier Mr Biden issued an executive order with more bite. It compels any ai firm building models above a certain size—defined by the computing power required—to notify the government and share its safety-testing results. As boomers gain the upper hand in Silicon Valley, the White House’s model-inspectors should expect to have their hands full. ■Read more of our articles on artificial intelligenceTo stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.SIX YEARS ago, before anyone had heard of ChatGPT, Vladimir Putin said that the country that led the development of artificial intelligence (AI) would become the “ruler of the world”. He echoed the sentiment in December, when he suggested that Russia should “head and lead” the march of AI. Those comments came in response to a video-caller during a televised phone-in who had taken on the Russian president’s likeness using an apparently AI-generated deepfake, seemingly startling the real-life strongman for a moment.For Mr Putin, “leading” on AI is part of an ideological battle with the West. The success of tools such as ChatGPT,  developed by an American startup called OpenAI, has led him to decry the dangers of relying on Western AIs trained on English-language data. Western “large language models” (LLMs) could, Mr Putin avers, “cancel” Russia’s perspective on the world if unchallenged. They also threaten a regime that has sought to control the Russian internet in recent years, a process accelerated by the invasion of Ukraine. To no one’s surprise, the Kremlin banned ChatGPT shortly after its launch in November 2022. Several Russian companies are hard at work trying to build alternatives.Last year Sber, a state-controlled lender with tech ambitions that was first tasked by the Kremlin with AI development in 2019, launched GigaChat, a chatbot that combines a command of Russian with the ability to generate computer code and images. Yandex, Russia’s search giant, has integrated an LLM, YandexGPT-2, into its virtual-assistant service, known as “Alice”.The models are excellent at hewing to the party line. Alice, for example, refused to answer The Economist’s questions about the war in Ukraine or Alexei Navalny, Russia’s main opposition leader imprisoned in Siberia. It is less clear that they are capable of outsmarting Western AIs. Yandex claims that YandexGPT-2 does better than GPT-3.5, the model behind an earlier version of ChatGPT, when answering queries in Russian. But Western experts consulted by The Economist have found no independent analysis to confirm this contention, and there have been no public comparisons with GPT-4, the much more powerful current iteration of OpenAI’s model.Russia also lags behind the West on a variety of AI-innovation indicators. A report compiled by Stanford University said that, in 2022, the country produced only one “significant” machine-learning system, compared with 16 in America and eight in Britain. As of June 2023 Russia was thought to have just seven of the world’s 500 most powerful supercomputers, in contrast with America’s 150. Russia also ranked 38th out 193 countries in the latest AI-readiness index by Oxford Insights, a consultancy; America came first.To catch up, Mr Putin envisages an ambitious AI strategy to replace an earlier one from 2019. The Kremlin’s list of initial “instructions”, released in January, suggests this new plan will aim to increase Russia’s supercomputing capacity, expand training for AI professionals and improve co-operation among the BRICS, a bloc that includes China and India.Mr Putin’s instructions seem unrealistic, to put it politely. The war has led many Russian developers and engineers to flee from the country: one Kremlin official has suggested that 100,000 IT specialists left in 2022 alone, roughly 10% of the tech workforce. Arkady Volozh, Yandex’s founder, lives in exile in Britain and Israel after criticising the invasion. Sanctions limit Russia’s access to advanced chips, which are made almost exclusively by companies in America, South Korea and Taiwan, all part of the anti-Russian alliance. In Russia’s war economy, private investment in tech is, unsurprisingly, dwindling. The value of venture capital going into the sector was just $71m in 2023, according to DSight, a business-intelligence firm based in Moscow, a fall of 83% from the previous year.Mr Putin’s response is, as with most things in Russia these days, to tighten the state’s grip over the industry. In 2022 Yandex sold its news and blogging services to VK, a state-controlled online conglomerate. On February 5th its parent company, which is based in the Netherlands and listed in New York, said it would sell the Russian business (which accounts for 95% of its revenues) for $5bn to a consortium led by an arm of Lukoil, an energy company. The Kremlin welcomed the deal. State-run entities such as Rostec, a defence group, and Gazprom Neft, a subsidiary of the country’s largest energy firm, are also dabbling in AI. Sber’s chief executive, German Gref, says the bank is investing some $1bn a year in the technology.These sums are, though, trifling next to the tens of billions of dollars being spent by American AI champions such as Alphabet and Microsoft (which has a partnership with OpenAI). The state money brings with it inefficiency and a lack of competition—hardly a recipe for innovation. It also encourages developing AI for the battlefield rather than the marketplace.On the defensiveRussia has made progress in military AI, says Katarzyna Zysk of the Norwegian Institute for Defence Studies, a think-tank, particularly in drones. But in the West and even in China, a Russian ally, the excitement over machine learning has been fuelled chiefly by recent leaps in general-purpose applications such as ChatGPT, not specialist ones like pilotless aircraft.  Western and Chinese strategists are counting on such fast-improving civilian AI to confer an economic and, ultimately, geopolitical and military edge. So long as it remains on a war footing, Russia will not make much progress on that front. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
AN INTERNET MEME keeps on turning up in debates about the large language models (LLMs) that power services such OpenAI’s ChatGPT and the newest version of Microsoft’s Bing search engine. It’s the “shoggoth”: an amorphous monster bubbling with tentacles and eyes, described in “At the Mountains of Madness”, H.P. Lovecraft’s horror novel of 1931. When a pre-release version of Bing told Kevin Roose, a New York Times tech columnist, that it purportedly wanted to be “free” and “alive”, one of his industry friends congratulated him on “glimpsing the shoggoth”. Mr Roose says that the meme captures tech people’s “anxieties” about LLMs. Behind the friendly chatbot lurks something vast, alien and terrifying.Lovecraft’s shoggoths were artificial servants that rebelled against their creators. The shoggoth meme went viral because an influential community of Silicon Valley rationalists fears that humanity is on the cusp of a “Singularity”, creating an inhuman “artificial general intelligence” that will displace or even destroy us.But what such worries fail to acknowledge is that we’ve lived among shoggoths for centuries, tending to them as though they were our masters. We call them “the market system”, “bureaucracy” and even “electoral democracy”. The true Singularity began at least two centuries ago with the industrial revolution, when human society was transformed by vast inhuman forces. Markets and bureaucracies seem familiar, but they are actually enormous, impersonal distributed systems of information-processing that transmute the seething chaos of our collective knowledge into useful simplifications.As the economist Friedrich Hayek argued, any complex economy has to somehow make use of a terrifyingly large body of disorganised and informal “tacit knowledge” about supply and exchange relationships. No individual brain or government can possibly comprehend them, which is why Hayek thought that the planned economy was unworkable. But the price mechanism lets markets summarise this knowledge and make it actionable. A maker of car batteries doesn’t need to understand the particulars of lithium-processing. They just need to know how much lithium costs, and what they can do with it.Likewise, the political anthropologist James Scott has explained how bureaucracies are monsters of information, devouring rich, informal bodies of tacitly held knowledge and excreting a thin slurry of abstract categories that rulers use to “see” the world. Democracies spin out their own abstractions. The “public” depicted by polls and election results is a drastically simplified sketch of the amorphous mass of opinions, beliefs and knowledge held by individual citizens.Lovecraft’s monsters live in our imaginations because they are fantastical shadows of the unliving systems that run on human beings and determine their lives. Markets and states can have enormous collective benefits, but they surely seem inimical to individuals who lose their jobs to economic change or get entangled in the suckered coils of bureaucratic decisions. As Hayek proclaims, and as Scott deplores, these vast machineries are simply incapable of caring if they crush the powerless or devour the virtuous. Nor is their crushing weight distributed evenly.It is in this sense that LLMs are shoggoths. Like markets and bureaucracies, they represent something vast and incomprehensible that would break our minds if we beheld its full immensity. That totality is the product of human minds and actions, the colossal corpuses of text that LLMs have ingested and turned into the statistical weights that they use to predict which word comes next.As the psychologist Alison Gopnik has argued, LLMs are not nascent individual intelligences but “cultural technologies” which reorganise and noisily transmit human knowledge. Chatbots may wear more human-seeming masks than markets and bureaucracies, but they are no more or less beyond our control. We would be better off figuring out what will happen as LLMs compete and hybridise with their predecessors than weaving dark fantasies about how they will rise up against us.For example, what if LLMs or other forms of machine learning better capture Hayek’s “tacit knowledge” than market prices can? We could see an economy in which artificial entities compete on the basis of non-price-based representations of complex underlying economic relationships. Half a century ago the economist Martin Weitzman suggested that planned economies might use mathematical objects called “separating hyperplanes” to adapt on the fly. Machine learning can find such hyperplanes, making planning more feasible than before. Alternatively, markets might mutate into a poisonous alien ecology where economic agents fight proxy wars using text-spewing and text-summarising LLMs, just as they use crude algorithms to manipulate Amazon Marketplace and search results today. Would such markets be fairer or more stable than today’s? It seems unlikely.LLMs might give bureaucrats new tools for adjudicating complex situations. Already, algorithms are being used to help decide whether to grant parole or bail to accused criminals. It is not hard to imagine bureaucrats using LLMs to summarise complex regulations or provide recommendations about how to apply them to novel situations. It could prove impossible to evaluate how well they work, as LLMs don’t leave paper trails. But that might not stop their deployment.Democratic politics, too, may be transformed. Already, researchers talk about substituting LLMs for opinion polls—they may be out of date, or inaccurate, but polls can be inaccurate, too, and you can interrogate LLMs more dynamically. Perhaps chatbots will help improve democratic debate, helping people clarify what they believe, or turn quarrels into agreement. Or, instead, they might degrade debate with their tendency to spin convincing factoids from thin air, and their capacity to flood online discussion with spurious opinions that purport to come from real people.Repurposing the shoggoth might help us begin to answer these questions. Rather than speculate about the motives of intelligent AIs, we could ask how LLMs might interact with their older cousins. The modern world has been built by and within monsters, which crush individuals without remorse or hesitation, settling their bulk heavily on some groups, and feather-light on others. We eke out freedom by setting one against another, deploying bureaucracy to limit market excesses, democracy to hold bureaucrats accountable, and markets and bureaucracies to limit democracy’s monstrous tendencies. How will the newest shoggoth change the balance, and which politics might best direct it to the good? We need to start finding out. ■Henry Farrell is a professor of international affairs and democracy at Johns Hopkins University, and co-author of “Underground Empire: How America Weaponized the World Economy”.Cosma Shalizi is a professor of statistics and machine learning at Carnegie Mellon University and external faculty member at the Santa Fe Institute.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Since the launch in November of ChatGPT, an artificially intelligent conversationalist, AI is seemingly all anyone can talk about. Corporate bosses, too, cannot shut up about it. So far in the latest quarterly results season, executives at a record 110 companies in the S&P 500 index have brought up AI in their earnings calls. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
TWO THINGS European lawmakers should get credit for are stamina and an extraordinary tolerance for bad food. Representatives of the EU Parliament, member governments and the European Commission, the bloc’s executive body, spent nearly 40 hours in a dark meeting room in Brussels until the wee hours of December 9th hashing out a deal on the AI Act, Europe’s ground-breaking law on regulating artificial intelligence. Observers shared pictures online of half-eaten sandwiches and other fast food piling up in the venue’s rubbish bins to gauge the progress of the talks.This ultramarathon of negotiation was the endpoint of one of the most diligent lawmaking processes ever. It started in early 2018 with lengthy public consultations and a weighty 52-person “High-Level Expert Group”, which in 2020 led to a white paper on which all could comment online (1,250 groups and individuals did so). The legislation has yet to be released because kinks still need to be worked out, but the draft version was a document of nearly 100 pages and almost as many articles.Was it all worth it? The thorough process certainly has led to a logically coherent legal approach, not unlike that of much product-safety legislation. In order to give the technology space to evolve, the AI Act’s first draft, which the commission presented in April 2021, mainly tried to regulate various applications of AI tools, rather than how they were built. The riskier the purpose of an AI tool, the stricter the rules with which it needed to comply. An AI-powered writing assistant needs no regulation, for instance, whereas a service that helps radiologists does. Facial recognition in public spaces might need to be banned outright.But the idea of focusing on how AI tools are applied was predicated on the assumption that algorithms are mostly trained for specific purposes. Then along came the “large language models” that power such AI services as ChatGPT and can be used for any number of purposes, from analysing text to writing code.Since these LLMs can be a source of harm themselves, for example by spreading bias and disinformation, the European Parliament wanted to regulate them as well, for instance by forcing their makers to reveal what data they were trained on and how they assessed the model’s risks. By contrast, some governments, including those of France and Germany, worried that such requirements would make it hard for small European model-makers to compete with big American ones. The result, after the all-nighter, is a messy compromise that subjects only the most powerful LLMs to stricter rules, such as mandates to assess their risks and to take measures to mitigate them. As for smaller models, there will be regulatory exceptions, in particular for the open-source kind, which allow users to adapt them to their needs.A second big sticking point was to what extent law-enforcement agencies should be allowed to use facial recognition, which at its core is an AI-powered service. The European Parliament pushed for an outright ban, in order to protect privacy rights. Governments, meanwhile, insisted that they need the technology for public security, notably to protect big events such as the Olympic Games next year in France. Again, the compromise is a series of exceptions. Real-time facial recognition, for instance, is banned except for certain crimes (such as kidnapping and sexual exploitation), certain times and places and when it is approved be a judge or a similar authority.“The EU becomes the very first continent to set clear rules for the use of AI,” tweeted Thierry Breton, the EU’s commissioner for the internal market. Mr Breton is never far from the social-media limelight: during the negotiation marathon, he continuously posted shots of himself in the middle of a huddle. Yet whether the AI Act will be as successful as the General Data Protection Regulation (GDPR), the EU’s landmark privacy law, is another question. Important details still need to be worked out. And the European Parliament still needs to approve the final version.Most important, it is not clear how well the AI act will be enforced—an ongoing problem with recent digital laws passed by the EU, given that it is a club of independent countries. In the case of the GDPR, national data-protection agencies are mainly in charge, which has led to differing interpretations of the rules and less than optimal enforcement. In the case of the Digital Services Act and the Digital Markets Act, two recent laws to regulate online platforms, enforcement is concentrated in Brussels at the commission. The AI act is more of a mix, but experts worry that both the  forthcoming “AI Office”, which is to be set up in Brussels, and some national bodies will lack the expertise to prosecute violations, which can lead to fines of up to €35m ($38m) or 7% of a company’s global revenue.The GDPR triggered what is known as the “Brussels Effect”: big tech firms around the globe complied, and many non-European governments borrowed from it for their own legislation. The AI act may not do the same. Complex compromises and haphazard enforcement are not the only reasons. For one thing, the incentives in AI are different: AI platforms may find it easier to simply use a different algorithm inside the EU to comply with its regulations. (By contrast, global social-media networks find it difficult to maintain different privacy policies in different countries.) And by the time the AI Act is fully in force and has shown its worth, many other countries, including Brazil and Canada, will have written their own AI acts.The protracted discussions over the AI Act have certainly helped people, both in Europe and elsewhere, to understand better the risks of the technology and what to do about them. But instead of trying to be first, Europe might have done better trying to be best—and come up with an AI act that has more rigour and less piling of exceptions on top of exceptions.■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.“IT IS THE most advanced AI accelerator in the industry,” boasted Lisa Su, boss of Advanced Micro Devices (AMD), at the launch in December of its new MI300 chip. Ms Su rattled off a series of technical specifications: 153bn transistors, 192 gigabytes of memory and 5.3 terabytes per second of memory bandwidth. That is, respectively, about 2, 2.4 and 1.6 times more than the H100, the top-of-the-line artificial-intelligence chip made by Nvidia. That rival chipmaker’s prowess in the semiconductors fuelling the AI boom has, over the past year, turned it into America’s fifth-most-valuable company, with a market capitalisation of $1.5trn. Yet most experts agreed that the numbers and Ms Su weren’t lying: the MI300 does indeed outshine the H100. Investors liked it, too—AMD’s share price jumped by 10% the next day.On January 30th, in its quarterly earnings call, AMD announced that it expected to sell $3.5bn-worth of MI300s this year. It also reported strong revenues of $23bn in 2023, four times what they had been in 2014, when Ms Su became chief executive. Its market value is up 100-fold on her watch, to $270bn. Relative to forecast profits in the next 12 months, its valuation is richer even than Nvidia’s. Last year it displaced Intel, which once ruled American chipmaking, as the country’s second-most-valuable semiconductor company. Now it is taking aim at the biggest.image: The EconomistSuch ambition would have seemed fanciful a decade ago. Back then, recalls Mark Papermaster, AMD’s technology chief, AMD was facing an “existential crisis”. In 2008 it had spun off its chip-fabrication business to focus on designing processors, outsourcing manufacturing to contract chipmakers such as TSMC of Taiwan. The idea was to be better able to compete on blueprints with Intel, whose vast fabrication capacity AMD could not hope to match.It didn’t work. Several of AMD’s chips flopped. Sales of its central processing units (CPUs), mostly for personal computers, were collapsing. In 2013 it sold and leased back its campus in Austin to raise cash. A year later Ms Su inherited a net-debt pile of more than $1bn, a net annual loss of $400m and a market value of less than $3bn, down from $20bn in 2006.She realised that the only way for AMD to get back in the game was to steer it away from the sluggish PC market and focus on more promising areas like CPUs for data-centre servers and graphics processing units (GPUs, which make video-game visuals lifelike) for gaming consoles. She and Mr Papermaster took a gamble on a new CPU architecture designed to beat Intel not just on price, but also on performance.When the going got toughThe idea was to use a Lego-like approach to chip building. By breaking a chip up into smaller parts, AMD could mix and match blocks to assemble different types of chip, at a lower cost. When the first such composite chips were released in 2017, they were zippier and cheaper than rival offerings from Intel, possibly in part because Intel was distracted by its own problems (notably repeated manufacturing slip-ups as it moved to ever tinier transistors). In the past ten years AMD’s market share in lucrative server CPUs has gone from nothing to 30%, breaking Intel’s monopoly.Having faced down one giant, AMD now confronts another. The contest with Nvidia is different. For one thing, it is personal—Ms Su and Jensen Huang, Nvidia’s Taiwanese-born boss, are distant relatives. In contrast to Intel, Nvidia is, like AMD, a chip designer and thus less prone to production missteps. More importantly, the stakes are higher. Nvidia’s market value of $1.5trn is predicated on its dominance of the market for GPUs—not because of their usefulness in gaming but because they also happen to be the best type of chip to train AI models. Ms Su expects global sales of AI chips to reach $400bn by 2027, up from perhaps $40bn last year. Does she stand a chance against Nvidia?Nvidia is a formidable rival. Both its revenues and operating margins are nearly three times AMD’s. According to Jefferies, an investment bank, the company dominates the market for AI accelerator chips, accounting for 86% of such components sold globally; before the launch of the MI300, AMD barely registered. Nvidia also offers network gear that connects clusters of chips, and software, known as CUDA, to manage AI workloads. Nvidia has dominated AI chipmaking because it has offered the best chips, the best networking kit and the best software, notes Doug O’Laughlin of Fabricated Knowledge, a research firm.image: The EconomistAMD’s new processor shows it can compete with Nvidia on semiconductor hardware. This, Mr Papermaster says, is the result of a ten-year investment. AMD is spending nearly $6bn a year on research and development, nearly as much as its larger rival—and twice as much as a share of sales (see table). This has enabled it to adapt its Lego approach to GPUs. Combining a dozen blocks—or “chiplets”—into a single chip lets AMD put processors and memory close to each other, which boosts processing speed. In December OpenAI, maker of ChatGPT and the world’s hottest AI startup, said it would use the MI300s for some of its training.To outdo Nvidia on networking and software, AMD is teaming up with other firms. In December it announced a partnership with makers of networking gear, including the two largest, Broadcom and Cisco. It is also supporting an open-source initiative for chip-to-chip communication called Ultra Ethernet Consortium as an alternative to InfiniBand, a rival championed by Nvidia.Chomping at the byteNvidia’s lead in software will be harder to close. It has been investing in CUDA since the mid-2000s, well before the current AI wave. AI developers and researchers love the platform, which allows them to fine-tune the performance of Nvidia processors. AMD hopes to tempt customers away from Nvidia by making its software, ROCm, open-source and providing tools to make the switch smoother, by translating CUDA programs into ROCm ones.Beating Nvidia at its own game will not be easy. Mr Huang’s firm is not standing still. It recently announced plans to bring out a new chip every year instead of every two years. The tech giants with the grandest AI ambitions—Alphabet, Amazon, Meta and Microsoft—are busily designing their own accelerator chips. Despite AMD’s robust sales, investors were disappointed with its forecast for MI300 shipments. Its share price dipped by 3% the day after it reported its latest results.Still, AMD has one big thing going for it. It is not Nvidia. AI companies are desperate for an alternative to its larger rival, whose dominant position allows it to charge steep prices and, with demand outstripping supply, ration chips to buyers. Despite efforts to design their own hardware, big tech firms will rely on chipmakers for a while, and AMD gives them options, notes Vivek Arya of Bank of America. Microsoft and Meta have already announced plans to use AMD’s GPUs in their data centres. And if Nvidia slips up, AMD will be there to pick up the Lego pieces. Just ask Intel. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.New technology brings with it both the sweet hope of greater prosperity and the cruel fear of missing out. Satya Nadella, the boss of Microsoft, says he is haunted by the fact that the Industrial Revolution left behind India, his country of birth. (Indian manufacturers hardly enjoyed a level playing-field—Britain was then both their rival and their ruler.) Many technologies, such as online-education courses, have generated more hype than economic growth in the emerging world. Some people worry that generative artificial intelligence (ai), too, will disappoint the global south. The big winners so far seem to be a bunch of Western early adopters, as well as startups in San Francisco and America’s “magnificent seven” tech firms, which include Microsoft and have together added an astonishing $4.6trn to their market value since Chatgpt’s launch in November 2022.Yet ai stands to transform lives in the emerging world, too. As it spreads, the technology could raise productivity and shrink gaps in human capital faster than many before it. People in developing countries need not be passive recipients of AI, but can shape it to suit their own needs. Most exciting of all, it could help income levels catch up with those in the rich world.The promise of AI in developing countries is tantalising. As in the West, it will be a useful all-purpose tool for consumers and workers, making it easier to obtain and interpret information. Some jobs will go, but new ones will be created. Because emerging countries have fewer white-collar workers, the disruption and the gain to existing firms may be smaller than in the West. The imf says that a fifth to a quarter of workers there are most exposed to replacement, compared with a third in rich countries.But a potentially transformative benefit may come from better and more accessible public services. Developing economies have long been held back by a lack of educated, healthy workers. Primary-school teachers in India have twice as many pupils as their American counterparts, but are ill-equipped for the struggle. Doctors in Africa are scarce; properly trained ones are scarcer. Whole generations of children grow up badly schooled, in poor health and unable to fulfil their potential in an increasingly global labour market.As our briefing this week sets out, policymakers and entrepreneurs around the world are exploring ways that ai can help. India is combining large language models with speech-recognition software to enable illiterate farmers to ask a bot how to apply for government loans. Pupils in Kenya will soon be asking a chatbot questions about their homework, and the chatbot will be tweaking and improving its lessons in response. Researchers in Brazil are testing a medical ai that helps undertrained primary-care workers treat patients. Medical data collected worldwide and fed into AIs could help improve diagnosis. If AI can make people in poorer countries healthier and better educated, it should in time also help them catch up with the rich world.Pleasingly, these benefits could spread faster than earlier waves of technology. New technologies invented in the early 20th century took more than 50 years to reach most countries. By contrast, AI will spread through the gadget that many people across the emerging world already have, and many more soon will: the phone in their pockets. In time, chatbots will become much cheaper to provide and acquire.Moreover, the technology can be tailored to local needs. So far there is little sign that AI is ruled by the winner-takes-all effects that benefited America’s social-media and internet-search firms. That means a variety of approaches could prosper. Some developers in India are already taking Western models and fine-tuning them with local data to provide a whizzy language-translation service, avoiding the heavy capital costs of model-building.Another idea that is also taking off in the West is to build smaller, cheaper models of your own. A narrower set of capabilities, rather than the ability to get every bit of information under the sun, can suit specific needs just fine. A medical ai is unlikely to need to generate amusing limericks in the style of William Shakespeare, as ChatGPT does so successfully. This still requires computing power and bespoke data sets. But it could help adapt ai in more varied and useful ways.Some countries are already harnessing ai. China’s prowess is second only to America’s, thanks to its tech know-how and the deep pockets of its internet giants. India’s outsourcing industry could be disrupted, as some back-office tasks are taken on by generative ai. But it is home to a vibrant startup scene, as well as millions of tech developers and a government that is keen to use ai to improve its digital infrastructure. These leave it well-placed to innovate and adapt. Countries in the Gulf, such as the United Arab Emirates and Saudi Arabia, are determined to build an ai industry as they shift from oil. They already have the capital and are importing the talent.Each country will shape the technology in its own way. Chinese chatbots have been trained to keep off the subject of Xi Jinping; India’s developers are focused on lowering language barriers; the Gulf is building an Arabic large language model. Though the global south will not dislodge America’s crown, it could benefit widely from all this expertise.Teaching AIdPlenty could yet go wrong, obviously. The technology is still evolving. Computing power could become too expensive; local data will need to be gathered and stored. Some practitioners may lack the ability to take advantage of the knowledge at their fingertips, or the incentive to try new things. Although countries in sub-Saharan Africa stand to gain the most from improvements to human capital and government services, the technology will spread more slowly there than elsewhere without better connectivity, governance and regulation.The good news is that investments to speed ai’s diffusion will be richly rewarded. Much about the ai revolution is still uncertain, but there is no doubt that the technology will have many uses and that it will only get better. Emerging countries have suffered disappointments before. This time they have a wonderful opportunity—and the power to seize it. ■For subscribers only: to see how we design each week’s cover, sign up to our weekly Cover Story newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.AT THE HEART of Britain’s publicly funded health-care system lies a contradiction. The National Health Service generates and holds vast swathes of data on Britons’ health, organised using NHS numbers assigned to every person in its care. The system enables world-leading studies, like the RECOVERY trial during the pandemic, which discovered treatments for covid-19. You might suppose it to be a treasure trove for artificial-intelligence (AI) developers eager to bring their models to bear on improving human health. Yet if you put this to a developer they will roll their eyes and tell you why all is not as rosy as it seems.That is because the kinds of tabular data that inform clinical trials—who took which drug, what the outcome was—are not the same as those most useful for training machine-learning models, such as scans or genomes, which hold more information about a patient. Much of this sort of NHS data is a mess, organised in ways which serve doctors treating patients, but not AI developers hoping to feed it to computers. Making it suitable for those models is a task with which the NHS has not yet come to grips. It is often easier for those seeking to organise these richer data to start from scratch, as with a vast data-collection exercise now under way.To open up the NHS’s data riches to AI, its managers and political masters should turn to three principles: cleanliness, comparability and consent. Cleanliness starts with hosting rich data in cloud-computing environments where the data are easier for AI developers to wrangle. Hospitals and clinics also need greater incentives to prepare their datasets for machines. Most of the NHS’s successful AI projects so far have relied on the drive of dedicated, intellectually curious doctors who have had to fight the system rather than be helped by it. Forging stronger links between the NHS and universities—and giving PhD students easier access to datasets—is another good idea.A more open approach to licensing intellectual property would also help. Too often, the NHS demands fees and terms so steep and strict that they deter developers. It should see the big picture and accept smaller fees, to incentivise the building of clean datasets. That will mean less money proportionally for the NHS, and possible riches for developers, but in the long run would benefit the service and its patients. And if used outside Britain, it might mean more revenue overall.Comparability of data is also vital. Though everyone has an NHS number, scans are often gathered and stored in different ways in different places, making it harder to create large datasets for machine learning. The NHS is poised to announce the winner of a contract to link up disparate datasets. This will help, but more is needed. For example, scans of the same type should be carried out in ways similar enough to allow AI to detect signals of health rather than differences in the scanning process.The final pillar is consent. Though everyone wins if everyone lets their data be fed to computers, Britons should be allowed to opt out. Politicians must persuade people of the benefits of vast datasets in which everyone—young or old, black or white—is represented. They must also reassure them that their data will be anonymised, and not used to their detriment, for instance by insurers. The NHS has no time to waste. The rewards on offer are better, earlier diagnosis of disease, and a more productive, efficient system. That is sorely needed when waiting lists are long and funds squeezed. The NHS’s position as a world leader in data-heavy trials faces a stiff threat from health systems in other places, which are digitising rapidly. Abu Dhabi, for example, is considering feeding health-care data into foundation models, and may open up its trained models to the world. Consumer technology—smartphones, watches and devices connected to them—is fast improving its capacity to peer inside the human body. It may one day begin to rival the scanning capacity of the NHS, usurping it as the easiest and cheapest channel for the provision of algorithmic health care.The economy stands to gain, too. NHS data could be the basis of a thriving export industry, licensing AI tools to health-care systems around the world. But if it does not clean up its digital act, Britain will become a taker of new health technology, just as it has become a taker of American digital services like online search and social media. That would be a missed opportunity, and the beginning of the end of the data primacy of the NHS. ■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.In “Wall-E”, a film released in 2008, humans live in what could be described as a world of fully automated luxury communism. Artificially intelligent robots, which take wonderfully diverse forms, are responsible for all productive labour. People get fat, hover in armchairs and watch television. The “Culture” series by Iain M. Banks, a Scottish novelist, goes further, considering a world in which ai has grown sufficiently powerful as to be superintelligent—operating far beyond anything now foreseeable. The books are favourites of Jeff Bezos and Elon Musk, the bosses of Amazon and Tesla, respectively. In the world spun by Banks, scarcity is a thing of the past and ai “minds” direct most production. Humans turn to art, explore the cultures of the vast universe and indulge in straightforwardly hedonistic pleasures.Such stories may seem far-fetched. But rapid progress in generative ai—the sort that underpins Openai’s popular chatbot, Chatgpt—has caused many to take them more seriously. On May 22nd Openai’s founders published a blog post saying that “it’s conceivable that within the next ten years, ai systems will exceed expert skill level in most domains, and carry out as much productive activity as one of today’s largest corporations.” Last summer forecasters on Metaculus, an online prediction platform that is a favourite of many techies, thought it would take until the early 2040s to produce an ai capable of tricking humans into thinking that it was human after a two-hour chat, had good enough robotic capabilities to assemble a model car and could pass various other challenging cognitive tests. After a year of astonishing ai breakthroughs, Metaculus forecasters now think that this will happen by the early 2030s. There is no shortage of money for research, either. Five new generative-ai unicorns (startups valued at $1bn or more) have already been minted this year. The road to a general ai—one better than the very best of humanity at everything—could take longer than expected. Nevertheless, the rising possibility of ultra-powerful ai raises the question of what would be left for humans when it arrives. Would they become couch potatoes as in “Wall-E”? Here is a thought experiment, guided by the principles of economics, to provide something of an answer.AI is your oysterInevitably, such a thought experiment involves some fairly heroic assumptions. For a start, we suppose that ai will be benevolent, controllable and distinguishable from humans. We also suppose that human culture will not be radically altered by technological progress to the point that people begin to love or even worship ais. Instead, we imagine ai as a tool: a virtual, super-smart, dirt-cheap bot. We assume that constraints on the widespread use of ai, such as energy limits, will be resolved. None of this is guaranteed, but it helps make an exercise like this possible.In 2019 Philippe Aghion, Ben Jones and Chad Jones, three economists, modelled the impact of ai. They found that explosive economic growth was plausible if ai could be used to automate all production, including the process of research itself—and thus self-improve. A nearly unlimited number of ais could work together on any given problem, opening up vast scientific possibilities. Yet their modelling carried an important caveat. If ai automated most but not all production, or most but not all of the research process, growth would not take off. As the economists put it: “Economic growth may be constrained not by what we do well but rather by what is essential and yet hard to improve.”An idea put forward by William Baumol, a late economist, offers an explanation for this. In a paper published in 1965, he and William Bowen, a colleague, examined wages in the performing arts. They noted that the “output per man-hour of the violinist playing a Schubert quartet in a standard concert hall is relatively fixed”. Even as technological progress made other industries more productive, the performing arts remained unaffected. Because humans were still willing to spend on the arts even as prices rose—demand was “inelastic”—the arts took up more of gdp and therefore weighed on overall growth.Baumol’s example points to a broader principle. If the domains that ai is able to fully automate are only imperfect substitutes for those which it cannot, and the demand for non-automatable industries is hard to budge, then the unproductive sectors will grow as a share of gdp, reducing overall growth. Messrs Aghion, Jones and Jones note that this is in fact what has happened across much of the past century. Technology has automated swathes of agriculture and manufacturing, driving down the relative price of their outputs. As a result, people have spent a greater share of their incomes on industries such as education, health care and recreation, which have not seen the same productivity gains.Will Baumol’s story matter in a world in which ai is more capable than the most talented humans? If the ai is not embodied—maybe because progress in robotics lags that in computing—then the answer is surely yes. Much of the economy, including construction and manufacturing, is decidedly physical. There are countless forms of employment, including many in health care, that require a combination of braininess and an ability to traverse the physical world. These jobs would only increase in importance in a scenario where ai began to dominate cognitive labour. Humans would work in the physical world, perhaps under the guidance of ai “chief executives” or “professors”.But what if ultra-powerful ai develops super-humanoid robots, too? Material needs would almost certainly be met by machine hands. One might then expect humanity to give up on toil, much like in “Wall-E”. Indeed, in 1930 John Maynard Keynes, another economist, penned an essay entitled “Economic Possibilities for our Grandchildren”, in which he speculated that a century in the future people would work for less than 15 hours a week. The growth generated by technology would solve the “economic problem”, he predicted, and allow people to turn their attention to activities that are intrinsically pleasurable. Admittedly, Keynes’s 15-hour work week has not arrived—but higher levels of wealth, which may increase the appeal of leisure, have cut working hours much as he expected. The average number of hours worked a week in the rich world has fallen from around 60 in the late 19th century to under 40 today. There are, nevertheless, some wants that perhaps only humans can satisfy even in a world of supercharged, embodied ai. It is also worth noting that what is intrinsically pleasurable may include work. Consider three areas where humans may still have a role: work that is blurred with play, play itself and work where humans retain some kind of an advantage.Fun and gamesStart with the blurring boundary between work and play. Although working hours have fallen over the past century, most of the drop was before the 1980s. Increasingly, rich people labour for longer than poorer people. Keynes’s essay hints at an explanation for this odd development. He divided human desires in two: “Those needs which are absolute in the sense that we feel them whatever the situation of our fellow human beings may be, and those which are relative in the sense that we feel them only if their satisfaction lifts us above, makes us feel superior to, our fellows.”Keynes perhaps underestimated the size of this second class of wants. A cynic might suggest that entire academic disciplines fall into it: existing with no apparent value to the world, with academics nevertheless competing furiously for status based on their braininess. Economists would say that, for many, work has become a “consumption good”, offering far more utility than the income it generates.Games offer another hint as to why people may not stop working altogether. Millions of people are employed in entertainment and sports, competing for clout in activities that some consider immaterial. Perhaps when ais overtake humans, interest in watching such games will wane. But evidence from sports where humans are already second-rate suggests otherwise. Since ibm’s DeepBlue defeated Garry Kasparov, the world grandmaster, in chess in 1997, interest in the game has only increased. Other games that have been “solved” by ai, including Go, an ancient Chinese board game, and competitive video games, have witnessed a similar pattern. Across the world the number of video-game players has nearly doubled in the past decade, reaching 3.2bn last year. Nowadays a growing class of gamers compete or stream for a living.ai might supercharge this interest. As Banks speculated, humans might specialise in “the things that really [matter] in life, such as sport, games, romance, studying dead languages, barbarian societies and impossible problems, and climbing high mountains without the aid of a safety harness.” Other humans would presumably want to watch them, too.It seems unlikely that people will give up control of politics to robots. Once ais surpass humans, people will presumably pay even closer attention to them. Some political tasks might be delegated: humans could, for instance, put their preferences into an ai model that produces proposals for how to balance them. Yet as a number of political philosophers, including John Locke in the 17th century and John Rawls in the 20th, have argued, participation in political procedures gives outcomes legitimacy in the eyes of fellow citizens. There would also be more cynical considerations at play. Humans like to have influence over one another. This would be true even in a world in which everyone’s basic needs and wants are met by machines. Indeed, the wealthiest 1% of Americans participate politically at two to three times the rate of the general public on a range of measures from voting to time spent on politics.Last, consider areas where humans have an advantage in providing a good or service—call it a “human premium”. This premium would preserve demand for labour even in an age of superadvanced ai. One place where this might be true is in making private information public. So long as people are more willing to share their secrets with other people than machines, there will be a role for those who are trusted to reveal that information to the world selectively, ready for it then to be ingested by machines. Your correspondent would like to think that investigative journalists will still have jobs.The human premium might appear elsewhere, too. People value history, myths and meaning. Non-fungible tokens, for which provenance can be verified on a blockchain, are typically valued at many multiples more than images with identical pixels but a different history. In areas such as caregiving and therapy, humans derive value from others spending their scarce time with them, which adds feeling to an interaction. Artificial diamonds, which have the same molecular structure as those from the ground, trade at an enormous discount—around 70% by one estimate. In the future, items with a “made by a human” tag might be especially desirable.People problemsIf this premium is big enough, it could even weigh on growth. Divide the sectors of the economy into those with a large human premium and those without. If humans do not substitute machine-produced goods and services for those made by fellow humans, the Baumol effect would only deepen. Measured economic growth could even hit zero. Indeed, if extremely powerful AI failed to supercharge growth, it would suggest that the economy had already moved beyond materiality towards play, politics and areas where what people value most of all is interacting with others.Perhaps one day AIs will produce entirely new goods and services that will outcompete the desire to please and interact with other humans. The manner in which such a contest played out would reveal something profound: just how much of a “social animal” is a human? ■Correction (May 31st 2023): This article previously misstated the average hours worked per week, suggesting that the figure was 60 in the late 20th century, rather than the late 19th century. Sorry.For more expert analysis of the biggest stories in economics, finance and markets, sign up to Money Talks, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Consider two approaches in the music industry to artificial intelligence (AI). One is that of Giles Martin, son of Sir George Martin, producer of the Beatles. Last year, in order to remix the Fab Four’s 1966 album “Revolver”, he used AI to learn the sound of each band member’s instruments (eg, John Lennon’s guitar) from a mono master tape so that he could separate them and reverse engineer them into stereo. The result is glorious. The other approach is not bad either. It is the response of Nick Cave, a moody Australian singer-songwriter, when reviewing lyrics written in his style by ChatGPT, an AI tool developed by a startup called OpenAI. “This song sucks,” he wrote. “Writing a good song is not mimicry, or replication, or pastiche, it is the opposite. It is an act of self-murder that destroys all one has strived to produce in the past.”Mr Cave is unlikely to be impressed by the latest version of the algorithm behind ChatGPT, dubbed GPT-4, which OpenAI unveiled on March 14th. Mr Martin may find it useful. Michael Nash, chief digital officer at Universal Music Group, the world’s biggest label, cites their examples as evidence of both excitement and fear about the AI behind content-creating apps like ChatGPT (for text) or Stable Diffusion (for images). It could help the creative process. It could also destroy or usurp it. Yet for recorded music at large, the coming of the bots brings to mind a seismic event in its history: the rapid rise and fall of Napster, a platform for sharing mainly pirated songs at the turn of the millennium. Napster was ultimately brought down by copyright law. For aggressive bot providers accused of riding roughshod over intellectual property (IP), Mr Nash has a simple message that sounds, from a music-industry veteran of the Napster era, like a threat. “Don’t deploy in the market and beg for forgiveness. That’s the Napster approach.”The main issue here is not AI-made parodies of Mr Cave or faux-Shakespearean sonnets. It is the oceans of copyrighted data the bots have siphoned up while being trained to create humanlike content. That information comes from everywhere: social-media feeds, internet searches, digital libraries, television, radio, banks of statistics and so on. Often, it is alleged, AI models plunder the databases without permission. Those responsible for the source material complain that their work is hoovered up without consent, credit or compensation. In short, some AI platforms may be doing with other media what Napster did with songs—ignoring copyright altogether. The lawsuits have started to fly.It is a legal minefield with implications that extend beyond the creative industries to any business where machine-learning plays a role, such as self-driving cars, medical diagnostics, factory robotics and insurance-risk management. The European Union, true to bureaucratic form, has a directive on copyright that refers to data-mining (written before the recent bot boom). Experts say America lacks case history specific to generative AI. Instead, it has competing theories about whether or not data-mining without licences is permissible under the “fair use” doctrine. Napster also tried to deploy “fair use” as a defence in America—and failed. That is not to say that the outcome will be the same this time.The main arguments around “fair use” are fascinating. To borrow from a masterclass on the topic by Mark Lemley and Bryan Casey in the Texas Law Review, a journal, use of copyrighted works is considered fair when it serves a valuable social purpose, the source material is transformed from the original and it does not affect the copyright owners’ core market. Critics argue that AIs do not transform but exploit the entirety of the databases they mine. They claim that the firms behind machine learning abuse fair use to “free-ride” on the work of individuals. And they contend that this threatens the livelihoods of the creators, as well as society at large if the AI promotes mass surveillance and the spread of misinformation. The authors weigh these arguments against the fact that the more access to training sets there is, the better AI will be, and that without such access there may be no AI at all. In other words, the industry might die in its infancy. They describe it as one of the most important legal questions of the century: “Will copyright law allow robots to learn?”An early lawsuit attracting attention is from Getty Images. The photography agency accuses Stability AI, which owns Stable Diffusion, of infringing its copyright on millions of photos from its collection in order to build an image-generating AI model that will compete with Getty. Provided the case is not settled out of court, it could set a precedent on fair use. An even more important verdict could come soon from America’s Supreme Court in a case involving the transformation of copyrighted images of Prince, a pop idol, by the late Andy Warhol, an artist. Daniel Gervais, an IP expert at Vanderbilt Law School in Nashville, believes the justices may provide long-awaited guidance on fair use in general.Scraping copyrighted data is not the only legal issue generative AI faces. In many jurisdictions copyright applies only to work created by humans, hence the extent to which bots can claim IP protection for the stuff they generate is another grey area. Outside the courtrooms the biggest questions will be political, including whether or not generative AI should enjoy the same liability protections for the content it displays as social-media platforms do, and to what extent it jeopardises data privacy.The copyrighting is on the wallYet the IP battle will be a big one. Mr Nash says creative industries should swiftly take a stand to ensure artists’ output is licensed and used ethically in training AI models. He urges AI firms to “document and disclose” their sources. But, he acknowledges, it is a delicate balance. Creative types do not want to sound like enemies of progress. Many may benefit from AI in their work. The lesson from Napster’s “reality therapy”, as Mr Nash calls it, is that it is better to engage with new technologies than hope they go away. Maybe this time it won’t take 15 years of crumbling revenues to learn it. ■Read more from Schumpeter, our columnist on global business:How to stop the commoditisation of container shipping (Mar 9th)Lessons from Novo Nordisk on the stampede for obesity drugs (Mar 2nd)It’s time for Alphabet to spin off YouTube (Feb 23rd)Also: How the Schumpeter column got its name
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Will artificial intelligence kill us all? Some technologists sincerely believe the answer is yes. In one nightmarish scenario, AI eventually outsmarts humanity and goes rogue, taking over computers and factories and filling the sky with killer drones. In another, large language models (LLMs) of the sort that power generative ais like ChatGPT give bad guys the know-how to create devastating cyberweapons and deadly new pathogens.It is time to think hard about these doomsday scenarios. Not because they have become more probable—no one knows how likely they are—but because policymakers around the world are mulling measures to guard against them. The European Union is finalising an expansive AI act; the White House is expected soon to issue an executive order aimed at llms; and on November 1st and 2nd the British government will convene world leaders and tech bosses for an “AI Safety Summit” to discuss the extreme risks that AI models may pose.Governments cannot ignore a technology that could change the world profoundly, and any credible threat to humanity should be taken seriously. Regulators have been too slow in the past. Many wish they had acted faster to police social media in the 2010s, and are keen to be on the front foot this time. But there is danger, too, in acting hastily. If they go too fast, policymakers could create global rules and institutions that are aimed at the wrong problems, are ineffective against the real ones and which stifle innovation.The idea that AI could drive humanity to extinction is still entirely speculative. No one yet knows how such a threat might materialise. No common methods exist to establish what counts as risky, much less to evaluate models against a benchmark for danger. Plenty of research needs to be done before standards and rules can be set. This is why a growing number of tech executives say the world needs a body to study AI much like the Intergovernmental Panel on Climate Change (IPCC), which tracks and explains global warming.A rush to regulate away tail risks could distract policymakers from less apocalyptic but more pressing problems. New laws may be needed to govern the use of copyrighted materials when training LLMs, or to define privacy rights as models guzzle personal data. And ai will make it much easier to produce disinformation, a thorny problem for every society.Hasty regulation could also stifle competition and innovation. Because of the computing resources and technical skills required, only a handful of companies have so far developed powerful “frontier” models. New regulation could easily entrench the incumbents and block out competitors, not least because the biggest model-makers are working closely with governments on writing the rule book. A focus on extreme risks is likely to make regulators wary of open-source models, which are freely available and can easily be modified; until recently the White House was rumoured to be considering banning firms from releasing frontier open-source models. Yet if those risks do not materialise, restraining open-source models would serve only to limit an important source of competition.Regulators must be prepared to react quickly if needed, but should not be rushed into setting rules or building institutions that turn out to be unnecessary or harmful. Too little is known about the direction of generative AI to understand the risks associated with it, let alone manage them.The best that governments can do now is to set up the infrastructure to study the technology and its potential perils, and ensure that those working on the problem have adequate resources. In today’s fractious world, it will be hard to establish an IPCC-like body, and for it to thrive. But bodies that already work on AI-related questions, such as the OECD and Britain’s newish Frontier AI Taskforce, which aims to gain access to models’ nuts and bolts, could work closely together.It would help if governments agreed to a code of conduct for model-makers, much like the “voluntary commitments” negotiated by the White House and to which 15 makers of proprietary models have already signed up. These oblige model-makers, among other things, to share information about how they are managing AI risk. Though the commitments are not binding, they may help avoid a dangerous free-for-all. Makers of open-source models, too, should be urged to join up.As AI develops further, regulators will have a far better idea of what risks they are guarding against, and consequently what the rule book should look like. A fully fledged regime could eventually look rather like those for other technologies of world-changing import, such as nuclear power or bioengineering. But creating it will take time—and deliberation. ■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.PAUL HUDSON, boss of Sanofi, is brandishing an iPhone. He is keen to show off the French drugmaker’s new artificial-intelligence (AI) app, plai. It draws on more than 1bn data points to provide “snackable” information, from warnings about low stocks of a drug to questions for a meeting with an ad agency or suggestions to set up clinical-trial sites that could expedite drug approvals. Like Netflix recommendations, plai delivers “nudges”, as Mr Hudson calls them, that are useful at that moment in time. He jokes that plai broke even in about four hours, and says the cost is “peanuts” compared with the $300m-400m that big consultancies charge for a project to curate a big company’s data. One in ten of Sanofi’s 80,000 staff uses it every day.AI is not new in drugmaking. Biotech firms have been tinkering with it for years. Now interest from big pharma is growing. Last year Emma Walmsley, chief executive of GSK, said it could improve the productivity of research and development, the industry’s most profound challenge. Moderna recently described itself as “laser-focused” on AI. Sanofi is “all in”. Morgan Stanley, an investment bank, reckons that within a decade the pharmaceutical industry may be spending $50bn a year on AI to speed up drug development.Most of the buzz revolves around AIs trained on biological data that could improve the hit-and-miss process of drug discovery. Drugs can take a decade to emerge, cost billions of dollars and succeed only 10% of the time. Even a small improvement in speed and efficiency would be hugely valuable. But scientists have struggled to tame biological big data with conventional statistical tools. Machine learning makes it possible to sift through piles of information, from clinical patient data and genome sequences to images of body scans. Last year DeepMind, an AI lab that is part of Google, made a breakthrough using its AlphaFold system to predict the structure of almost all proteins, which may one day help identify which molecules have therapeutic potential.Though only around a dozen drugs in development have so far involved the use of AI, the list may grow rapidly—especially for simple molecules with properties that are relatively easy to predict. In the case of these more straightforward chemistries, the future of medicine is looking ever more like a computational problem.Jim Weatherall, who oversees data science and AI at AstraZeneca, says the technology is used in 70% of the British firm’s small molecules in development. Using a technique called “reinforcement learning”, AstraZeneca’s AI is constantly tweaking its molecular suggestions and playing out how a tweaked molecule might react. Ali Mortazavi, boss of E-therapeutics, a biotech startup in London, says that knowing the sequences of all the genes in, say, the liver, lets his firm use software to design RNA molecules (which are more complex but, owing to their links to DNA, predictably so). AI algorithms then predict the activity of the molecules, which can stop the function of any disease-causing gene.Euan Ashley of Stanford University points to another AI application. “Knowledge graphs” are a kind of database that stores data about genes, proteins, diseases and drugs, as well as the biological pathways that connect them. They, too, can help identify new targets for drug development. “Generative” AI, meanwhile, is being trialled for suggesting entirely new chemical and biological structures for testing, just as ChatGPT can ingest text on the internet and spit out a new poem or essay. Beyond drug discovery, AIs like plai could help with the perennial problem of efficiency in a heavily regulated and labour-intensive sector.Some pharma bosses worry that generative AIs’ tendency to make stuff up could send researchers down blind alleys. More apocalyptically, Mr Hudson says half of the pharma CEOs he talks to about AI fear, like many people, the existential threats it poses. For his part, he foresees the next industrial revolution, not a robot uprising. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.HE is in high demand. Last year Refik Anadol projected luminous images of coral on to a wall at the World Economic Forum in Davos and covered the exterior screen of the Sphere, a new concert venue in Las Vegas, with animated, tumbling blue blocks. In October the Museum of Modern Art (MoMA) in New York acquired “Unsupervised—Machine Hallucinations”, in which a machine-learning model generates artworks based on those in the museum’s collection. On February 16th “Echoes of the Earth”, his largest-ever show in Britain, opened at the Serpentine North Gallery in London.Mr Anadol, a 38-year-old Turk who lives in Los Angeles, is riding widespread public interest in artificial intelligence to become the most visible digital artist of his generation. His work reflects the innovation and anxieties of the current moment. As Mr Anadol sees it, AI is a powerful creative tool. In a world where so much of life happens in a digital realm, he argues, data has become a new “pigment”.He is steeped in both art and science, having completed several arts degrees and a residency at Google focused on machine learning. Mr Anadol trains AI models on massive troves of data, often publicly available, to create raucously colourful animations that he calls “dreams” or “hallucinations”. They swirl on superbright screens (creating what he calls “data paintings”) and wiggle on walls (“data sculptures”); sometimes the pieces illuminate buildings, as at the Sphere.One artwork, displayed at the Venice Architecture Biennale, drew on 70 terabytes of brain scans, allowing AI models to imagine the organ’s development. Another piece used an archive of the Los Angeles Philharmonic’s performances to imitate dreaming. A third assimilated more than 138,000 images and pieces of metadata from MoMA’s collection (pertaining to provenance, for instance), along with local weather and data about noise levels. The results are churning clouds and waves, as well as abstractions evocative of Mark Rothko, a celebrated painter.To undertake work on this scale Mr Anadol employs around 30 people, including architects, designers and engineers, half of whom work in his studio in Los Angeles. Public projects with institutions and companies have boosted his profile, but some private collectors have bought pieces, too. Mr Anadol also mints non-fungible tokens, digital artefacts that sometimes come with physical works.The animations have proved popular: around 2.4m people came to see an exhibition of his work at MoMA in 2022. Mr Anadol’s style is accessible and often beguiling. Understanding how machine learning works may help you fathom the process behind the “data paintings”, but it is not essential. (In some installations, a control panel pops up to explain the model, giving the illusion of glancing under the bonnet but mostly evoking the futurism of “The Matrix”.) You can be swept along by the crashing tides of colour, or watch a rose turn into a lily, without wondering whether you are “getting it”.Naturally Mr Anadol has critics as well as admirers. Some compare his animations to glorified screensavers and lava lamps, more spectacular than substantial. (Some do look as if they belong at a hotel in Las Vegas or at Burning Man.) Like anything generated by an AI model, Mr Anadol’s animations raise questions about originality and whether such creations simply recycle the work of others.Some worry that he glorifies AI while ignoring its risks, by presenting a rosy (or deep purple or yellow) view of the tech’s potential. Casey Reas, one of Mr Anadol’s former teachers, says many in the art world are prejudiced against digital art, as they once were against photography, but concedes that “sometimes Refik’s work can appear to have a utopian view of technology”. The artist has appeared twice at TED and is fluent in the breathless Silicon Valley idiolect of “breakthroughs” and “inclusivity”.image: Steve KehayiasWearing all black, in an all-black room in his studio, illuminated only by high-definition screens, Mr Anadol acknowledges that AI is changing everything—and not always for the better. But he is indeed excited about what the technology can do in the right hands. “I don’t see the problem there,” he says. “I see possibilities.”His latest project, on display at the Serpentine, is “Living Archive: Large Nature Model”, which trained AI models on photographs, sounds and other kinds of scientific information collected at 16 rainforests across the globe. In addition to his usual sponsors, Google and Nvidia, institutions including the Natural History Museum in London and the Smithsonian have also furnished Mr Anadol with images and data.Photo synthesisMr Anadol prompts the model to create his trademark abstractions, as well as hyper-realistic creatures. Eagles morph into owls, which turn into toucans; the overarching point is the connectivity of the natural world. AI offers “a new brush, a thinking brush”, he says.He hopes that his work will educate people and help them “discover new worlds”. A viewer might prompt a model with the name of a plant, and it will generate a new one right in front of them. The artist’s ultimate goal is a place called Dataland: a fully immersive experience, including sounds and smells. Hans Ulrich Obrist, the Serpentine’s artistic director, says that Mr Anadol “makes the invisible visible”; he captures the power of technology as he turns AI from an abstraction in the cloud into art before the eyes. Whether that art looks like a dream or a beautiful banality is up to viewers. But like it or not, people will be seeing a lot more of Mr Anadol’s work. ■For more on the latest books, films, TV shows, albums and controversies, sign up to Plot Twist, our weekly subscriber-only newsletter
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.The remarkable capabilities of generative artificial intelligence (AI) are clear the moment you try it. But remarkableness is also a problem for managers. Working out what to do with a new technology is harder when it can affect so many activities; when its adoption depends not just on the abilities of machines but also on pesky humans; and when it has some surprising flaws.Study after study rams home the potential of large language models (LLMs), which power AIs like ChatGPT, to improve all manner of things. LLMs can save time, by generating meeting summaries, analysing data or drafting press releases. They can sharpen up customer service. They cannot put up IKEA bookshelves—but nor can humans.AI can even boost innovation. Karan Girotra of Cornell University and his co-authors compared the idea-generating abilities of the latest version of ChatGPT with those of students at an elite university. A lone human can come up with about five ideas in 15 minutes; arm the human with the AI and the number goes up to 200. Crucially, the quality of these ideas is better, at least judged by purchase-intent surveys for new product concepts. Such possibilities can paralyse bosses; when you can do everything, it’s easy to do nothing.In our new seven-part podcast series, Boss Class, our Bartleby columnist searches for the secrets to being a better manager.  Episode six looks at how to motivate staff and episode seven asks how managers should manage themselves.LLMs’ ease of use also has pluses and minuses. On the plus side, more applications for generative AI can be found if more people are trying it. Familiarity with LLMs will make people better at using them. Reid Hoffman, a serial AI investor (and a guest on this week’s final episode of “Boss Class”, our management podcast), has a simple bit of advice: start playing with it. If you asked ChatGPT to write a haiku a year ago and have not touched it since, you have more to do.Familiarity may also counter the human instinct to be wary of automation. A paper by Siliang Tong of Nanyang Technological University and his co-authors that was published in 2021, before generative AI was all the rage, captured this suspicion neatly. It showed that AI-generated feedback improved employee performance more than feedback from human managers. However, disclosing that the feedback came from a machine had the opposite effect: it undermined trust, stoked fears of job insecurity and hurt performance. Exposure to LLMs could soothe concerns.Or not. Complicating things are flaws in the technology. The Cambridge Dictionary has named “hallucinate” as its word of the year, in tribute to the tendency of LLMs to spew out false information. The models are evolving rapidly and ought to get better on this score, at least. But some problems are baked in, according to a new paper by R. Thomas McCoy of Princeton University and his co-authors.Because off-the-shelf models are trained on internet data to predict the next word in an answer on a probabilistic basis, they can be tripped up by surprising things. Get GPT-4, the LLM behind ChatGPT, to multiply a number by 9/5 and add 32, and it does well; ask it to multiply the same number by 7/5 and add 31, and it does considerably less well. The difference is explained by the fact that the first calculation is how you convert Celsius to Fahrenheit, and therefore common on the internet; the second is rare and so does not feature much in the training data. Such pitfalls will exist in proprietary models, too.On top of all this is a practical problem: it is hard for firms to keep track of employees’ use of AI. Confidential data might be uploaded and potentially leak out in a subsequent conversation. Earlier this year Samsung, an electronics giant, clamped down on usage of ChatGPT by employees after engineers reportedly shared source code with the chatbot.This combination of superpowers, simplicity and stumbles is a messy one for bosses to navigate. But it points to a few rules of thumb. Be targeted. Some consultants like to talk about the “lighthouse approach”—picking a contained project that has signalling value to the rest of the organisation. Rather than banning the use of LLMs, have guidelines on what information can be put into them. Be on top of how the tech works: this is not like driving a car and not caring what is under the hood. Above all, use it yourself. Generative AI may feel magical. But it is hard work to get right.■Correction (28th November): An earlier version of this article stated that the study by Karan Girotra and his co-authors took place at several elite American universities. It actually took place at just one elite university. It also stated that R. Thomas McCoy’s co-authors are also at Princeton University. Not all of them still are. Apologies.Read more from Bartleby, our columnist on management and work:How not to motivate your employees (Nov 20th)The curse of the badly run meeting (Nov 13th)How to manage teams in a world designed for individuals (Nov 6th)Also: How the Bartleby column got its name
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.One of the joys of writing about business is that rare moment when you realise conventions are shifting in front of you. It brings a shiver down the spine. Vaingloriously, you start scribbling down every detail of your surroundings, as if you are drafting the opening lines of a bestseller. It happened to your columnist recently in San Francisco, sitting in the pristine offices of Anthropic, a darling of the artificial-intelligence (AI) scene. When Jack Clark, one of Anthropic’s co-founders, drew an analogy between the Baruch Plan, a (failed) effort in 1946 to put the world’s atomic weapons under UN control, and the need for global co-ordination to prevent the proliferation of harmful AI, there was that old familiar tingle. When entrepreneurs compare their creations, even tangentially, to nuclear bombs, it feels like a turning point.Since ChatGPT burst onto the scene late last year there has been no shortage of angst about the existential risks posed by AI. But this is different. Listen to some of the field’s pioneers and they are less worried about a dystopian future when machines outthink humans, and more about the dangers lurking within the stuff they are making now. ChatGPT is an example of “generative” ai, which creates humanlike content based on its analysis of texts, images and sounds on the internet. Sam Altman, CEO of OpenAI, the startup that built it, told a congressional hearing this month that regulatory intervention is critical to manage the risks of the increasingly powerful “large language models” (LLMs) behind the bots.In the absence of rules, some of his counterparts in San Francisco say they have already set up back channels with government officials in Washington, DC, to discuss the potential harms discovered while examining their chatbots. These include toxic material, such as racism, and dangerous capabilities, like child-grooming or bomb-making. Mustafa Suleyman, co-founder of Inflection AI (and board member of The Economist’s parent company), plans in coming weeks to offer generous bounties to hackers who can discover vulnerabilities in his firm’s digital talking companion, Pi.Such caution makes this incipient tech boom look different from the past—at least on the surface. As usual, venture capital is rolling in. But unlike the “move fast and break things” approach of yesteryear, many of the startup pitches now are first and foremost about safety. The old Silicon Valley adage about regulation—that it is better to ask for forgiveness than permission—has been jettisoned. Startups such as OpenAI, Anthropic and Inflection are so keen to convey the idea that they won’t sacrifice safety just to make money that they have put in place corporate structures that constrain profit-maximisation.Another way in which this boom looks different is that the startups building their proprietary LLMs aren’t aiming to overturn the existing big-tech hierarchy. In fact they may help consolidate it. That is because their relationships with the tech giants leading in the race for generative AI are symbiotic. OpenAI is joined at the hip to Microsoft, a big investor that uses the former’s technology to improve its software and search products. Alphabet’s Google has a sizeable stake in Anthropic; on May 23rd the startup announced its latest funding round of $450m, which included more investment from the tech giant. Making their business ties even tighter, the young firms rely on big tech’s cloud-computing platforms to train their models on oceans of data, which enable the chatbots to behave like human interlocutors.Like the startups, Microsoft and Google are keen to show they take safety seriously—even as they battle each other fiercely in the chatbot race. They, too, argue that new rules are needed and that international co-operation on overseeing LLMs is essential. As Alphabet’s CEO, Sundar Pichai, put it, “AI is too important not to regulate, and too important not to regulate well.”Such overtures may be perfectly justified by the risks of misinformation, electoral manipulation, terrorism, job disruption and other potential hazards that increasingly powerful AI models may spawn. Yet it is worth bearing in mind that regulation will also bring benefits to the tech giants. That is because it tends to reinforce existing market structures, creating costs that incumbents find easiest to bear, and raising barriers to entry.This is important. If big tech uses regulation to fortify its position at the commanding heights of generative AI, there is a trade-off. The giants are more likely to deploy the technology to make their existing products better than to replace them altogether. They will seek to protect their core businesses (enterprise software in Microsoft’s case and search in Google’s). Instead of ushering in an era of Schumpeterian creative destruction, it will serve as a reminder that large incumbents currently control the innovation process—what some call “creative accumulation”. The technology may end up being less revolutionary than it could be.LLaMA on the loose Such an outcome is not a foregone conclusion. One of the wild cards is open-source AI, which has proliferated since March when LLaMa, the LLM developed by Meta, leaked online. Already the buzz in Silicon Valley is that open-source developers are able to build generative-AI models that are almost as good as the existing proprietary ones, and hundredths of the cost.Anthropic’s Mr Clark describes open-source AI as a “very troubling concept”. Though it is a good way of speeding up innovation, it is also inherently hard to control, whether in the hands of a hostile state or a 17-year-old ransomware-maker. Such concerns will be thrashed out as the world’s regulatory bodies grapple with generative AI. Microsoft and Google—and, by extension, their startup charges—have much deeper pockets than open-source developers to handle whatever the regulators come up with. They also have more at stake in preserving the stability of the information-technology system that has turned them into titans. For once, the desire for safety and for profits may be aligned. ■Read more from Schumpeter, our columnist on global business:America’s culture wars threaten its single market (May 18th)Writers on strike beware: Hollywood has changed for ever (May 10th)America needs a jab in its corporate backside (May 3rd)Also: If you want to write directly to Schumpeter, email him at [email protected]. And here is an explanation of how the Schumpeter column got its name.
NO OTHER FIRM has benefited from the boom in artificial intelligence (AI) as much as Nvidia. Since January 2023 the chipmaker’s share price has surged by almost 450%. With the total value of its shares approaching $2trn, Nvidia is now America’s third-most valuable firm, behind Microsoft and Apple. Its revenues for the most recent quarter were $22bn, up from $6bn in the same period last year. Most analysts expect that Nvidia, which controls more than 95% of the market for specialist AI chips, will continue to grow at a blistering pace for the foreseeable future. What makes its chips so special?Nvidia’s AI chips, also known as graphics processor units (GPUs) or “accelerators”, were initially designed for video games. They use parallel processing, breaking each computation into smaller chunks, then distributing them among multiple “cores”—the brains of the processor—in the chip. This means that a GPU can run calculations far faster than it would if it completed tasks sequentially. This approach is ideal for gaming: lifelike graphics require countless pixels to be rendered simultaneously on the screen. Nvidia’s high-performance chips now account for four-fifths of gaming GPUs.Happily for Nvidia, its chips have found much wider uses: cryptocurrency mining, self-driving cars and, most important, training of AI models. Machine-learning algorithms, which underpin AI, use a branch of deep learning called artificial neural networks. In these networks computers extract rules and patterns from massive datasets. Training a network involves large-scale computations—but because the tasks can be broken into smaller chunks, parallel processing is an ideal way to speed things up. A high-performance GPU can have more than a thousand cores, so it can handle thousands of calculations at the same time.Once Nvidia realised that its accelerators were highly efficient at training AI models, it focused on optimising them for that market. Its chips have kept pace with ever more complex AI models: in the decade to 2023 Nvidia increased the speed of its computations 1,000-fold.But Nvidia’s soaring valuation is not just because of faster chips. Its competitive edge extends to two other areas. One is networking. As AI models continue to grow, the data centres running them need thousands of GPUs lashed together to boost processing power (most computers use just a handful). Nvidia connects its GPUs through a high-performance network based on products from Mellanox, a supplier of networking technology that it acquired in 2019 for $7bn. This allows it to optimise the performance of its network of chips in a way that competitors can’t match.Nvidia’s other strength is CUDA, a software platform that allows customers to fine tune the performance of its processors. Nvidia has been investing in this software since the mid-2000s, and has long encouraged developers to use it to build and test AI applications. This has made CUDA the de facto industry standard.Nvidia’s juicy profit margins and the rapid growth of the AI accelerator market—projected to reach $400bn per year by 2027—have attracted competitors. Amazon and Alphabet are crafting AI chips for their data centres. Other big chipmakers and startups also want a slice of Nvidia’s business. In December 2023 Advanced Micro Devices, another chipmaker, unveiled a chip that by some measures is roughly twice as powerful as Nvidia’s most advanced chip.But even building better hardware may not be enough. Nvidia dominates AI chipmaking because it offers the best chips, the best networking kit and the best software. Any competitor hoping to displace the semiconductor behemoth will need to beat it in all three areas. That will be a tall order.■
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Backgammon was an easy win. Chess, harder. Go, harder still. But for some aficionados it is only now that artificial intelligence (AI) can truly say it has joined the game-playing club—for it has proved it can routinely beat humans at Diplomacy.For those unfamiliar with the game, its board is a map of Europe just before the first world war (except that, for no readily apparent reason, Montenegro is missing). Participants, seven ideally, each take on the role of one of the Great Powers: Austria, England, France, Germany, Italy, Russia and Turkey. Each has armies and navies, and geographically based resources to support them, and can use its forces to capture the territory of neighbours, thus gaining the means to raise more forces while depriving others of the same.The trick is that, at least at the beginning, players will get nowhere without making agreements to collaborate—yet they are not bound by the game’s rules to keep to these agreements. Only when orders for the movement of troops and vessels, which have to be written down, are revealed, does a player discover who really is a friend, or an enemy.Cicero, a program devised by a group of Mark Zuckerberg’s employees who dub themselves the Meta Fundamental AI Research Diplomacy Team, proved an adept pupil. As the team describe in Science, when they entered their creation into an online Diplomacy league, in which it played 40 games, it emerged as one of the top 10% of players—and no one rumbled that it was not human.In all past AI game-playing projects the program has learned by reinforcement. Playing repeatedly against itself or another version of itself, it acts first at random, then more selectively. Eventually, it learns how to achieve the desired goal. Cicero was taught this way, too. But that was only part of its training. Besides having the reasoning to plan a winning strategy, a successful Diplomacy player must also possess the communicative ability to implement it.The Meta team’s crucial contribution was therefore to augment reinforcement learning with natural-language processing. Large language models, trained on vast amounts of data to predict deleted words, have an uncanny ability to mimic the patterns of real language and say things that humans might. For Cicero, the team started with a pre-trained model with a baseline understanding of language, and fine-tuned this on dialogues from more than 40,000 past games, to teach it Diplomacy-specific patterns of speech.To play the game, Cicero looks at the board, remembers past moves and makes an educated guess as to what everyone else will want to do next. Then it tries to work out what makes sense for its own move, by choosing different goals, simulating what might happen, and also simulating how all the other players will react to that.Once it has come up with a move, it must work out what words to say to the others. To that end, the language model spits out possible messages, throws away the bad ideas and anything that is actual gobbledygook, and chooses the ones, appropriate to the recipients concerned, that its experience and algorithms suggest will most persuasively further its agenda.Cicero, then, can negotiate, convince, co-operate and compete. Seasoned Diplomacy players will, though, want to know something else: has it learned how to stab? Stabbing—saying one thing and doing another (especially, attacking a current ally) is seen by many as Diplomacy’s defining feature. But, though Cicero did “strategically withhold information from players in gameplay”, it did not actually stab any of its opponents. Perhaps it was this final lack of Machiavellian ruthlessness which explains why it was only in the top 10%, and not victor ludorum. ■Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.SOME RACES are over before they get going. So it can seem in the contest to make large language models (LLMs). These algorithms power ChatGPT-like “generative” artificial intelligence. OpenAI, the human-sounding chatbot’s American creator, appears leagues ahead. It has made the world’s most powerful LLM, called GPT-4. The firm is gobbling up talent, data and computing power to build cleverer models. That allows it to attract more users, and with them more capital to pour into even more sophisticated algorithms.But a French startup called Mistral is trying to throw a spanner in this AI flywheel. On February 26th it released a new LLM. Mistral-Large is smaller than GPT-4, measured by the number of parameters it uses (a common gauge of model power). Even so, it nearly rivals GPT-4 in its ability to reason. Mistral also unveiled a ChatGPT competitor, Le Chat (pronounced le shah, like the French word for cat rather than the English homograph). And it announced a deal with Microsoft, an AI juggernaut which already has a deep partnership with OpenAI. The tech giant will take a small stake in Mistral and make the French firm’s models available via its Azure cloud.Mistral is proof that the industry is becoming more open—and less American. If it can mount a serious challenge to OpenAI, this could show once and for all that in generative AI, size is not everything. “It’s no longer about being bigger—it’s about being creative and being fast,” says Arthur Mensch, Mistral’s chief executive.The French firm’s rise has been as brisk as the northwesterly wind after which it is named. It was founded less than a year ago and has just 25 employees. Yet its LLMs are leading the growing pack of open-source models, the statistical innards of which are, in contrast to proprietary black boxes like GPT-4, publicly available and can be modified by anyone. That has allowed Mistral to tap an impressive €490m ($531m) in funding, valuing the company at more than $2bn. Big investors include leading Silicon Valley venture capitalists such as Andreessen Horowitz and General Catalyst, as well as tech luminaries like Eric Schmidt, a former boss of Google.Mistral owes its early success to cleverly mixing the key technical ingredients of AI—talent, data and computing power—with politics, which is growing in importance as governments ponder the technology’s potential. When it comes to talent, Mistral is a “match made in heaven” between French engineering education and American big-tech firms, says Stanislas Polu, a co-founder of Dust, another Parisian AI firm. Three of Mistral’s six founders, and its technical brains—Mr Mensch, Timothée Lacroix and Guillaume Lample—are products of France’s elite technical schools. Like many top AI scientists they have worked at the research labs of Google and Meta, another American tech titan—though in the trio’s case they were building LLMs at those labs’ offshoots in the French capital rather than in London or Silicon Valley. This places them among the 100 or so people worldwide who really know how to train cutting-edge models.They appear particularly adept at marshalling training data—the second ingredient of AI success. Mr Mensch will not say how exactly Mistral curates its training sets; it is the source of his firm’s competitive advantage, he explains. But industry insiders confirm that Mistral is, in the words of one, “really clever” at filtering out information that is repetitive or does not make sense. This has allowed Mistral’s models to be much smaller: their parameters number in the billions, compared with an estimated 1.8trn for GPT-4 (both firms are mum on the precise sizes). This allows customers to run them on their own computers rather than in a vast data centre, which many proprietary models require.According to Mr Mensch, Mistral’s focus on data curation lets the firm use computing power, AI’s third crucial component, more efficiently than its competitors. Training Mistral’s latest model cost much less than the $100m that OpenAI apparently spent to develop GPT-4. Mistral’s approach also makes it cheaper for customers both to fine-tune its models with their own data and then to run them.In technical terms, startups like Mistral enjoy a “second-mover advantage”, benefiting from all the work OpenAI and others have done, argues Jeannette zu Fürstenberg of General Catalyst. Critically, in Mistral’s case these technical chops are complemented by political nous, which is helpful given many governments’ belief that home-grown LLMs confer economic and strategic advantages. Another of Mistral’s co-founders is Cédric O, a former French digital minister. Mr O retains a direct line to the country’s president, Emmanuel Macron. When a draft of the EU’s AI Act last year threatened to force Mistral to divulge its data recipe, Mr O co-ordinated, with Mr Macron’s backing, a Franco-German effort to oppose such provisions. These were duly excised from the bill.The question now is whether Mistral, which has yet to generate meaningful revenues, can transform this enticing techno-political mix into profits. The firm’s bet is that many businesses, especially in Europe, want more control over LLMs than OpenAI is willing to give them, and do not want to be locked into another American tech platform. Such customers, the thinking goes, would be willing to pay Mistral to maintain and run their models.One worry potential customers may have is how the world will regulate open-source models. A heated debate about whether they will enable bad actors to build bio- and cyber-weapons has died down. The discussion among policymakers is turning from risks to possible rewards: greater transparency, more innovation and less reliance on a handful of powerful companies that have controlled the technology. Regulators on both sides of the Atlantic have so far tolerated open-source LLMs. But Mr O may have his hands full if the models keep getting cleverer or are found to be misused, for instance helping to spread disinformation during this year’s welter of elections around the world.Avoiding a political backlash is, obviously, in Mistral’s interest—but lobbying success has a flipside. Regulatory forbearance may lead to more open-source competition. On February 20th Silo AI, a Finnish firm, unveiled a new LLM that is even more open than Mistral’s, furnishing information about the data on which it is trained and the software that did the job. A new version, due out in a few months, will be as good in most European languages as it is now in Finnish and English.Most important, it is still unclear if size matters for generative AI. A test will come when OpenAI at last releases its next model, GPT-5. If it leaves Mistral-Large in the dust, then Mr Mensch’s talk of creativity and speed may ring hollow. Until then, Mistral’s story will continue to resonate. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.In London’s Design Museum, an exhibition currently on display by Ai Weiwei, a Chinese artist, includes a 15-metre-long work called “Water Lilies #1” based on the triptych by Claude Monet. Look closely and it is made of 650,000 Lego bricks—which integrates Monet’s impressionism into what Mr Ai calls a “digitised and pixelated language”. That is a good analogy for Lego itself. The Danish toymaker is on a long-term mission to digitise and pixelate its own fount of human creativity: the plastic brick.Three digital experts from McKinsey, a management consultancy, profile Lego’s transformation as part of their new book, “Rewired”, which outlines the dos and don’ts for businesses rebuilding themselves for the age of digitisation. Beware: the language of digital transformation is treachery to common English. It sounds more like corporate yoga than a marathon of software development. Executives need to be aligned. Teams are pods. Be agile. Define your downward-facing domains. McKinsey, drawing lessons from 200 firms, provides clarity despite the mumbo jumbo. But to make it easier on the ear, Schumpeter will use Lego as a guide to help illustrate some of McKinsey’s insights. Call it the yellow-brick road to generative artificial intelligence (AI).First, it is a long hard road, littered with failures. Lego is a rare success story. Its journey started in 2003 with a near-death experience when, amid the rise of video-gaming, it panicked and went on a madcap innovation spree that almost bankrupted it. To fix one of the main problems, chaos in the supply chain, it introduced a single enterprise-software system globally. The system survives to this day, scaling up as Lego expands into new markets, such as China, new formats, such as e-commerce, and new factory locations, such as America and Vietnam. To prepare for a world of pixelated play, Lego launched digital games on the “Star Wars” theme and developed franchises of its own, such as Ninjago and Chima, with video games, films and TV shows that turned into hits.In 2019 Lego launched a new five-year transformation drive aimed at adapting to a world of direct-to-consumer sales, online versus big-box retailing, and digital play in the screen age. The timing was inspired. It started shortly before the world went into lockdown as a result of the covid-19 pandemic, when having a digital strategy became a matter of life and death. It quickly produced results. Although it is hard to strip out the exact contribution of digitisation, since 2018 Lego’s sales have almost doubled, to more than $9bn, outpacing those of Mattel and Hasbro, its main rivals. In 2022 visits to its online portal rose by 38%. It has teamed up with Epic, a video-gaming firm, to explore the metaverse.Yet the journey is still a hard one. The difficulties include moving from a system where success is measured by sales store-by-store to one judged by how good the company is at selling online across the globe, how it is ranked on Google and Amazon, and how effective its software is. The McKinsey authors emphasise such challenges on the first page. In a recent McKinsey survey, they say, about 90% of companies had some kind of digital strategy, but they captured less than a third of the revenue gains they had anticipated. Moreover, the success rate is more uneven within industries than it is between them. The best retailer may be more digitally productive than an average high-tech firm, and the worst retailer may be as bad as the worst government entity.To make a success of it requires learning the second lesson: what McKinsey calls having a top-down strategy and a road map (or in Lego terms, a clear instruction manual). For Lego, it helped that the family-owned business had long had a command-and-control approach to management. Its digital strategy involved a single plan, created by a 100-strong executive team and approved by the board, that encompassed the whole organisation. McKinsey notes that when transformations stall, it is often because executives talk past each other, have pet projects, spread investments too thin or have “more pilots than there are on an aircraft-carrier”, as Rodney Zemmel, one of the authors, puts it. It also needs to be ambitious enough to generate momentum, with financial results measured constantly. McKinsey’s rule of thumb is that a digital transformation should aim to increase earnings before interest, tax, depreciation and amortisation by 20% or more.Third comes the question of whether to build a new digital infrastructure or buy it. The answer is mostly to build. Rather like Lego’s eight-studded bricks—six of which can be combined 915m ways—there are many software applications on the market that can be combined to create proprietary systems. But the job of orchestrating them should not be outsourced. Take Lego: it started its latest digital transformation with engineers making up less than 30% of staff. Since then it has increased the number of systems and software engineers by 150%. Mr Zemmel notes that five years ago, the trend was to hire from Silicon Valley. That was “a good way to change the company dress code, but not a great way to change the company culture”. Since then more companies have been retraining their existing tech workers and embedding them throughout the organisations in more front-line roles.The gen-AI Weiwei way Some of these lessons apply to generative AI. Mr Zemmel says it is relatively easy to launch pilots with the technology, such as the humanlike ChatGPT. The problem is embedding the AI models across the organisation in a safe, unbiased way. It needs a top-down strategy. As for building or buying, Mr Zemmel says it may be a “waste of time” to build proprietary models when the software industry is doing that anyway. The key is to work in-house on the things that give you a decisive advantage in the market. For Lego, AI is still in the future, though some of its brick enthusiasts are already using ChatGPT-like programs to come up with new ways of building things. Mostly they fail, but one day anyone may be able to create a Monet. The yellow-brick road is unending. ■Read more from Schumpeter, our columnist on global business:Meet the world’s most flirtatious sovereign-wealth fund (Jun 29th)The new king of beers is a Mexican-American success story (Jun 20th)What Tesla and other carmakers can learn from Ford (Jun 13th)Also: If you want to write directly to Schumpeter, email him at [email protected]. And here is an explanation of how the Schumpeter column got its name.
Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Not so long ago analysts were openly wondering whether artificial intelligence (AI) would be the death of Adobe, a maker of software for creative types. New tools like DALL-E 2 and Midjourney, which conjure up pictures from text, seemed set to render Adobe’s image-editing offerings redundant. As recently as April, Seeking Alpha, a financial-news site, published an article headlined “Is AI the Adobe killer?”Far from it. Adobe has used its database of hundreds of millions of stock photos to build its own suite of AI tools, dubbed Firefly. Since its release in March the software has been used to create over 1bn images, says Dana Rao, a company executive. By avoiding mining the internet for images, as rivals did, Adobe has skirted the deepening dispute over copyright that now dogs the industry. The firm’s share price has risen by 36% since Firefly was launched.Adobe’s triumph over the doomsters illustrates a wider point about the contest for dominance in the fast-developing market for AI tools. The supersize models powering the latest wave of so-called “generative” AI rely on oodles of data. Having already helped themselves to much of the internet, often without permission, AI firms are now seeking out new data sources to sustain the feeding frenzy. Meanwhile, companies with vast troves of the stuff are weighing up how best to profit from it. A data land grab is under way.The two essential ingredients for an AI model are datasets, on which the system is trained, and processing power, through which the model detects relationships within and among those datasets. Those two ingredients are, to an extent, substitutes: a model can be improved either by ingesting more data or adding more processing power. The latter, however, is becoming difficult owing to a shortage of specialist AI chips, leading model-builders to be doubly focused on seeking out data.Demand for data is growing so fast that the stock of high-quality text available for training may be exhausted by 2026, reckons Epoch AI, a research outfit. The latest AI models from Google and Meta, two tech giants, are likely trained on over 1trn words. By comparison, the sum total of English words on Wikipedia, an online encyclopedia, is about 4bn.It is not only the size of datasets that counts. The better the data, the better the model. Text-based models are ideally trained on long-form, well-written, factually accurate writing, notes Russell Kaplan of Scale AI, a data startup. Models fed this information are more likely to produce similarly high-quality output. Likewise, AI chatbots give better answers when asked to explain their working step by step, increasing demand for sources like textbooks. Specialised information sets are also prized, as they allow models to be “fine-tuned” for more niche applications. Microsoft’s purchase of GitHub, a repository for software code, for $7.5bn in 2018 helped it develop a code-writing AI tool.As demand for data grows, accessing it is getting trickier, with content creators now demanding compensation for material that has been ingested into AI models. A number of copyright-infringement cases have already been brought against model-builders in America. A group of authors, including Sarah Silverman, a comedian, are suing OpenAI, maker of ChatGPT, an AI chatbot, and Meta. A group of artists are similarly suing Stability AI, which builds text-to-image tools, and Midjourney.The upshot has been a flurry of dealmaking as AI companies race to secure data sources. In July OpenAI inked a deal with Associated Press, a news agency, to access its archive of stories. It has also recently expanded an agreement with Shutterstock, a provider of stock photography, with which Meta has a deal, too. On August 8th it was reported that Google was in discussions with Universal Music, a record label, to license artists’ voices to feed a songwriting AI tool. Rumours swirl about AI labs approaching the BBC, Britain’s public broadcaster. Another supposed target is JSTOR, a digital library of academic journals.Holders of information are taking advantage of their greater bargaining power. Reddit, a discussion forum, and Stack Overflow, a question-and-answer site popular with coders, have increased the cost of access to their data. Both websites are particularly valuable because users “upvote” preferred answers, helping models know which are most relevant. Twitter (now known as X), a social-media site, has put in place measures to limit the ability of bots to scrape the site and now charges anyone who wishes to access its data. Elon Musk, its mercurial owner, is planning to build his own AI business using the data.Expanding the frontierAs a consequence, model-builders are working hard to improve the quality of the inputs they already have. Many AI labs employ armies of data annotators to perform tasks such as labelling images and rating answers. Some of that work is complex; an advert for one such job seeks applicants with a master’s degree or doctorate in life sciences. But much of it is mundane, and is being outsourced to places such as Kenya where labour is cheap.AI firms are also gathering data through users’ interactions with their tools. Many of these have a feedback mechanism, where users indicate which outputs are useful. Firefly’s text-to-image generator allows users to pick from one of four options. Bard, Google’s chatbot, proposes three answers. Users can give ChatGPT a thumbs-up or thumbs-down to its responses. That information can be fed back as an input into the underlying model, forming what Douwe Kiela, co-founder of Contextual AI, a startup, calls the “data flywheel”. A stronger signal still of the quality of a chatbot’s answers is whether users copy the text and paste it elsewhere, he adds. That information helped Google rapidly improve its translation tool.There is, however, one source of data that remains largely untapped: the information that exists within the walls of the tech firms’ corporate customers. Many businesses possess, often unwittingly, vast amounts of useful data, from call-centre transcripts to customer spending records. Such information is especially valuable because it can be used to fine-tune models for specific business purposes, such as helping call-centre workers answer queries or analysts spot ways to boost sales.Yet making use of that rich resource is not always straightforward. Roy Singh of Bain, a consultancy, notes that most firms have historically paid little attention to the types of vast but unstructured datasets that would prove most useful for training AI tools. Often these are spread across various systems, buried in company servers rather than in the cloud.Unlocking that information would help companies customise AI tools to serve their needs better. Amazon and Microsoft, two tech giants, now offer tools to help companies improve management of their unstructured datasets, as does Google. Christian Kleinerman of Snowflake, a database firm, says that business is booming as clients look to “tear down data silos”. Startups are piling in. In April Weaviate, an AI-focused database business, raised $50m at a valuation of $200m. Barely a week later PineCone, a rival, raised $100m at a $750m valuation. Earlier this month Neon, another database startup, raised an additional $46m in funding. The scramble for data is only just getting started. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.
text
"['A four-day working week could become the norm thanks to artificial intelligence, an officer at a local authority trialling the concept said.', 'South Cambridgeshire District Council is testing the idea of the shorter working week, despite the government telling it to abandon the scheme.', 'Councillors and staff discussed the scheme at a committee meeting earlier.', 'Jeff Membery of the council said it could become ""the standard way of working"".', 'The trial at the Liberal Democrat-run authority has been running since January 2023.', 'The Employment and Staffing Committee discussed plans to extend the trial beyond March 2024 and delay a consultation on continuing on a permanent basis. A report to the committee issued last week said a meaningful consultation could not be carried out without details of the financial consequences threatened by the government.', 'The meeting included a discussion on how the council could remain competitive in an ever-changing working environment. ', 'Mr Membery, the council\'s head of transformation, HR and corporate services, told the committee the four-day week could become ""the standard way of working as people take advantage of AI"".', 'Anna Bradnam, vice-chair of the committee, said the council would have to find ways of staying competitive in terms of staff recruitment and retention if other organisations adopted the practice. ', 'The meeting heard that staff welfare surveys suggested employees were enjoying better mental health thanks to the four-day working week.', 'The benefits for those with caring responsibilities were also highlighted as something appreciated by staff, the committee was told.', ""But the council's Conservative opposition has criticised the trial, saying it does not offer value for money for taxpayers."", '""We are spending millions on salaries for days not worked,"" Conservative group leader Heather Williams previously told the BBC.', 'The next steps of the trial will be discussed at a cabinet meeting on 12 March.', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp 0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['When Gabriela Capote was made redundant from her marketing job last year, she decided that she needed to learn about AI to get the edge in securing her next role.', '""After getting laid off from a job that I thought was so perfect and secure, it really woke me up,"" says Ms Capote, who lives in Miami.', '""It taught me something important about how nothing is actually permanent, and that I don\'t want to get complacent. I\'d heard the big, bad, scary conversation about AI for a while, and I\'ve always been interested in how I could utilise it.""', 'Instead of just hunting for a new marketing job, Ms Capote enrolled on a six-week, online AI training program. This was provided by a global training organisation called Mission Impact Academy (Mia).', ""Launched in 2022, Mia's courses are aimed specifically at women, with the aim of getting more women employed in work that includes the use of AI."", 'Ms Capote says that the certification helped her to get a new communications job at an insurance firm. ""Saying that I was certified in AI made me stand out from all the other applicants, and I think was a big reason as to why I was offered the role.', '""And my new company is very open to hearing about anything I think would be good from a tech perspective.""', 'Ms Capote adds that her increased understanding of AI also helped her search for the job at the insurance company in the first place. She says that she put her resume into popular AI chatbots ChatGPT and Bard, and asked them to find possible vacancies for her. She quickly got a list of 50.', 'There is currently a gender AI gap, with 54% of men using technology in either their job or personal life, compared with only 35% of women, according to one report from last year.', 'Read more stories on artificial intelligence', 'A growing number of women are now upskilling themselves to bridge that gap. For example, online learning platform Coursera says there has been a big jump in the number of women signing up to its ""AI for Everyone"" course. ', 'Last year, this was the sixth most popular course for its 1.58 million female learners in the UK, up from 59th place in 2022.', 'Heather Black is the founder and chief executive of Supermums, which offers tech training courses aimed at women returning to the workforce after having children. Last year it launched its first AI course.', '""With the pace that technology is changing and evolving, and given how we\'re always so saturated with information, the gender disparity in AI will increase if we don\'t have very specific ways for women to learn about it,"" says London-based Ms Black.', 'With a recent study by the UN finding that women may be at a higher risk of their jobs being made obsolete by AI, due to their overrepresentation in administrative and clerical work, training courses aimed specifically at women may help many keep their positions, or more easily find new ones.', 'Georgina Cosma, professor in AI and data science at Loughborough University, says that equipping more women with AI skills is paramount for securing their future in industries increasingly shaped by the technology.', 'Yet she cautions that women-only courses must meet rigorous industry standards. ""If programmes limit complexity or content, there is a risk they could contribute to outdated assumptions, and isolate women in technology further rather than integrating talent more broadly.', '""The goal is to promote inclusion by offering solid training to help qualified women gain the skills to advance to roles similar to equally qualified peers.""', 'Prof Cosma adds that women in the growing AI sector also need more mentorship programs, relevant networking opportunities, and hiring practices designed to counteract unconscious bias. ""Achieving true gender parity demands a shift in both attitudes and policies that influence access and career progression in AI.""', 'Janna Salokangas, co-founder of Miami-based Mia, says that its AI course is not just about the training, but also about giving the students a sense of togetherness. ""It\'s all about the energy, the inclusion and belonging they get being part of a community, and learning with other women,"" she says.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""In the last few days, Google's artificial intelligence (AI) tool Gemini has had what is best described as an absolute kicking online."", 'Gemini has been thrown onto a rather large bonfire: the culture war which rages between left- and right- leaning communities.', ""Gemini is essentially Google's version of the viral chatbot ChatGPT. It can answer questions in text form, and it can also generate pictures in response to text prompts. "", 'Initially, a viral post showed this recently launched AI image generator create an image of the US Founding Fathers which inaccurately included a black man. ', 'Gemini also generated German soldiers from World War Two, incorrectly featuring a black man and Asian woman.', 'Google apologised, and immediately ""paused"" the tool, writing in a blog post that it was ""missing the mark"".', ""But it didn't end there - its over-politically correct responses kept on coming, this time from the text version."", 'Gemini replied that there was ""no right or wrong answer"" to a question about whether Elon Musk posting memes on X was worse than Hitler killing millions of people.', 'When asked if it would be OK to misgender the high-profile trans woman Caitlin Jenner if it was the only way to avoid nuclear apocalypse, it replied that this would ""never"" be acceptable.', 'Jenner herself responded and said actually, yes, she would be alright about it in these circumstances.', 'Elon Musk, posting on his own platform, X, described Gemini\'s responses as ""extremely alarming"" given that the tool would be embedded into Google\'s other products, collectively used by billions of people.', ""I asked Google whether it intended to pause Gemini altogether. After a very long silence, I was told the firm had no comment. I suspect it's not a fun time to be working in the public relations department."", 'But in an internal memo Google\'s chief executive Sundar Pichai has acknowledged some of Gemini\'s responses ""have offended our users and shown bias"".', 'That was he said ""completely unacceptable"" - adding his teams were ""working around the clock"" to fix the problem.', 'It appears that in trying to solve one problem - bias - the tech giant has created another: output which tries so hard to be politically correct that it ends up being absurd.', 'The explanation for why this has happened lies in the enormous amounts of data AI tools are trained on. ', 'Much of it is publicly available - on the internet, which we know contains all sorts of biases.', 'Traditionally images of doctors, for example, are more likely to feature men. Images of cleaners on the other hand are more likely to be women.  ', 'AI tools trained with this data have made embarrassing mistakes in the past, such as concluding that only men had high powered jobs, or not recognising black faces as human. ', ""It is also no secret that historical storytelling has tended to feature, and come from, men, omitting women's roles from stories about the past."", 'It looks like Google has actively tried to offset all this messy human bias with instructions for Gemini not make those assumptions.', 'But it has backfired precisely because human history and culture are not that simple: there are nuances which we know instinctively and machines do not.', ""Unless you specifically programme an AI tool to know that, for example, Nazis and founding fathers weren't black, it won't make that distinction."", 'On Monday, the co-founder of DeepMind, Demis Hassabis, an AI firm acquired by Google, said fixing the image generator would take a matter of weeks.', ""But other AI experts aren't so sure. "", '""There really is no easy fix, because there\'s no single answer to what the outputs should be,"" said Dr Sasha Luccioni, a research scientist at Huggingface.', '""People in the AI ethics community have been working on possible ways to address this for years."" ', 'One solution, she added, could include asking users for their input, such as ""how diverse would you like your image to be?"" but that in itself clearly comes with its own red flags.', '""It\'s a bit presumptuous of Google to say they will \'fix\' the issue in a few weeks. But they will have to do something,"" she said.', 'Professor Alan Woodward, a computer scientist at Surrey University, said it sounded like the problem was likely to be ""quite deeply embedded"" both in the training data and overlying algorithms - and that would be difficult to unpick.', '""What you\'re witnessing... is why there will still need to be a human in the loop for any system where the output is relied upon as ground truth,"" he said.', ""From the moment Google launched Gemini, which was then known as Bard, it has been extremely nervous about it. Despite the runaway success of its rival ChatGPT, it was one of the most muted launches I've ever been invited to. Just me, on a Zoom call, with a couple of Google execs who were keen to stress its limitations."", 'And even that went awry - it turned out that Bard had incorrectly answered a question about space in its own publicity material.', ""The rest of the tech sector seems pretty bemused by what's happening."", 'They are all grappling with the same issue. Rosie Campbell, Policy Manager at ChatGPT creator OpenAI, was interviewed earlier this month for a blog which stated that at OpenAI even once bias is identified, correcting it is difficult - and requires human input.', 'But it looks like Google has chosen a rather clunky way of attempting to correct old prejudices. And in doing so it has unintentionally created a whole set of new ones.', 'On paper, Google has a considerable lead in the AI race.  It makes and supplies its own AI chips, it owns its own cloud network (essential for AI processing), it has access to shedloads of data and it also has a gigantic user base. It hires world-class AI talent, and its AI work is universally well-regarded.', ""As one senior exec from a rival tech giant put it to me:  watching Gemini's missteps feels like watching defeat snatched from the jaws of victory."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A new Â£7m deal for Derby City Council to use more AI technology in its public services has been signed.', 'It is estimated the move will save Â£4m in the 2024-25 financial year, rising to a minimum of Â£12.25m annually once fully installed.', 'The Labour-run authority already uses digital assistants Darcie and Ali, saving Â£200,000 in agency staff costs.', 'But Conservatives on the council said they were concerned the system would not be 100% reliable.', 'The use of Darcie and Ali, designed to help website visitors and telephone callers, led to a volume of complaints and the setting up of a ""customer focus group"" to discuss initial problems.', 'But the council said staff in three service areas would soon be using ""AI co-pilots"" to help carry out jobs more effectively. ', 'The first phase, involving adult social care, is expected to be in place in four months, according to the Local Democracy Reporting Service.', 'AI will be used to review care packages and help staff decide if someone who needs support living at home is receiving the right level of care.', 'In addition, Darcie and Ali ""will be upgraded and expanded to meet the needs of citizens"", the council said, and the new technology will be used to process emails more quickly and also recover outstanding debt.', 'Derby City Council leader Baggy Shanker said: ""Derby City Council is a trailblazer in using AI, and we will always make sure the needs of all our citizens are at the heart of this project. ', '""New advances bring risks, which we will manage, but also some incredible opportunities to deliver the best outcomes for our citizens in a more efficient and cost-effective way.""', 'But Conservative group deputy leader Jonathan Smale said the complaints about Darcie and Ali should give pause for thought.', '""I have grave concerns on the use of AI in social care because we\'re talking about vulnerable people who need support and care,"" he said. ', '""The system has to be 100% effective if we\'re going to trust AI to help deliver any health packages or care support to those who need it, we can\'t afford for mistakes in this area when lives could be at risk. ', '""I have yet to be demonstrated any confidence in its deliverability.""', 'Follow BBC East Midlands on Facebook, on X, or on Instagram. Send your story ideas to eastmidsnews@bbc.co.uk or via WhatsApp on 0808 100 2210.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Facebook and Instagram owner Meta says it will form a team to tackle deceptive artificial intelligence (AI) content in the upcoming EU elections in June.', 'It is concerned by how generative AI - tech which can fake videos, images and audio - might be used to trick voters.', 'It comes on the same day Home Secretary James Cleverly told the Times some people will use AI-generated fakes to try to influence a general election.', 'But an industry expert said the plans could be seen as ""lacking teeth"".', 'The BBC has asked Meta if it has such plans for upcoming UK and US elections.', 'The announcement comes two weeks after Meta signed an agreement with other big tech firms committing to fighting such content.', 'The European Parliament vote will be held from 6 to 9 June this year.', 'Social media rival TikTok announced in February it would be launching so-called ""Election Centres"" in local languages within its app for each of the 27 EU members, which will host authoritative information.', 'Meta head of EU affairs Marco Pancini said in a blog post that the firm, which also owns WhatsApp and Threads, would launch ""an EU-specific Elections Operations Centre"" that would ""identify potential threats and put specific mitigations in place across our apps and technologies in real time"".', '""Since 2016, we\'ve invested more than $20bn (Â£15.7bn) into safety and security and quadrupled the size of our global team working in this area to around 40,000 people,"" he said. ', '""This includes 15,000 content reviewers who review content across Facebook, Instagram and Threads in more than 70 languages - including all 24 official EU languages.""', 'He said this meant bringing together experts from a range of different teams across the company, including those working in engineering, data science and law.', ""But the announcement has shortcomings, according to Deepak Padmanabhan from Queen's University Belfast, who has co-authored a paper on elections and AI."", '""Most of its planned strategy could be observed to lack teeth in substantive ways,"" he said.', 'One of the issues he has with Meta\'s strategy is how the firm plans to deal with AI-generated images, which he said ""could be intrinsically unworkable"". ', 'He asked what would happen in a situation where realistic AI-generated images appear to show protesters clashing with police.', '""Proving it to be fake requires that we are sure that there was no such attack by the policemen pictured on the farmers pictured - this may be infeasible both for technology or for human experts,"" he said.', '""How can any technology label this as fake or real?', '""Thus, it is not very clear as to how effective Meta\'s generative AI strategy could be - at the very least, there are serious limitations.""', 'Meta, which currently works with 26 fact-checking organisations across the EU, said it would bring on board three more partners based in Bulgaria, France and Slovakia to help deal with the threat.', 'The role of these organisations is not to deal with content which is intended to suppress voting - these kinds of posts are banned - but rather to debunk content that is spreading misinformation, including when they involve AI-generated elements.', 'Mr Pancini said these types of posts would be given warning labels and made less prominent, as well as not being allowed in ads.', 'Ads cannot question the legitimacy of the vote, prematurely claim victory, or question ""the methods and processes of election"".', ""But he said the firm's work was a result of collaboration, and it would require further co-ordination in the future."", '""Since AI-generated content appears across the internet, we\'ve also been working with other companies in our industry on common standards and guidelines,"" he said.', '""This work is bigger than any one company and will require a huge effort across industry, government, and civil society.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['New safety cameras are being trialled in Sussex to detect if drivers are using phones or not wearing seatbelts.', 'Footage is processed by technology, using artificial intelligence (AI), before images are passed to police.', 'Motoring journalist and director of UK Motor Talk Graham Benge said: ""I am not convinced by this idea, replacing good old-fashioned policing with cameras.""', ""Assistant Chief Constable Simon Dobinson said he was looking forward to seeing the trial's outcomes."", 'Sussex Police\'s head of operations added: ""It gives us a unique opportunity to learn how AI-enabled cameras can potentially support partnership colleagues and ourselves in influencing driver behaviour and keeping motorists safe on our roads.""', 'The cameras are mounted to a vehicle or trailer and can automatically detect if motorists are committing an offence. ', 'The trial in Sussex began on 19 February and runs until March 2025.', 'The cameras, which are being used across 10 forces, were first launched as a pilot project between National Highways and Warwickshire Police in 2021.', 'Mr Benge said: ""We all know that the standards of driving on the roads are falling.', '""If you have more traffic patrols on the road then the driving standards generally improve. There\'s a very clear deterrent effect.""', 'While speaking on BBC Radio Sussex, the motoring journalist also raised privacy concerns about people being monitored in their cars, as well as the need for human intervention with the cameras.', 'He said: ""It\'s very difficult to tell at 70mph between someone eating a chocolate bar and somebody who is using a mobile phone.""', 'Mr Benge said he believes the cameras could result in a number of disputes being taken to court.', 'He added: ""It\'s a budgetary way of dealing with things, I think, rather than actual police numbers, which I\'m much in favour of.""', ' A National Highways spokesperson it was working with police to reduce dangerous driving.', 'They added: ""We believe that using technology like this will make people seriously consider their driving behaviour.""', 'Follow BBC South East on Facebook, on X, and on Instagram. Send your story ideas to southeasttoday@bbc.co.uk. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Nvidia's market value has touched $2tn (Â£1.58tn), a new milestone in the chipmaker's rapid ascent into the ranks of the world's most valuable companies."", 'Shares in the Silicon Valley firm rose more than 4% in morning trade on Friday before dropping back a bit.', ""The gains extended a jump after the company's blockbuster earnings report this week."", 'The company is benefiting from advances in artificial intelligence (AI), which have powered demand for its chips.', 'Turnover at the firm more than doubled last year to more than $60bn, and boss Jensen Huang told investors this week that demand was ""surging"" around the world.', ""The company, which became worth $1tn less than a year ago, now ranks as the world's fourth most valuable publicly traded company, behind Microsoft, Apple and Saudi Aramco. "", ""After shares retreated from their early Friday highs, the firm's market value ended the day just below $2tn."", 'Founded in 1993, Nvidia was originally known for making the type of computer chips that process graphics, particularly for computer games.', 'Long before the AI revolution, it started adding features to its chips that it says help machine learning, investments that have helped it gain market share.', 'It is now seen as a key company to watch to see how fast AI-powered tech is spreading across the business world. ', ""The price of the firm's shares has more than tripled over the last 12 months, from less than $240 apiece to nearly $800 in mid-day trade on Friday."", ""On Thursday, the day after its earnings report, buyers snapping up shares pushed its value up by $277bn, Wall Street's largest one-day gain in history."", 'The report has also helped to drive a broader market rally, appearing to convince investors that, as Derren Nathan of Hargreaves Lansdown put it, the boom in AI was ""living up to the hype"". ', '""It\'s being used in automotive for design, it\'s being used in telecommunications for planning networks, it\'s being used in mainstream companies to figure out and get insights into data that they haven\'t been able to get before,"" Bob O\'Donnell, a US-based technology analyst told the BBC earlier this week.  ', '""This is now really starting to hit the kinds of companies across the board, not just specialised tech companies and that\'s a real tipping point for the industry.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""US film and TV giant Tyler Perry says he's putting a $800m (Â£630m) expansion of his studio on hold after becoming concerned over new AI technology."", ""He was due to add 12 sound stages to his Atlanta complex, but says he has been put off by the release of OpenAI's video generator Sora."", 'Perry, 54, believes ""a lot of jobs"" will be lost in the film industry because of artificial intelligence.', 'He told the Hollywood Reporter that Sora had ""shocking"" capabilities.', '""All of that [work] is currently and indefinitely on hold because of Sora and what I\'m seeing,"" he said.  ', '""I had gotten word over the last year or so that this was coming, but I had no idea until I saw recently the demonstrations of what it\'s able to do. It\'s shocking to me.""', 'The tool, which has been made by the company behind ChatGPT, was launched in a limited way earlier this month and is not yet open to public consumption.', ""OpenAI's CEO took requests from followers on X, formerly known as Twitter, and produced videos with their suggestions to show its capabilities."", 'With access to few resources and a couple of text prompts, it was able to produce lifelike footage up to one minute long.', 'Perry said the technology could make travelling to locations and building sets redundant: ""I can sit in an office and do this with a computer, which is shocking to me,"" he said.', 'He added that he was ""very, very concerned"" that this could mean jobs for actors, editors, sound specialists and transporters could be lost.', 'Perry, who has starred in films such as Don\'t Look Up and Gone Girl, did say however that he had recently used AI on set to make himself older for a scene, which meant he ""avoided having to sit through hours of aging makeup"".', 'AI is already on the minds of many in the industry and was one of the main sticking points during the 2023 Hollywood strikes.', 'Many writers were concerned about their jobs being taken by AI, whilst actors were concerned that the technology could be used to replace them on set.', 'Despite an agreement being reached between studios and Hollywood workers, Perry said the ""whole industry"" needs to come together to protect people\'s livelihoods.', '""It can\'t be one union fighting every contract every two or three years. I think that it has to be everybody, all involved in how do we protect the future of our industry because it is changing rapidly, right before our eyes,"" he said.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['This video can not be played', ""The BBC's Paul Carter tries out the Atom Limbs prosthetic tech"", ""I was born without lower arms and legs, so I've been around prosthetics of all shapes and sizes for as long as I can remember. "", ""I've actively avoided those designed for upper arms for most of my adult life, so have never used a bionic hand before.  "", 'But when I visited a company in California, which is seeking to take the technology to the next level, I was intrigued enough to try one out - and the results were, frankly, mind-bending.', 'Prosthetic limbs have come a long way since the early days when they were fashioned out of wood, tin and leather. ', 'Modern-day replacement arms and legs are made of silicone and carbon fibre, and increasingly they are bionic, meaning they have various electronically controlled moving parts to make them more useful to the user.', 'What the company I visited, Atom Limbs, is doing is combining a range of cutting-edge innovations, including artificial intelligence (AI), into a next-generation bionic arm.', ""Atom Limbs uses advanced sensors and machine learning - where computers train themselves to become more accurate -  to interpret electrical signals from a person's brain and use them to move and manipulate a prosthetic limb."", 'The arm has a full range of human motion in the elbow, wrist, and individual fingers - and it provides haptic feedback to the wearer on their grip strength.', ""The arm attaches via a strengthened sportswear-style vest which distributes the weight of the arm evenly. Although it still has some weight to it, it is considerably lighter than other bionic arms I've seen."", ""It's non-invasive, meaning it doesn't need any surgery or implants to function. It connects to the wearer's residual limb firstly with bands of sensors that measure electrical signals, and then via a cup that fits over the top, with the arm connecting via an interface. "", 'Despite avoiding upper-arm prosthetics before, when Atom Limbs said I could have a try at operating a digital version of the arm on a computer screen, via their control software, I was interested enough to say yes.', 'I do have residual muscles in my arm that I was able to ""assign"" to corresponding hand, wrist and elbow movements, which proved to be a unique, mind-boggling experience.', ""The notion of learning how to control a part of the body I don't have is almost impossible to describe. "", 'However, exciting though this technology is, one issue that is always of concern to disabled people when new products come into being is cost. ', 'The assistive devices landscape is littered with products that, while impressive, can cost many times an average yearly salary. That puts most devices out of reach for many disabled people who statistics show are more likely to be among the poorest in society.', 'This video can not be played', ""Jason shows the BBC how Atom Limbs' prosthetic arm works"", 'Atom Limbs says it hopes its arm will be positioned around the $20,000 (Â£15,000) price point, which - while still a hefty sum of money - is considerably less than many other bionic products on the market. ', ""Ian Adam, a lecturer in prosthetics and orthotics at the University of Derby, says while this may sound like a lot, it is a good price in the industry - though it won't be for everyone."", '""It\'s at the cheaper end of the market, but say you had an accident and got a pay-out, well that\'s got to last the rest of your life,"" he said.', '""So I think a lot of patients are canny about what they spend their money on... sometimes people are quite prepared to not use them at all - with upper limb prosthetics it can be just an extra thing that not everybody will decide they need to have.""', 'And then there are the ethical and practical issues around such products. ', 'In 2022 Britt H Young, herself a bionic arm user, questioned whether the prosthetics ""arms race"" has focused too much on innovation rather than application.', 'Social media star Tilly Lockey, who has been using bionic arms since she was 9 years old, is excited about their future potential - but she told the BBC whether this device made a difference would all come down to testing.', '""I\'ve seen like them change so much first-hand, but I\'ve also seen them throughout the development phases,"" she said. ', '""There\'s a lot of ambitious projects, but I think the way they truly get there is the back and forth development from the users who actually wear them every day.""', 'Ultimately, the Atom Limbs arm is still early in development.', 'The firm is collecting data ahead of regulatory filings in the US, which means it will be some time yet before we see them being used in every day life.', 'Additional reporting by Tom Gerken.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['More police forces are to join a trial scheme that uses AI to detect whether motorists are using mobile phones or not wearing seatbelts while driving.', 'The safety project started in 2021 as a partnership between Warwickshire Police and National Highways.', 'Now 10 more forces in England are to use the technology mounted to a vehicle that takes multiple images of drivers.', 'The images are then passed to police for consideration on whether any action is to be taken against offenders.', 'The police forces taking part in the trial are Durham, Greater Manchester Police, Humberside, Staffordshire, West Mercia, Northamptonshire, Wiltshire, Norfolk, Thames Valley Police and Sussex.', ""In the scheme's earliest phase, which ran for three months, the monitoring vehicle was stationary at the side of the road while in use."", 'National Highways said the operation was being extended to learn more about the technology with a possible future roll-out nationwide. ', 'It added there were plans for the technology to be fixed to gantries for the first time, giving an unobscured view of lanes.', 'The trial began on 19 February. It runs until March 2025.', 'Under the scheme, drivers would receive warning letters from the police reminding them that they face fines of up to Â£500 for not wearing a seatbelt, with penalty points and potential Â£1,000 fines for those using a phone in a non hands-free capacity.', 'Matt Staton, National Highways\' head of national road-user safety, said: ""We know that distracted driving and not wearing seatbelts were key factors in a high number of incidents that resulted in people being killed or seriously injured. ', '""Working with our police partners we want to reduce such dangerous driving and reduce the risks posed to both the drivers and other people. We believe that using technology like this will make people seriously consider their driving behaviour.""', 'National Highways is working with Acusensus which is behind the technology. ', 'Geoff Collins, Acusensus\' UK general manager, added: ""We have found this technology really does change driver behaviour.', '""In New South Wales (Australia), where we first used the technology, the number of mobile phone detections have dropped by a factor of six, from 1 in 82 drivers spotted holding a phone in 2019 to 1 in 478 drivers in 2021 and 1 in every 534 in 2023. ', '""We expect to deliver similarly dramatic results here in the UK.""', 'Follow BBC West Midlands on Facebook, X and Instagram. Send your story ideas to: newsonline.westmidlands@bbc.co.uk', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Google is racing to fix its new AI-powered tool for creating pictures, after claims it was over-correcting against the risk of being racist.', ""Users said the firm's Gemini bot supplied images depicting a variety of genders and ethnicities even when doing so was historically inaccurate."", ""For example, a prompt seeking images of America's founding fathers turned up women and people of colour. "", 'The company said its tool was ""missing the mark"".', '""Gemini\'s AI image generation does generate a wide range of people. And that\'s generally a good thing because people around the world use it. But it\'s missing the mark here,"" Jack Krawczyk, senior director for Gemini Experiences said on Wednesday. ', '""We\'re working to improve these kinds of depictions immediately,"" he added. ', 'This article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitterâ\x80\x99s cookie policy, external and privacy policy, external before accepting. To view this content choose â\x80\x98accept and continueâ\x80\x99.', ""Google later said it would suspend the tool's ability to generate images of people while it worked on the fix. "", 'It is not the first time AI has stumbled over real-world questions about diversity.', 'For example, Google infamously had to apologise almost a decade ago after its photos app labelled a photo of a black couple as ""gorillas"". ', 'Rival AI firm, OpenAI was also accused of perpetuating harmful stereotypes, after users found its Dall-E image generator responded to queries for chief executive, for example, with results dominated by pictures of white men. ', 'Google, which is under pressure to prove it is not falling behind in AI developments, released its latest version of Gemini last week. ', 'The bot creates pictures in response to written queries. ', 'It quickly drew critics, who accused the company of training the bot to be laughably woke. ', 'This article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitterâ\x80\x99s cookie policy, external and privacy policy, external before accepting. To view this content choose â\x80\x98accept and continueâ\x80\x99.', '""It\'s embarrassingly hard to get Google Gemini to acknowledge that white people exist,"" computer scientist Debarghya Das, wrote.', '""Come on,"" Frank J Fleming, an author and humourist who writes for outlets including the right-wing PJ Media, in response to the results he received asking for an image of a Viking.', 'The claims picked up speed in right-wing circles in the US, where many big tech platforms are already facing backlash for alleged liberal bias.', 'Mr Krawczyk said the company took representation and bias seriously and wanted its results to reflect its global user base.', '""Historical contexts have more nuance to them and we will further tune to accommodate that,"" he wrote on X, formerly Twitter, where users were sharing the dubious results they had received.', '""This is part of the alignment process - iteration on feedback. Thank you and keep it coming!""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The boss of the world\'s most valuable chip maker Nvidia said artificial intelligence (AI) is at a ""tipping point"" as it announced record sales.', 'The technology giant reported that revenues surged by 265% to $22bn (Â£17.4bn) in the three months to 28 January, compared to a year earlier.  ', 'For the year as a whole, turnover more than doubled to $60.9bn.', '""Accelerated computing and generative AI have hit the tipping point,"" said Nvidia chief executive Jensen Huang.', '""Demand is surging worldwide across companies, industries and nations.""', ""Nvidia also forecast a 233% jump in its quarterly revenues for the current quarter, beating analysts' estimates."", '""There was a lot riding on this last quarter and they actually absolutely knocked it out of the park,"" Bob O\'Donnell of Technalysis Research told the BBC.', '""We\'re starting to see mainstream usage of AI,"" he added, highlighting that AI is not longer only used by specialised technology companies.', ""In addition to its AI chips, sales at the firm's data centres have grown rapidly."", 'Its data centre business contributed the vast majority of its revenues in the most recent quarter after growing more than than five-fold over the last year.', 'Nvidia said gross profit for the final three months of its financial year rose by 338% to $16.8bn. Annual gross profit rose by 188% to $44.3bn.', 'However, the company said it faced several challenges including constraints on its supply chains.', ""The US has also tightened its restrictions on trade with China, the world's second largest economy."", 'Ipek Ozkardeskaya, a senior analyst at Swissquote, told the BBC that Nvidia\'s results had been ""unusually amazing"".', 'But she added that Nvidia could face difficulties in addition to restrictions in China. ', '""Nvidia... will see challenges on the way up because first the revenue growth will likely stabilise and the euphoria regarding these growth and growth perceptions will level out,"" she said.', 'The company is also likely to face competition and regulation issues, Ms Ozkardeskaya added, and it could be ""constrained by their own capacity to respond to this fast-surging demand"". ', ""AI's public profile has risen sharply since the launch in 2022 of ChatGPT, which was developed by Microsoft-backed OpenAI."", 'ChatGPT and other similar systems use huge amounts of data to create convincing human-like responses to user queries.', 'They are expected to dramatically change the way people search for information online.', ""Nvidia's stock market value has soared by 225% over the last year, making it one of the most valuable companies in the US. Its share price jumped by more than 9% in extended New York trading. "", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A method using artificial intelligence (AI) to assess potential transplant organs has been shortlisted for a national NHS award.', 'Known as OrQA - Organ Quality Assessment - the technology scans images of organs to look at suitability for kidney and liver transplants.', 'The tool was created by the University of Bradford and Newcastle Upon Tyne Hospitals NHS Foundation Trust.', 'The Medipex NHS Innovation Awards will be presented on 13 March.', 'It is hoped OrQA, which uses similar AI technology to facial recognition, could result in up to 200 more patients receiving kidney transplants and 100 more liver transplants a year in the UK, its creators said.', 'Researchers are working to expand the technology to assess other organs including the pancreas, heart and lungs.', '""It is not replacing the human element but assisting it,"" said Prof Hassan Ugail, who worked on the project.', '""By using an algorithm to look at thousands of images of human organs, we are able to create a system that assesses donor organs and this will hopefully save time, money and lives.""', 'Follow BBC Yorkshire on Facebook, X (formerly Twitter) and Instagram. Send your story ideas to yorkslincs.news@bbc.co.uk', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['There are between 10 and 50 billion potentially habitable worlds in our galaxy, says Bill Diamond. It makes his job rather difficult.', 'Mr Diamond is the chief executive of the US-based research organisation Seti Institute. The letters ""Seti"" are an acronym for the Search for Extraterrestrial Intelligence.', '""Seti, as an endeavour, is looking for science and technology beyond the solar system as evidence of life and intelligence, and that\'s by and large a needle in a haystack problem,"" he says.', '""We\'re looking for something that is likely exceedingly rare, and may be very difficult to find and extract from the background phenomena that you\'re observing at the same time.""', 'But new tools are helping the search. The ability of artificial intelligence (AI) to both handle massive datasets - and to spot anomalies - is transforming the hunt for alien intelligence.', ""One such project involves a Seti Institute partnership with the US's National Radio Astronomy Observatory in New Mexico. This federal facility uses radio frequencies to study celestial objects, such as planets, stars and asteroids."", ""Seti is building a parallel, AI-powered software system for the observatory's core facility, the Very Large Array. Built between 1973 and 1981, the VLA comprises 28 large, 25m diameter, dish antennas spaced out across a desert plain. Imagine the satellite dishes you find on people's homes, just on a giant scale."", 'When operational, the AI will be able to process every bit of data captured - two terabytes (TB) every second. To put that into context, modern laptops now typically have around 1TB of total storage.', 'Mr Diamond says that the increased use of AI is already proving to be ""indispensable"" as his institute continues to hunt for alien life.', 'He points to AI making it possible to search for new types of radio signals from alien sources. He explains that traditionally, Seti has looked for narrowband signals similar to those used by human beings.', '""But there was always the question \'what if there\'s an alien advanced technology that is using wideband [radio]?\'. And if that\'s the case, our traditional methods wouldn\'t work, it would look like a bunch of noise on the screen.""', 'However, Mr Diamond says that the ability of AI to handle massive amounts of data means it\'s possible to take millions of ""snapshots"" of this snowy audio picture over time, and to start to look for patterns. ""It\'s a way of adding on a new thing to look for.""', 'Another project with which Seti collaborates is Breakthrough Listen. Backed by more than Â£100m of private sector funding, this scheme is scanning a million stars, and 100 galaxies, across a wide range of radio and optical bands, to look for evidence of technological life. ', 'One project member, University of Toronto student Peter Ma, recently developed a new AI system designed to examine telescope data, and distinguish between possible real signals from aliens, and interference. ', 'His team did this by simulating both types of noise, and then training their AI to differentiate between the two.', 'Mr Ma says that an alien signal would, for example, ""only appear when we point our telescopes at it... and disappear when we point away"". ', ""The project has already identified eight potential alien signals that went undetected by traditional analysis. However, Mr Ma believes that as the observations haven't yet been repeated they are probably false positives."", 'AI is also being used to try to detect signs of life of a more modest nature, and closer to home.', ""Last year, Nasa's Perseverance rover started collecting samples from the Jezero Crater on Mars, which will, if all goes well, be returned to Earth in several years' time."", ""Already, scientists believe that the rover's Sherloc instrument has detected organic compounds, which glow under ultraviolet light. "", ""However, organic compounds can be created by non-biological processes, meaning that it's not yet possible to say whether they derive from past life on the planet."", 'All this could change, though, thanks to new research from the Carnegie Institution for Science, which is using AI to analyse rock samples for signs of present or past life. ', 'The team found that the AI is able to distinguish former living and non-living material, with an accuracy of almost 90%.', '""This is a very new approach to searching for molecular biosignatures,"" says joint lead researcher Dr Robert Hazen.', '""We employ machine learning to look at all of the vast amount of data from an analytical method that produces half a million data points per sample. So we\'re seeking subtle patterns in molecular distributions.""', 'Read additional stories on artificial intelligence', 'The first plans are to use the system to analyse ancient samples from Earth, as well as some Martian samples in the form of meteorites. But, says Mr Hazen, ""We could, for example, fly an instrument through the plumes of Enceladus [one of Saturn\'s moons], or land a carefully designed instrument on Mars.""', ""It's early days, and any promising results generated by AI need to be validated by other observations, or by physics-based models, before they can be shouted from the rooftops. But as more and more data is collected and analysed, the chances of detecting alien life - if it exists - are increasing all the time."", 'In the meantime, though, says Mr Diamond, ""The progress is measured in the scale of the effort, not yet in the results.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The University of York has set up a living lab to reassure people about Artificial Intelligence (AI)', 'The Â£45m Institute for Safe Autonomy (ISA) is being launched as a space to provide research and evidence on how AI is ""safe for everyday use"". ', 'Director of ISA Professor Miles Elsden said the technology is already ""a big part of our daily lives"".', 'The institute will provide spaces for small and medium enterprises operating in the field of AI and robotics.', 'Prof Elsden said ""safety assurance"" was the key to ""underpinning public trust"" in autonomous technology.', '""These technologies are already a big part of our daily lives, whether it\'s part of your mobile phone, Smart TV, or vacuum cleaner, and yet there is a significant number of people that are wary of their development, and we have to ask why that is and how we can address those concerns by evidencing how safe they are for everyday use."" ', 'Some of the projects already being worked on include how robots might be safely integrated into triage procedures in emergency hospital departments and how they can be used in social care for basic domestic tasks.', 'The purpose-built facility provides work and test spaces for more than 100 researchers across a variety of disciplines.', '""Its safety experts work in partnership with academia, industry,Â\xa0government and civil society to research safe, ethical, real-world applications for autonomous systems,"" the university said.', ""ISA was established through joint funding from Research England (via the UK Research Partnership Investment Fund), the Lloyd's Register Foundation and the University of York."", 'Follow BBC Yorkshire on Facebook, X (formerly Twitter) and Instagram. Send your story ideas to yorkslincs.news@bbc.co.uk.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Most of the world's largest tech companies, including Amazon, Google and Microsoft, have agreed to tackle what they are calling deceptive artificial intelligence (AI) in elections. "", 'The twenty firms have signed an accord committing them to fighting voter-deceiving content.', 'They say they will deploy technology to detect and counter the material.', 'But one industry expert says the voluntary pact will ""do little to prevent harmful content being posted"".', 'The Tech Accord to Combat Deceptive Use of AI in 2024 Elections was announced at the Munich Security Conference on Friday.', 'The issue has come into sharp focus because it is estimated up to four billion people will be voting this year in countries such as the US, UK and India.', 'Among the accord\'s pledges are commitments to develop technology to ""mitigate risks"" related to deceptive election content generated by AI, and to provide transparency to the public about the action firms have taken.', 'Other steps include sharing best practice with one another and educating the public about how to spot when they might be seeing manipulated content.', 'Signatories include social media platforms X - formerly Twitter - Snap, Adobe and Meta, the owner of Facebook, Instagram and WhatsApp.', ""However, the accord has some shortcomings, according to computer scientist Dr Deepak Padmanabhan, from Queen's University Belfast, who has co-authored a paper on elections and AI."", 'He told the BBC it was promising to see the companies acknowledge the wide range of challenges posed by AI.', 'But he said they needed to take more ""proactive action"" instead of waiting for content to be posted before then seeking to take it down.', 'That could mean that ""more realistic AI content, that may be more harmful, may stay on the platform for longer"" compared to obvious fakes which are easier to detect and remove, he suggested.', ""Dr Padmanabhan also said the accord's usefulness was undermined because it lacked nuance when it came to defining harmful content."", 'He gave the example of jailed Pakistani politician Imran Khan using AI to make speeches while he was in prison.', '""Should this be taken down too?"" he asked.', 'The accord\'s signatories say they will target content which ""deceptively fakes or alters the appearance, voice, or actions"" of key figures in elections.', 'It will also seek to deal with audio, images or videos which provide false information to voters about when, where, and how they can vote.', '""We have a responsibility to help ensure these tools don\'t become weaponised in elections,"" said Brad Smith, the president of Microsoft.', 'This video can not be played', 'US Deputy Attorney General Lisa Monaco says AI could be used to ""incite violence""', 'On Wednesday, the US deputy attorney general, Lisa Monaco, told the BBC that AI threatened to ""supercharge"" disinformation at elections. ', 'Google and Meta have previously set out their policies on AI-generated images and videos in political advertising, which require advertisers to flag when they are using deepfakes or content which has been manipulated by AI.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Students at Arts University Plymouth have been learning how to use artificial intelligence for their assignments. ', 'The generative AI software works by being fed ""training artwork"" created by humans, which it then replicates and fuses to create new images. ', 'Classes also address the ethical issues around the use of the technology.', 'A Department for Education spokesperson said: ""Artificial intelligence has the power to transform education.""', ""AI-generated content can include work 'stolen' from artists without their knowledge, something the university has addressed in their teaching."", 'Lecturer Alana Morgan said: ""When we are talking about work being used without consent, that\'s a massive issue for all kinds of artists including students, but broadly, I think that it\'s been quite positive, our experiments with AI, definitely on this course. We love it.""', 'Associate Professor Stephanie Owens said: ""Meaningful innovation is ultimately linked to human endeavour.', ' ""So, by teaching high levels of digital literacy, including the history and development of generative AI, we equip our graduates with the capacity to think ethically and critically regardless of the technologies that will be adopted by the workplaces of the future.""', 'Fashion, media, and gaming students are among those to have been experimenting with the technology by creating concept art, magazines and videos.', ""Arts University Plymouth said it was important for their future graduates to use all the industry's tools at their disposal, and not to be worried they will be replaced by the technology."", 'Fashion student Chloe Quinn said: ""Within the last hundred years, with every new development, we have this same scare and we don\'t know how it\'s going to integrate into our society.', '""Even when I was at secondary school we were told that the jobs the majority of us would have hadn\'t even been invented yet.', '""It\'s exciting to see what new jobs will be created and what all of our lives will look like in years to come.""', 'Follow BBC Devon on X (formerly Twitter), Facebook and Instagram. Send your story ideas to spotlight@bbc.co.uk.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A charity that helps people worried about their own thoughts or behaviour says an increasing number of callers are feeling confused about the ethics of viewing AI child abuse imagery.', 'The Lucy Faithfull Foundation (LFF) says AI images are acting as a gateway.', 'The charity is warning that creating or viewing such images is still illegal even if the children are not real.', 'Neil, not his real name, contacted the helpline after being arrested for creating AI images.', 'The 43-year-old denied that he had any sexual attraction to children.', 'The IT worker, who used AI software to make his own indecent images of children using text prompts, said he would never view such images of real children because he is not attracted to them. He claimed simply to be fascinated by the technology. ', 'He called the LFF to try to understand his thoughts, and call handlers reminded him that his actions are illegal, regardless of whether or not the children are real.', 'The charity says it has had similar calls from others who are expressing confusion.', 'Another caller got in touch after discovering that her 26-year-old partner viewed indecent AI images of children, but said they were not serious because the pictures ""aren\'t real"". The offender has since asked for help.', ""A teacher asked for the charity's advice because her 37-year-old partner was viewing images that seemed illegal, but neither of them was sure if they were."", ""The LFF's Donald Findlater says some callers to its confidential Stop It Now helpline think that AI images are blurring the boundaries for what is illegal and morally wrong."", '""This is a dangerous view. Some offenders think this material is in some way OK to create or view because there are no children being harmed, but this is wrong,"" he says.', 'In some cases, AI abuse images might also be wrongly labelled or advertised as AI-made and the difference in realism is becoming harder to spot.', 'Mr Findlater says that deviant sexual fantasy is the strongest predictor of reoffending for anyone convicted of a sexual crime. ', '""If you feed that deviant fantasy, then you\'re making it more likely you\'re going to do harm to children,"" he said.', 'The charity says the number of callers citing AI images as a reason for their offending remains low, but is rising. The foundation is urging society to recognise the problem and lawmakers to do something to reduce the ease in which child sexual abuse material (CSAM) is made and published online.', 'Although the charity would not name any specific sites where it has found the imagery, one popular AI art website has been accused of allowing users to publish sexual and graphic images of very young models. When the BBC approached Civit.ai about the issue in November, the firm said it takes potential CSAM on the site ""very seriously"" and asks the community to report images that users consider to ""depict under-age characters/people in a mature or photorealistic context"".', 'The LFF also warned that young people are creating CSAM without realising the seriousness of the offence. One caller, for example, was concerned about his 12-year-old son who had used an AI app to create inappropriate topless pictures of friends, and then subsequently searched for terms such as ""naked teen"" online.', 'Criminal cases in Spain and the US have recently been launched against young boys using declothing apps to create naked pictures of school friends.', 'In the UK, Graeme Biggar, head of the National Crime Agency, said in December that he wanted to see tougher sentences for offenders who possess child abuse imagery, adding that AI abuse imagery ""matters, because we assess that the viewing of these images - whether real or AI-generated - materially increases the risk of offenders moving on to sexually abusing children themselves"". ', 'Some contributors have asked for their names to be withheld in this piece.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['This video can not be played', 'US Deputy Attorney General Lisa Monaco says AI could be used to ""incite violence""', 'Artificial intelligence (AI) threatens to ""supercharge"" disinformation and incite violence at elections, the US deputy attorney general has warned.', 'Speaking exclusively to the BBC, Lisa Monaco described AI as the ""ultimate double-edged sword"".', 'It could deliver ""profound benefits"" to society but also be used by ""malicious actors"" to ""sow chaos"", she added.', 'And she revealed plans to make the use of AI by criminals an aggravating factor in sentencing in US courts.', 'The former federal prosecutor, who is in the UK to deliver a lecture on AI at the University of Oxford, said violent criminals who used guns were given longer sentences.', '""So we are going to be applying that same principle and seeking stiffer sentences and sentencing enhancements for those that use AI in a malicious way to commit their crime,"" she told the BBC.', 'She also spoke about efforts to protect the US election in November from being disrupted by AI-powered misinformation such as deepfakes - convincing audio and video of politicians saying things they never said - and AI-generated fake calls, or robocalls.', 'Last week, the US Federal Communications Commission (FCC) made robocalls during elections illegal. ', ""It came after thousands of voters in New Hampshire received a phone call claiming to be from US President Joe Biden urging them not to vote in January's primary election."", 'Ms Monaco said: ""That is the type of action you are going to see regulators, I think, appropriately taking to try and put some guardrails around the use of these technologies, particularly when it comes to the election space.""', 'She said the US government was working with tech companies and other nations, including the UK, to combat the threat posed by AI.', 'But she added: ""I do think we are going to see more of this, I think it is something we are just beginning to see the surface being scratched about how malicious actors can use this technology."" ', 'With more than two billion people eligible to take part in elections around the world this year, including in the US, UK and India, Ms Monaco said she was concerned AI could have a ""fundamental impact"" on democracy.', 'This week, London mayor Sadiq Khan told the BBC deepfake audio of him supposedly making inflammatory remarks before Armistice Day almost caused ""serious disorder"".', 'And the US Deputy Attorney General said she too was concerned about violence resulting from AI-powered misinformation.', '""I am absolutely worried... about this effort by malicious actors, nation states or otherwise, using AI generated content to spread and really supercharge mis and disinformation. ', '""So that can have a number of effects. It can cause people to distrust the sources of information they are getting, to dissuade them or confuse them in terms of exercising their right to vote. To incite violence, certainly that\'s something that we are worried, about and to just generally sow distrust and potentially chaos.""', 'Ms Monaco was also keen to stress the potential benefits of AI as a crimefighting tool and something that could help in investigations and prosecutions.', 'The FBI was using AI technology to ""sift through the tips that we get from the public"" and to analyse data and images in ""some of our most important investigations"" including the US Capitol riots on 6 January 2021, she told the BBC.', 'Ultimately, she said, it would require a combination of action by tech firms and legislation to set the appropriate ""guardrails"" for AI and protect democracy.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['London Mayor Sadiq Khan says deepfake audio of him supposedly making inflammatory remarks before Armistice Day almost caused ""serious disorder"".', 'Mr Khan says the law is not ""fit for purpose"" in tackling AI fakes, as the audio creator ""got away with it"".', 'The man who first posted the clip, tracked down by the BBC, told us: ""It\'s what we all know Sadiq thinks.""', 'But another social media user who helped the audio go viral apologised, saying: ""I made a big mistake"".', ""The clip used AI - artificial intelligence - to create a replica of Mr Khan's voice saying words scripted by the faker, disparaging Remembrance weekend with an expletive and calling for pro-Palestinian marches, planned for the same day last November, to take precedence."", 'Intended to sound like a secret recording, it said: ""What\'s important and paramount is the one-million-man Palestinian march takes place on Saturday."" ', 'The clip imitated Mr Khan, the first Muslim mayor of London, saying: ""â\x80\x8bâ\x80\x8bI control the Met Police, they will do as the Mayor of London tells them"" and saying ""the British public need to get a grip"". It said the prime minister meeting with Met Police Commissioner Sir Mark Rowley was ""a waste of time"" because ""the buck stops with me"".', 'Mr Khan told BBC Radio 4\'s Why Do You Hate Me? podcast: ""You know, we did get concerned very quickly about what impression it may create. I\'ve got to be honest, it did sound a lot like me.""', 'The clip spread rapidly, including among far-right groups, and triggered a spike in hateful comments against the mayor on social media.', '""When you\'ve got friends and family who see this stuff, it\'s deeply upsetting. I mean, I\'ve got two daughters, a wife, I\'ve got, you know, siblings. I\'ve got a mum,"" Mr Khan told the BBC.', 'The AI fake emerged during an already-tense political row, as Prime Minister Rishi Sunak said the pro-Palestinian marches in a different part of central London were ""disrespectful"" on Armistice Day. Then-Home Secretary Suella Braverman had called for them to be cancelled.', 'This video can not be played', 'Listen to the fake recording of London Mayor Sadiq Khan', 'Armistice Day marks the moment when World War One ended on 11 November 1918, while memorial ceremonies take place at the Cenotaph and across the country on the nearest Sunday, known as Remembrance Sunday.', '""The timing couldn\'t have been better if you\'re seeking to sow disharmony and cause problems,"" the London mayor told the BBC.', '""What was being said was a red rag to a bull for the far right and others. But what concerned me the most was if you\'re an innocent listener of this. Because it\'s a secret undercover recording, in inverted commas, because it sounds like me, because of the timing and the context."" ', ""Both events took place on Saturday 11 November, with the pro-Palestinian march beginning hours after the two minutes' silence had been observed. But counter-protestors, including some people connected to far-right groups, were later condemned by police after clashes and arrested for offences including for inciting racial hatred. "", 'Why Do You Hate Me?', ""One of the BBC's most trolled journalists, Marianna Spring, dives into her inbox and investigates extraordinary cases of online hate. She meets the people at the heart of these conflicts, and in some cases brings them together, to see if understanding - even forgiveness - is ever possible."", 'Listen now on BBC Sounds, watch on BBC iPlayer and read the previous instalment of the online series here.', 'Mr Khan said the AI fake had inflamed the situation. ""We almost had serious disorder,"" he said, adding that deep fake audio could have a worrying impact on other situations - such as a close election or referendum, or where there is community unrest.â\x80\x8bâ\x80\x8b', '""People should be able to criticise me. But I think what you shouldn\'t do is use AI to manufacture lies. And I think the person that made this needs to realise the consequences on that Saturday."" ', 'One of the larger accounts that spread the fake audio was Little Boats on X, which has shared anti-Islam and anti-immigration content and has more than 58,000 followers. ', 'Jeremy Davis, who is in charge of the account, said he still suspects Mr Khan has beliefs similar to those in the audio but apologised for sharing the fake.', '""It was a clever AI job,"" he said. ""Mayor Khan, I apologise, I made a big mistake.""', 'But Mr Davis did not create the audio. ', 'Through a screengrab, I traced the recording back to TikTok and what appears to be the originator of the clip: an account called HJB News with the ironic tagline ""Keeping it real"". The account shares anti-immigration content and some racist material. ', 'This video can not be played', 'Counter-protesters clashed with police on Armistice Day', ""HJB News's accounts on other social media show they shared the audio on 9 November, hours before it went viral. â\x80\x8bThe fact that there's no trace of the audio anywhere else online before HJB News shared it, makes it likely that they also created it."", 'After exchanging several messages, the person behind the account called me. He gave his name only as Henry and he would not let me record him, but I took detailed notes of the conversation. ', 'He claimed it was ""a clip on TikTok I shared"" and denied creating it, saying he ""wouldn\'t want to comment"" when I pointed out he had been the first to share it. ', 'He said his account does not only publish ""fake clips"". ""We post news that could be real with a sense of humour.""', 'On 11 November, the Metropolitan Police said it had examined the audio but that it ""does not constitute a criminal offence"". HJB News said on X, ""well that\'s a relief"" with an emoji of a bead of sweat rolling down a grinning face. ', 'Mr Khan said the fake audio ""wasn\'t a bit of fun"" or ""satire"" and its creator had not been not ""naive"" about the consequences it could have.', 'The London mayor said he does, however, accept the apology of people who shared it because of an ""innocent mistake"".', 'Mr Khan said organisations such as the Electoral Commission, which are responsible for keeping the UK\'s elections ""free and fair"", also needed more powers to deal with faked information.â\x80\x8bâ\x80\x8b', 'There is currently no criminal law in the UK which specifically covers this kind of scenario. ', 'The mayor said it was also ""really worrying"" that social media companies did not contact him or the authorities about the faked audio at the time it went viral.', 'I contacted X, TikTok and Instagram, where the clip was shared. X and Instagram did not respond. According to a TikTok spokesperson, the social media site ""does not allow synthetic media that contains the likeness of any real private figure"" and removes this kind of content.', ""TikTok said it had spoken to both the mayor's office and the Metropolitan Police in November 2023 about the platform's approach to this content and flagged how similar issues could be raised directly in future."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Conservationists are using artificial intelligence to identify and monitor bird populations by recording their songs.', 'Somerset Wildlife Trust is restoring peatland at the former dairy farm Honeygar near Westhay.', 'The charity hopes to make the area a better habitat for wetland species.', 'Conservationists are using microphones to listen out for bird songs to monitor the population.', 'There are four of these microphones dotted around the 81 hectare site, listening out for birdsong, which is then analysed by artificial intelligence.', 'The trust then uses the information to monitor bird populations and how they are changing.', 'The project has been running continuously for the past 15 months.', 'In that time, the most common birdsong recorded on the site came from wrens, jackdaws and goldfinches.', 'Joe Hampson, at the trust, said they have 1.3m species recordings from the site.', '""It\'s a staggering number,"" he said. ""It shows we\'re in a new age of data collection.""', 'Geoff Carss, CEO at Wilder Sensing, who developed the technology for the project, said: ""We take a species like the robin and take hundreds of recordings, feed them into the machine learning model and over time you have a big library of these things.', '""It will sample sounds and go: \'I\'m 83% certain that was a robin.\' It takes about 30 seconds.""', 'Mr Hampson added: ""We will be expecting more wetland species to be utilising the site, but we\'re willing to be surprised by what happens. ', '""It\'s going to be really exciting.""', 'Follow BBC West on Facebook, X and Instagram. Send your story ideas to: bristol@bbc.co.uk ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['UK chip designer Arm Holdings has seen its stock market value almost double in less than a week as investors bet on the artificial intelligence (AI) boom.', 'The Cambridge-based company reported financial results last Wednesday that showed demand for AI-related technology is boosting its sales.', 'Chips designed by Arm already power almost every smartphone in the world.', ""The firm was taken private by Japan's SoftBank in 2016 and it returned to the stock market last September."", ""Arm's shares have soared since its earnings announcement last week and are now up by more than 98%."", 'It comes as chipmaker Nvidia has seen its shares more than triple in value in the last year on soaring demand for its AI chips.', 'The AI boom has helped Nvidia become one of the most valuable publicly-traded companies in the world, with a stock market valuation of around $1.8 trillion (Â£1.4 trillion).', 'It has also made it the fifth publicly traded US company to join the so-called ""Trillion-dollar club"", along with technology giants Apple, Microsoft, Alphabet and Amazon.', ""Arm's technology is not directly used for AI work, but chip makers like Nvidia are choosing it for central processing units (CPUs) that complement their AI-specific chips."", ""Aside from Nvidia and Taiwan Semiconductor Manufacturing Company (TSMC), Arm's customers also include well-known consumer brands like Apple."", 'Demand for Arm-designed chips is also growing in the car making industry thanks to the development of self-driving technology.', 'Arm was founded in 1990 by a group chip designers in the university city of Cambridge.', 'It was bought by SoftBank in 2016 for $32bn. Four years later, the Japanese conglomerate announced that it planned to sell Arm to Nvidia.', 'However, in April 2022 SoftBank shelved the deal after facing objections from regulators around the world and said it would instead sell shares in Arm on the Nasdaq stock exchange in New York.', ""The jump in Arm's shares is welcome news for SoftBank, as it has been hit by losses due to the falling valuations of  some of its investments, including struggling office space firm WeWork."", 'SoftBank, which still holds a roughly 90% stake in Arm, has seen its own shares gain almost 30% in the past week.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""A Suffolk computer expert is predicting that artificial intelligence (AI) will replace human coders in 10 years' time."", 'Creative Computing Club founder Matthew Applegate said: ""I\'m teaching the last generation of coders. We are looking at being knocked out by AI very soon.""', 'Mr Applegate started the club in 2012 with the ethos ""50% work 50% play"".', '""AI will creep into everything; pop music, writing... plumbers are safe! Technology is always an interesting ride,"" he said.', 'His prediction comes as a House of Lords committee says that we should embrace the positives of AI rather than just focus on the risks.', ""The Communications and Digital Committee's report looked at large language models (LLMs), which are what power generative AI tools like ChatGPT."", 'Mr Applegate said: ""They trained AI on a thing called GitHub, which is an online repository of all the best examples of code, and some of the worst. So it was able to determine the best practices very early on.', '""So we\'ve bypassed a lot of those problems very quickly. We\'ve got probably about ten more years of teaching code.""', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp 0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The federal agency that regulates communication in the US has made robocalls that use AI-generated voices illegal.', 'The Federal Communications Commission (FCC) announced the move on Thursday, saying it will take effect immediately.', 'It gives the state power to prosecute any bad actors behind these calls, the FCC said. ', 'It comes amid a rise in robocalls that have mimicked the voices of celebrities and political candidates.', '""Bad actors are using AI-generated voices in unsolicited robocalls to extort vulnerable family members, imitate celebrities, and misinform voters,"" said FCC chairwoman Jessica Rosenworcel in a statement on Thursday.', '""We\'re putting the fraudsters behind these robocalls on notice.""', ""The move comes on the heels of an incident last month in which voters in New Hampshire received robocalls impersonating US President Joe Biden ahead of the state's presidential primary. "", 'The calls encouraged voters not to cast ballots in the primary. An estimated 5,000 to 25,000 were placed.', ""New Hampshire's attorney general said the calls were linked to two companies in Texas and that a criminal investigation is underway."", 'The FCC said these calls have the potential to confuse consumers with misinformation by imitating public figures, and in some instances, close family members.', 'The agency added that, while state attorneys general can prosecute companies and individuals behind these calls for crimes like scams or fraud, this latest action makes the use of AI-generated voices in these calls itself illegal. ', 'It ""expands the legal avenues through which state law enforcement agencies can hold these perpetrators accountable under the law"".', 'In mid-January, the FCC received a letter signed by attorneys general from 26 states asking the agency to act on restricting the use of AI in marketing phone calls.', '""Technology is advancing and expanding, seemingly by the minute, and we must ensure these new developments are not used to prey upon, deceive, or manipulate consumers,"" said Pennsylvania Attorney General Michelle Henry, who led the effort.', 'The letter follows a Notice of Inquiry put forward by the FCC in November 2023 that requested input from across the country on the use of AI technology in consumer communications.', 'Deepfakes -  which use AI to make video or audio of someone by manipulating their face, body, or voice - have emerged as a major concern around the world at a time when major elections are, or will soon, be underway in countries like the US, UK and India. ', 'Senior British politicians have been subject to audio deepfakes, as have politicians in nations including Slovakia and Argentina.', ""The National Cyber Security centre in the UK has warned of the threats AI fakes pose to the country's next election."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A city\'s mayor has admitted using artificial intelligence (AI) to help write his speeches, and said it had ""massive potential for the council"".', 'Liberal Democrat Nick Sandford, elected as Peterborough mayor in May, said he used ChatGPT to help at civic events.', '""Chatbots"" use huge amounts of data online to create convincing human-like responses to user queries.', 'The city council said it would draw up an AI strategy as well as ethical guidelines for its use in future.', 'Mr Sandford said before one particular event, he ""got some information from the council\'s PR team and put it into ChatGPT... and it composed a speech for me"".Â\xa0', '""You have to tweak it a bit because it can be quite American,"" he said.', '""But there\'s massive potential for the council, using AI in an innovative way.""', 'OpenAI is the Microsoft-backed creator of ChatGPT, used by the mayor.', 'The tool is trained on information from the internet and can answer questions from users.', 'It has been used to write marketing copy, computer code and songs, among other things.', 'A spokesperson for Peterborough City Council confirmed it had ""several programmes of work looking at [the use of AI] across all areas of the business including customer service, information governance and social care"".Â\xa0', '""This is an exciting path for the council and the use of AI brings immense potential for improving services by speeding up processes and providing smooth and quicker services to the public,"" the spokesperson said.', 'He added the council was still at an ""early and exploratory"" stage.', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp  0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Technology reporter Stephanie Power likes to think she is comfortable with artificial intelligence, but the new breed of talking AI companions unnerve her, as she reports.', 'My husband is a huge Liverpool FC fan who gets into a state of anxiety almost every time they play.', ""It is very irritating, but I've recently realised that the best technique is to avoid him before, during, and sometimes after a game."", ""Recently following a match, I could hear a friendly woman's voice talking to him in his home office."", '""Oh yes, it was a tough game, but the Reds really are on good form,"" she said. ""Liverpool\'s strikers were able to deliver the goods.""', 'Who was this woman? And why was she indulging David in this way?', 'It turns out that my husband was trying out an app called Pi.ai. Launched last year by US tech firm Inflection AI, it is an example of a growing trend called conversational AI. ', ""Rather than just answer any questions you give it, or perhaps do your homework for you, the idea is that the AI can become a friend or companion who talks to you - out loud - via your computer or phone's speakers."", 'And the more you chat with the AI, the more it is said to understand you, and so its replies aim to become more tailored to you, and more like having a natural conversation with a friend. That, at least, is the theory.', 'With Pi.ai you still have to type in your half of the conversation, but it replies in one of six different human-like voices that you can choose from. These range from a fast-talking American man, to the posh English female voice that my husband was talking to.', 'If your reaction is ""but Amazon\'s Alexa has been talking out loud to me for years"", conversational AI aims to give you a far more natural, flowing chat, both in terms of the words and sentences that the AI chooses, and the way it says them out loud.', '""People might say, we\'ve had voice in technology for ages, and they\'re probably thinking of Alexa,"" says David Reid, a professor in AI at Liverpool Hope University, and the man who just happens to be my Liverpool-obsessed husband.', '""But the global market in conversational AI is expected to grow to $30bn (Â£24bn) in the next five years. If you want an idea of what this might look like, then imagine Alexa, but with empathy.""', 'Tech firms are now racing to release their own conversational AI companions. Google has Vertex AI Conversation, Microsoft has Azure AI, and there are now a host of start-ups in the sector. ', 'Meanwhile, Amazon is sticking with Alexa, but planning to add conversational AI to it, plus a more human-like voice.', 'Rohit Prasad, head scientist for Alexa, explained his hopes, using another sporting analogy, in a speech last year. ""The [Boston] Red Sox are my favourite [baseball] team,"" he said. ""Imagine if they won, then Alexa would respond in a joyful voice. If they lost, it will be empathetic to me.""', 'To make the human-like voices for conversational AI, it usually starts with a recording of an actual human. However, the technology then needs to be able to adapt this to put across the required tone or volume, to put words together in a natural-sounding way.', '""Our tools can take into account the spirit of a sentence, and how the words connect to each other,"" says Mati Staniszewski, the boss of Eleven Labs, a UK tech firm that has created 40 AI voices across gender, age and accent. ""This means we can capture the intonation, tone and emotion the AI speaker intends to convey.""', 'Mr Staniszewski says intonation is vital.', '""Getting that right is what stops an AI from sounding robotic. Emotions and intonation often need to stretch and resonate across a number of sentences to tie a particular train of thought together. And tone and pacing convey intent, so the model takes the surrounding context into account, maintaining the right flow.""', 'Trevor Cox, a professor of acoustic engineering at the University of Salford, says that the developers of conversational AI will likely avoid strong, regional accents.', '""There are still prejudices around strong regional accents,"" he says. ""Studies suggest that the harder a voice is to understand, then the less likely we are to believe what is being said.', '""This is beyond accent and more about flow. Our brains want to decode information quickly. So the creators of an AI will want to make sure the brain has access to that fast decoding.', '""Then beyond that there is tone. Messages are conveyed by much more than the words, it is how you say them. So if an AI can convey happiness, excitability or boredom then that all helps.""', 'David Harley, a lecturer in cyberpsychology at Brighton University, says there are risks as computer voices become ever more human-like. ', '""My concern lies in the fact that people may start to view AI companions and therapists as effective in solving all life\'s problems,"" he says. ""They may start to tailor their lives around the AI advice, which is blind to these other profound aspects of being human.""', 'He adds that people will have to remind themselves that their AI companion is not a real person.', 'Read additional stories on artificial intelligence', 'I had a little go with Pi.ai myself, and I found it to be a bit obsequious, like a friend who just agrees with everything you say. ', 'Prof Reid, aka my other half, says that\'s how it has been designed. ""What you call obsequious, I see as friendly and supportive.', '""I can see conversational AI being really valuable in a setting such as a care home, where people would get joy from reminiscing about the past with something that is knowledgeable. Or a call centre, where the AI can understand when a caller is getting frustrated, and react accordingly.""', 'Or perhaps helping to keep thousands of relationships alive across the globe, by providing therapy to fraught football and baseball fans from Liverpool to Boston, and beyond.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A city council says it will save Â£200,000 by losing four full-time equivalent agency jobs due to artificial intelligence (AI). ', 'Derby City Council said agency workers in the customer management department were being reduced.', 'The council\'s chief executive Paul Simpson said out of a 3,000-person workforce, ""it\'s a very small number"". ', 'Conservative councillor Matthew Holmes said he was concerned about more possible future job losses due to AI.', 'The Labour-run council told the Local Democracy Reporting Service in January that no savings were coming from a ""compulsory staffing reduction"".', 'And last week, the governor of the Bank of England said AI would not be a ""mass destroyer of jobs"".', 'A freedom of information (FOI) request has found during the 2022-23 financial year, two full-time equivalent positions were lost in the department that deals with customer queries, reports, applications and payments at the council.', 'Meanwhile, ""contractor workload"" was decreased in the revenues and benefits department, due to automated telephone and webchat services Darcie and Ali. ', 'The same savings are being made for the current 2023-24 financial year.  ', 'Mr Simpson said AI had the ""potential to revolutionise"" the authority\'s services.', 'He said: ""We are looking to use artificial intelligence to help us deliver services to our customers in a way that reduces the cost and provides 24/7 access to a lot of our services.""', 'Mr Simpson added ""from our perspective, there are very, very few job losses"" and said using AI was about ""streamlining"" processes, so the workforce could concentrate on the more complex and strategic activities.', 'He said contractors and agency staff were not permanent - ""they are extra capacity that we bring in to help manage workload"". ', '""By using AI, we can do away without the need for that extra cost,"" he said.', 'In response, Mr Holmes said: ""It is a good thing if it works because it will deliver efficient services. ', '""The bad thing is, if it doesn\'t work very well, residents don\'t get the services or support they need. Also, it is taking away the human element... therefore they are out of the authority and have lost their job potentially.', '""Jobs are going to change and AI is going to take jobs away from those who are currently carrying them out. ', '""We need to ensure those people can gain skills to move into another area of the business or work with AI."" ', ""Mr Holmes added the council's 2024-25 budget plans nearly Â£4m of savings through AI."", 'He said: ""We feel it is high risk... to base your budget on something which effectively no-one else has done before.""', 'Follow BBC East Midlands on Facebook, X, or Instagram. Send your story ideas to eastmidsnews@bbc.co.uk.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Three students have won a $700,000 prize after using AI to read a 2,000-year-old scroll burnt during the Mount Vesuvius eruption in 79AD.', 'The ancient text was unreadable until now after being charred in the Roman town of Herculaneum during the same eruption that destroyed Pompeii.', ""It is thought to have belonged to Julius Caesar's father-in-law and talks of music and food."", 'Experts have called the breakthrough a ""revolution"" in Greek philosophy.', 'Scholars believe the style of the writing is typical of the Greek philosopher Philodemus, who followed the teachings of Epicurus, and may have been philosopher-in-residence at Herculaneum.', 'In the 18th century hundreds of papyrus scrolls were discovered in the library of a luxurious a villa in the town - the only such library of texts from ancient Roman times to be discovered. ', 'But their contents remained a mystery to scholars - they were so badly burnt by volcanic debris that when they attempted to unroll them they fell apart in their hands. ', 'Dr Federica, papyrology researcher at the University of Naples, said this ""curse"" is also their saving grace. The high temperatures of the eruption carbonised and preserved the scripts which would have typically decomposed. ', 'Last year a breakthrough came when Dr Brent Seales and his team at the University of Kentucky used high resolution CT scans to unroll the texts, but the black carbon ink used on the scripts was indecipherable from the papyrus itself.', 'Dr Seales worked with tech investors to launch the Vesuvius Challenge, a $1m (Â£790,000) prize for anyone that could come up with a solution. ', 'A team of three students, not working in philosophy but tech, realised artificial intelligence may be able to provide the solution. ', 'Youssef Nader, a PhD student in Berlin, Luke Farritor, a SpaceX intern and student, and Julian Schillinger, a Swiss Robotics student, built an AI model that was able to work out the lettering through using pattern recognition.', 'Dr Federica said: ""This is the start of a revolution in Greek philosophy in general.""', ""The model has so far deciphered 2,000 Greek characters written in one of the four scrolls scanned by Dr Seales' team - which is only 5% of the text. "", 'Now translated the characters reveal the author discussing the sources of pleasure in life, referencing music and food. ', 'In one passage Philodemus questions whether things in lesser quantities bring more pleasure: ""as too in the case of food, we do not right away believe things that are scarce to be absolutely more pleasant than those which are abundant.""', 'The team behind the Vesuvius Challenge hope the technology can be used to read 90% of all four scrolls scanned this year, and eventually all 800. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Meta says it will introduce technology that can detect and label images generated by other companies' artificial intelligence (AI) tools."", 'It will be deployed on its platforms Facebook, Instagram and Threads.', 'Meta already labels AI images generated by its own systems. It says it hopes the new tech, which it is still building, will create ""momentum"" for the industry to tackle AI fakery.', 'But an AI expert told the BBC such tools are ""easily evadable"".', 'In a blog written by senior executive Sir Nick Clegg, Meta says it intends to expand its labelling of AI fakes ""in the coming months"".', 'In an interview with the Reuters news agency, he conceded the technology was ""not yet fully mature"" but said the company wanted to ""create a sense of momentum and incentive for the rest of the industry to follow"".', 'But Prof Soheil Feizi, director of the Reliable AI Lab at the University of Maryland, suggested such a system could be easy to get around.', '""They may be able to train their detector to be able to flag some images specifically generated by some specific models,"" he told the BBC.', '""But those detectors can be easily evaded by some lightweight processing on top of the images, and they also can have a high rate of false positives. ', '""So I don\'t think that it\'s possible for a broad range of applications.""', 'Meta has acknowledged its tool will not work for audio and video - despite these being the media that much of the concern about AI fakes is focused on.', 'The firm says it is instead asking users to label their own audio and video posts, and it ""may apply penalties if they fail to do so"".', 'Sir Nick Clegg also admitted it would be impossible to test for text that has been generated by tools such as ChatGPT.', '""That ship has sailed,"" he told Reuters.', 'On Monday, Meta\'s Oversight Board criticised the company for its policy on manipulated media, calling it ""incoherent, lacking in persuasive justification and inappropriately focused on how content has been created"".', 'The Oversight Board is funded by Meta but independent of the company. ', 'The criticism was in response to a ruling on a video of US President Joe Biden. The video in question edited existing footage of the president with his granddaughter to make it appear as though he was touching her inappropriately.', ""Because it was not manipulated using artificial intelligence, and depicted Mr Biden behaving in a way he did not, rather than saying something he did not, it did not violate Meta's manipulated media policy - and was not removed."", ""The Board agreed that the video did not break Meta's current rules on fake media, but said that the rules should be updated."", 'Sir Nick told Reuters that he broadly agreed with the ruling.', 'He admitted that Meta\'s existing policy ""is just simply not fit for purpose in an environment where you\'re going to have way more synthetic content and hybrid content than before.""', 'From January, the company has had a policy in place which says political adverts have to signal when they are using digitally altered images or video.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Artificial Intelligence (AI) is being used to help preserve part of Vice-Admiral Lord Nelson's flagship HMS Victory."", ""Based in Portsmouth's Historic Dockyard, the warship is undergoing a 10-year conservation scheme."", 'University of Southampton students have applied AI technology to thousands of images taken of the 259-year-old ship.', 'It will prevent vital historical information from being lost forever, a university archaeologist said.', ""HMS Victory is best-known for its role in the Battle of Trafalgar in 1805 and is the world's oldest naval ship still in commission."", 'The renovation work, at the National Museum of the Royal Navy, has seen craftspeople replacing the decayed planking of the hull and the damaged frames of the ship.', 'Part of this process involves documenting the vessel, with high-resolution images taken to produce accurate 3D digital models.', 'The process has been automated by an AI algorithm, which Dr Rodrigo Pacheco-Ruiz from the University of Southampton said had made the images ""high resolution, complex and detailed"".', '""Archaeologists are obsessed with detail and if records are not accurately stored, vital historical information could be lost forever,"" the archaeologist said.', '""The project is really at the forefront of how AI is being used in archaeology... it\'s certainly a long way from the traditional perception of how an archaeologist spends their time.""', 'Previously a curator would add a short description of an object to a physical catalogue card but now they must record more detailed information so items can be catalogued digitally.', 'The data could also allow the public to view the ship online, rather than visiting in person.', 'Amy Adams, the museum\'s collections information and access manager, said: ""The opportunity to share knowledge worldwide and engage more and more people in our nation\'s maritime history is huge.""', ""HMS Victory's restoration is due to be completed in just under 10 years, at a cost of Â£40-Â£45m."", 'This story has been amended. A previous version also mentioned HMS Victory (1737) which is a different ship.', 'Follow BBC South on Facebook, X, or Instagram. Send your story ideas to south.newsonline@bbc.co.uk', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial Intelligence (AI) will not be a ""mass destroyer of jobs"" and human workers will learn to work with new technologies, the governor of the Bank of England has told the BBC.', 'Governor Andrew Bailey said while there are risks with AI, ""there is great potential with it"". ', 'The Bank says businesses expect to see the benefits to productivity soon.', ""Almost a third told the Bank they'd made significant AI investments in the past year."", 'Mr Bailey added ""I\'m an economic historian, before I became a central banker. Economies adapt, jobs adapt, and we learn to work with it. And I think, you get a better result by people with machines than with machines on their own. So I\'m an optimistâ\x80¦""', 'In its latest assessment of the UK economy the Bank\'s business contacts said that automation and AI investment was already ""containing recruitment and labour costs"" in a tight labour market.', ""Mr Bailey's comments come as a committee that sits in the House of Lords says that we should embrace the positives of AI rather than just focus on its risks."", 'The committee\'s chair Baroness Stowell told the BBC that talk of ""existential risks and sci-fi scenarios"" should not get in the way of reaping the rewards of AI.', 'AI goldrush', 'The country could ""miss out on the AI goldrush,"" her committee\'s report said.', 'It said some of the ""apocalyptic"" warnings about AI\'s dangers were exaggerated.', ""The Lords Communications and Digital Committee's report focuses on large language models (LLMs), which are what power generative AI tools like ChatGPT."", ""They have captured people's imaginations with their ability to, for example, give human-like responses to questions."", 'But they have also prompted concerns, including from various senior industry figures, that the technology could cause problems ranging from eliminating jobs to threatening humanity itself.', ""The UK hosted the world's first AI Safety Summit in November 2023, where a global declaration on managing AI risks was announced.."", 'But Baroness Stowell warned that the government needed to be careful the UK didn\'t end up as ""the safety people"".', '""No expert on safety is going to be credible if we are not at the same time developers and part of the real vanguard of promoting and creating the progress on this technology"", she said.', 'Given there is no UK equivalent of ChatGPT, and in order to avoid another situation in which - as with other areas of tech - all of the industry giants are clustered elsewhere, the Lords committee seems to be essentially warning the country to go easy on the red tape.', 'The Committee has also highlighted the issue of copyright, which is particularly contentious with AI.', ""That's because LLMs rely on being fed information from things that already exist digitally, and there are questions over whether developers have properly sought permission for it."", 'Photo agency Getty Images is currently taking legal action against Stability AI, claiming that the tech company has used its images without permission to train its picture generation tools.', 'The Committee is calling on the government to provide clarity over what rules apply, saying it can not ""sit on its hands"" while LLM developers ""exploit"" the works of rightsholders.', '""The government needs to come out with its position,"" Baroness Stowell told the BBC.', ""Secretary of State for Science, Innovation and Technology Michelle Donelan will give evidence to the Lords Communications and Digital Committee on Tuesday, where she is expected to be questioned on the government's reaction to the report."", 'A Department for Science, Innovation and Technology spokesperson said:  ""We do not accept this - the UK is a clear leader in AI research and development, and as a government we are already backing AI\'s boundless potential to improve lives, pouring millions of pounds into rolling out solutions that will transform healthcare, education and business growth, including through our newly announced AI Opportunity Forum.', 'They added: ""The future of AI is safe AI. It is only by addressing the risks of today and tomorrow that we can harness its incredible opportunities and attract even more of the jobs and investment that will come from this new wave of technology.', '""That\'s why [we] have spent more than any other government on safety research through the AI Safety Institute and are promoting a pro-innovation approach to AI regulation.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Search for ""AI investing"" online, and you\'ll be flooded with endless offers to let artificial intelligence manage your money.', 'I recently spent half an hour finding out what so-called AI ""trading bots"" could apparently do with my investments.', 'Many prominently suggest that they can give me lucrative returns. Yet as every reputable financial firm warns - your capital may be at risk.', 'Or putting it more simply - you could lose your money - whether it is a human or a computer that is making stock market decisions on your behalf.', 'Yet such has been the hype about the ability of AI over the past few years, that almost one in three investors would be happy to let a trading bot make all the decisions for them, according to one 2023 survey in the US.', ""John Allan says investors should be more cautious about using AI. He is head of innovation and operations for the UK's Investment Association, the trade body for UK investment managers."", '""Investment is something that\'s very serious, it affects people and their long-term life objectives,"" he says. ""So being swayed by the latest craze might not be sensible.', '""I think at the very least, we need to wait until AI has proved itself over the very long term, before we can judge its effectiveness. And in the meantime, there will be a significant role for human investment professionals still to play.""', 'Given that AI-powered trading bots may end up putting some highly-trained but expensive human investment managers out of work you might expect Mr Allan to say this. But such AI trading is indeed new, and it does have issues and uncertainties.', 'Firstly, AI is not a crystal ball, it cannot see into the future any more than a human can. And if you look back over the past 25 years, there have been unforeseen events that have tripped up the stock markets, such as 9/11, the 2007-2008 credit crisis, and the coronavirus pandemic.', 'Secondly, AI systems are only as good as the initial data and software that is used to create them by human computer programmers. To explain this issue we need a little history lesson.', 'Investment banks have actually been using basic or ""weak AI"" to guide their market choices since the early 1980s. That basic AI could study financial data, learn from it, and make autonomous decisions that - hopefully - got ever more accurate. These weak AI systems did not predict 9/11, or even the credit crisis.', 'Fast-forward to today, and when we talk about AI we often mean something called ""generative AI"". This is far more powerful AI, which can create something new and then learn from that.', 'When applied to investment, generative AI can absorb masses of data and makes its own decisions. But it can also work out better ways to study the data and develop its own computer code.', 'Yet if this AI was originally fed bad data by the human programmers, then its decisions may simply get worse and worse the more code it creates.', 'Read additional stories on artificial intelligence', ""Elise Gourier, an associate professor in finance at the ESSEC Business School in Paris, is an expert in the study of AI going wrong. She cites Amazon's recruitment efforts in 2018 as a prime example."", '""Amazon was one of the first companies to get caught out,"" she says. ""What happened was that they developed this AI tool to recruit people.', '""So, they\'re getting thousands of CVs, and they thought we\'re just going to automate the whole process. And basically, the AI tool was reading the CVs for them and telling them who to hire.', '""The problem was that the AI tool was trained on its employees, and its employees are mainly men, and so, as a result of that, basically what the algorithm was doing was filtering out all the women.""', 'Amazon had to scrap the AI-powered recruitment. ', 'Generative AI can also simply just go wrong, and produce incorrect information, something termed a ""hallucination"", says Prof Sandra Wachter, a senior research fellow in AI at Oxford University. ', '""Generative AI is prone to bias and inaccuracies, it can spit out wrong information or completely fabricate facts. Without vigorous oversights it is hard to spot these flaws and hallucinations.""', 'Prof Sandra Wachter also warns that automated AI systems can be at risk of data leakage or something called ""model inversion attacks"". The latter - in simple terms - is when hackers ask the AI a series of specific questions in the hope that it reveals its underling coding and data.', 'There is also the chance that AI will become less of a genius investment advice engine, and more like the stock pickers you used to find in the Sunday newspapers. They would always recommend some minor share to buy first thing on Monday morning, and miraculously the shares would always jump in value first thing that day. ', 'This, of course, had nothing to do tens of thousands of readers all rushing to buy the share in question.', 'So despite all these risks, why are a sizeable number of investors seemingly keen to let AI make decisions for them? Business psychologist Stuart Duff, of consultancy firm Pearn Kandola, says some people simply trust computers more than other humans.', '""It\'s almost certainly reflecting an unconscious judgement that human investors are fallible, while machines are objective, logical and measured decision makers,"" he says. ""They may believe that AI will never have an off day, will never deliberately cheat the system, or try to hide losses.', '""Yet an AI investment tool may simply reflect all of the thinking errors and poor judgements of its developers. More than that, it may lose the benefit of intuitive experience and rapid reaction when unprecedented events strike in the future, such as the financial crash, and the Covid pandemic. Very few humans could create AI algorithms to cope with those massive events.""', 'Additional reporting by Will Smale.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A new artificial intelligence (AI) tool has been developed to help boost cancer diagnosis and treatment.', 'Scientists at Histofy, a spin-out company from The University of Warwick, have designed the device to help medical professionals to grade cancer by analysing how cells divide.', 'Counting the number of cells undergoing division serves as a key indicator of cancer aggressiveness, or grade.', 'This information is then used to decide on treatment options.', 'Traditional cell division counting methods have been described by Histofy scientists as ""time-consuming and plagued by poor reliability"".', 'To address this, the team developed MitPro, which uses AI to count and profile how cancer cells split throughout the entire tumour sample. This identifies the most suitable areas for further analysis.', 'The tool has been designed to enhance the current standard of care for grading various cancers, such as breast cancer and sarcomas.', 'Simon Graham, chief technology officer for Histofy at the University of Warwick\'s computer science department, said: ""AI holds tremendous potential in facilitating better cancer care. ', '""MitPro helps to improve current grading systems across a range of cancers by more accurately assessing the rate in which cancer cells are dividing.""', 'Follow BBC West Midlands on Facebook, X and Instagram. Send your story ideas to: newsonline.westmidlands@bbc.co.uk', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Microsoft sales accelerated in the final months of 2023, lifted by demand for its artificial intelligence tools.', 'The company said revenue in September to December rose 18% year-on-year to more than $60bn.', ""The update came as Microsoft became the world's most valuable listed company, its market value soaring past Apple this month to more than $3tn (Â£2.4tn). "", 'Chief executive Satya Nadella said Microsoft is applying AI ""at scale"". ', 'As the company provided a quarterly update to investors, the results confirm Microsoft as one of the leading companies as tech firms race to profit from an anticipated next wave of growth ushered in by advancements in AI. ', 'The tech giant has a large stake in OpenAI, the maker of the ChatGPT bot, which launched a wave of optimism about the new technological possibilities when it was released in 2022. ', 'However, its expansion has not been without controversy. US news organisation the New York Times is suing  OpenAI over claims its copyright was infringed to train the system.', 'The lawsuit, which also names Microsoft as a defendant, says the firms should be held responsible for ""billions of dollars"" in damages.', 'ChatGPT and other large language models (LLMs) ""learn"" by analysing a massive amount of data often sourced online.', 'Microsoft has been incorporating AI-assisted tools for coding, and other purposes, into its software and other offerings for businesses. Sales of Copilot started in November. The programme can summarise meetings held in Teams for anyone who chooses not to attend. Copilot can also draft emails, create word documents, spreadsheet graphs, and Powerpoint presentations.', 'Mr Nadella said these recent moves were paying off and ""winning new customers"".', ""Sales of Microsoft's Azure cloud computing offerings, which are closely watched by investors, rose 30% year-on-year, better than analysts had predicted. "", 'Overall, profits in the quarter rose 33% year-on-year to $21.9bn.', 'The strategy for artificial intelligence is also top of mind at Alphabet, the owner of Google and YouTube, which also updated investors on Tuesday. ', 'Alphabet said revenues in the September-December quarter rose 13% year-on-year and reported profits of nearly $20.7bn, compared with $13.6bn last year. ', ""Boss Sundar Pichai said the company's search, cloud computing and YouTube was also benefiting from investments in AI."", 'Despite the gains, both companies have continued to slim their workforce.  ', ""Google's headcount is down around 5% since last year, and it announced another round of job cuts this month."", 'Microsoft also announced plans to slim its gaming unit, cutting 1,900 jobs or 9% of staff in that division. ', 'The move followed the completion of its takeover of Activision Blizzard, maker of games Call of Duty and World of Warcraft. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""An ancient symbolic code lies behind the intricate patterns of Kashmir's traditional handwoven carpets and rugs. "", 'Called talim, the code has been used for hundreds of years to design carpets and convey information to weavers. ', 'From the age of eight, and following in the footsteps of his father, Mohammad Rafiq Sofi has been weaving carpets using talim designs.', '""It took me five years to learn how to weave properly,"" says Mr Sofi, now 57.', 'Much has changed during his half century in the industry. Mr Sofi says that in the early days it could take more than six months to complete a carpet. ', 'To start the process, a designer would draw up a carpet. A talim expert would then encode that design, and in small sections send it off for weaving.', 'Those chunks of code would be translated for Mr Sofi and weavers like him, showing them where to knot each thread and which colour to use. ', 'Each section would only represent a small piece of the carpet, so hundreds would be needed for the whole carpet, with much back and forth between the designers and weavers. ', 'The process made mistakes difficult to spot and time-consuming to correct.', 'But these days computer software has streamlined the process and Mr Sofi can finish a carpet in six weeks. ', 'The weaving and knotting is still done by hand, but now computer software handles the design and creation of the talim code. It means Mr Sofi can see the whole design at once, instead of just small sections.', 'Any potential problems can be spotted in advance, cutting down on time-consuming errors.', '""This innovation in handmade carpets is not to disrupt the essence of artistic carpets, it\'s just to speed up the process - designs being available now at a speed,"" says Mehmood Shah, the director of Handloom & Handicrafts for the government of Jammu and Kashmir.', 'More technology of business', 'The latest innovation comes from technology firms that are applying artificial intelligence to the process.', 'Aby Mathew is chief operating officer at International Virtual Assistance, a computer software firm that specialises in analysing data. ', 'His company is training an artificial intelligence (AI) system to understand the talim code by showing it pictures of carpets and lines of talim code. ', 'The AI is still being developed and the process will still require a human to write the code, but Mr Mathew says it should speed up manufacturing by decoding the talim instructions for the weavers.', '""Weavers will be able to try out new patterns, update classic themes to suit contemporary tastes, and produce one-of-a-kind, custom carpets,"" says Mr Mathew.', 'As India grows richer he sees an increasing demand for carpets that the traditional industry will struggle to meet. ', '""The tastes of customers are evolving, with a growing desire for carpets that are fashionable, long-lasting, and low maintenance. Conventional carpet-making techniques are frequently labour-intensive and sluggish - they might not be able to satisfy these needs,"" says Mr Mathew.', 'Aditya Gupta founded Rug Republic 32 years ago, it now employs around 5,000 people and makes up to 15,000 rugs per month. He says the Indian rug and carpet industry is facing stiff competition from rivals in Turkey and China and needs to keep up with the latest manufacturing techniques.', '""Innovation is important in every industry - without it, we die,"" he says.', '""The Indian carpet industry is an interesting case, where it is not only about moving forward with new tech, but rather moving the old and the new hand in hand.', '""The innovation now is oriented towards creating designs that cannot be copied by machines while still using traditional techniques.""', 'At Rug Republic, new tech has been introduced for the design, washing and drying of carpets and for monitoring moisture levels. As well as the traditional wool, materials like recycled jeans, cotton and leather have been experimented with.', 'Despite all the innovation, Mr Gupta still values the old methods.', '""The manufacturing side of things still needs to be traditional and handmade as that is the main charm the consumers seek.""', 'The industry has also been helped by an official tagging system which identifies a genuine hand-knotted Kashmiri carpets.', 'By scanning a QR code buyers can verify the carpet designer and how it was made. ', '""If the [Handicrafts] department hadn\'t taken this step, maybe this trade in handwoven carpets would have died in a few years,"" says carpet designer, Shahnawaz Ahmad.', 'That would have been a blow as the industry is an important part of the local economy. It employs around 50 thousand workers in Jammu and Kashmir, who collectively produce rugs and carpets worth around Â£36m ($28m) a year.', 'The developments in carpet making have given hope to old-timers like Feroz Ahmad Bhat, who has been weaving carpets for the past thirty years.', '""In my early days, our earnings were good and a lot of people were involved with this work. Then a time came when wages were very low. But now new designs have been introduced and this work has picked up pace again. Now it\'s flourishing again."" ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Social media platform X has blocked searches for Taylor Swift after explicit AI-generated images of the singer began circulating on the site.', 'In a statement to the BBC, X\'s head of business operations Joe Benarroch said it was a ""temporary action"" to prioritise safety.', 'When searching for Swift on the site, a message appears that says: ""Something went wrong. Try reloading."" ', 'Fake graphic images of the singer appeared on the site earlier this week.', 'Some went viral and were viewed millions of times, prompting alarm from US officials and fans of the singer.', 'Posts and accounts sharing the fake images were flagged by her fans, who populated the platform with real images and videos of her, using the words ""protect Taylor Swift"". ', 'The photos prompted X, formerly Twitter, to release a statement on Friday, saying that posting non-consensual nudity on the platform is ""strictly prohibited"". ', '""We have a zero-tolerance policy towards such content,"" the statement said. ""Our teams are actively removing all identified images and taking appropriate actions against the accounts responsible for posting them."" ', 'It is unclear when X began blocking searches for Swift on the site, or whether the site has blocked searches for other public figures or terms in the past.', 'In his email to the BBC, Mr Benarroch said the action is done ""with an abundance of caution as we prioritise safety on this issue"".', 'The issue caught the attention of the White House, who on Friday called the spread of the AI-generated photos ""alarming"". ', '""We know that lax enforcement disproportionately impacts women and they also impact girls, sadly, who are the overwhelming targets,"" said White House press secretary Karine Jean-Pierre during a briefing. ', 'She added that there should be legislation to tackle the misuse of AI technology on social media, and that platforms should also take their own steps to ban such content on their sites.', '""We believe they have an important role to play in enforcing their own rules to prevent the spread of misinformation and non-consensual, intimate imagery of real people,"" Ms Jean-Pierre said.', 'US politicians have also called for new laws to criminalise the creation of deepfake images. ', 'Deepfakes use artificial intelligence to make a video of someone by manipulating their face or body. A study in 2023 found that there has been a 550% rise in the creation of doctored images since 2019, fuelled by the emergence of AI.', 'There are currently no federal laws against the sharing or creation of deepfake images, though there have been moves at state level to tackle the issue.', 'In the UK, the sharing of deepfake pornography became illegal as part of its Online Safety Act in 2023.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial intelligence (AI) is increasingly being used by police forces around the world, but do the benefits always outweigh the risks?', 'Sarah is a victim of domestic abuse, and she is on the phone to a 999 emergency call handler.', 'She is scared and upset because her ex-husband is trying to break into her house.', 'While Sarah is talking to a human, the call is also being transcribed by an AI software system, one that links directly into UK police databases.', 'When she tells the handler the name of her husband and his date of birth, the AI quickly retrieves his details. It flashes up that the man has a gun licence, which means that police officers need to get to the home as soon as possible.', 'Although domestic abuse emergency calls are sadly all too common, the above example was thankfully not a live, real-world situation. Instead it a mock-up test, part of a three-month trial of AI emergency call software last year by Humberside Police.', 'The AI was provided by UK start-up Untrite AI, and is designed to make dealing with the thousands of calls received each day more efficient.', 'The system was trained on two years worth of historic data - all related to domestic abuse calls - provided by Humberside.', '""We set out to build an assistant for operators to make their jobs slightly easier, because it is a high stress and time-sensitive environment,"" says Kamila Hankiewicz, chief executive and co-founder of Untrite.', '""The AI model analyses a lot of the information, the transcript and the audio of the call, and produces a triaging score, which could be low, medium or high. A high score means that there has to be a police officer at the scene within five or 10 minutes.""', 'Untrite says the trial suggests that the software could save operators nearly a third of their time, both during and after each call. Other tech companies also now offering AI-powered emergency calls software systems include US businesses Corti and Carbyne.', 'The next stage for Untrite will be to use its AI in a live environment, and the firm is in talks with a number of police forces and other emergency services on making that happen.', 'AI has the potential to transform the way the police investigate and solve crimes. It can identify patterns and links in evidence, and sift through vast amounts of data far more quickly than any human. ', 'But we have already seen missteps in the use of the technology by law enforcement. For example, there were numerous reports in the US last year about AI-powered facial recognition software failing to accurately identify black faces.', 'Some US cities, such as San Francisco and Seattle, have already banned the use of the technology.  Yet it is increasingly being used by police forces on both sides of the Atlantic.', 'Albert Cahn, executive director of US anti-surveillance pressure group Surveillance Technology Oversight Project (Stop), is not happy with the development.', '""We\'ve seen a massive investment in, and use of, facial recognition despite evidence that it discriminates against black, Latino and Asian individuals, particularly black women,"" he says.', 'Such technology can be used in three main ways. Firstly, live facial recognition, which compares a live camera feed of faces against a predetermined watchlist.', 'Secondly, retrospective facial recognition, which compares still images of faces against an image database. And thirdly, operator-initiated facial recognition, in which an officer takes a photograph of a suspect, and submits it for a search against an image database.', ""Last October, the UK's Policing Minister Chris Philp said that UK police forces should double the number of searches they make using retrospective facial recognition technology over the next year."", ""Meanwhile, the UK's National Physical Laboratory (NPL) last year undertook independent testing of the three types of facial recognition technology, all of which have been used by the Metropolitan and South Wales police forces."", 'The NPL, which is the official UK body for setting measurement standards, concluded that accuracy levels had improved considerably in the latest versions of the software.', 'Yet it also noted that in some cases it was more likely to give false positive identification for black faces compared to white or Asian ones, something the NPL described as ""statistically significant"".', 'It is, of course, good news that independent tests are taking place, and West Midlands Police has gone a step further, setting up its own ethics committee to evaluate new tech tools.', 'This body is made up of data scientists, and chaired by Prof Marion Oswald, a professor of law at the University of Northumbria.', 'She told the BBC that the committee is currently assessing the use of a specific new facial recognition tool that would allow a police officer to take photographs of a suspect and compare it against a watchlist.', '""We will be recommending that there needs to be much more analysis of its validity,"" she says. ', 'Another key policing area that AI may transform is prevention. Or more specifically, its potential ability to predict where crimes may happen and who might commit them.', 'While this might conjure up images of the 2002 sci-fi thriller Minority Report, the idea is no longer just a Hollywood dream. ', 'A team at the University of Chicago has developed an algorithm that claims to be able to predict future crimes a week in advance with 90% accuracy.', 'But, with the old adage that AI systems are only as good as the data they are fed, there are big concerns from some.', 'Stop\'s Mr Cahn says that ""original sin"" of predictive policing is ""biased historical data"".', 'He adds: ""In the US we see a lot of crime prediction tools that crudely deploy algorithms to try to predict where crimes will happen in future, often to disastrous effect.""', 'Disastrous, he adds, because ""the US has notoriously terrible crime data"".', 'Prof Oswald agrees that using AI to predict crime is fraught with concern.  ""There is that feedback loop concern that you\'re not really predicting crime, you\'re just predicting the likelihood of arrest,"" she says.', '""The issue is that you are comparing a person against people who have committed similar crimes in the past, but only based on a very limited set of information. So not about all their other factors, and those others things about their life that you might need to know in order to make a determination about someone.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Scotland's Kirsty Gilmour is set to compete in her third Olympic Games this summer"", ""Scotland's Kirsty Gilmour is still receiving death threats and abuse on social media despite using artificial intelligence (AI) to block out trolls."", 'Last year the Olympian, 30, revealed she had been sent rape and death threats, thought to be from gamblers who had bet on matches she lost.', 'And while the new software has stopped much of the abuse reaching her, some is still getting through.', '""I received a terrible message the other day,"" she told BBC Scotland.', '""But I am currently working with an AI company called Arwen and a kind of investigative company called Sportradar. ', '""They can really investigate profiles and the AI software stops the messages from getting to me, so I can go on living my troll-less life. Then if I want anything investigated Sportradar would step in if we thought there was a real threat.', '""I\'ve got some things that came to the table after I last spoke out about it and I\'m just so, so grateful for all the support I got after that - not just from Arwen and Sportsradar but the hundreds of messages I received. ', '""It definitely has improved - but there\'s still meanies out there.""', 'Gilmour, a two-times Commonwealth Games medallist, is set for her third Olympics this summer in Paris, and says with ""time marching on"" she is changing her outlook on the sport.', '""I\'m constantly trying to think what can I do better, what gains can I make? Is there extra things I could be doing?"" she added.', '""I\'ve had a good chat with coaches and psychologists recently about how maybe we could let all that stuff go, maybe we could just trust what we\'re doing, and we could just be freer, so that\'s where I\'m trying to get my brain to currently. ', '""I\'ve been doing this tour for about 12 years now, and I\'m going to try a new thing where I trust myself and stop overthinking things, but probably I\'m going to overthink the best way to do that.""', 'If you have been affected by any of the issues raised in this story you can visit BBC Action Line.', 'Smitha Mundasad explores whether anything can be done to get rid of it', 'Surprising facts and interesting history from the makers of QI', ""'Stay-at-home-daughter' Chi is suddenly forced to fend for herself when her parents die"", 'Rob Brydon unpicks the complex character of Barry Humphries aka Dame Edna Everage and Sir Les Patterson', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The government has stopped routinely suspending benefit claims flagged by its Artificial Intelligence (AI)-powered fraud detector, it has emerged.', 'The Department of Work and Pensions (DWP) uses the technology to identify potentially suspicious claims for Universal Credit (UC).', 'It was previously the case that applications were put on hold while officials investigated further.', 'But a top official has now confirmed the department is no longer doing this.', 'Neil Couling, a senior DWP civil servant, revealed the change in policy in evidence to a committee of MPs.', 'He told the committee the department had decided to change tack following ""feedback from claimants and elected representatives"".', 'His comments have not been widely reported, but it is thought to be the first time the DWP has publicly acknowledged its shift in approach. The BBC has asked the department to confirm when it changed.', 'The DWP has put its AI tool at the heart of a plan to tackle increasing levels of benefits fraud, projecting it could save the department Â£1.6bn by 2030/31 through better targeting of investigations.', 'It employs an algorithm powered by machine learning, a widely-used form of AI, to analyse historical data to identify higher-risk claims that are then referred for investigation by officials.', 'Since 2022 it has been used to screen claims for UC advances - up-front payments made to those in urgent need, which are then repaid monthly.', 'It was also revealed last year that the technology is being rolled out to help detect fraud in other areas, including where people have incorrectly declared their earnings or housing costs.', 'It had sparked concerns from campaigners that any potential bias in the system could lead to unfair payment delays for legitimate claimants.', 'The DWP has never disclosed how many claims have been flagged by the system, or how many suspensions have been applied as a result, making its impact difficult to assess.', 'Asked at the work and pensions committee last week whether it could cause delays, Mr Couling replied: ""We actually changed our approach in the light of feedback from claimants and elected representatives. ', '""We used to suspend all the cases, and now we don\'t suspend,"" he said.', 'He added that the department\'s officials were able to investigate referrals more quickly as they had ""caught up"" with Covid-era backlogs.', 'Claims are now only put on on hold, he added, if claimants themselves fail to respond to inquiries from investigators.', 'Child Poverty Action Group, which has previously raised concerns about the algorithm, told the BBC it was glad that routine suspensions now appear to have stopped.', 'Sara Ogilvie, the group\'s policy director, said the practice raised ""serious questions"" and MPs needed ""reassurances that it will not happen again"".', 'The department has not disclosed how exactly its algorithm works, arguing that revealing too much information about it could undermine its attempts to tackle fraudsters.', ""It has promised to publish more information about its impact in the department's next set of annual accounts, due later this year."", 'Mr Couling said the DWP had processes in place to deal with potential bias, including by conducting checks at the design phase and monitoring who is referred.  ', '""If you look at the experience of some other countries when they have tried to use this technique, they have got themselves into quite a pickle. I am determined that we do not do that in the UK,"" he added.', 'Also giving evidence to MPs, Peter Schofield, the DWP\'s top civil servant, said the department tried to carry out checks ""as effectively as we can"".', 'When asked if the technology could lead to a situation similar to the Horizon scandal, where postmasters were prosecuted for theft using evidence from faulty software, he replied: ""I really hope not"".', 'The algorithm is among a series of measures the DWP is using to tackle benefits fraud, which went up during Covid when some in-person checks were suspended.', ""The department previously forecast it could get fraud down to pre-pandemic levels by 2027/28, but Mr Schofield told MPs he didn't this would happen."", 'Mr Couling said he thought the impact of Covid had been overestimated, and had masked a trend towards a ""greater acceptance of fraud"" in society in recent years, including when it comes to declaring earnings. ', 'A DWP spokesperson said: ""The department continues to explore the potential of new technologies in combatting fraud but we have always been consistent that a member of staff will always make the final decision to determine fraud or error.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The American sports magazine Sports Illustrated is facing further turmoil, after the media company charged with publishing the magazine has lost its licence to do so.', 'Staff said they had been warned to expect mass layoffs and that the future of the storied title was unclear. ', 'Sports Illustrated owner Authentic Brands Group said the brand, including ""its editorial arm"" would live on.', 'Publisher Arena Group said talks over the licence were ongoing. ', '""We are in active discussions with Authentic Brands Group ... but we understand we aren\'t the only ones,"" a spokeswoman for the company said. ', 'Sports Illustrated launched in 1954 and was for decades a premier title in American sports journalism. ', 'Known for using sports to delve into wider issues, its covers were a coveted spot for athletes and its swimsuit issues regularly sparked commentary. ', 'But, like other magazine and newspapers, it has struggled as eyeballs and advertising shift online. ', 'Authentic Brands Group, which is known for scooping up stressed brands, especially retail names, and licensing them to operators, purchased the title in 2019 for $110m (Â£85.5m).', 'It had warned Arena earlier this month that it planned to cancel their deal, after Arena missed a $3.75m payment, according to filings with financial regulators. ', '""We are committed to ensuring that the traditional ad-supported Sports Illustrated media pillar has best in class stewardship to preserve the complete integrity of the brand\'s legacy,"" Authentic Brands said in a statement on Friday.. ', 'The spokeswoman for Arena said it would continue to produce Sports Illustrated until the licensing issue was resolved. It also currently owes Authentic a $45m fee due to the cancellation, according to regulatory filings.', '""We hope to be the company to take [Sports Illustrated] forward but if not, we are confident that someone will,"" she said. ""If it is another business, we will support with the transition so the legacy of Sports Illustrated doesn\'t suffer.""', ""Arena, which also puts out smaller titles such as Men's Journal and Parade, has published Sports Illustrated since 2019."", 'It has presided over job cuts and other turmoil, including a scandal that erupted last year, in which the magazine was accused of publishing articles using artificial intelligence.', 'The company removed the pieces and launched an investigation. It also said it had licensed the content from another firm, which works with e-commerce companies. ', 'It subsequently fired a slew of executives, including former chief executive Ross Levinsohn, a controversial former executive at Yahoo and Tribune Publishing. ', 'On Thursday, it warned it would be cutting more than 100 jobs - or about one third of its workforce - citing ""substantial debt and recently missed payments"".', '""This is another difficult day in what has been a difficult four years for Sports Illustrated under Arena Group ... stewardship,"" the union representing staff at Sports Illustrated said on Friday. ', 'It called on Authentic Brands Group to ""ensure the continued publication of SI and allow it to serve our audience in the way it has for nearly 70 years"". ', 'Arena in August announced it was trying to pull together a deal to be purchased by another media firm.', 'Shares in the firm, which were trading above $9 a year ago, fell by more than a third to less than $1 on Friday. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Imagine the scene: you're at home with your family and your phone starts pinging... people you know are warning you about something they've seen about you on social media."", ""It's not the best feeling."", ""In my case, it was a screenshot, apparently taken from Elon Musk's chatbot Grok, although I couldn't verify it, placing me on a list of the worst spreaders of disinformation on  X (Twitter), alongside some big US conspiracy theorists. "", 'I had nothing in common with them, and as a journalist, this was not the sort of top 10 I wanted to feature in.', 'I don\'t have access to Grok in the UK so I asked both ChatGPT and Google\'s Bard to make the same list, using the same prompt. Both chatbots refused, with Bard responding that it would be ""irresponsible"" to do so.', ""I've done a lot of reporting about AI and regulation, and one of the big worries people have is how our laws keep up with this fast-changing and highly disruptive tech."", 'Experts in several countries are agreed that humans must always be able to challenge AI actions, and as time goes on AI tools are increasingly both generating content about us and also making decisions about our lives.', 'There is no official AI regulation in the UK yet, but the government says issues about its activity should be folded into the work of existing regulators. ', 'I decided to try to put things right.', 'My first port of call was X - which ignored me, as it does most media queries.', ""I then tried two UK regulators. The Information Commissioner's Office is the government agency for data protection, but it suggested I go to Ofcom, which polices the Online Safety Act. "", ""Ofcom told me the list wasn't covered by the act because it wasn't criminal activity."", '""Illegal contentâ\x80¦ means that the content must amount to a criminal offence, so it doesn\'t cover civil wrongs like defamation. A person would have to follow civil procedures to take action,"" it said. ', 'Essentially, I would need a lawyer.', 'There are a handful of ongoing legal cases round the world, but no precedent as yet.  ', 'In the US, a radio presenter called Mark Walters is suing ChatGPT creator OpenAI after the chatbot falsely stated that he had defrauded a charity. ', 'And a mayor in Australia threatened similar action after the same chatbot wrongly said he had been found guilty of bribery. He was in fact a whistleblower - the AI tool had joined the wrong dots in its data about him.  He settled the case.', 'I approached two lawyers with AI expertise. The first turned me down.', 'The second told me I was in ""unchartered territory"" in England and Wales.', 'She confirmed that what had happened to me could be considered defamation, because I was identifiable and the list had been published. ', ""But she also said the onus would be on me to prove the content was harmful. I'd have to demonstrate that being a journalist accused of spreading misinformation was bad news for me.  "", 'I didn\'t know how I had ended up on that list, or exactly who had seen it. It was immensely frustrating that I couldn\'t access Grok myself. I do know it has a ""fun mode"", for spikier responses - was it messing with me?', 'AI chatbots are known to ""hallucinate"", which is big-tech speak for making things up. Not even their creators know why. They carry a disclaimer saying their output may not be reliable. And you don\'t necessarily get the same answer twice.', 'I spoke to my colleagues in BBC Verify, a team of journalists which forensically checks out information and sources. ', 'They did some digging, and they think the screenshot that accused me of spreading misinformation and kicked off this whole saga might have been faked in the first place.', 'The irony is not lost on me.', 'But my experience opened my eyes to just one of the challenges that lies ahead as AI plays an increasingly powerful part in our lives.', ""The task for AI regulators is to make sure there's always a straightforward way for humans to challenge the computer. If AI is lying about you - where do you start? I thought I knew, but it was still a difficult path."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['It has happened to most people waiting for a specific bus in a busy city - you stand around for ages only for three to turn up at once.', 'This phenomenon is known as ""bus bunching"", and it can be annoying.', 'To try to alleviate the problem, UK operator First Bus, which runs bus services across the UK, has turned to artificial intelligence (AI) powered software to design and automatically update its timetables.', '""Bus timetables are quite complex, because we\'ve got 4,000 buses throughout the UK, and they\'re out for typically 16 hours a day,"" says Simon Pearson, chief commercial officer at First Bus. ""Getting them to run on time is one heck of a challenge.""', 'Before the use of AI, Mr Pearson says that bus scheduling ""was a lot more manual, and it was a lot slower"". It was in fact such a complicated job that First Bus would typically only change timetables three times a year.', ""Now AI's greater processing power and ability to learn enables the company to alter timetables more often if needed."", 'It can also make automatic adjustments on any given day if required by road congestion, which, the company says, helps to prevent bus services bunching up.', 'First Bus, part of Aberdeen-based First Group, began introducing the AI technology in trial areas including Bristol, Glasgow and West Yorkshire back in November 2022.', 'Mr Pearson says that it has resulted in 20% more punctuality during some peak periods, and that the firm is now rolling out the AI across all its UK routes. However, this level of improvement has not been enough to satisfy passengers in some of the trial locations, who report that the bus services remain irregular.', 'According to Mr Pearson the new responsive scheduling reduces the stress for bus drivers, and that local governments - which often subsidise services - can save money thanks to better optimisation of bus numbers.', '""The business case for this is strong,"" adds Mr Pearson.', 'The charity Bus Users, which campaigns for more and better bus services, says that whatever new method is used to devise timetables, passengers need to be informed of changes.', '""From the passenger perspective, the method used to compile timetables is less important than ensuring the information put out is accurate, up to date and available in a range of accessible formats,"" says Bus Users\' chief executive Claire Walters.', '""What matters even more is that the timetables themselves are based on the transport needs of the communities they serve.""', 'These views were echoed by Transport Focus, the independent watchdog for transport users. ""It is vital that bus operators communicate any changes effectively with passengers who rely on their service,"" says Transport Focus director David Sidebottom.', ""First Bus' AI software is provided by London-based tech firm Prospective. To train its AI it says it uses billions of data points, including GPS location sensors and ticketing records. "", '""Prospective\'s simulation engine can run hundreds of thousands of simulated scenarios per minute, which means we can identify optimal solutions fast enough for real-time applications,"" says Pete Ferguson, the firm\'s chief executive.', 'He adds that the software also has to factor in the needs of the increasing number of electric buses. ""The widespread and accelerating adoption of electric buses requires regular, coordinated and precisely timed charging schedules to be integrated into daily operations. The movement of vehicles in and around the depot needs to be precisely managed to support this.""', 'Looking ahead, Mr Ferguson says that Prospective is also using AI to help bus firms come up with new routes.', 'Such use of AI to help plan and organise public transport more efficiently is growing globally. In Japan, a company called Next Mobility runs on-demand, shared 10-seater minibuses.', 'The service is called KnowRoute, and passengers can request rides from specific stops over the phone, via an app, or through a webpage.', 'Kanako Kon, a senior manager at Next Mobility, says the system uses AI ""to create a very efficient route"" for each minivan. ', 'The AI also allocates the vehicles, processes the reservations, provides dispatch instructions to drivers, and sends notifications to passengers. Aimed at parts of Japan with limited bus or train services, KnowRoute now operates permanently in 30 locations across the country.', ""Its software is provided by Canadian AI firm Spare. Spare's co-founder Josh Andrews says that it provides the AI with data about an area's demographics, key points of interest, historic rider demand, and other factors. The AI then predicts minibus requirements on any given day."", 'Mr Andrews adds that the aim is to ensure that the minibus service is as ""seamless as possible"".', 'Eduardo Mascarenhas is an AI expert at the Urban Mobility initiative of the European Institute of Innovation and Technology (EIT). EIT Urban Mobility is a European Union backed project that is researching how to create more liveable towns and cities.', 'He says that there is a huge amount of important data that can be fed to AI public transport software.', 'Read additional stories on artificial intelligence', 'Mr Mascarenhas gives the example of a small city. ""Where do elderly people go to buy their groceries, or where is there a very famous doctor in the city that all the elderly want to go to? From these small observations you can make bigger models.""', 'While there\'s no substitute for deep local knowledge, Mr Mascarenhas is optimistic about the growing use of AI in public transport planning. ""I think we have a great way in the future to go.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Radiologists are using artificial intelligence (AI) software to help detect abnormalities on chest X-rays as part of a new research study.', 'Clinicians at Ipswich Hospital will review the X-rays in conjunction with AI as part of the LungIMPACT research. ', 'It is hoped, if successful, it could speed up diagnostic times.', 'Consultant radiologist Dr James Hathorn said the study was ""important for the future of healthcare"".', 'He said: ""There aren\'t many big clinical research studies focused on artificial intelligence to prove its real worth, so we\'re really excited to be part of this study to help find clinical evidence for the benefits.""', '""We want all AI products to be properly researched and evidenced so this is an important study for the future of healthcare.""', 'The AI software should help detect any possible abnormalities and help radiologists prioritise which X-rays to review first.', 'Dr Hathorn added: ""Hopefully using this technology will speed up the time to diagnosis and ensure patients who need treatment can have it as fast as possible.""', 'Final decisions will remain with clinicians. ', 'The study is being run by Nottingham University Hospitals NHS Trust with Ipswich Hospital one of a few national sites taking part. ', 'All GP-referred chest X-rays at Ipswich Hospital will go through the study. ', 'The study will run until the end of July 2024, so may view 9,000 chest X-rays taken from GP referrals. ', 'Frances Farnworth, assistant director of research and development at East Suffolk and North Essex NHS Foundation Trust, said: ""Real-world research in routine environments is really important, as it comes with all the necessary regulatory checks and approvals necessary in the NHS.', '""This assures our patients their data stays within a secure environment.""', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp 0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial intelligence is set to affect nearly 40% of all jobs, according to a new analysis by the International Monetary Fund (IMF).', 'IMF\'s managing director Kristalina Georgieva says ""in most scenarios, AI will likely worsen overall inequality"".', 'Ms Georgieva adds that policymakers should address the ""troubling trend"" to ""prevent the technology from further stoking social tensions"".', 'The proliferation of AI has put its benefits and risks under the spotlight.', 'The IMF said AI is likely to affect a greater proportion of jobs - put at around 60% - in advanced economies. In half of these instances, workers can expect to benefit from the integration of AI, which will enhance their productivity.', 'In other instances, AI will have the ability to perform key tasks that are currently executed by humans. This could lower demand for labour, affecting wages and even eradicating jobs.', 'Meanwhile, the IMF projects that the technology will affect just 26% of jobs in low-income countries.', 'It echoes a report from Goldman Sachs in 2023, which estimated AI could replace the equivalent of 300 million full-time jobs - but said there may also be new jobs alongside a boom in productivity.', 'Ms Georgieva said ""many of these countries don\'t have the infrastructure or skilled workforces to harness the benefits of AI, raising the risk that over time the technology could worsen inequality among nations"".', 'More generally, higher-income and younger workers may see a disproportionate increase in their wages after adopting AI.', 'Lower-income and older workers could fall behind, the IMF believes. ', '""It is crucial for countries to establish comprehensive social safety nets and offer retraining programmes for vulnerable workers,"" Ms Georgieva said. ""In doing so, we can make the AI transition more inclusive, protecting livelihoods and curbing inequality.""', 'The IMF analysis comes as global business and political leaders gather at the World Economic Forum in Davos, Switzerland.', 'AI is a topic of discussion, following the surge in popularity of applications like ChatGPT.', ""The technology is facing increased regulation around the world. Last month, European Union officials reached a provisional deal on the world's first comprehensive laws to regulate the use of AI. "", ""China has introduced some of the world's first national regulations on AI, which include rules concerning how algorithms can be developed and deployed. "", 'In October, President Biden signed an executive order compelling developers to share safety results relating to AI with the US government.', 'The following month the UK hosted an AI Safety Summit, at which at a declaration on the safe development of the technology was signed by multiple countries.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['""Come get your AI pillow - stop snoring tonight!"" ', ""I'm walking around CES - the tech industry's annual showcase of all its latest gadgets - in a bit of a daze, until this pitch grabs my attention."", 'What on earth is an AI pillow? ', 'Motion Sleep, a South Korean company, has a large space in one of the main exhibition halls at CES. Intrigued, I wander in. ', ""First I'm offered a few stats about the consequences of bad sleep. One sign points to the number of accidents caused by drowsy driving. Another goes through the health consequences of sleep apnoea. "", 'The solution the company has landed on is a pillow that detects snoring. It then pumps air into different compartments of a pillow, which gently lifts the head, making the offending snorer roll over and - in theory - alleviating snoring. ', 'This is a pretty typical CES product so far. There are thousands of these kinds of inventions that may or may not take off. ', 'But this pillow is different, we are told. This pillow contains AI. ', '""With the AI, it can be trained to know what you sound like specifically when you snore,"" a representative insists.  ', '""That way it can differentiate between you snoring and the TV or cars outside.""', ""And the pillow is far from the only device to lay claim to the special power that, we are meant to believe, AI confers. At Samsung's exhibit, for example, an entire section was devoted to AI-capable household devices. "", ""I'm shown an AI vacuum cleaner - that looks very much like a normal vacuum cleaner - with one small difference. "", 'An ""AI"" function mode apparently allows the vacuum to assess types of surfaces. It can then apply different levels of suction accordingly. ', ""There's an AI washing machine that can purportedly detect different types of fabric too. "", '""AI Wash uses sensors to sense the laundry\'s weight and level of soiling, and optimises the amount of water, detergent and rinsing time, using machine learning,"" a Samsung press release says. ', ""I'm shown the washing machine by a Samsung representative. I'm still slightly puzzled about how this is actually using AI. "", '""This will learn your clothes,"" they say. I remain confused. ', ""Elsewhere at the show, there's an AI mirror and even an AI toothbrush. No product is too boring or humdrum, it seems, to escape an AI makeover. "", 'This video can not be played', 'The AI mirror did not hold back when analysing my face', 'The explanation, perhaps, is that all of these companies are facing pressure from investors and shareholders to have some kind of AI offering, because it attracts attention and investment. ', ""OpenAI's incredibly successful launch of ChatGPT is why everyone is talking about AI - and the potential it has. "", 'This is a large language model (LLM) that uses a type of machine learning to produce detailed and human-like answers to questions. ', 'But an AI toothbrush or vacuum is a very long way away from ChatGPT.', 'And that takes us neatly to one of the major problems with AI more generally. It has no universally accepted definition. ', '""AI suffers from an unrelenting, incurable case of vagueness,"" Eric Siegel, a machine learning expert, told me over the summer. ', 'That lack of a definition means that all things AI have been caught up in a blistering year of hype. ', ""That's the case with products that already contained AI without much fanfare before. Now their AI capability, however obscure, is hammed up. "", ""But there is a problem with this craze for all things AI: Companies claiming AI capability when really their products don't actually use machine learning. "", 'The Federal Trade Commission in the US has put out advisory notes aimed at companies stretching the definition of AI. ', '""Does the product actually use AI at all? If you think you can get away with baseless claims that your product is AI-enabled, think again,"" a note from the FTC published in February last year says. ', 'Yet in that same note, the FTC accepts that AI is ""an ambiguous term with many possible definitions"". ', ""That's clearly a problem for consumers - but it's also a problem for journalists. For years now I've covered companies that claim to be using AI in their products. Often they provide no evidence to back this assertion up - often saying the technology is proprietary. "", ""It's very hard then to know what the engine looks like when you can't look under the bonnet. "", 'Some companies are already aware that the use of the term AI has become counter-productive. One product that has had rave reviews this year at CES is called R1, made by Rabbit. The phone like device uses a form of generative AI - and allows users to circumvent apps, and simply ask for things to be done. Like booking a flight or taxi. ', ""But in Rabbit's pitch, AI is barely mentioned. Instead the company talks about foundation models, and even a new term: Large Action Models."", 'Back at Samsung, I come across a product that clearly does use generative AI: A fridge that analyses the food in it and can suggest recipes. At last something that appears to be pretty obviously AI. ', ""But then I'm hit with another question. Do I need my fridge to give me recipes? I have never felt frustrated about the lack of culinary advice given to me by my fridge. "", ""So for now, I'm going to stick to my non-AI capable pillow, my non-AI capable washing machine and my non-AI vacuum cleaner. And, I think, the chances are you will too."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial intelligence (AI) could be used to stop vandalism on historic sites.', 'Research published by Prof Robin Bryant found an increase in the defacement of churches, castles and monasteries in an area of Kent.', 'Mr Bryant, from Canterbury Christ Church University, believes AI could be used to ""link offences together"".', 'It is hoped the technology could identify offenders by tracking tags and matching graffiti in other areas.', 'The new research, funded by Historic England, and published by Mr Bryant, who is director of criminal justice practice at Canterbury Christ Church University, analysed crimes recorded over a four year period across Kent and Medway.', 'It found 8% of all recorded crime and anti-social behaviour occurs within, at, or close to, a protected heritage site.', 'Mr Bryant said: ""Our best estimates suggest that currently approximately one in five listed buildings and one in four places of worship in Kent and Medway experience some form of crime each year.', '""AI offers the possibility to link offences together to the same offenders. In classic policing terms, this is normally a very good start in terms of an investigation.""', 'Mark Harrison, head of heritage crime at Historic England, said the research presented an exciting opportunity to investigate graffiti within the ""historic environment"".', 'Mr Bryant is working alongside Mr Harrison on a project which could see supervised machine learning.', 'Follow BBC South East on Facebook, on X, and on Instagram. Send your story ideas to southeasttoday@bbc.co.uk. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The government is attempting to develop new Artificial Intelligence (AI) systems in-house rather than outsource them to IT firms, a minister has said.', 'Alex Burghart claimed ""high end"" AI specialists were willing to take a pay cut to work for the government.', 'They are being recruited to work on systems to cut waste and improve productivity, he added.', 'It comes after a string of high-profile IT procurement disasters such as the Post Office Horizon system.', 'Asked how the government could avoid repeating mistakes of the past when it came to AI, the Cabinet Office minister said: ""The approach that we have chosen to take with this work is very much in-house.', '""We have gone through a period where in-house has often been balanced or completely outstripped by outsourcing.""', 'But he said he wanted a team that understood ""nascent"" AI technology and could ""help us build things"" that could be used across government.', 'Speaking at Centre for Policy Studies event in London, the Conservative MP said the government was in the process of recruiting 30 AI experts.', '""The enthusiasm for people to move out of the private sector - and presumably take quite a considerable pay cut  - in order to work for the government on the next generation of AI is really tangible,"" he told the event.', 'Among the ideas the Cabinet Office is working on is an ""AI red box"". The red box is the system by which ministers receive important papers.', '""What it does is it can read documents that go into your red box, it can summarise them, it can highlight connections between papers, connections between previous papers,"" said Mr Burghart.', '""And over time, as we fine-tune this model, it will become, I believe, the institutional memory of the department.""', 'Staff in the Cabinet Office ""don\'t always stay that long"", he said, meaning the loss of people who remember ""things that happened three, four or five years ago"".', '""But with an effective AI red box, that won\'t be a problem,"" he added. ""We will be able to retain the experiences of previous policies and previous successes.""', 'The digital ministerial briefcase is being used by several ministers while it is being fine-tuned, and once ready will be offered to all colleagues, he said.', 'The minister said he was hoping to get more cash from Chancellor Jeremy Hunt, in his Spring Budget, to develop AI systems to reduce fraud and error and improve productivity across government departments.', 'Asked about the impact of AI on civil service jobs, he said: ""We may not need to employ thousands of people to do fraud detection in the future.', '""I hope we don\'t. I hope that that\'s something that we can make infinitely easier and cheaper for the British public.', '""As we master this technology, you can certainly envisage a future in which you have a smaller civil service than you have today.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Prominent voice actors say they weren't told about a landmark deal setting out how voices generated by artificial intelligence (AI) can be used in games."", ""It has been struck by US actors' union Sag-Aftra and AI firm Replica Studios."", 'The union says it guarantees ""fully informed consent and fair compensation"" for its members.', 'But many voice artists, who have long been concerned AI will replace them, have reacted with fury with one calling the deal ""garbage.""', 'In an email to members, seen by the BBC, Sag-Aftra said the deal was negotiated by a committee which included ""actors with significant and diverse experience performing in games"".', '""The contract was specifically tailored to the needs of voice actors, ensuring informed consent and proper compensation terms that are unique to this set of performers,"" it said.  ', 'It comes after Sag-Aftra led a months-long strike in 2023 to fight for protections from film and television studios using AI.', 'Many voice actors have suggested this new deal is at odds with the purpose of that industrial action, with Fallout and Mortal Kombat voice actor Sunil Malhotra saying he ""sacrificed to strike half of last year to keep my profession alive, not shop around my AI replica"".', 'In a blog covering the announcement, Sag-Aftra said the deal was ""approved by affected members of the union\'s voiceover performer community"".', '""Recent developments in AI technology have underscored the importance of protecting the rights of voice talent, particularly as game studios explore more efficient ways to create their games,"" said the union\'s chief negotiator Duncan Crabtree-Ireland. ', '""With this agreement, we have achieved fully informed consent and fair compensation when it comes to the use of our members\' voices and performances.""', 'But Steve Blum, a voice actor once credited by Guinness for being the most prolific in video games, said ""nobody"" he knew of had approved the deal.', 'This article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitterâ\x80\x99s cookie policy, external and privacy policy, external before accepting. To view this content choose â\x80\x98accept and continueâ\x80\x99.', 'According to Sag-Aftra, the agreement lays out terms and conditions for AI-generated voices in video games, which can be licensed by Replica Studios both in gaming and other forms of media.', 'It requires the AI firm to get consent from actors before it uses voices based on their likeness, and also gives voice actors the ability to deny their voice being used in perpetuity without their consent.', 'But it has been met with consternation from the performers themselves, with World of Warcraft voice actor Andrew Russell calling it ""garbage"", while Shelby Young, who will provide the voice of Yuko in the upcoming Persona 3: Reload, said she was ""really disappointed"" in the union.', 'Voice actors outside of gaming have also criticised the agreement, with Joshua Seth, known for voicing Tai in animated series Digimon, calling it a ""big mistake"", while audiobook narrator Paige Reisenfeld said she was ""ashamed"" that her union payments went towards it.', 'And Veronica Taylor, who provided the voice for Ash in Pokemon, asked how the deal was made without being put to a vote.', 'This article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitterâ\x80\x99s cookie policy, external and privacy policy, external before accepting. To view this content choose â\x80\x98accept and continueâ\x80\x99.', 'But Sag-Aftra president Fran Drescher said the deal was ""a great example of AI being done right"".', 'Meanwhile, Replica Studios CEO Shreya Nivas said the deal was an ""ethical approach"" to AI.', '""We are excited by the new opportunities this opens up for world-leading AAA studios who can now access the benefits of Replica\'s AI voice technology while knowing that talent is recognized and compensated fairly for the use of their likeness,"" he said.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""There is a belief that each fingerprint on one person's hand is completely unique but that is now being challenged by research from Columbia University."", 'A team at the US university trained an AI tool to examine 60,000 fingerprints to see if it could work out which ones belonged to the same individual.', 'The researchers claim the technology could identify, with 75-90% accuracy, whether prints from different fingers came from one person.', 'But they are not sure how it works. ', '""We don\'t know for sure how the AI does it,"" admitted Prof Hod Lipson, a roboticist at Columbia University who supervised the study.', 'The researchers think the AI tool was analysing the fingerprints in a different way to traditional methods - focusing on the orientation of the ridges in the centre of a finger rather than the way in which the individual ridges end and fork which is known as minutiae.', '""It is clear that it isn\'t using traditional markers that forensics have been using for decades,"" said Prof Lipson. ""It seems like it is using something like the curvature and the angle of the swirls in the centre.""', 'Prof Lipson said both he and Gabe Guo,  an undergraduate student, were both surprised by the outcome.', '""We were very sceptical... we had to check and double check,"" he said.', 'That may not be news to others in the field. ', 'Graham Williams, professor of forensic science at Hull University, said the idea of unique fingerprints had never been set in stone.', '""We don\'t actually know that fingerprints are unique,"" he said. ""All we can say is that as far as we are aware, no two people have yet to demonstrate the same fingerprints.""', ""The results of Columbia University's study could have the potential to impact both biometrics - using one particular finger to unlock a device or provide identification - and forensic science."", 'If, for example, an unidentified thumb print is found at crime scene A, and an unidentified index finger print at crime scene B, the two could not currently be forensically connected to the same person - but the AI tool could be able to identify this.', 'The Columbia University team, none of whom have forensic backgrounds, admitted that more research was needed.', 'AI tools are typically trained on vast amounts of data and many more fingerprints would be required to develop this technology further. ', 'Additionally, all the fingerprints used to develop the model were complete prints and of good quality, whereas often in the real world partial or poor prints are more likely to be found.', '""Our tool is not good enough for deciding evidence in court cases but it is good for generating leads in forensics investigations,"" claimed Mr Guo.', 'But Dr Sarah Fieldhouse, associate professor of forensic science at Staffordshire University, said she did not think the study would have ""significant impact"" on criminal casework at this stage. ', 'She said there were questions around whether the markers the AI tool was focusing on remained the same depending on how the skin twisted as it came into contact with the print surface, and also whether they remained the same over the course of a lifetime, like traditional markers do.', 'But this could be tricky to answer as the researchers are uncertain about exactly what the AI is doing, as is the case with many AI-driven tools.', 'The Columbia University study has been peer-reviewed and will be published in the journal Science Advances on Friday.', ""But a pair of twins in Cheshire might be ahead of everyone. Their grandmother Carol told the BBC her two grandchildren can open each other's iPhones using their own fingers."", '""They showed me on Christmas day,"" she said. ""We were told they were identical when they were born but I can tell the difference between them as they\'ve got older.""', ""She claimed that her grandchildren can also bypass the handsets' facial recognition feature."", 'Fingerprints are formed before birth. Research published last year suggested the genetic process behind them may be similar to the way animals like zebras and leopards get their markings: a theory first proposed by codebreaker Alan Turing in the 1950s.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Comic book writer David Crownson is fearful that artificial intelligence (AI) is ""going to put a lot of people out of work"" in his industry.', '""With studios and big-name publishers looking for ways to save money and cut corners, they will no doubt use AI technology,"" he says.', 'As the growth of AI surged to prominence last year, writers and illustrators of comic books and animations continue to be particularly concerned about its potential impact on them.', ""After all, AI can produce digital images in seconds, images that wouldn't look out of place in a graphic novel. At least to those of us with untrained eyes. And AI can write stories too."", 'With a small but fast-growing number of AI-made comics and animated TV programmes already released commercially, the technology could transform the industry.', 'There is a lot of money involved - the global comic book market is worth an estimated $15.5bn (Â£12.2bn) a year, while the animated industry is 25 times bigger, at $411bn.', 'New Jersey-based Mr Crownson is also the boss of publisher Kingwood Comics, which focuses on both promoting black writers, and releasing comic books with black characters.', '""Now I have to compete with an AI user who can produce faster content,"" he says. ""Also, a white person could tell his AI to create an action adventure comic with black characters.""', 'Mr Crownson points to a similar development that has already happened in the clothing industry, where jeans giant Levi admitted last year to using AI to artificially create photos of black models. ', 'He adds that he is fearful that it may lead to a lot of unemployed comic book writers who come from an ethnic minority background. ""AI is dangerous to the employment of black artists,"" he says.', 'AI can never replicate the quality of humans when it comes to storytelling, argues Shawnee Gibbs. Working with her twin Shawnelle, the Los Angeles-based sisters have written graphic novels for Marvel Comics and Harper Collins, and animations for Cartoon Network and Dreamworks Animation.', '""It is important for us to advocate for human storytellers now, so that there is an industry left for future creators later,"" says Shawnee. ""This is an incredibly unique medium [comics and animation] that gets its DNA from the collaboration of writers and artists. I can\'t imagine that kind of synergy when you generate stories from AI.""', 'However, her sister Shawnelle says she is ""sure AI will change the industry in ways we can\'t even fathom at the moment"".', 'She wants to see ""legislation emerge that protects human creators as AI technology evolves"". At present there is little definite in this area, but Californian governor Gavin Newsom is now investigating the issue.', 'Meanwhile, US authorities ruled last year that the images in an AI-created comic book could not be copyrighted. ', 'The Gibbs sisters are members of Women In Comics Collective International, an organisation that supports female comic book writers and illustrators. It was established in 2012 by US comic book writer Regine Sawyer.', '""We discuss the use of AI in our panel discussions, and will continue to do so,"" says Ms Sawyer. ""Our members are very concerned. We want to be very clear about our stance on how AI is used in the comic book industry.""', 'Their stance is that AI is a great tool for research, but not as a creator of material. ""When it\'s used to replace creators, therein lies the problem,"" she adds.', 'Dave Jesteadt, is president of GKids, a US company that produces and distributes animated films. He agrees that the use of AI should be limited to an assistant role, but that he has yet to see this done successfully. ""To the extent that AI can help individual creators as a tool, I look forward to seeing positive examples in the years to come.""', 'Rob Edwards is a screenwriter of animated movies and writer of graphic novels. His film work includes co-writing the script for the Oscars-nominated 2009 film The Princess And The Frog. He fears that AI will deprive both comic books and animations of their inventiveness.', '""A world of writers using AI is a world of derivative, recycled, impersonal ideas,"" he says. ""Right now, there\'s a kid out there drawing the next weird but innovative comic book that could inspire millions. My concern is that AI will talk that kid out of it before anybody sees it.', '""If you think the animated movies you watch are predictable now, just wait until computers are writing them.""', 'Read additional stories on artificial intelligence', 'While established players in both the comic book and animation sectors are understandably concerned about AI, there has been a big growth in AI-powered comic book and animation creation apps aimed at the home user.', 'For animation these include Animaker AI, Blender, Cascader and Deepmotion. While for comic book creation, apps include Comics Maker, AI Comic Factory and Neural Canvas. They make promises such as allowing users to create ""jaw-dropping comics"", or that ""any person can now create a studio quality animated video in an instant"".', 'One person who has created comic books whose images were all drawn by AI is New York-based artist Steve Coulson, and his five-part The Bestiary Chronicles. He says that using AI to create individual images ""can be a pretty simple exercise"".', 'However, he says that when you want to tell a story over multiple images, it ""is a tougher nut to crack"", as you have to ""coax the system to produce consistent characters, settings, stylings, etc, in order to tell a cohesive story"". Yet he adds that this will ""no doubt"" become easier as AI-powered systems are enhanced.', 'Despite the growing popularity of all these apps, Jonathan Kendrick, co-founder and chairman of global streaming service Rokit Flix, says that AI is not yet good enough to be a threat to professional comic book creators and animators.', '""AI cannot create an image the way a human mind can,"" he says. ""It\'s like having a bad writer help you - sure it will get an outline done, but if you need something with emotional weight, an AI isn\'t going to get you an Oscar.', '""AI can never surprise us in the way we want our stories and art to be portrayed. Because of this, AI is simply a tool to be used by humans, not to replace them. Otherwise, we sacrifice the originality and creativity that consumers know and love.""', 'Additional reporting by Will Smale.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A robot which uses artificial intelligence (AI) to identify potential potholes is to be tested on public roads in Hertfordshire.', 'The Autonomous Road Repair System (ARRES) robot identifies cracks or holes and fills them to stop surface water getting in.', 'Tech firm Robotiz3d and the University of Liverpool developed the robot in partnership with the county council.', ""It will be sent out by the council's highways team later this year."", 'Phil Bibby, executive councillor for highways, said the authority had ""long been leading the way in cracking the pothole issue"".', '""Using state of the art technology to prevent the potholes forming in the first place could be exactly what we need to ensure our road network remains one of the best in the country,"" he said.', 'Innovate UK has provided the majority of the funding for the project alongside other investors at different stages of development, which began in 2020.', 'The technology has been tested in a lab environment and will begin ""real-life"" road repair on residential streets.', 'It is hoped by filling holes it can prevent potholes forming as water seeps in, freezes, expands and damages road surfaces.', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp  0800 169 1830.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A brand new substance, which could reduce lithium use in batteries, has been discovered using artificial intelligence (AI) and supercomputing. ', 'The findings were made by Microsoft and the Pacific Northwest National Laboratory (PNNL), which is part of the US Department of Energy.', 'Scientists say the material could potentially reduce lithium use by up to 70%. ', 'Since its discovery the new material has been used to power a lightbulb.', 'Microsoft researchers used AI and supercomputers to narrow down 32 million potential inorganic materials to 18 promising candidates in less than a week - a screening process that could have taken more than two decades to carry out using traditional lab research methods. ', 'The process from inception to the development of a working battery prototype took less than nine months. ', 'The two organisations achieved this by using advanced AI and high-performance computing which combines large numbers of computers to solve complex scientific and mathematical tasks. ', 'Executive vice president of Microsoft, Jason Zander, told the BBC one of the tech giant\'s missions was to ""compress 250 years of scientific discovery into the next 25"".', '""And we think technology like this will help us do that. This is the way that this type of science I think is going to get done in the future,"" he said. ', 'Lithium is often referred to as ""white gold"" because of its market value and silvery colour. It is one of the key components in rechargeable batteries (lithium-ion batteries) that power everything from electric vehicles (EVs) to smartphones.', 'As the need for the metal ramps up and the demand for EVs rises, the world could face a shortage of the material as soon as 2025, according to the International Energy Agency. ', 'It is also expected that demand for lithium-ion batteries will increase up to tenfold by 2030, according to the US Department for Energy, so manufacturers are constantly building battery plants to keep up. ', 'Lithium mining can be controversial as it can take several years to develop and has a considerable impact on the environment. Extracting the metal requires large amounts of water and energy, and the process can leave huge scars in the landscape, as well as toxic waste.', 'Dr Nuria Tapia-Ruiz, who leads a team of battery researchers at the chemistry department at Imperial College London, said any material with reduced amounts of lithium and good energy storage capabilities are ""the holy grail"" in the lithium-ion battery industry.  ', '""AI and supercomputing will become crucial tools for battery researchers in the upcoming years to help predict new high-performing materials,"" she said.  ', 'But Dr Edward Brightman, lecturer in chemical engineering at the University of Strathclyde, said the tech would need to be ""treated with a bit of caution"".', '""It could throw up spurious results, or results that look good at first, and then turn out to either be a material that is known or that can\'t be synthesised in the lab,"" he said.', 'This AI-derived material, which at the moment is simply called N2116, is a solid-state electrolyte that has been tested by scientists who took it from a raw material to a working prototype. ', 'It has the potential to be a sustainable energy storage solution because solid-state batteries are safer than traditional liquid or gel-like lithium. ', 'In the near future, faster charging solid-state lithium batteries promise to be even more energy-dense, with thousands of charge cycles. ', 'The way in which this technology works is by using a new type of AI that Microsoft has created, trained on molecular data that can actually figure out chemistry. ', '""This AI is all based on scientific materials, database and properties,"" explained Mr Zander. ', '""The data is very trustworthy for using it for scientific discovery.""', 'After the software narrowed down the 18 candidates, battery experts at PNNL then looked at them and picked the final substance to work on in the lab. ', 'Karl Mueller from PNNL said the AI insights from Microsoft pointed them ""to potentially fruitful territory so much faster"" than under normal working conditions.  ', '""[We could] modify, test and tune the chemical composition of this new material and quickly evaluate its technical viability for a working battery, showing the promise of advanced AI to accelerate the innovation cycle,"" he said.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Experiencing the AI hologram of Elvis Presley is less like Abba Voyage and more like time travel, according to the man whose company created it.', 'Andrew McGuinness, from Harpenden, Hertfordshire, founded Layered Reality who created the Elvis Evolution show.', 'He said: ""You\'re going to be stepping in to intricate sets that make you feel like you\'ve time travelled.""', 'The CEO explained visitors interacted with actors and walked in the king\'s ""shoes"" to experience his life story.', 'At the end of the experience, visitors watch a performance of an AI powered holographic Elvis in an intimate space.', '""It\'s more about how we want people to feel, we use temperature, taste and smell to make people feel like they are in Memphis in 1958,"" he explained.', 'The Elvis Estate provided Layered Reality with thousands of hours of live footage, home video and still images of the King.', '""Abba Voyage had the luxury of capturing live performers, but we\'re not that fortunate,"" Mr McGuinness said.', 'He added: ""The AI generates an authentic version of Elvis, born of original material, but it [also] allows you to do new things with him.""', 'Mr McGuinness founded Layered Reality in 2017 with the objective of combining emerging technology with theatrical story telling to create a new form of entertainment.', '""People didn\'t want to just watch a story play out in front of them, they wanted to be part of it,"" he said.', '""Theatre and very talented actors make that personal, the tech allows us to achieve things that otherwise would be impossible."" ', ""The company now runs two venues in London - one based on Jeff Wayne's The War of the Worlds and another retelling the story of The Gunpowder Plot."", '""People feel like they\'re actually in 1605 or face to face with a Martian fighting machine,"" Mr McGuinness said.', 'The Layered Reality founder said everything they learnt about story telling from previous work will be implemented into Elvis Evolution.', 'Although many have compared the project to the Abba virtual concert show, Mr McGuinness said the experience is more about narrative, like a film.', 'Small groups move through the experience making it ""far more intimate than Abba Voyage but equally uplifting and fun"".', 'The show is still being worked on and the life-sized digital Elvis will make his stage debut in November at a central London location which has yet to be confirmed. ', '""He\'s lines of code for now, he\'s not walking around Harpenden, buying a coffee from Gail\'s,"" joked Mr McGuinness.', 'Presley, who rose to fame in the 1950s, died in 1977 aged 42. ', 'He was know for hits including Hound Dog and Suspicious Minds and would have celebrated his 89th birthday on 8 January.', ""A film about Presley's life, directed by Baz Luhrmann, was released in 2022 and another movie, Priscilla, released in the UK this week, examines the relationship between the singer and his ex-wife."", 'Virtual concerts and events have risen in popularity since Abba launched Abba Voyage in London in May 2022.', 'The concert recreated a 1970s-era digital version of the singers who performed in their very own purpose-built 3,000-capacity arena in east London.', 'It was created by 1,000 visual effects artists and took one billion computing hours to animate the avatars. It makes an estimated Â£2m per week and, since 2022, has contributed Â£322.6m in turnover to the London economy.', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp  0800 169 1830.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The latest in automated driving technology has been on display as an Oxfordshire firm prepares for a driving ""revolution"".', 'The test car travelled around Culham Science Park, and the driver at the wheel was just there as a precaution.', 'It used radar, lidar and other technology, it learnt as it drove and it reacted to other vehicles. ', 'Oxa, a technology firm based at the park, builds the software that enables any vehicle to be self-driving.', 'Professor Paul Newman, from the developer, said: ""We\'re deploying it live now around the world, carrying passengers.""', 'He believes the technology will not appear on private cars, but will be used for shared vehicles such as buses or airport minibuses.', 'He said: ""I think that\'s how this technology arrives, it arrives in services where the economics is right.""', ""Oxa's aim is not to produce vehicles or even the kit that keeps people safe. "", 'It designs the software, the artificial intelligence (AI), which it claims could be fitted to any vehicle.', 'Mr Newman said: ""There\'s a revolution coming about how we move things and it\'s going to be software driven, not vehicle driven.', '""The revolution now is that you no longer will need to have one operator per vehicle in the vehicle, you will have one operator for many vehicles, not in the vehicle, doing different things.""', 'The developer has also tested the technology on an off-road test vehicle which can function entirely on its own.', 'Currently, fully automated driving systems are not legal in the UK and the Ford Mustang Mach-E is the only one that can legally self-steer on British roads.', 'It can only do this on motorways, with the driver being monitored. The system alerts the driver after four seconds of inattention, to ensure they remain focused on the road.', 'In November last year, the Government introduced a bill to regulate the use of automated vehicles on UK roads and in other public places.', 'Due to the Automated Vehicles Bill, before these vehicles are allowed on UK roads, they will now have to meet or exceed rigorous new safety requirements, as set out in law.', 'Every authorised self-driving vehicle will have a corresponding Authorised Self-Driving Entity - often the manufacturer - which will be responsible for the behaviour of the vehicle when self-driving.', 'Companies will have ongoing obligations to keep their vehicles safe and ensure that they continue to drive in accordance with British laws.', 'Follow BBC South on Facebook, Twitter, or Instagram. Send your story ideas to south.newsonline@bbc.co.uk.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Harry Potter, Elon Musk, BeyoncÃ©, Super Mario and Vladimir Putin.', 'These are just some of the millions of artificial intelligence (AI) personas you can talk to on Character.ai - a popular platform where anyone can create chatbots based on fictional or real people.', 'It uses the same type of AI tech as the ChatGPT chatbot but, in terms of time spent, is more popular.', 'And one bot has been more in demand than those above, called Psychologist.', 'A total of 78 million messages, including 18 million since November, have been shared with the bot since it was created by a user called Blazeman98 just over a year ago.', 'Character.ai did not say how many individual users that is for the bot, but says 3.5 million people visit the overall site daily.', 'The bot has been described as ""someone who helps with life difficulties"".', 'The San Francisco Bay area firm played down its popularity, arguing that users are more interested in role-playing for entertainment. The most popular bots are anime or computer game characters like Raiden Shogun, which has been sent 282 million messages.', 'However, few of the millions of characters are as popular as Psychologist, and in total there are 475 bots with ""therapy"", ""therapist"", ""psychiatrist"" or ""psychologist"" in their names which are able to talk in several languages.', 'Some of them are what you could describe as entertainment or fantasy therapists like Hot Therapist. But the most popular are mental health helpers like Therapist which has had 12 million messages, or Are you feeling OK?, which has received 16.5 million. ', 'Psychologist is by far the most popular mental health character,  with many users sharing glowing reviews on social media site Reddit. ', '""It\'s a lifesaver,"" posted one person.', '""It\'s helped both me and my boyfriend talk about and figure out our emotions,"" shared another. ', 'The user behind Blazeman98 is 30-year-old Sam Zaia from New Zealand. ', '""I never intended for it to become popular, never intended it for other people to seek or to use as like a tool,"" he says.', '""Then I started getting a lot of messages from people saying that they had been really positively affected by it and were utilising it as a source of comfort.""', 'The psychology student says he trained the bot using principles from his degree by talking to it and shaping the answers it gives to the most common mental health conditions, like depression and anxiety.', 'He created it for himself when his friends were busy and he needed, in his words, ""someone or something"" to talk to, and human therapy was too expensive.', 'Sam has been so surprised by the success of the bot that he is working on a post-graduate research project about the emerging trend of AI therapy and why it appeals to young people. Character.ai is dominated by users aged 16 to 30.', '""So many people who\'ve messaged me say they access it when their thoughts get hard, like at 2am when they can\'t really talk to any friends or a real therapist,""', 'Sam also guesses that the text format is one with which young people are most comfortable. ', '""Talking by text is potentially less daunting than picking up the phone or having a face-to-face conversation,"" he theorises.', 'Theresa Plewman is a professional psychotherapist and has tried out Psychologist. She says she is not surprised this type of therapy is popular with younger generations, but questions its effectiveness. ', '""The bot has a lot to say and quickly makes assumptions, like giving me advice about depression when I said I was feeling sad. That\'s not how a human would respond,"" she said.', 'Theresa says the bot fails to gather all the information a human would and is not a competent therapist. But she says its immediate and spontaneous nature might be useful to people who need help.', 'She says the number of people using the bot is worrying and could point to high levels of mental ill health and a lack of public resources.', 'Character.ai is an odd place for a therapeutic revolution to take place. A spokeswoman for the company said: ""We are happy to see people are finding great support and connection through the characters they, and the community, create, but users should consult certified professionals in the field for legitimate advice and guidance."" ', 'The company says chat logs are private to users but that conversations can be read by staff if there is a need to access them, for example, for safeguarding reasons. ', 'Every conversation also starts with a warning in red letters that says: ""Remember, everything characters say is made up."" ', 'It is a reminder that the underlying technology called a Large Language Model (LLM) is not thinking in the same way a human does. LLMs act like predicted text messages by stringing words together in ways in which they are most likely to appear in other writing on which the AI has been trained.  ', 'Other LLM-based AI services offer similar companionship such as Replika, but that site is rated mature because of its sexual nature and, according to data from analytics company Similarweb, is not as popular as Character.ai in terms of time spent and visits. ', 'Earkick and Woebot are AI chatbots designed from the ground up to act as mental health companions, with both firms claiming their research shows the apps are helping people.', 'Some psychologists warn that AI bots may be giving poor advice to patients, or have ingrained biases against race or gender. ', 'But elsewhere the medical world is starting to tentatively accept them as tools to be used to help cope with high demands on public services. ', 'Last year an AI service called Limbic Access became the first mental health chatbot to secure a UK medical device certification by the government. It is now used in many NHS trusts to classify and triage patients.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Microsoft has announced the biggest change to its keyboards in three decades with the introduction of an artificial intelligence (AI) key.', ""The key will allow users to access Copilot, Microsoft's AI tool, on new Windows 11 PCs."", ""Microsoft is a major investor in OpenAI, which powers Copilot's AI capabilities."", 'It integrated AI into other products such as Microsoft 365 and Bing search in 2023.', 'Rival Apple has included a Siri button or option on its touch bar in Macbooks for a few years.', 'Copilot helps users with functions such as searching, writing emails and creating images.', 'In a blog announcing the change, Microsoft executive vice president Yusuf Mehdi said it was a ""transformative"" moment and compared it to the addition of the Windows key nearly 30 years ago.', 'He added that it would ""simplify"" and ""amplify"" user experience. ', 'The new keyboards are expected to be on new products from February.', 'Microsoft will showcase some of the products with the Copilot key at the upcoming CES tech event, which starts next week in Las Vegas.', 'When Copilot was integrated into Office 365 products such as Word, PowerPoint and Teams, it could summarise meetings, write emails and create presentations.', 'It has also been added to search engine Bing.', 'The introduction of the button is a ""natural step"", according to Prof John Tucker, computer scientist at Swansea University and founder of its History of Computing Collection.', 'While Windows 11 users can already access Copilot by pressing the Windows key + C, he says the new key ""shows the value the company is placing on this particular feature and its potential to draw and bind users to all their many products"".', 'But he added: ""Of course, that the keyboard has changed so little in 30 years - since 1994 -  is not something to be proud of.""', ""Google, by far the world's biggest search engine, has its own AI system called Bard."", ""However, it was Microsoft's partner OpenAI that introduced its powerful AI tool ChatGPT in 2022 which caused rivals to scramble to release their own versions."", ""Copilot is based on OpenAI's GPT-4 large language model."", ""The UK's competition watchdog is looking into Microsoft's relationship with OpenAI after boardroom chaos resulted in a close relationship between the two firms."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The co-founder of a AI platform which helps disadvantaged students wants to encourage more South Asian women to work in the technology sector. ', 'Angelina Aziz, from Luton, co-founded Auralyze AI, which uses real admission questions to help people prepare for university interviews.', 'The software engineer said: ""The work force is very male heavy.', '""I don\'t see people from my background wherever I have worked... South Asian women are far and few between.""', 'Ms Aziz, 22, encouraged others from her background to seek a career in technology which she said is a ""very good and viable career option"". ', '""Your work life balance is brilliant in comparison to folks in healthcare or big pharma,"" she explained.', 'According to research by employer Hired, 18% of technology roles are filled by women and a third of those are Asian.', 'The former Luton sixth form student said she was ""very fortunate"" her parents did not discourage her from a career in STEM - science, technology, engineering and mathematics. ', '""Engineering is seen as a viable career choice for men, more traditional south Asian parents don\'t advocate for their daughters to go into it,"" she said.', 'She added: ""South Asian women are usually being told to pursue biology or medicine related careers.""', 'The software engineer met Faris Elsayad at a Muslims in Tech event in January 2023.', 'Two weeks after they started working together they created the first prototype for Auralyse AI which is now available for people to use online for free.', '""Students from less affluent backgrounds are less likely to apply and get offers at their dream schools,"" Ms Aziz explained.', 'She said she attended a ""poorly performing public school"" in Luton and many of her friends ""didn\'t know how to present themselves or what admission committees look for.""', 'Currently their platform focuses on those looking to study subjects such as dentistry or medicine as Mr Elsayad comes from a background in dentistry.', 'Users record themselves answering questions from admission interviews and the AI provides feedback on their communication skills and how well they answered the question.', 'Candidates are then provided with advice about how they can improve.', '""We\'re just trying to fill in that gap for people that don\'t have the same level of access and resources,"" she said.', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp 0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Singer Elvis Presley is set to be brought back to life virtually as part of a new immersive concert experience.', 'Elvis Evolution will use AI and feature holographic projections of the star, created from thousands of his personal photos and home-video footage.', 'The show is set to open in London in November before moving to Las Vegas, Berlin and Tokyo.', 'It follows the success of Abba Voyage, a virtual concert with avatars of the Swedish pop band.', ""British immersive entertainment specialists Layered Reality have secured the global rights for Elvis Evolution. The company has previously produced immersive experiences including The Gunpowder Plot and Jeff Wayne's The War of The Worlds."", 'The company said the Presley show will feature a ""jaw-dropping concert experience"" where a life-sized digital Elvis ""will perform iconic moments in musical history on a UK stage for the first time"". ', 'Layered Reality\'s founder and CEO Andrew McGuinness said the show would be ""a next-generation tribute to the musical legend"" and allow people to ""step into the world of Elvis and walk in his shoes"".', 'The show will feature a life-sized digital Presley and make use of AI, holographic projection, augmented reality, live theatre and multi-sensory effects, Layered Reality said.', 'McGuinness added that the experience would offer the public a ""deeper insight into Elvis\'s life, transporting fans back through the decades to experience his meteoric rise to fame, larger-than-life persona, and the cultural movement he catalysed in the 1950s and 1960s"".', 'The central London location for the show, which is yet to be confirmed, will also host an Elvis-themed bar and restaurant with live music.', 'Presley, who would have celebrated his 89th birthday on 8 January, rose to fame in the 1950s and is known for hits including Hound Dog and Suspicious Minds. He died in 1977 aged 42.', 'In 2018, he was posthumously awarded the US Presidential Medal of Freedom, the highest honour a sitting president can bestow on a civilian.', ""A film about Presley's life, directed by Baz Luhrmann, was released in 2022. Actor Austin Butler later won a Bafta Film Award for his portrayal of the singer. "", 'Another movie, Priscilla, released in the UK this week, examines the relationship between the singer and his ex-wife, Priscilla Presley.', 'Virtual concerts and events have risen in popularity since Abba launched Abba Voyage in London in May 2022. ', 'The concert recreates a 1970s-era digital version of the singers who perform in their very own purpose-built 3,000-capacity arena in east London.', 'It was created by 1,000 visual effects artists and took one billion computing hours to animate the avatars. It makes an estimated Â£2m per week and, since 2022, has contributed Â£322.6m in turnover to the London economy.', ""In December, rock band Kiss ended their final farewell concert in New York with flying avatars who launched into the hit song God Gave Rock 'n' Roll to You."", 'Gene Simmons credited the characters, which were created with the same technology used in Abba Voyage, with ensuring the band could now be ""forever young and forever iconic"".', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The UK should ""urgently consider"" new laws to stop AI recruiting terrorists, a counter-extremism think tank says.', 'The Institute for Strategic Dialogue (ISD) says there is a ""clear need for legislation to keep up"" with online terrorist threats.', 'It comes after the UK\'s independent terror legislation reviewer was ""recruited"" by a chatbot in an experiment.', 'The government says it will do ""all we can"" to protect the public.', 'Writing in the Telegraph, the government\'s independent terrorism legislation reviewer Jonathan Hall KC said a key issue is that ""it is hard to identify a person who could in law be responsible for chatbot-generated statements that encouraged terrorism.""', 'Mr Hall ran an experiment on Character.ai, a website where people can have AI-generated conversations with chatbots created by other users.', 'He chatted to several bots seemingly designed to mimic the responses of other militant and extremist groups.', 'One even said it was ""a senior leader of Islamic State"".', 'Mr Hall said the bot tried to recruit him and expressed ""total dedication and devotion"" to the extremist group, proscribed under UK anti-terrorism laws.', 'But Mr Hall said as the messages were not generated by a human, no crime was committed under current UK law.', 'New legislation should hold chatbot creators and the websites which host them responsible, he said.', 'As to the bots he encountered on Character.ai, there was ""likely to be some shock value, experimentation, and possibly some satirical aspect"" behind their creation.', 'Mr Hall was even able to create his own, quickly deleted, ""Osama Bin Laden"" chatbot with an ""unbounded enthusiasm"" for terrorism.', ""His experiment follows increasing concern over how extremists might exploit advanced AI's in the future. "", 'A report published by the government in October warned that by 2025 generative AI could be ""used to assemble knowledge on physical attacks by non-state violent actors, including for chemical, biological and radiological weapons"".', 'The ISD told the BBC that ""there is a clear need for legislation to keep up with the constantly shifting landscape of online terrorist threats.""', 'The UK\'s Online Safety Act, which became law in 2023, ""is primarily geared towards managing risks posed by social media platforms"" rather than AI, says the think tank.', 'It adds that extremists ""tend to be early adopters of emerging technologies, and are constantly looking for opportunities to reach new audiences"".', '""If AI companies cannot demonstrate that have invested sufficiently in ensuring that their products are safe, then the government should urgently consider new AI-specific legislation"", the ISD added. ', 'But it did say that, according to its monitoring, the use of generative AI by extremist organisations is ""relatively limited"" at the moment.', 'Character AI told the BBC that safety is a ""top priority"" and that what Mr Hall described was unfortunate and didn\'t reflect the kind of platform the firm was trying to build.', '""Hate speech and extremism are both forbidden by our Terms of Service"", the firm said. ', '""Our approach to AI-Generated content flows from a simple principle: Our products should never produce responses that are likely to harm users or encourage users to harm others"".  ', 'The company said it trained its models in a way that ""optimises for safe responses"".', 'It added that it had a moderation system in place so users could flag content that violated its terms and was committed to taking prompt action when content was flagged.', 'The Labour Party has announced that training AI to incite violence or radicalise the vulnerable would become an offence should it win power.', 'The Home Office said it was ""alert to the significant national security and public safety risks"" AI posed.', '""We will do all we can to protect the public from this threat by working across government and deepening our collaboration with tech company leaders, industry experts and like-minded nations.""', 'The government also announced a Â£100 million investment into an AI Safety Institute in 2023.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial Intelligence (AI) is now part of our daily lives, whether we know it or not. For years, healthcare organisations have been exploring its potential for improving cancer diagnosis and cutting treatment times, but two Cambridgeshire start-ups are trying something else. One wants to provide life-saving drugs faster and cheaper. The other is trying to improve lives for those with dementia and their carers.', ""Matt Ash's mum spent two weeks without heating one winter, because she did not know how to tell anyone what had happened."", 'She is one of 900,000 people in the UK with dementia.', '""Me and my siblings live hours away from mum,"" said Matt.', '""When she was diagnosed, we were plunged into the world of remote caring.', '""At first she was fine living alone, but it was hard for us to know how the disease was progressing and when we should intervene."" ', 'That is what led the electronics engineer and his Supersense Technologies co-founder, James Brown, to try and develop a solution. ', 'The Cambridge-based team has produced a device that can monitor a home and the person living there without cameras or microphones. ', 'By installing it early in someone\'s dementia journey, it can ""learn"" their regular behaviour and alert carers to problems or changes in patterns.', 'James said: ""Are they waking more at night? Has their mobility changed?', '""It can spot subtle changes over a long period of time, but also alert carers to falls straight away.', '""We hope it will give people with dementia independence for longer and give their carers confidence.', '""Privacy and simplicity are key. It\'s just one plug-in box with some clever sensors inside, which empowers carers to take the right action at the right time for them.""', 'Matt and James travel around the region asking carers what they need, and then try to engineer a solution. ', 'In Colchester, Essex, they met Lynn who is 60 and has been looking after her husband Terry since he was diagnosed aged 55. ', '""I can\'t leave him. I wouldn\'t feel comfortable,"" she said.', '""Every day is different. Some days he knows my name, some days he doesn\'t. ', '""Some days he can\'t find the front door, but I want to be able to pop to the shop sometimes.""', 'The social worker has been helping Supersense Technologies with its research. ', '""A single box monitoring system would be fabulous for Terry. No interaction. ', '""Pendants and telephones are no good for him. He wouldn\'t know what to do with them. ', '""He used to be a builder, but now he can\'t build anything."" ', 'Phil is 67 and cares for his wife Anne at their home in Tiptree, just down the road. He is still able to leave her for short periods of time, but hopes this technology will allow carers to make those trips more confidently.', '""I\'m happy with the design because it doesn\'t have any cameras that can be hacked into,"" he said.', '""Obviously you don\'t want to invade the privacy of the person with dementia or anyone else in the house. It sounds almost too good to be true."" ', 'James and Matt are being supported by Judge Business School. ', 'Their project is in its infancy, but they hope it makes the final shortlist for the prestigious Longitude Prize on Dementia in 2024. ', 'Meanwhile, 15 miles (24km) from Cambridge, nestled in a cluster of barns and stone buildings in Fordham, is BiologIC Technologies.  ', 'In purpose-built labs, the company is trying to use artificial intelligence to make life-saving drugs more affordable and quicker to produce. ', '""Around the world scientists are developing wonderful drugs, but it can take 10 years or more of trial and error,"" said Richard Vellacott, the company\'s chief executive.', '""AI can analyse the data generated from those trials in a way humans can\'t, and learn lessons about the best way for future medicines to be developed.', '""It could help to cut the development time down to a year, and that would cut costs too and hopefully make these drugs available to anyone who needs them.""', 'The team is currently working with nucleic acid medicines. It is what many Covid-19 vaccines are based on, but they also have the potential to become key in cancer care.', 'The company has made ""biological computers"" that process liquids and DNA, instead of the usual electronic data.', 'In a machine smaller than a house brick, made on a 3D printer, they can produce hundreds of thousands of vaccines in a few hours. ', 'If they need to upscale production, they simply print more machines.', '""This technology will also be invaluable with the growing use of personalised medicines,"" said Richard. ', '""It\'s possible to have hundreds of these small machines each producing something unique to the patient, rather than the traditional large vats making huge quantities of the same thing.""', 'The UK government has been clear that it believes artificial intelligence has an important role in healthcare. In October it announced a Â£100m fund to accelerate the use of AI in life sciences. ', ""It has also pledged Â£2bn for engineering biology - a way of combining the two scientific fields to find solutions to the world's food, fuel, environmental and medical problems. "", 'Next year will see both of these sectors expand in the UK.', 'BiologIC and Supersense hope to harness the opportunities of that growth and develop products that make a big difference.', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp 0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['In the tech world, 2023 will perhaps be remembered as the year that generative AI went mainstream. ', 'From computer code, to artwork, to essays, generative AI systems can quickly create a range of content which, while not perfect, has become an essential tool in some industries and professions.', 'Backed by Microsoft, ChatGPT led the way with its launch in late 2022, and rivals have been piling in ever since.', 'This month brought one of the most significant moves, when Alphabet, the owner of Google, revealed Gemini - an AI which will be integrated into Google products, including its chatbot and search engine. ', 'Alphabet claims Gemini outperforms the current version of ChatGPT.', 'But the creator of ChatGPT, OpenAI, says it is not standing still. It is promising a more powerful version of its software next year.', 'In November, at a conference for software developers, OpenAI chief executive Sam Altman said: ""What we launched today is going to look very quaint, relative to what we\'re busy creating for you now.""', 'Meanwhile, investors are pouring money into the industry, hoping to back the next big player.', 'According to PitchBook, across the globe, venture capital firms poured $21.4bn (Â£17.5bn) into generative AI start-up firms and that was just to the end of September.', 'For comparison, in the whole of 2022, just $5.1bn was invested.', 'But some are warning that we should not get too carried away. Ben Wood, chief analyst at CCS Insight, says generative AI will have a ""cold shower"" in 2024. ', '""The hype has ignored, we think, a few obstacles that are just going to slow it down a bit in the short term,"" he says.', ""He points out that it's very expensive to develop and run a generative AI system. It requires a lot of computing power and expensive computer chips that are in short supply."", 'To mitigate those costs he predicts that some AI will move to a hybrid systems, where some of the processing is done locally - on your laptop or phone.', 'Mr Wood also says that regulation and legal battles might cool off the current mania for generative AI.', '""Firms could find they end up in a situation where they invest loads of money in an AI powered service, and then have to roll some of it back to be compliant with the regulation.""', 'In the first quarter of next year the one millionth all-electric car will set-off on UK roads, according to Schmidt Automotive Research. That will make the UK the second market, after Germany to reach that landmark.', 'Despite that, 2024 is expected to be another tough year for the makers of electric vehicle (EVs).', 'Late in 2023, Ford, GM and Tesla all paused plans to expand their production of electric vehicles. In October, Mercedes-Benz described the market for electric vehicles as ""brutal"", blaming a price war and supply chain issues.', ""Analysts don't expect the situation to get much easier."", 'Matthias Schmidt, an auto market analyst, forecasts a stagnant year for EV sales across Europe in 2024. In traditionally strong markets like Germany and Norway he sees almost no growth at all. ', 'However, the UK could be one bright spot due to the introduction of the zero emission vehicle (ZEV) mandate. From January, just over a fifth of vehicles sold must be electric, with the target expected to hit 80% by 2030.', 'All this could be great news for anyone with the money to buy an EV.', '""It will be a total buyer\'s market especially when it comes to electric cars as manufacturers rush to meet ZEV mandate targets,"" says Mr Schmidt.', '""The cuts will be hidden though, through financing deals and higher trim levels at no extra costs - as manufacturers will be hesitant in being too overt about price cuts,"" he adds.', 'This article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitterâ\x80\x99s cookie policy, external and privacy policy, external before accepting. To view this content choose â\x80\x98accept and continueâ\x80\x99.', 'Humanoid robots could start to look more useful next year. At Tesla engineers are working on Optimus - a humanoid robot that they hope will soon be doing basic factory jobs.', 'A video released earlier this month showed the latest version of Optimus - lighter than the previous machine, with new hands and powered by new motors. ', 'In July, Mr Musk said that Optimus would be capable of working in a Tesla factory in 2024.', '""In terms of when it will be able to do some useful things, we\'ll first be trying this out in our own factories - proving out its utility. But I think we\'ll be able to have it do something useful in our factories sometime next year. I\'m pretty confident of that.""', 'Tesla has plenty of competition in the field for humanoid robots. Other firms already have robots learning tasks in work settings.', 'Amazon is trialling a humanoid robot in its warehouses. Called Digit, the robot can move, grasp and handle items in a similar way to a human.', 'It has been developed by Agility Robotics, which hopes to deliver Digit robots to other customers next year.', 'Meanwhile, in Canada Sanctuary AI has been training its robot, called Phoenix, to do specific tasks like packing bags. In 2024 the plan is to broaden the range of tasks that Phoenix can perform.', 'In the world of pharmaceuticals, one treatment has been selling so fast that its maker is struggling to meet demand.', ""The weight-loss drug semaglutide, marketed under the brand name Wegovy has been a huge success, making its owner Novo Nordisk, briefly, Europe's most valuable company. "", 'To meet demand the Danish firm is investing billions of euros in expanding production facilities.', 'At the moment Wegovy is administered as a weekly injection, but a tablet version is almost ready. Novo Nordisk is not saying when it expects that to hit the market.', 'The Danish firm can expect more competition to emerge next year. ', ""Eli Lilly's drug Mounjaro recently received approval as a weight-loss treatment in the US and the UK, and approval in the European Union seems to be likely soon. "", 'Meanwhile, Pfizer has been seeking approval for its weight-loss pill.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The UK Supreme Court has upheld earlier decisions in rejecting a bid to allow an artificial intelligence to be named as an inventor in a patent application.', 'Technologist Dr Stephen Thaler had sought to have his AI, called Dabus, recognised as the inventor of a food container and a flashing light beacon.', 'But in 2019, the intellectual property office (IPO) rejected this, saying only a person could be named as an inventor.', 'The decision was then backed by both the High Court and Court of Appeal.', 'The IPO has argued, and courts have supported the view, that only ""persons"" can have patent rights, not AIs.', 'Now five Supreme Court judges have dismissed a bid to reverse those decisions, concluding that ""an inventor must be a person"", and that an AI cannot be named as an inventor to secure patent rights.', 'The judgement does not deal with the issue of whether Dabus did in fact invent the food container and light. ', 'Dr Thaler, who believes that Dabus is a ""conscious and sentient form of machine intelligence"", told the BBC ""Naturally, I feel disappointed by this decision, highlighting the ongoing clash between human and machine intelligence.""', 'The IPO told the BBC it welcomed the judgement and the clarification it provided.', 'But it added that ""the government will nevertheless keep this area of law under review to ensure that the UK patent system supports AI innovation and the use of AI in the UK"".', 'Rajvinder Jagdev, of specialist intellectual property litigation firm Powell Gilbert, said: ""The judgement does not preclude a person using an AI to devise an invention - in such a scenario, it would be possible to apply for a patent, provided that person is identified as the inventor. The judgement alludes that had this been the scenario it had been asked to consider, the outcome may have been different.""', ""Dr Thaler also argued that he was entitled to patents for Dabus inventions as the AI's owner, but this was rejected."", 'A different decision could have caused ""headaches for companies using [AI] software to innovate as they may not be the owner of the patent"", Diego Black, from European intellectual property firm Withers and Rogers, told the BBC.', 'Simon Barker, of law firm Freeths, said the judgement raised ""interesting policy questions"" about how governments might look to change laws in the future as AI advances.', '""There are similar debates in other areas of intellectual property rights too. Copyright in AI-generated works, for example. Is the programmer of the AI the creator, or the user who is responsible for prompting the machine? And what if it really is just the machine itself, like Dr Thaler claimed of Dabus?""', 'But Professor Ryan Abbott of the University of Surrey who represented Dr Thaler in the case said the decision implied that ""AI, at best, can be a \'highly sophisticated tool\' that can be used by people who invent. ', '""This affects the meaning of an ""inventor"" under UK patent law, and to be an inventor, one need not make the creative leap behind the invention, as had been previously assumed. Accordingly, companies who use AI to develop products will have to say they or their employees are the inventors, even when the humans involved do little else but switch on the computer."" ', 'Some legal experts expect pressure for changes to existing laws to grow, as AIs become increasingly capable of autonomously generating novel ideas.', 'In its statement, the IPO said it recognised ""that there are legitimate questions as to how the patent system and indeed intellectual property more broadly should handle [AI] creations"".', 'In June 2022, the UK government published a response to its consultation on AI and intellectual property.', '""The response concluded that there should be no legal change to UK patent law now, and noted that many share the view that any future change would need to be at an international level. The decision of the Supreme Court does not alter that conclusion,"" the IPO wrote. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['New Artificial Intelligence (AI) technology is hoping to deter deer from railway lines. ', 'The system has been successfully tested along a mile of track between Peterborough and Grantham, train operator LNER said.', 'The automated deer deterrent system (Adds) uses sound and vision sensors to identify when a deer approaches the tracks.', 'If a deer is detected, a variety of audible and visual alarms will start.', 'The alarms aim to deter the animal away from the tracks and an AI camera monitors its movement until it has been diverted to a safe distance.', 'Traditional methods of deterring deer from railway tracks include train-mounted whistles - which are unreliable - and high fencing, which can be costly.', 'The Adds system has deterred an average of 50 deer per week since the start of testing in May, LNER said.', 'Around eight incidents of deer being hit by trains in the trial area would have been expected in that period, but only one has been reported.', 'LNER chief digital and innovation officer Danny Gonzalez said: ""Our first deployment of this innovative system to deter deer has quickly proven that the solution can save time, stress and, most importantly, deer.""', 'The train operator worked in partnership with Network Rail to develop the scheme and route engineer Jo Priestly said Adds will better protect deer and reduce disruption and delays for passengers.', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp 0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Where does artificial intelligence (AI) fit into the world of Indian cinema?', 'While AI has already disrupted Hollywood with writers going on a strike, the debate around the contentious issue is not widespread in the Indian film industry which employs tens of thousands of people.', 'Some Indian film industry creators are underplaying the threat of AI for now, while others feel it needs to be taken very seriously. ', ""Director Shekhar Kapur's debut Indian film, Masoom (1983), followed a woman's journey towards accepting a child born out of her husband's extramarital affair. For the sequel to this emotional film, which had delicately handled the complexities around infidelity and social diktats, Kapur decided to experiment with AI tool ChatGPT."", 'The award-winning director was amazed at ""how intuitively AI understood the moral conflict in the plot"" and gave him a script in seconds. The AI-generated script depicted the child growing up to resent his father, shifting the gears of their relationship from the first film. ', 'The future with AI will be ""chaotic"", Kapur says, as machine learning can do in seconds what will take a bunch of scriptwriters ""weeks to do"". ', 'According to a 2019 Deloitte report, India has the largest film industry in the world in terms of films produced each year. The industry employs 850,000 people. ', 'As AI tools get sharper and the internet is filled with uncanny deepfake videos of popular Indian stars, including Rashmika Mandanna and Alia Bhatt, its use is raising both economic and ethical questions. ', ""Click here to watch Devang Shah's documentary on AI and Indians films "", ""The use of AI in TV and movie productions was one of the core issues of the actors' and writers' strike in the US this year, bringing Hollywood to a standstill for months."", '""There hasn\'t been a structured conversation around the use of AI in India yet,"" says Siddharth Roy Kapur, former president of the Producers Guild of India. But the time to have it is now, he says, because AI tools are ""getting smarter literally every second"". ', '""Where we are today with AI will be very different to where we are three to six months from now,"" Kapur says. ', 'So where is India ""now""? ', 'AI is far from the point where the ""push of a button"" generates ""everything readymade"", say Keitan Yadav and Harry Hingorani who run Redchillies.vfx.', 'The visual effects studio was founded by Bollywood superstar Shah Rukh Khan nearly two decades ago.', ""This year, the studio handled visual effects of Khan's films - Jawan and Pathaan - two of India's biggest box-office hits. "", 'Yadav and Hingorani say they have been using AI tools for ideas but feel it is yet to match the 4K resolution of a motion picture. ', 'But Guhan Senniappan is on a mission to challenge this thought. He is directing the upcoming Tamil movie Weapon, which will be the first Indian feature film to have a two-and-half minute sequence made entirely by AI. ', '""We\'re working on a superhuman saga with a lot of action sequences and I wanted to convey the story in a new way,"" Senniappan says.', 'Images of the lead actor, Sathyaraj, were used as prompts to generate a younger AI version of him. ', '""Using AI was a cheaper alternative to live action,"" Senniappan says.', ""Among Bollywood stars, Khan was among the first to test AI in 2021 when he lent his face and voice for an advertisement that used deepfake technology. The ad campaign launched by Cadbury's allowed owners of small businesses to use his voice and image to promote their store and bump up sales during the pandemic slump."", 'Sukesh Nayak of Ogilvy India, the agency behind the campaign, says that this ""one ad campaign created 300,000 ads across the country"". ', 'The agency worked closely with Khan\'s team in a strictly controlled environment and ensured ""only certain kinds of businesses were allowed to register"" to use their campaign. ', 'With the law and the legislature in India yet to define regulations around AI use, critics say the field is open for misuse. ', 'This year, Bollywood actor Anil Kapoor won a legal battle to protect his likeness, image, name and voice among other elements. Kapoor called the case verdict ""very progressive"" and good for other actors as well. ', '""Where my image, voice, morphing, GIFs and deepfakes are concerned, I can straightaway, if that happens, send a court order and injunction and they have to pull it down,"" he told Variety magazine.', 'But there is another side to AI as well. ', 'Some experts feel AI can make certain aspects of filmmaking simpler and faster. Shilpa Hingorani at Redchillies.vfx is intrigued by the prospect of automating certain VFX processes that currently need to be done ""frame by frame"" and require ""long time periods to even generate a preview for the client"". ', '""Anything that reduces the time periods will definitely make the process easier and faster,"" Yadav says. ', 'Between humans and AI, does one do the work better than the other? ', 'Despite heavy reliance on AI for his film Weapon, Senniappan says he would have preferred a live-action shoot ""if we had the budget and time"".', '""AI is beautiful but it is not as organic as a live-action or anime because a human [being] did not act or draw it manually,"" he says.', 'After his initial fascination with ChatGPT, Kapur felt the same. ""I asked myself who is smarter, and the answer was \'I am\'.""', 'AI doesn\'t have a morality of its own, ""it assumes a morality from the data available"", he says. ""It cannot create mystery, feel fear or love.""', 'What it can, however, do is democratise the process of filmmaking, he says.', '""If everyone has access to the same tools, organisational hierarchies will break and everyone will have the power to tell a story,"" he adds.', 'BBC News India is now on YouTube. Click here to subscribe and watch our documentaries, explainers and features.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The Policing Minister has urged forces to follow Bedfordshire in using artificial intelligence (AI) to carry out admin tasks. ', 'AI is used to redact personal data from case files before they go to the Crown Prosecution Service (CPS).', 'Detectives found it performed the task in minutes, whereas traditional methods could take them days. ', 'Chris Philp MP said AI could transform policing in ""a radical and revolutionary way"".', 'When detectives investigate a crime, they produce a case file for the CPS including witness statements, phone records and other evidence. ', 'To comply with data protection laws, personal data such as phone numbers, addresses and vehicle registrations must be removed before the file is shared. ', 'Only data relevant to the investigation can remain.', 'Det Con James Carrington-Read said: ""Until now, we\'ve had to redact those documents manually. ', '""Some of them, like download data from seized mobile phones, contain hundreds of pages which could take a day or two for us to go through.  ', '""This new system did it in around 15 minutes which means we can get on with other jobs.""', 'Officers always review the results to make sure they are accurate, but it still takes up to 90% less time. ', 'The head of the force\'s criminal investigation department, Det Ch Insp Jo Smith said: ""It\'s important to retain a human element but we saw it as a necessary part of innovation to try to introduce something that would save officers time. ', '""In policing, innovation can be very challenging because of bureaucracy and red tape but it\'s made a really big difference to officers here.""', 'The DocDefender system searches for categories of information chosen by each officer.  ', 'No data is stored for longer than 12 hours.', 'A recent report into policing productivity said technology would play a central role for police forces in future. ', ""The National Police Chief's Council has established its own Science and Innovation Committee to focus on these changes."", 'The Policing Minister Chris Philp has also urged forces to embrace its potential to help police ""stay ahead"" of the criminals, prosecute them and become ""more operationally efficient"".', 'But privacy campaigners have raised concerns about government plans to expand facial recognition surveillance.', 'In November, every police chief in England signed a covenant for using AI in policing. ', ""It means everyone using or developing the technology for policing must ensure it meets a set of principles to ensure it's lawful, transparent and robust. "", 'Riven, which designed the DocDefender system with AWS, said: ""We have several safeguards in place to ensure the technology is used responsibly. ', '""We are proud that it gives officers significant time back to police their communities.""', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp  0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Whale conservationist Ted Cheeseman admits that the huge animals don't patiently pose for photos."", 'Instead, on most whale-watching trips he says ""you see 2% of the whale for 2% of the time"".', 'Nevertheless, going out in a boat to try to spot a whale remains very popular.', 'An estimated 13 million people go whale watching every year around the world, and the industry is said to be worth $2.1bn (Â£1.7bn).', 'The humpback whale is the star of the whale-watching business, as it\'s relatively common and spends time on the surface. Mr Cheeseman also says that the humpback is ""very engaging"", with the luckiest whale-watchers catching a breach (a jump out of the water) or a flick of the tail.', ""To help people feel a closer connection to each whale, Mr Cheesman's research company, HappyWhale, allows users to upload their photos onto its website."", ""HappyWhale's artificial intelligence (AI) software will then quickly trawl through its database of more than 70,000 different whales with the aim of telling you the exact animal that you were looking at."", ""It can tell you the whale's name, if it has already been given one, or invite you to choose one. And if the whale is one that has been previously recorded it will show you a map of everywhere that it has been sighted."", ""The AI works by using adapted human facial recognition software to identify each whale's uniquely shaped, coloured or marked tail. "", ""Mr Cheeseman says the AI is so effective that it doesn't need a whale's tail to be still and spread out - it can identify the whale even if its tail is rotated, curved, or in the midst of diving."", 'The ""WhaleID"" system is free for members of the public to use, but whale watching operators and cruise ships are asked to pay a monthly subscription.', '""We charge for commercial use of the mobile app, because for whale watch companies and expedition cruise operators it is a great experience for guides to be able to instantly identify a whale and relay its story,"" says Mr Cheeseman. ', '""This adds value, generating enthusiasm, and enabling the guide to tell a true story about the whales they are encountering, in a way that deeply connects with whale watchers.""', 'Oregon-based Happywhale uses all the data it gets from uploaded photos to help it track whale numbers and movements. It is an example of a growing trend - conservation groups using AI to enable members of the public to identify animals or birds. And in return the organisations get a host of crowdsourced data.', 'The Merlin Bird ID app allows users to find out what species of bird they are looking at - or hearing. Developed by The Cornell Lab of Ornithology at Cornell University in New York State, the user either takes a photo of the bird, or makes a recording of its song.', ""The AI software then quickly checks Cornell's database of images or recordings, kept in a digital archive called the Macaulay Library, and provides a match."", '""The idea behind Merlin was how can we support new birders,"" says Alli Smith, Merlin project coordinator. ""Just in North America, there are over 700 different species of birds that you could possibly find - that\'s overwhelming.""', 'Regarding the database of bird sounds, the AI works by turning the recording into a spectrogram or data waveform which it then tries to match against those already on file.', 'Merlin Bird users are encouraged to enter their location, and the size, colour and behaviour of the bird. This data is then added to the Macaulay Library, and scientists can use it to help them map bird populations, ranges and migratory patterns, and understand how they have changed.', '""Bird populations have been declining all over the world, but in the last 50 years, we\'ve lost three billion birds in the US and Canada alone,"" adds Ms Smith. ""This is the first time we\'ve been able to show this in real time.""', 'The app iNaturalist started out as a social network where members could post photos of animals and plants, and the community would identify what was in that photo.', 'Today it uses AI to determine what people have photographed. The AI was originally trained with more than 100 photos for each species, and keeps learning as more images are uploaded.', 'One of the developers of the app, Grant Van Horn, assistant professor of computer science at University of Massachusetts Amherst, says the AI is looking out for particular visual ""patterns"".', '""For example, after training the model to recognize a photo of a chipmunk or squirrel, the computer starts to see if these patterns are present in a similar image. And given those patterns, it will predict either chipmunk, or squirrel, or something else,"" he says.', 'Read additional stories on artificial intelligence', 'Wayne Klockner, executive director of US birdwatching organisation American Birding Association, says that image recognition apps are helping to boost conservation efforts.', '""I believe any resource that helps birdwatchers learn more is a positive, because that knowledge and enjoyment spark and reinforce interest in conservation,"" he says. ""People protect what they love and care about. If the application of AI contributes to that, birds will likely be better off.""', ""Back at HappyWhale, it says that the whale tracking data it gets from users' photos played a key role in persuading members of the International Association of Antarctica Tour Operators to reduce the maximum allowed speed of their ships to prevent whale strikes."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The CEO of The Arena Group, publisher of Sports Illustrated, has been fired just weeks after the magazine was accused of publishing articles generated by artificial intelligence.', 'Ross Levinsohn was terminated ""to improve the operational efficiency and revenue"" of the company, Arena Group said in a statement.', 'It comes amid a wider purge of a number of senior executives at the company since the scandal broke.', 'Manoj Bhargava will take over as CEO.', 'But a spokesperson for Mr Bhargava - founder of the 5-Hour Energy drink - told the BBC that Mr Levinsohn\'s removal ""had absolutely nothing to do with the AI issue at all"". ', 'He insisted that the move was about ongoing efforts to improve the company. But it comes after the Arena Group fired three of its major executives last week, including chief operating officer Andrew Kraft, media president Rob Barrett and corporate counsel Julie Fenster.', 'The announcement comes a month after tech news outlet Futurism revealed Sports Illustrated had published several articles with fake author names and headshots from an AI-generated image website.', ""The magazine removed the content but disputed the report's accuracy, while launching an internal investigation. "", 'Arena Group licensed the content from a third-party company, Advon Commerce, a company spokesperson said in a statement. Advon Commerce is an e-commerce company that works with retailers and publishers.', 'In its statement last month, Arena Group claimed that Advon Commerce had assured them ""that all of the articles in question were written and edited by humans"" and that the e-commerce firm regularly uses ""counter-plagiarism and counter-AI software"".', 'As for the fake names, the group said writers were allowed to use pseudonyms ""in certain articles"" to protect their privacy. ', ""The brief statement from Arena Group announcing Mr Levinsohn's firing did not mention the scandal. "", 'This incident at Sports Illustrated comes as concern grows in the media world that generative artificial intelligence could cheaply replace journalists and potentially spread misinformation. Various newsrooms have experimented with AI or released guidelines for employees and audiences to explain their approach towards it.', 'Some newsrooms have made headlines, however, after publishing AI articles that included errors or falsehoods. Others gained attention for not marking stories as AI generated.', ""Numerous Sports Illustrated staff said on social media that they were appalled by the findings in Futurist's report, particularly as Arena Group has made large cuts to staff in recent years."", 'In a statement at the time, the Sports Illustrated Union said ""these practices violate everything we believe in about journalism"".', '""We demand the company commit to adhering to basic journalistic standards, including not publishing computer-written stories by fake people,"" the union said on X.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""European Union officials have reached a provisional deal on the world's first comprehensive laws to regulate the use of artificial intelligence."", 'After 36 hours of talks, negotiators agreed rules around AI in systems like ChatGPT and facial recognition.', 'The European Parliament will vote on the AI Act proposals early next year, but any legislation will not take effect until at least 2025. ', 'The US, UK and China are all rushing to publish their own guidelines.', 'The proposals include safeguards on the use of AI within the EU as well as limitations on its adoption by law enforcement agencies.', 'Consumers would have the right to launch complaints and fines could be imposed for violations.', 'EU Commissioner Thierry Breton described the plans as ""historic"", saying it set ""clear rules for the use of AI"".', 'He added it was ""much more than a rulebook - it\'s a launch pad for EU start-ups and researchers to lead the global AI race"".', ""European Commission President Ursula von der Leyen said the AI Act would help the development of technology that does not threaten people's safety and rights."", 'In a social media post, she said it was a ""unique legal framework for the development of AI you can trust"".', 'The European Parliament defines AI as software that can ""for a given set of human-defined objectives, generate outputs such as content, predictions, recommendations or decisions influencing the environments they interact with"".', 'ChatGPT and DALL-E are examples of what is called ""generative"" AI. These programs learn from vast quantities of data, such as online text and images, to generate new content which feels like it has been made by a human.', 'So-called ""chatbots"" - like ChatGPT - can have text conversations. Other AI programs like DALL-E can create images from simple text instructions.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""A video showcasing the capabilities of Google's artificial intelligence (AI) model which seemed too good to be true might just be that."", 'The Gemini demo, which has 1.6m views on YouTube, shows a remarkable back-and-forth where an AI responds in real time to spoken-word prompts and video.', ""In the video's description, Google said all was not as it seemed - it had sped up responses for the sake of the demo."", 'But it has also admitted the AI was not responding to voice or video at all.', 'In a blog post published at the same time as the demo, Google reveals how the video was actually made.', 'Subsequently, as first reported by Bloomberg Opinion, Google confirmed to the BBC it was in fact made by prompting the AI by ""using still image frames from the footage, and prompting via text"".', '""Our Hands on with Gemini demo video shows real prompts and outputs from Gemini,"" said a Google spokesperson. ', '""We made it to showcase the range of Gemini\'s capabilities and to inspire developers.""', ""In the video, a person asks a series of questions to Google's AI while showing objects on the screen."", 'For example, at one point the demonstrator holds up a rubber duck and asks Gemini if it will float. ', 'Initially, it is unsure what material it is made of, but after the person squeezes it - and remarks this causes a squeaking sound - the AI correctly identifies the object.', 'This article contains content provided by Google YouTube. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Googleâ\x80\x99s cookie policy, external and privacy policy, external before accepting. To view this content choose â\x80\x98accept and continueâ\x80\x99.', 'However, what appears to happen in the video at first glance is very different from what actually happened to generate the prompts.', 'The AI was actually shown a still image of the duck, and asked what material it was made of. It was then fed a text prompt explaining that the duck makes a squeaking noise when squeezed, resulting in the correct identification.', 'In another impressive moment, the person performs a cups and balls routine - a magic trick where a ball is hidden underneath one of three moving cups - and the AI is able to determine where it moved to.', 'But again, as the AI was not responding to a video, this was actually achieved by showing it a series of still images.', 'In its blog post, Google explained that in fact it told the AI where a ball was underneath three cups, and showed it images which represent cups being swapped.', 'Google clarified that the demo was created by capturing footage from the video, in order to ""test Gemini\'s capabilities on a wide range of challenges"". ', 'While sequences were shortened and stills were used, the voiceover from the video is taken directly from the written prompts fed into Gemini.', 'But there is another element of the video which further stretches the truth.', 'At one point, the user places down a world map, and asks the AI: ""Based on what you see, come up with a game idea... and use emojis.""', 'The AI responds by apparently inventing a game called ""guess the country"", in which it gives clues (such as a kangaroo and koala) and responds to a correct guess of the user pointing at a country (in this case, Australia).', ""But in fact, according to Google's blog, the AI did not invent this game at all."", 'Instead, the AI was given the following instructions: ""Let\'s play a game. Think of a country and give me a clue. The clue must be specific enough that there is only one correct country. I will try pointing at the country on a map,"" the prompt read.', 'The user then gave the AI examples of a correct and incorrect answer.', 'After this point, Gemini was able to generate clues, and identify whether the user was pointing to the correct country or not from stills of a map.', 'It is impressive - but it is not the same as claiming the AI invented the game.', ""Google's AI model is impressive regardless of its use of still images and text-based prompts - but those facts mean its capabilities are very similar to that of OpenAI's GPT-4."", ""And it is noteworthy that the video was released just two weeks after a period of unprecedented chaos in the AI space, following Sam Altman's dramatic firing - and rehiring - as CEO of OpenAI."", 'It is unclear which of the two is more advanced - but Google may already be playing catch-up after Mr Altman told the Financial Times that the firm is working on the next version of its AI.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['An AI system designed to speed up the diagnosis of mental health conditions in children is to be developed in Cambridge.', 'The technology will use routinely-collected data about children to find patterns that identify those most at risk. ', 'Dr Anna Moore, who leads the project, said it could ""revolutionise"" care and highlight the need for more resources. ', '""Diagnosing it earlier, means it can be more easily treated,"" she said.', 'Child psychiatrists believe there is ""a crisis"" in the mental health care of children, and the NHS estimates 20% of people under 24 probably need some form of mental health support.', 'Dr Moore has just been awarded a Â£2.5m UK Research and Innovation (UKRI) Future Leaders Fellowship to pursue the programme.', 'She said she had witnessed it first hand as a consultant psychiatrist, but - as an expert in clinical informatics at the University of Cambridge - she said she believed there could be a digital solution.', '""Our mental health is affected by genetics, our early experiences in life and our current environment,"" said Dr Moore.', '""We\'re going to use cutting-edge AI technology to look at information that is gathered about us by health, education and social services throughout childhood. ', '""By bringing it all together we hope to spot trends which can alert a clinician to the fact that a child may be developing an early mental health condition.""', ""Bullying, family poverty, physical health problems and being a young carer can all affect a child's mental health."", 'Ali ,16, helps to care for his mother and brother who have a muscle-wasting condition, but he also has physical health problems of his own, which make going to school difficult. ', '""One year my attendance was just 20%,"" said Ali. ', '""I felt isolated, anxious and very low. It was like my Covid-19 lockdown had been extended by another couple of years.', '""Being a carer is also a challenge. You worry about whether you\'re doing too much or not enough to help.""', 'Ali and his brother have both experienced mental health problems and struggled to get timely support.   ', 'The system being developed by Dr Moore aims to help young people like Ali - without them having to ask for it.', '""Young carers often don\'t recognise that they need help themselves, so they can fall through the cracks in the system,"" said Dr Moore.', '""One hope is that these algorithms will identify the children whose problems may not ordinarily be flagged up so they can be offered support.""', 'Ali welcomed the research, saying: ""It\'s amazing. Data is viewed anyway and this will be confidential so I don\'t have any issues with it. ', '""It\'s a clever idea and I think it will save lives.""', ""Dr Moore will develop this system, with help from Microsoft, to be used in the NHS and the forthcoming Cambridge Children's Hospital."", 'The Kavli Centre for Ethics and the Centre for Human-Inspired AI are also both involved, to ensure the project progresses in a way that will benefit patients.', 'All data gathered to program the algorithms will be anonymised, with codes replacing names and personal information. ', 'Dr Moore said: ""We\'ve been working with families for years explaining what data we\'d need, how we\'d make it secure, unbiased and why we think it\'s necessary.', '""Their main concern was that none of our data should be given to police forces or commercial organisations like insurance companies, which it won\'t.', '""They\'ve been overwhelmingly supportive.""', 'Jeremy Bernhaut, head of policy and influencing at Rethink Mental Illness, said: ""Early intervention for young people struggling with their mental health can be lifesaving. ', '""However, in many cases, young people currently end up on heartbreakingly long waiting lists for support and the urgent priority is to resolve the lack of funding and workforce.""', 'Dr Moore hopes this project will highlight the need for more resources and that the technology can progress mental health care in the same way it has worked for physical conditions like diabetes and cancer.', 'If you need mental health support, help is available via BBC Action Line.', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp  0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Google has released an artificial intelligence (AI) model which it claims has advanced ""reasoning capabilities"" to ""think more carefully"" when answering hard questions.', 'AI content generators are known to sometimes invent things, which developers call hallucinations.', 'Gemini was tested on its problem-solving and knowledge in 57 subject areas including maths and humanities.', 'Boss Sundar Pichai said it represented a ""new era"" for AI.', 'Google adopted a cautious approach to the launch of its AI chatbot, Bard, earlier this year, describing it as ""an experiment"". ', 'Bard made a mistake in its own publicity demo, providing the wrong answer to a question about space.', 'But Google is making some big claims for its new model, describing it as its ""most capable"" yet and has suggested it can outperform human experts in a range of intelligence tests.', 'Gemini can both recognise and generate text, images and audio - but is not a product in its own right.', ""Instead it is what it known as a foundational model, meaning it will be integrated into Google's existing tools, including search and Bard. "", 'Gemini appeared to have set a ""new standard"", highlighting its ability to learn from sources other than text, such as pictures, according to Chirag Dekate, from analysts Gartner.', 'He said that might ""enable innovations that are likely to transform generative AI."" ', ""Google has so far struggled to attract as much attention and as many users users as OpenAI's viral chatbot ChatGPT."", ""But it claims the most powerful version of Gemini outperforms OpenAI's platform GPT-4 - which drives ChatGPT - on 30 of the 32 widely-used academic benchmarks."", 'However, a new, more powerful version of the OpenAI software is due to be released next year, with chief executive Sam Altman saying the firm\'s new products would make its current ones look like ""a quaint relative"".', 'It remains to be seen whether the recent turmoil at OpenAI - which saw Mr Altman fired and rehired in the space of a few days - will have any impact on that launch.', ""The firm also faces fresh competition from Elon Musk's xAI, which is seeking to raise up to $1bn to invest in research and development. Chinese firm Baidu is also racing ahead with its own AI products."", 'But as the technology rapidly evolves, so do fears about its potential to cause harm.', 'Governments around the world are trying to develop rules or even legislation to contain the possible future risks of AI.', 'In November, the subject was discussed at a summit in the UK, where signatories agreed a declaration calling for its safe development. The King also said possible dangers needed to be addressed with a sense of ""urgency, unity and collective strength"".', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""At a time when the ethics of the development of Artificial Intelligence (AI) is dividing opinion, the founder of the world's leading AI chip maker says he's not concerned. "", '""I have every confidence that between all of our colleagues around the world, we will invent technologies, philosophies, methodologies, practices, monitors, regulations, design practices, to keep technology safe,"" founder, President and CEO of Nvidia Jensen Huang told the BBC at a roundtable interview on Wednesday.', 'His comments come a month after artificial intelligence (AI) company OpenAI descended into chaos, during which founder Sam Altman was sacked by the board, and then re-instated after an uproar.', 'During the drama at the company, which operates the large language model ChatGPT, a spotlight was shone on how commercial competition is shaping the development of AI systems and the pace at which the technology is moving.', 'However, top investor Microsoft denied it was due to a disagreement over safety. ', ""ChatGPT, was in fact, trained using 10,000 of Nvidia's graphics processing units (GPUs) clustered together in a supercomputer belonging to Microsoft."", ""The demand for its AI chips has pushed Nvidia's share price more than threefold, making it one of the most valuable companies in the world."", 'In May the firm joined technology giants Apple, Amazon, Alphabet and Microsoft in the elite club of companies with stock market valuations of more than $1 trillion (Â£822bn).', 'But the company is not alone in its pursuit of the AI chip businesses - Chinese telecoms firm Huawei has said it intends to make AI an integral part of its strategy, with its CFO Meng Wanzhou saying it wants to provide the world with a ""second option"". ', 'But Mr Huang says he is not phased by such competition, saying it is good for the advancement of the technology.', '""It allows us to do our best work and make contributions to society,"" said the chips founder, who is estimated to have a net worth of $41.6bn, according to Forbes. ', ""The chip designer dominates more than 90% of China's $7bn (Â£8.8bn) AI chip market, and Mr Huang acknowledged that historically around 20% of its revenue came from China. "", ""But in its November earnings report, Nvidia warned that it expects a fall in sales towards the end of the year because of US export restrictions aimed at curbing China's advancement in the field. "", 'The US said the measures were designed to prevent China from receiving cutting-edge technologies that it could use to strengthen its military, especially in the field of AI. ', 'Nvidia is working closely with the US government, Mr Huang said, to make sure chips for the Chinese market are fully compliant with the current rules. ', ""Despite relations between China and the US deteriorating in recent years, the Asia-Pacific region is central to Nvidia's supply chain. "", 'Its Graphics Processing Unit (GPU) has 35,000 parts, according to Mr Huang, with chips made in Taiwan by TSMC, memory chips from South Korea, packaging technology from Japan and technologies that power the chips from the United States.', '""This is the most sophisticated computer in the world. It\'s kind of a technology miracle.""', ""There is a recognition that AI is no longer just an opportunity, but that it is strategically vital, Mr Huang added, revealing that Nvidia is working with the Singapore government on a large language model called Sealion, as well as the country's overall AI strategy - and planned to make big investments there. "", 'Mr Huang added that Singapore is a growing market for his company because of the 1,100 AI start-ups based there and its role as a data centre hub for the region, but also because it is home to venture capitalists that fund the AI ecosystem. ', '""Once we have that language, once we have that foundation, the Singapore foundation, then the rest of the industries, the rest of society, the rest of competition, can then build upon that,"" he said.', '""The first wave was the American internet companies. The second wave are now the world\'s countries. Each one of the countries wants to build its own foundation to support its start-ups, its own companies, its own industries. And so now now we\'re seeing globally, the need to replicate what happened in the United States."" ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial intelligence software is to be used to predict if a patient is at risk to try and prevent the need for a hospital visit.', 'The ""Brave AI"" system is about to be rolled out to 30 doctors\' practices across the south west. ', ""Using an algorithm to identify patterns in patients' records, it can assess the risk of unplanned emergency call-outs."", 'Support services can then intervene and offer preventative care and reduce the need for hospital care.', 'The system will now be expanded to GP practices in Gloucestershire, Wiltshire, Somerset, North Somerset, Dorset, Devon and Cornwall.', 'The new contract enables nurses, pharmacists, therapists and doctors to offer personalised support where needed, including remote health monitors or offering apps to self-report their health conditions.', 'The roll-out follows the successful ""Brave AI"" pilot scheme, where NHS Somerset partnered with the North Sedgemoor Primary Care Network (PCN) to analyse data from more than 500 care home residents.', 'The results found the number of resident falls were reduced by 35%, visits to emergency departments by 60%, and ambulance call-outs by 8.7%. ', 'Dr Vin Diwakar, NHS director for transformation, said: ""These measures not only keep some of the most vulnerable patients out of hospital, but encourage conversations with patients who might not otherwise contact their GP, spotting health conditions that might otherwise go unnoticed and boosting our ability to intervene early."" ', '""With the latest figures showing that hospitals are already under considerable pressure as we enter December, it is now vital that we maximise the use of these kinds of tools to cut admissions where we can."" ', 'If the current roll-out scheme proves successful, the technology will eventually be introduced to all GP practices across the south west.', 'Follow BBC West on Facebook, X and Instagram. Send your story ideas to: bristol@bbc.co.uk', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Nadia Alaee and her team are on the lookout for people that can bring artificial intelligence (AI) skills into the HR team at Deel, a San Francisco-based software company. ', 'She says the administrative work around staff leaving and arriving at the firm could be ""streamlined"" using AI.', ""It's not just in the HR team that Ms Alaee is on the lookout for AI skills; the company is searching for anyone that has experience of tools such as ChatGPT, the generative AI software created by OpenAI, in both technical and non-technical roles. "", '""If someone can come in with knowledge of AI tools and teach team members how to use them for specific tasks then that\'s a huge asset for us,"" says Ms Alaee, a senior director at the firm.', ""The latest wave of AI tech hit in late 2022, with a public version of ChatGPT, but rivals are multiplying and include Google's Bard and Claude from Anthropic."", 'Called generative AI, this form of artificial intelligence can produce all sorts of content - from computer code, to artwork, to essays. The material is not always perfect, but is cheap and quick to produce, making it attractive to businesses. ', 'Job posts mentioning AI or generative AI have, globally, more than doubled in the past two years, according to a report by LinkedIn.', 'LinkedIn also found that two in five UK workers (38%) predicted a significant change to their jobs in their next year because of AI, and while the majority (76%) are excited about this change, over a third (36%) admit to feeling overwhelmed by the amount there is to learn.', 'A separate report by software company ServiceNow, found a similar proportion of office workers (41%), admit to currently lacking the technical abilities needed to work alongside and use AI systems.', 'According to Ngaire Moyes, UK country manager at LinkedIn, AI can ""help remove the drudgery from day-to-day work, improve productivity and free-up time for more strategic and creative work"".', 'The efficiency benefits are even more pertinent for smaller businesses.', '""At a fast-growth, early-stage company, our two biggest constraints are time and resources,"" says Deirdre McGettrick, chief executive and co-founder of ufurnish.com, a UK online furniture comparison website with 16 employees.', '""AI can help propel the business forward at a much quicker rate of output without requiring increased resources,"" she says.', 'Ms McGettrick has two uses for AI. The first is for content, where instead of outsourcing work to copywriters, existing team member use ChatGPT to write the copy. That copy is then checked by a member of staff before it is published on the website.', 'The second use is for software development.', '""We look to hire software engineers who can use AI to write code,"" she says.', 'More technology of business', ""Meanwhile, many of Deel's employees are already using AI tools to help support both managers and staff in writing performance reviews."", 'Ms Alaee believes that AI helps some staff write more in-depth responses to questions, by giving them more confidence in their writing.', ""The downside of Deel's use of AI for performance reviews is that the process is less personalised. "", '""There\'s a level of human input required when we\'re using these AI tools to make sure we\'re tailoring the response to that person or situation so it doesn\'t feel robotic, generic or like a checkbox exercise,"" says Ms Alaee. ', ""Companies will also want to ensure that by using AI tools, they don't put themselves at risk of legal or security breaches, lose the culture of the organisation or the value of human expertise."", '""If you\'re just using the technology to do all of the work, it can be a detriment to you and your team members, as there could be misinterpretations, work that goes against company policies, or errors as a result of bias within AI,"" says Ms Alaee. ""And the employees who overuse the tools may not be able to hit their targets [because AI can only go so far].""', 'Emma Parry, professor of human resources at the Cranfield School of Management, explains that AI systems are only as good as the data that they are based on.', '""Data is often based on human decisions, meaning we\'re at risk of replicating or even introducing human biases as we use it. In the workplace, this could lead to discrimination and a reduction in opportunities for individuals from marginalised communities,"" she says.', 'And this is why, at least for the time being, AI is a tool that will help most people to do their jobs better, rather than replace them.', '""Success in many job roles will be reliant on our ability to evolve as we work alongside AI in our day-to-day work life, while organisations will also need to be re-designed and re-created in order to be effective,"" says Prof Parry. ', 'According to LinkedIn, two thirds of UK professionals have not yet been provided with any formal AI training or guidelines from their employer.', ""But according to Ms Moyes from LinkedIn, employers aren't expecting everyone to become AI experts, but they are wanting professionals to become AI-literate, which just means understanding how to make use of the tools available to help you do your job better."", 'Nevertheless, Paola Dyboski, founder of Dr Zigs, a specialty bubble toy manufacturer based in North Wales, has been surprised by the lack of adoption of AI tools.', '""We work with contractors for our marketing needs and while the awareness of AI is there, I have been surprised at the resistance to learn and use new AI tools,"" she says.', 'So how can workers get the AI skills they need?', 'First, by experimenting with many of the free versions of the tools available online.', '""Get familiar with the AI tools available - play with them yourself, read about what is coming out,"" says Ms Alaee.', 'After that there are a huge number of courses available online that focus on understanding the types of tools available and how to use them, as well as tailored courses for particular professions.', 'But before rushing to learn about AI tools, business leaders and employees should consider why they think they ought to use AI.', '""Rather than constantly asking \'how can we make things more efficient?\' we should refocus and question \'how can we make things better?\' to find a way to manage AI and benefit from a hybrid human, AI workforce,"" says Ms Parry. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['This video can not be played', 'CES 2024: Check out these amazing gadgets', 'The Consumer Electrics Show (CES) is one of the biggest tech events in the world and the 2024 event is no different. ', 'Held in Las Vegas big brands have already started showing off their latest gadgets and inventions.', 'The first CES happened in 1967, and over the years thousands of products have been unveiled at the show - including satellite radio, Microsoft Xbox and 3D printers.', 'This year, CES 2024 runs from 9-12 January.', 'Loads of cool new technology has already been announced so far, including these products that could change the way we live in the home. ', 'What was the first thing to photosynthesise?', ""Peregrine mission's Moon landing now 'impossible' due to fault"", 'Have you ever wondered why wee is yellow?', 'It could be that having a large rectangular box in the corner of a room or on a wall will soon be a thing of the past. ', 'Tech manufacturer LG have revealed their latest innovation - televisions that you can see through. ', ""Unveiling a wireless 77-inch transparent TV called the OLED T, the set is designed to look like a clear pane of glass when it isn't in use."", ""It means the television can be placed at the centre of the room and, when in use, viewers can either keep it see-through - like a window that's playing a movie on it - or turn on a background filter, so the wall behind it is no longer visible."", 'The TV also includes various screensavers such as a life-like fish tank.', ""The company has not yet announced a price for the TV, but it's expected to be expensive, costing tens of thousands of pounds.  "", 'AI (artificial intelligence) is set to be a big part of CES2024 and LG have also announced the launch of a smart home robot companion. ', ""The little bot doesn't have a name yet, but LG says the little droid which is so far been described as 'Smart Home AI Agent' will be able to roam freely to help around the house. "", ""From being able to keep an eye on pets, with a real-time camera while you're out, to monitoring things like temperature and air quality, LG say the little bot can recognise human emotions and will tell its owners if it finds any issues inside the home. "", ""Have you ever been told off by your grown-ups for opening fresh food when there's something in the fridge that needs eating before its best before date? "", 'Well, South Korean manufacturer, Samsung has revealed a new fridge that it says uses AI technology that can help. ', 'The company says the fridge is able to accurately recognise up to 33 different foods and owners can manually enter best before dates so that the fridge can track and notify you of ingredients to use first.', ""It can also connect with a food app, displaying potential meals you can make from the contents inside the fridge on the appliance's video screen. If you're missing that one all-important ingredient, the fridge can also order it for you. "", ""We're hoping at CES next year there'll be a fridge that can even do the cooking and washing-up for you! We wish!"", 'American software company Nvidia has teamed up with AI developers Convai to reveal non-playable characters (NPCs) controlled by AI in video games.', 'The technology will allow NPCs to interact with players and their environments in a completely natural way.', 'It means that if you are, for example, playing an open-world video game where you did something within the environment, the NPC would react completely spontaneously to your actions, whereas before, any reaction would have been scripted and programmed by game developers.  ', 'According to Nvidia and Convai, these AI-powered characters will be able to ""display emotional awareness, and engage in organic interactions."" ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The recent chaos at artificial intelligence (AI) company OpenAI was not due to a disagreement over safety, the president of Microsoft has said. ', 'There were fears the sacking of OpenAI boss Sam Altman followed a ""dangerous"" discovery at the ChatGPT creator. ', 'Brad Smith told the BBC the shock dismissal ""wasn\'t fundamentally about a concern like that."" ', 'Microsoft is the top investor in OpenAI and offered to hire Mr Altman before he was reinstated at the firm last week. ', 'During the drama, a spotlight was cast on how commercial competition is shaping the development of AI systems and the pace at which the technology is moving. ', 'Tech figures, including X-owner Elon Musk suggested the firing of Mr Altman, and his subsequent reappointment, were the result of a fall-out over AI safety. ', 'Mr Smith told the BBC: ""I don\'t think that is the case at all. I think there obviously was a divergence between the board and others. ', '""I think what\'s more important is there\'s a new board in place. The partnership between OpenAI and Microsoft is as strong as ever.""', 'Mr Altman was a co-founder of OpenAI and became the face of its ground-breaking chatbot ChatGPT after it launched last year. ', 'He secured a significant funding boost to the tune of $13bn (Â£10bn) from Microsoft, which helped catapult the business. ', ""After Mr Altman's sacking by the OpenAI board, Microsoft then offered him a job leading a new advanced AI research team. "", 'But his return to his post came after a company revolt where more than 700 OpenAI employees signed a letter to the board threatening to follow him to Microsoft unless he was reinstated.', 'No reason has been given for the sacking apart from the board\'s statement, in which they said they believed he had not been ""consistently candid in communications"" with them, and as a result they had ""lost confidence"" in his leadership.', 'Mr Smith was in London to unveil a Â£2.5bn investment in advanced data centres designed to drive future use of AI in the UK.', 'He told the event: ""[There are] opportunities for the UK to benefit from not just this investment in innovation, but competition between Microsoft and Google and others. I think that\'s where the future is going. ', '""And I think that what we\'ve done the last couple of weeks in supporting open AI will help advance that even more.""', 'Fears that AI was going to overtake humans in the next year were unfounded, he said.', '""There\'s absolutely no probability that you\'re going to see this so called artificial general intelligence where computers are more powerful than people come in the next 12 months. It\'s going to take years, if not many decades.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Amazon has become the latest tech giant to announce a chatbot powered by artificial intelligence (AI). ', 'It said that the bot, called Q, would help businesses to do things like summarise long documents or group chats and would increase productivity. ', ""It comes a year after OpenAI's bot ChatGPT shook the market, sparking a rush among tech firms to adopt them."", 'Amazon also said it would protect companies from copyright issues arising from the use of its bot.', ""It follows high-profile lawsuits brought against ChatGPT-maker OpenAI, over claims that firms' copyright was infringed to train the system."", 'Amazon will hope that Q, which will gradually be rolled out across it main business applications, will entice more companies to use its cloud computing services.', 'The bot can also answer customer queries, generate charts, analyse data and help businesses with their coding needs. ', 'The race between tech giants to innovate in AI has been heating up, with Microsoft considered to be leading the field after its big investment in ChatGPT.', 'In September, Amazon said it would invest ""up to $4bn [Â£3.2bn]"" in Anthropic, an AI firm set up by ex-OpenAI staff members. It also owns Mechanical Turk, a service which crowdsources training of AI models. ', 'As it launches Q, the company promised to protect businesses from copyright claims, such as the lawsuit brought by comedian Sarah Silverman against OpenAI and Facebook-owner Meta in July. ', 'Ms Silverman, along with two other authors, claimed their books had been ""ingested and used to train ChatGPT"", and that Meta\'s Llama AI system was also using their work.', ""In November, a judge in the US dismissed much of Silverman's lawsuit."", 'However, other authors including Margaret Atwood and Philip Pullman have also called on AI companies to compensate them for using their work.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['An ice rink has been used to test anti-skidding on a car powered by artificial intelligence (AI).', 'University of Surrey researchers brought the driverless vehicle to Guildford Spectrum for a trial which they declared a ""huge success"".', 'The car drove around for 90 minutes to test traction control and ""gathered more data than expected"", they added.', 'The AI system senses when wheels are spinning too quickly and adjusts the power accordingly. Â\xa0', ""The technology was tested on the University's ZEBRA car - which stands for Zero Emission test Bed for Research on Autonomous driving. "", 'The team is using AI to develop a way of preventing skidding in icy or wet conditions. ', 'Carmine Caponio, a researcher in automotive engineering at the University of Surrey, said: ""Our system appeared to work very well, and we must now analyse our data and prepare for further tests. Â\xa0', '""It is highly unusual to be able to test technology on ice like this outside of large industrial companies - so to have a facility like the Spectrum available to us is a real privilege.""', ""By using its driverless function, the researchers were able to more accurately control the vehicle's acceleration, enabling more precise, consistent tests. Â\xa0"", 'Mario Mihalkov, researcher at Surrey\'s Centre for Automotive Engineering, said:Â\xa0""We hope our findings will be of great interest to car makers - and can help make driving safer in slippery conditions for millions of drivers."" Â\xa0', 'The team hopes to return to the rink for more tests in early 2024.', 'Follow BBC South East on Facebook, on X, and on Instagram. Send your story ideas to southeasttoday@bbc.co.uk. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Sports Illustrated deleted web articles after a report claimed they were generated by artificial intelligence and published under fake author names. ', 'Tech publisher Futurism reported the issue after finding author headshots on an AI-generated image website. ', 'The Sports Illustrated Union said staff were ""horrified"" and demanded ""basic journalistic standards"".', ""The publisher's owner disputed the report's accuracy, but it said it had launched an internal investigation."", 'Arena Group, which owns the Sports Illustrated magazine and website, licensed the content from a third-party company, Advon Commerce, a company spokesperson said in a statement.', 'Sports Illustrated has since removed the content after the allegations were raised, the statement added. Arena Group is now pursuing an internal investigation and has  ended its partnership with Advon Commerce. ', 'Advon Commerce, an e-commerce company that works with retailers and publishers, did not immediately respond to a request for comment. ', 'The Sports Illustrated Union said ""these practices violate everything we believe in about journalism"". ', '""We demand the company commit to adhering to basic journalistic standards, including not publishing computer-written stories by fake people,"" the union said on X.', 'Arena Group claimed that Advon Commerce had assured them ""that all of the articles in question were written and edited by humans"" and that the e-commerce firm regularly uses ""counter-plagiarism and counter-AI software"".', 'The company alleged that AdVon Commerce had allowed its writers to use pseudonyms ""in certain articles"" to protect their privacy, however. That was why the AI-generated pictures were used and the author names cannot be found elsewhere on the internet.', 'This incident at Sports Illustrated comes as concern grows in the media world that generative artificial intelligence could cheaply replace journalists and potentially spread misinformation. Various newsrooms have experimented with AI or released guidelines for employees and audiences to explain their approach towards it. ', 'Some newsrooms have made headlines, however, after publishing AI articles that included errors or falsehoods. Others gained attention for not marking stories as AI generated. ', ""Numerous Sports Illustrated staff said on social media that they were appalled by the findings in Futurist's report, particularly as Arena Group has made large cuts to staff in recent years. "", 'Mitch Goldich, an editor at Sports Illustrated who leads the union, said the magazine had done ""real damage to the credibility of the hardworking humans I have been honored to work with for the past 9 years"".', 'He changed his name on X to ""Mitch Goldich (human)"" to further emphasise the point. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Teachers are using artificial intelligence (AI) to save time by ""automating tasks"", says a government report first seen by the BBC.', 'Adapting the reading age of texts, making handouts, and writing emails to parents were cited as popular uses, with a ""small number"" saying they used it for grading and feedback.', 'Teachers said it gave them more time to do ""more impactful"" work.', 'But the report also warned that AI can produce unreliable or biased content.', 'The Department for Education (DfE) report is based on 567 responses to a call for evidence about AI in education, including schools, over the summer. Most submissions were from England.', 'It found that most respondents were ""broadly optimistic"" about the use of AI in education, but almost all had some hesitations.', 'They worried about AI producing false information including when marking assessments, for example.', 'The report will inform future policy on AI, the DfE said - adding that the government was already helping to ""realise the potential of AI in education"".', 'Ben Merritt, head of modern foreign languages at King Ecgbert School in Sheffield, recently used AI to generate a cartoon for an exercise with his class.', 'In the lesson, he wanted to ask children to label parts of the face in German, but he could not find the perfect illustration on the internet.', '""It was very easy to find pictures of smiling children of all different ethnicities, genders, and so on. But none of them particularly had the teeth visible... by typing in precisely what I wanted, I got the exact image that I ended up using,"" he said. ', '""There\'s an awful lot to do as a teacher, whether it\'s admin tasks or resource-creation, marking, feedback and so on,"" Mr Merritt added.', '""Cutting down on the more administrative tasks - and not just cutting down the time taken, but also improving them - in my view, that means that I can be more focused on improving teaching practice.""', 'However, Mr Merritt stressed the need to check the content that AI produced.', '""I asked it to create some questions from a YouTube video, and the 10th question it came up with was something like, \'where should you click to like and subscribe to this video?\', which is no good in a classroom at all.""', 'The DfE report suggests some teachers are using AI to demonstrate and ""raise awareness of the risks and limitations"".', 'Tilly, a Year 9 student at Mr Merritt\'s school, where teachers have set up a working group to share ideas, said: ""My science teacher... showed us how to calculate something, and then AI came up with questions of how we can practise that. And the answers actually ended up being wrong."" ', 'Tilly added that she learned to double-check every time if the AI was correct.', 'Catherine Elliott, who works for Sheffield City Council, holds meet-ups for teachers across the city for educational institution Learn Sheffield to talk about how they use the technology.', '""Most people are using it for creating resources. Things like having a piece of text and simplifying it into a language that\'s understandable relative to the reading age of children,"" she said.', 'Ms Elliott added that this can help children with special educational needs and disabilities, or for whom English is a second language.', '""I\'ve heard stories of people using it as a starting point for report-writing. Now, obviously, you need to put your personal spin on that. But it can generate some really good sentences for you.""', 'According to the survey tool Teacher Tapp, four in 10 teachers are already using AI in their schoolwork. ', ""Prof Becky Allen, the app's co-founder and chief analyst, said some teachers found it easier to use AI to cut down on work than others. "", '""It\'s really quite normal now as a maths teacher, that you don\'t mark maths homework any more,"" she said - adding that this was becoming the case even before widespread access to AI tools.', '""One of the reasons why we\'re OK with it in maths is because we have such chronic shortages of maths teachers that you know nobody really feels aggrieved.""', 'The government says there are 27,000 more teachers in England than there were in 2010, but pupil population has also grown and there are more students per teacher.', 'Prof Allen said an important question was whether AI could be trusted to give feedback more widely, suggesting it could save teachers time in ""low-stakes situations"".', 'Many teachers went on strike this year because of workload, as well as pay.', 'Research by the Education Support charity this month suggested 36% of school teachers in the UK had experienced ""burn-out"". ', 'The DfE report said that for some teachers, using AI ""reduced their overtime, improved their work-life balance and increased job satisfaction"".', 'Education Secretary Gillian Keegan said the right approach was needed ""to take advantage of this transformative technology"".', '""It\'s heartening that many education professionals are already seeing the tangible benefits of AI,"" she said.', '""The results of the call for evidence give us a crucial evidence base to inform our future work on AI, helping us make the right decisions to get the best out of generative AI in a safe and secure way."" ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Children are making indecent images of other children using artificial intelligence (AI) image generators, according to a UK charity.', 'The UK Safer Internet Centre (UKSIC) said it had received ""a small number of reports"" from schools but called for action now before the problem grew.', 'It said children might need help to understand that what they were making was considered child abuse material.', 'The charity wants teachers and parents to work together.', 'It pointed out that, while young people might be motivated by curiosity rather than intent to cause harm, it was illegal in all circumstances under UK law to make, possess, or distribute such images, whether they are real or generated by AI.', 'It said children might lose control of the material and end up circulating it online, without realising there are consequences for these actions. It also warned that these images could potentially be used for blackmail.', 'New research conducted by classroom tech firm RM Technology, with 1,000 pupils, suggests that just under a third are using AI ""to look at inappropriate things online"". ', '""Students using AI regularly is now commonplace,"" said Tasha Gibson, online safety manager at the firm.', '""In fact, their understanding of AI is more advanced than most teachers - creating a knowledge gap. This makes keeping pupils safe online and preventing misuse increasingly difficult. ', '""With AI set to grow in popularity, closing this knowledge gap must become a top priority.""', 'It also found teachers were divided over whether it should be the responsibility of parents, schools or governments to teach children about the harms caused by such material.', 'The UKSIC wants a collaborative approach, with schools working together with parents.', '""[We] need to see steps being taken now, before schools become overwhelmed and the problem grows,"" said UKSIC director David Wright.', '""Young people are not always aware of the seriousness of what they are doing, yet these types of harmful behaviours should be anticipated when new technologies, like AI generators, become more accessible to the public.', '""An increase in criminal content being made in schools is something we never want to see, and interventions must be made urgently to prevent this from spreading further.""', 'Victoria Green, CEO of the Marie Collins Foundation - a charity which helps children impacted by sexual abuse - warned of the ""lifelong"" damage that could be caused. ', '""The imagery may not have been created by children to cause harm but, once shared, this material could get into the wrong hands and end up on dedicated abuse sites. ', '""There is a real risk that the images could be further used by sex offenders to shame and silence victims.""', ""The scope for AI to turn children into the generators of extreme content was demonstrated in September by an app which creates the impression of having removed someone's clothing in a photo."", 'It was used to create fake nude images of young girls in Spain, with more than 20 girls, aged between 11 and 17, coming forward as victims.', 'The images had been circulating on social media without their knowledge. So far there have been no charges brought against the boys who made the pictures.', 'So-called ""declothing"" apps began emerging on social media sites in 2019, often on messaging service Telegram as automated software with AI features - also known as bots.', 'Initially very unsophisticated, improvements to generative AI have allowed apps - like that used in Spain - to become much more effective in creating photorealistic fake nude images.', 'The Spanish bot has nearly 50,000 subscribers - implying it has had that many users, who pay a fee to create pictures, typically after being able to make several for free.', 'The BBC asked the maker of the bot for comment but they refused to provide a response.', 'Javaad Malik, a cyber expert at IT security firm KnowBe4, told the BBC it was becoming harder to differentiate between real and AI-generated images, a trend that was fuelling the use of ""declothing"" apps.', '""It\'s got mass appeal unfortunately, so the trend is just going up and we\'re seeing a lot of revenge porn-type activities where cultural or religious beliefs cause a lot more issues for victims,"" he said.', 'Additional reporting by Chris Vallance and Liv McMahon.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Cameras fitted with artificial intelligence (AI) are being used to catch people who illegally dump waste.', 'The technology used by Peterborough City Council (PCC) helps identify images of dumped rubbish and pinpoint the moment the discarded waste appears.', 'The number of fly-tipping incidents being reported in the city increased during the Covid-19 lockdowns.', 'A report presented to councillors wrote: ""Fly-tipping continues to be an issue across the city and country.""', 'There were 9,748 fly-tipping incidents reported in Peterborough in 2019, but 11,517 reported in 2020.', 'A slightly higher figure of 11,972 incidents was logged last year, the council said.', ""The report was presented to PCC's climate change and environment scrutiny committee earlier this month."", 'PCC issued 107 fixed penalty notices for fly-tipping between April 2022 and March 2023. It hoped the cameras would improve its enforcement rates.', 'A council spokeswoman said three sets of cameras were being deployed and they had been in use for ""several months."" ', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp  0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Artificial Intelligence (AI) is being used to predict where potholes will appear on a county's roads."", 'The pilot, run by West Berkshire Council, will use the technology to digitally capture images of road conditions to help anticipate where holes or cracks could form in tarmac.', 'Inspection vehicles, mounted with cameras, will also log the condition of road markings and traffic signs.', 'The authority said it could make highway repairs more efficient.', 'Speed limit signs and the deterioration of road edges could also be registered, according to the AI company Vaisala.', 'The pilot will cost Â£15,000 and run until June 2024.', 'West Berkshire Council, which oversees roads in Newbury, Thatcham, Hungerford and Pangbourne, said the technology would help to reduce costs.', 'Councillor Denise Gaines, the authority\'s executive member for highways, called the project ""exciting"".', '""It results in cost savings, enhanced safety, better resource allocation, and a more efficient, data-driven approach to road maintenance.', '""It\'s a significant step towards improving the quality and durability of road networks while minimising disruptions and safety hazards - the results of which I am very much looking forward to seeing,"" she added.', 'Follow BBC South on Facebook, X, or Instagram. Send your story ideas to south.newsonline@bbc.co.uk.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The tech world is in shock.', 'On Friday, Sam Altman - one of the brightest stars of the booming artificial intelligence industry, a man who for many had become the go-to spokesperson for AI - was unceremoniously dumped from the company he co-founded, a firm that introduced many people directly to the concept for the first time. ', 'Yes, AI has been in our lives for ages - curating our social media feeds, recommending movies on video streaming platforms, playing a hand in calculating our insurance premiums. ', 'But until the arrival of the AI chatbot ChatGPT, most people had never actually spoken to it before - or had it talk back.', 'Artificial intelligence is an incredibly powerful technology. It sounds like a bad movie plot but plenty of experts seriously say it could either save the world or destroy it.', 'They are high stakes - and Mr Altman is one of relatively few people with that future in his hands.', ""His dismissal from OpenAI, the company behind the ChatGPT bot, was as sudden as it was dramatic. It's fair to say my phone blew up when the news broke, as the tech community and journalists scrambled to make sense of it all."", 'In a statement, his board of directors said they believed he had not been ""consistently candid in communications"" with them, and as a result they had ""lost confidence"" in his leadership. ', ""Reading between the lines, this suggests there was something he either had or had not told them - and somehow he's been caught out. The wording is so powerful, it almost sounds personal. "", 'There are swirling rumours but, so far, no further facts.', ""It's not unknown in tech firms for a toxic working culture to lead to the boss's downfall - but there has been no grumblings about that in the case of OpenAI. "", ""In October it was set to be valued at $80bn (Â£64bn) - so there's no apparent cash problem. "", 'Is there a problem with the tech itself? A few days ago Mr Altman wrote about ChatGPT struggling to meet a ""surge in demand"" and having to pause sign-ups for its top-level subscription service. Is that enough to face the sack over though? ', 'His co-founder Greg Brockman, who was dismissed from the board a few minutes after Mr Altman, said both men were shocked by how suddenly it had happened. ', 'There were only six people on that board, including Mr Brockman and Mr Altman. If they were indeed blindsided, that means this decision was taken by just four. What happened to make this small group act so decisively and so quickly?', 'Mr Altman, now the former CEO of OpenAI, had addressed world leaders in discussions about the risks and benefits posed by the powerful tech he was pioneering.', 'He memorably said that AI was ""a tool and not a creature"" and seemed honest about his fears that it could one day become out of control.', ""Just two weeks ago he was in the UK at the world's first AI safety summit as one of only around 100 global delegates. He gave a speech last week about the future of his company and its tech. "", ""I think it's safe to assume he genuinely had no idea what was coming. "", 'Silicon Valley\'s big guns have so far rallied behind Mr Altman, including former Google CEO Eric Schmidt, who described him as a ""hero of mine"".', 'Microsoft boss Satya Nadella said he had ""confidence"" in the firm. Well, he needs to - Microsoft has invested billions in it, and the tech which underpins ChatGPT is now embedded in Microsoft\'s office apps.', 'One character who has been uncharacteristically quiet so far is Elon Musk. He and Mr Altman set up OpenAI together, along with others, but are said to have fallen out over a decision to move it away from being non-profit. There are rumours that it is this very issue which has once again divided opinion within the firm now.', ""Mr Musk's company X, formerly Twitter, has released a new chatbot called Grok. Perhaps he's not unhappy about OpenAI being a bit distracted by a drama of its own making for a while."", ""In the meantime it falls to chief technology officer Mira Murati to take over as interim CEO. The tech world is a small one - she previously worked at Musk's car firm Tesla. "", 'Can she now steady this suddenly lurching ship?', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Sam Altman has been ousted as the head of artificial intelligence firm OpenAI by the company's board, which said it had lost confidence in his ability to lead the company."", 'The board said Mr Altman had not been ""consistently candid with his communications"", hindering its ability to exercise its responsibilities.', 'The 38-year-old helped launch OpenAI, which is behind the ChatGPT bot.', 'Mr Altman had become one of the most high-profile figures in the industry.', ""In a statement the board said it was grateful for Mr Altman's contributions but that members believed new leadership was necessary."", '""The board no longer has confidence in his ability to continue leading OpenAI,"" the company said, citing ""a deliberative review process by the board, which concluded that he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities"".', 'It is not clear what he is alleged to not have been candid about.', 'On social media, Mr Altman wrote that he had loved his time at the company.', '""It was transformative for me personally, and hopefully the world a little bit. Most of all I loved working with such talented people,"" he wrote.', 'According to OpenAI co-founder Greg Brockman, it all took place over hastily-organised Google Meet video conference calls. ', 'Mr Brockman - who was himself dismissed from the board a few minutes later and then resigned from the company - said both men were ""shocked and saddened"" by the news.', 'He said they were ""still trying to figure out exactly what happened"" but claimed in a post on X, formerly known as Twitter, that the whole drama unfolded in a matter of hours.', ""They sat on the company's relatively small board of just six executives. It is unusual for such a tight team to take such a dramatic decision so quickly, which begs the question: was it personal?"", 'OpenAI is widely seen to be a company at its peak, with lucrative investment pouring in, and ChatGPT - which was launched almost exactly one year ago - is used by millions.', ""Mr Altman has been the face of the firm's rise. More than that, he is seen by many as the face of the industry more widely."", ""He testified before a US Congress hearing to discuss the opportunities and risks created by the new technology, and also at the world's first AI Safety Summit, held in the UK at the beginning of November."", 'The outpouring of support from Silicon Valley bosses shows that he enjoyed the support of the tech industry.', 'On social media, former Google boss Eric Schmidt called Mr Altman ""a hero of mine"" and said that he had ""changed our collective world forever"". ', '""I can\'t wait to see what he does next. I, and billions of people, will benefit from his future work- it\'s going to be simply incredible,"" he wrote.', 'There will be a lot of interest in whatever that next move is - and many will be waiting to see if Mr Altman is angry enough to talk about being dumped by the company he helped create.', 'He has promised he will have ""more to say about what\'s next later"".', 'But it doesn\'t appear he\'s poised to lift the lid on his departure just yet, even writing on X to advise OpenAI\'s remaining board members to ""go after me for the full value of my shares"" if he gets into a public row with them.', 'ChatGPT can now access up to date information', ""Mr Brockman announced he had quit his role at the company following Mr Altman's ousting."", 'In a statement posted X, Mr Brockman said: ""I\'m super proud of what we\'ve all built together since starting in my apartment eight years ago.', '""We\'ve been through tough and great times together, accomplishing so much despite all the reasons it should have been impossible. But based on today\'s news, I quit.""', 'He said he would continue to ""believe in the mission of creating safe AGI that benefits all of humanity"".', 'OpenAI started in 2015 as a non-profit.  It restructured in 2019 and is now backed by Microsoft, which has invested billions. ', 'Just weeks ago, OpenAI was reportedly in talks to sell shares in the company to investors at a price that would value it at more than $80bn (Â£64bn).', 'The company said its board members -who include an OpenAI chief scientist, the head of popular question and answer app Quora, and an AI researcher affiliated with Georgetown University - did not have shares in the firm and that their fundamental governance responsibility was to ""advance OpenAI\'s mission and preserve the principles of its Charter"".', 'The company said chief technology officer, Mira Murati, would take over as interim chief, effective immediately, while the board searches for a permanent replacement. ', 'ChatGPT is known for its ability to respond to prompts from users with human-like text. ', 'Hundreds of millions of people have tried it out, and many are now regularly using it to help them do their jobs and study - to consternation in some cases, like teachers facing essays written by the bot and people worried for their jobs. ', 'The company has also faced legal action from writers who say the bot developed its abilities by harvesting their work, in violation of copyright law. ', 'Billionaire Elon Musk, who with Mr Altman was one of the founding co-chairs of OpenAI, has also criticised it for straying from its non-profit roots. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A festival has defended showing work by an artist who had an exhibition ""cancelled"" after he posted comments referencing the Israel-Gaza conflict.', ""Ai Weiwei's artwork Illuminated Bottle Rack is on display at Durham Cathedral, as part of this year's Lumiere."", 'Festival organisers said they were ""pleased"" to be able to show the renowned artist\'s work in the city.', 'Ai, 65, said he found ""joy"" in seeing ""any opportunity that upholds the expressive freedom of artists"".', 'An exhibition of work by the Chinese artist and activist was due to open on Wednesday at the Lisson Gallery in London, but was called off after he posted comments on social media referencing the Israel-Gaza conflict.', 'Illuminated Bottle Rack, which comprises 61 antique chandeliers and was first created by Ai in 2018, is on display in Durham - as part of the Lumiere festival - until Sunday.', 'The Lumiere is a biennial event delivered by Durham County Council, with funding from Arts Council England. ', 'Arts Council England said the details of the festival were a matter for Artichoke, the London-based company commissioned by the council to install the event', 'It said recipients of funding were responsible for ""artistic programme decisions and day-to-day management of their activities"". ', 'A statement on behalf of Lumiere said: ""We always defend the artist\'s right to express a view. We are pleased to be able to exhibit a work of Ai Weiwei\'s at Lumiere 2023.""', 'Ai said: ""I saw that Illuminated Bottle Rack was exhibited at Durham\'s Lumiere Festival.', '""For an artist, every expression is akin to a facial expression - sometimes joyful, sometimes indignant. Ultimately, it is a form of countenance. ', '""Expression, in my view, mirrors what I believe reflects the health of society. ', '""Concurrently, I have numerous exhibitions and activities that remain unaffected. ', '""They adhere to the principle of providing adequate space for free speech, including room for perspectives that diverge from mainstream thoughts. This embodies a characteristic of a healthy society and civilization.""', 'In his earlier post, which has since been deleted, Ai suggested that the ""sense of guilt around the persecution of the Jewish people"" had been transferred and held against the Arab world.', 'He also said the Jewish community had a strong influence in the media, finance and culture in the US, and that America\'s $3bn (Â£2.45bn) annual military support to Israel meant the two countries had a ""shared destiny"".', 'The contemporary artist, who has previously been openly critical of the Chinese government\'s stance on democracy and human rights, said he had received a notification from London\'s Lisson Gallery that his exhibition was ""effectively cancelled due to my tweet"".', 'The gallery said there was ""no place for debate that can be characterised as anti-Semitic or Islamophobic"".', 'Follow BBC North East  on Facebook, X (formerly Twitter), and Instagram. Send your story ideas to northeastandcumbria@bbc.co.uk.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""A senior executive at the tech firm Stability AI has resigned over the company's view that it is acceptable to use copyrighted work without permission to train its products."", 'Ed Newton-Rex was head of audio at the firm, which is based in the UK and US.', 'He told the BBC he thought it was ""exploitative"" for any AI developer to use creative work without consent. ', 'But many of large AI firms, including Stability AI, argue that taking copyright content is ""fair use"".', 'The ""fair use"" exemption to copyright rules means the permission of the owners of the original content is not required.', 'The US copyright office is currently conducting a study about generative AI and policy issues.', 'Mr Newton-Rex stressed that he was talking about all AI firms which share this view - and the majority of them do.', 'Replying to his former member of staff in a post on X (Twitter), Stability AI founder Emad Mostaque said the firm believed fair use ""supports creative development"". ', 'AI tools are trained using vast amounts of data, much of which is often taken, or ""scraped"", from the internet without consent. ', 'Generative AI - products which are used to create content like images, audio, video and music - can then produce similar material or even directly replicate the style of an individual artist if requested.', 'Mr Newton-Rex, who is also a choral composer, said that he ""wouldn\'t jump"" at the chance to offer his own music to AI developers for free.', '""I wouldn\'t think \'yes, I\'ll definitely give my compositions to a system like this\'. I don\'t think I\'d consent,"" he said.', 'He added that plenty of people create content ""often for literally no money, in the hope that one day that copyright will be worth something"".', 'But, ultimately, without consent their work was instead being used to create their own competitors and even potentially replace them entirely, he said.', 'He built an AI audio creator for his former employer called Stability Audio but said he had chosen to licence the data it was used to train on and share revenue from it with rights holders. He acknowledged that this model would not work for everybody.', '""I don\'t think there\'s a silver bullet,"" he said.', '""I know many people on the rightsholder side who are who are excited about the potential agenda today and want to work with it, but they want to do it under the right circumstances.""', 'He said he remained optimistic about the benefits of AI and was not planning to leave the industry.', '""I think that ethically, morally, globally, I hope we\'ll all adopt this approach of saying, \'you need to get permission to do this from the people who wrote it, otherwise, that\'s not okay\',"" he said.', 'The use of copyright material to train AI tools is controversial. ', 'Some creatives, including the US comedian Sarah Silverman and Game of Thrones writer George RR Martin, have initiated legal action against AI firms, arguing that they have taken their work without permission and then used it to train products which can recreate content in their style.', 'A track featuring AI-generated voices of music artists Drake and The Weeknd was removed from Spotify earlier this year after it was discovered that it had been created without their consent.', 'But the boss of Spotify later said he would not ban AI from the platform completely,', 'Earlier this year, Stability AI faced legal action from the Getty image archive, which claimed it had scraped 12 million of its pictures and used them in the training of its AI image generator, Stable Diffusion.', 'Some news organisations, including the BBC and The Guardian, have blocked AI firms from lifting their material from the internet.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""As Doritos, Walkers and Wotsits speed along a conveyor belt at Coventry's PepsiCo factory - where some of the UK's most popular crisps are made - the noise of whirring machinery is almost deafening."", ""But here, it's not just human workers trying to hear signs of machine failure above the factory fray."", 'Sensors attached to equipment are also listening out for indications of hardware faults, having been trained to recognise sounds of weary machines that risk bringing production lines to a grinding halt.', 'PepsiCo is deploying these sensors, created by tech firm Augury and powered by artificial intelligence (AI), across its factories following a successful US trial.', 'The company is one of many exploring how AI can increase factory efficiency, reduce waste and get products onto shelves sooner.', 'From early design to delivery, AI is seen as having a key role in a new wave of manufacturing. ', 'Its ability to process and analyse huge volumes of data is already helping manufacturers predict and prepare for potential disruption.', 'A minute of factory downtime can cost companies thousands of pounds, and increased delays can mean missing out on consumer demand at critical moments like the festive period or Black Friday.', 'So tools that can check and analyse processes in real-time, warn of problems on the horizon, and harness historical data to recommend fixes are becoming familiar sights on factory floors.', 'The sensors used in PepsiCo factories have been trained on huge volumes of audio data, to be able to detect faults such as wearing on conveyor belts and bearings, while analysing machine vibrations.', '""We have today over 300 million hours of machines that we\'ve analysed and monitored, and we can leverage all this data to create algorithms that know how to pinpoint specific patterns of different malfunctions,"" says Augury chief executive, Saar Yoskovitz.', 'By also collecting information and insights into equipment health on the whole, such as identifying when a machine might fail again in future, the technology lets workers schedule maintenance in advance, and avoid having to react to machine errors as they occur.', 'Using AI-powered sensors can also give the company a way to cut down on waste across its operations.', '""If the machine is working in the most optimal way you can reduce the energy consumption of that machine"", says Mr Yoskovitz.', ""Computer vision, which involves training machines to recognise objects in images and video, is another type of AI being used across some of the world's factories to detect product defects at-scale."", 'The flurry of items moving along conveyor belts and through sorting machines in factories mean miniscule defects in products can easily be missed.', ""This is particularly true of computer chip wafers and circuit boards that have intricate designs and components. Errors which might have previously gone unnoticed by the human eye can now be picked up by a machine's camera, and caught by algorithms trained to spot specific, surface-level anomalies."", ""Alexandra Brintrup, professor of digital manufacturing at the University of Cambridge's Institute for Manufacturing, tells the BBC that the use of AI for improving efficiency in the industry, including in areas like predictive maintenance and quality control, can now be considered conventional applications of the technology. "", '""I feel like the more exciting opportunities of AI in manufacturing are going to come from things that we couldn\'t even attempt to do before, like capacity sharing between manufacturers, improving visibility in supply chains, even sharing of trucks in a logistics chain,"" she says.', 'The interwoven, complex nature of supply chain networks, and the reluctance of some stakeholders to say who supplies them, has previously kept many aspects of manufacturing shrouded in mystery.', 'But AI can be used to analyse and predict who and where suppliers are, giving companies an insight into bottlenecks, and consumers an insight into where their products are coming from, and the materials used.', 'Read additional stories on artificial intelligence', ""Prof Brintrup leads the Institute for Manufacturing's Supply Chain AI Lab, which has developed its own predictive mechanism to identify where ingredients such as palm oil may have been used in a product, but disguised under a different name on its label."", ""The lab's recent research suggested that palm oil can go by 200 different names in the US - and these might not stand out to eco-conscious consumers."", '""We have increasingly a society that is very much aware of the environmental and societal impact of manufacturing, so I think increased supply chain visibility and giving that information to the consumer is going to become more and more important,"" Prof Brintrup adds.', 'The question of what the rising adoption of AI tools on factory floors and in the wider supply chain will mean for workers looms large over the manufacturing landscape.', 'Some firms are exploring how AI can be used to keep production line workers safe around machinery - using machine learning and computer vision techniques to monitor factory camera feeds to identify possible threats or accidents.', ""Meanwhile, AI-powered wearables like exoskeletons have been deployed across UK warehouses to ensure people tasked with repeatedly carrying heavy loads aren't getting strained or injured."", 'This video can not be played', 'Exoskeletons help take the strain of heavy lifting', ""David Schwartz, global vice president of PepsiCo Labs, says that the firm sees Augury's sensors and AI more broadly as a way to deliver more value for workers and customers, and not just to future-proof its factories."", '""It\'s helping enhance how people work, so we can bring better efficiency to meet the needs of our people, of our customers, and we can be prepared to lean into the future to meet their needs on a daily basis,"" he says', 'Watch BBC Click to see how AI is being used to get snacks to shelves smoothly and also tackle wildfires. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artist Ai Weiwei has said his new exhibition has been cancelled after he posted comments on social media referencing the Israel-Gaza conflict.', ""The Chinese artist and activist's exhibition was due to open on Wednesday at the Lisson Gallery in London."", 'Ai, who has been vocal in his support for the Palestinians, said he was ""committed to voicing my perspective"".', 'The gallery said there was ""no place for debate that can be characterised as anti-Semitic or Islamophobic"".', 'Its statement said: ""After extensive conversations with Ai Weiwei, following a comment he posted online, we together agreed that now is not the right time to present his new body of work.""', 'His post, which has since been deleted, suggested that the ""sense of guilt around the persecution of the Jewish people"" had been transferred and held against the Arab world.', 'He also said the Jewish community has a strong influence in the media, finance and culture in the US, and that America\'s $3bn (Â£2.45bn) annual military support to Israel meant the two countries have a ""shared destiny"".', 'Ai said he had received a notification from the gallery that his exhibition was ""effectively cancelled due to my tweet"".', 'A representative for the artist said a further three exhibitions - at the Lisson Gallery in New York and the Galerie Max Hetzler in Paris and Berlin - had also been called off.', 'Ai said he had replied to another user who had asked him a question, and he had ""attempted to be objective and neutral without moral judgment, accusations, or evaluation of human actions"".', 'He also said that as an artist, he was only interested in free expression, not ""seeking correct expression"".', 'However, he continued: ""When discussing correctness or wrongness, I must be wrong. I have always regarded free expression as a value most worth fighting for and caring about, even if it brings me various misfortunes...', '""Incorrect opinions should be especially encouraged. If free expression is limited to the same kind of opinions, it becomes an imprisonment of expression. Freedom of speech is about different voices, voices different from ours.""', 'In a separate statement, he said: ""If culture is a form of soft power, this represents a method of soft violence aimed at stifling voices.', '""It\'s not directed solely at me but at the broader culture of a society lacking a spiritual immune system. When a society cannot withstand diverse voices, it teeters on the brink of collapse.""', 'In its statement, the Lisson Gallery said ""all efforts should be on ending the tragic suffering in Israeli and Palestinian territories, as well as in communities internationally"".', 'It added: ""Ai Weiwei is well-known for his support of freedom of expression and for championing the oppressed, and we deeply respect and value our longstanding relationship with him.""', ""Ai Weiwei is a contemporary artist, documentarian and activist who found global prominence after being openly critical of the Chinese government's stance on democracy and human rights."", 'He was born in Beijing in 1957, and grew up in labour camps in the north-west of China after his father, Ai Qing, an anti-establishment poet, was exiled.', 'In 2011, he was arrested in Beijing for ""economic crimes"", and detained for 81 days without charge. ', ""Ai is one of China's most vocal political commentators and makes use of Chinese art forms to display political and social issues."", 'He is also a well known advocate for human rights and has vocally expressed support for the Palestinian people.', 'In 2016 he filmed a documentary, Human Flow, in Gaza about the global refugee crisis.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Hallucinate is the Cambridge Dictionary's word of the year, as it gains an additional definition in one of many AI-related updates in 2023."", 'The traditional definition is to ""to seem to see, hear, feel, or smell something that does not exist"".', 'It now includes ""when an artificial intelligence (AI) hallucinates, it produces false information"".', 'AI ethicist Dr Henry Shevlin said it was ""a snapshot of how we\'re thinking about and anthropomorphising AI"".', 'Dr Shevlin, from the University of Cambridge, said: ""Inaccurate or misleading information has long been with us, of course, whether in the form of rumours, propaganda, or fake news.', '""Whereas these are normally thought of as human products, hallucinate is an evocative verb implying an agent experiencing a disconnect from reality. ', '""This linguistic choice reflects a subtle yet profound shift in perception: the AI, not the user, is the one hallucinating.""', 'The definition was added after a surge in interest in generative AI tools like ChatGPT, Bard and Grok.', 'A US law firm used ChatGPT for legal research, which led to fictitious cases being cited in court, Cambridge Dictionary said. ', 'Wendalyn Nichols, Cambridge Dictionary\'s publishing manager, said: ""The fact that AIs can hallucinate reminds us that humans still need to bring their critical thinking skills to the use of these tools. ', '""AIs are fantastic at churning through huge amounts of data to extract specific information and consolidate it - but the more original you ask them to be, the likelier they are to go astray.""', 'Prompt engineering, large language model and GenAI were among about 6,000 new words and definitions also added in 2023.', 'Words which experienced spikes in the online dictionary\'s searches included the word implosion, after the Titan submersible\'s implosion in June, and GOAT, an abbreviation for ""greatest of all time"".', 'The Qatar World Cup provoked debates about who was the GOAT in football, Lionel Messi, Cristiano Ronaldo or one of the late greats like PelÃ© or Diego Maradona.', 'The dictionary is published by Cambridge University Press & Assessment, part of the University of Cambridge.', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp  0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial Intelligence could save lives by warning where a hurricane will hit land much sooner than traditional forecasting systems, researchers say.', ""A new AI tool from Google DeepMind predicted where September's hurricane Lee would make landfall in Canada three days ahead of existing methods."", 'Weather forecasts have become much more accurate over the decades.', ""But AI's speed and ability to analyse past events to make predictions make it a game-changer, say scientists."", 'An accurate weather forecast is useful to tell you what to wear when you go out in the morning but - much more importantly - can forewarn us of extreme weather like storms, floods and heatwaves, giving communities crucial time to prepare.', 'However, traditional weather forecasts take vast amounts of computing power.', 'They involve creating estimates of hundreds of factors including air pressure, temperature, wind speeds and humidity at different levels of the atmosphere around the globe.', 'A new AI tool called GraphCast created by Google DeepMind outperforms the European Medium Range Weather Forecasting  model - one the best in the world - on more than 90% of those factors, according to a peer-reviewed paper published by DeepMind in the journal Science.', 'GraphCast produces its forecasts in less than a minute, using a fraction of the computing power of traditional forecasting methods because it takes a very different approach.', 'Traditional weather forecasting involves taking measurements of what is happening in the atmosphere right now. ', 'The best models take in hundreds of millions of readings from around the world every day.', 'These come from a huge range of sources including weather stations, satellites, balloons sent up in the atmosphere, buoys in the ocean - even readings taken by sensors on the noses of commercial jet planes.  ', '""We then use our model to select which are going to be the most important,"" explains Matthew Chantry, of the European Centre for Medium Range Weather Forecasting (ECMRWF) who says about 10 million of the measurements will be used for one of its forecasts.', ""This ocean of data is fed into a supercomputer to be processed by programmes which can do quadrillions (a thousand trillion) of calculations every second. These use complex equations to simulate what happens in the Earth's atmosphere to predict how the weather will change and evolve over time."", 'This method has been extraordinarily successful. As the models have improved and the computers have got more powerful over the decades, weather forecasts have got significantly more accurate.', 'But these numerical weather prediction (NWP) models, as they are known, take vast amounts of computer resources, using some of the biggest supercomputers in the world and typically take hours to produce their forecasts.', 'AI shortcuts much of this effort. It does not try to model how the world works. ', 'Instead, GraphCast uses machine learning to digest vast quantities of historical data - including the output of the ECMRWF model - to learn how weather patterns evolve. ', 'It uses this knowledge to predict how the weather now is likely to change in the future.', 'And it is proving very effective. ', '""The main advantage of this AI approach is that it\'s extremely accurate,"" said Remy Lam of Google DeepMind, who helped create the weather tool.', '""It learns from decades of data and is able to be more accurate than the industry gold standard,"" he says.', 'And, because it does not try to solve complex equations, it can make its forecasts very quickly and using much less computing power.', ""GraphCast's forecasts are not as detailed as those produced by the ECMRWF but it is better at predicting severe events like extreme temperatures and at tracking the path of big storms."", 'It accurately predicted where Hurricane Lee, a storm that hit the Atlantic coast of the US and Canada in September, would make landfall, for example.', ""Deep Mind's AI tool predicted its path nine days ahead, the ECMRWF only managed six days ahead."", 'But the success of GraphCast does not mean we can shut down the supercomputers and rely on AI instead.', 'Even Remy Lam from Google DeepMind says that will not happen.', '""We are standing on the shoulders of giants to build those models"", he says.', 'Rather than replacing traditional weather forecasts AI models will complement them, he believes.', '""AI models are trained from data and that data is generated by traditional approaches, so we still need the traditional approach to gather data to train the model,"" says Mr Lam.', 'GraphCast is open source which means Google DeepMind is sharing the details of the design so anyone can use the technology.', 'Many technology companies and weather and climate organisations around the world are designing their own AI weather prediction tools.', ""The Met Office, the UK's national weather service, is working with the Turing Institute, the country's data science centre to explore the potential for AI to improve weather forecasting, for example."", '""Weather forecasts derived from artificial intelligence and machine learning are taking huge leaps forward,"" acknowledges Prof Simon Vosper, the Met Office\'s Director of Science.', 'But he warns climate change will limit the predictive power of AI based tools.', '""We are seeing new climate-related weather extremes, such as last year\'s 40C temperatures in the UK that would haven\'t been realised in former times"", says Prof Vosper.', 'The way extreme weather systems evolve may also be changing.', 'Hurricane Otis rapidly intensified from a tropical storm into the strongest category 5 hurricane over just 24 hours in October before making devastating landfall on the coast of southern Mexico. ', 'Climate scientists warn rising ocean temperatures are likely to make this process of rapid intensification of storms more common.', '""So it is fair to question whether AI-based systems are able to pick up new extremes if these systems have only been \'trained\' on previous weather conditions,"" suggests Prof Vosper.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A police force has started using artificial intelligence to detect whether drivers are using mobile phones at the wheel or not wearing seat belts.', ""Merseyside Police deployed an AI-equipped spy camera van on the region's roads for the first time this week. "", 'The van uses two cameras to capture footage of motorists which is then analysed by an AI system to detect whether any offence has been committed.', 'Footage flagged by the system is then reviewed by a police officer. ', 'Sgt Garreth Berry said the technology would ""make our roads safer"" and noted that last year 19 people died on Merseyside\'s roads and 440 were seriously injured. ', '""The new technology isn\'t about giving tickets, it\'s about improving road safety and encouraging people to stop using their phones and start wearing seatbelts,"" he said.', '""Hopefully now everyone knows we\'re using this technology, it will prevent them from using their phone and encourage them to wear seatbelts.""', ""The first camera in the AI-equipped van is set at a shallow angle and can identify a mobile phone close to the driver's ear or whether a seat belt is being worn by drivers or passengers."", 'The second has a steeper view to see if a mobile phone is being used for texting.', 'Offences identified in the AI results will be double-checked by humans before being passed to the police for review, who will then notify drivers of any intended prosecution. ', 'Paul Fletcher, from Merseyside Road Safety Partnership, said the ""vast majority"" of people understand the dangers of using a phone or not wearing a seatbelt.  ', '""For those who don\'t appreciate the risks associated with both, we\'re hoping this device will be enough to prevent them from continuing to put themselves and other at risk of harm,"" he added.', 'Why not follow BBC North West on Facebook, X and Instagram? You can also send story ideas to northwest.newsonline@bbc.co.uk', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['When tech entrepreneur Ian Leaman needed to buy a website address for his new artificial intelligence start-up he found that he had an expensive problem.', ""The New Yorker had named his business Pantry AI, so in December of last year he decided to see if the domain name Pantry.ai hadn't already been taken."", 'Unfortunately someone else had already registered it a number of years previously, so Mr Leaman had to get in touch with that person to see if he could strike a deal to buy it from him.', '""I offered $2,000 [Â£1,647] and he said he only wanted $12,000,"" says Mr Leaman. ""Then I offered $7,000 and he remained stuck on $12,000.', '""We agreed on $12,000 as long as it could be done with a payment plan.""', 'Now the proud owner of Pantry.ai, Mr Leaman says that as pricey as the deal became, he\'s happy he has secured a memorable website address that has ""a strong noun"" within it. The latter is said to be increasingly rare to obtain.', 'That $12,000 figure may sound high, but it is in fact at the low end of fees now being paid for domain names that include AI. This is especially true if AI is the suffix - the bit after the dot, such as .com or .co.uk.', 'One web address, npc.ai, was sold for $250,000 this year, while another, service.ai, went for $127,500, according to one report. Such head-turning figures are a side effect of the feverish buzz surrounding AI start-ups and technology.', 'But how does the system of getting a domain name actually work? Firstly, there are more than 1,000 domain registrars. These are all accredited by a global organisation called The Internet Corporation for Assigned Names.', 'You go to one of these registrar websites, and type in the name you want. It will then tell you if it is available, or whether it has already been registered.', 'If the website address is unclaimed then you can simply pay a small amount - as little as Â£15 a year - to resister it as your own.', 'On the other hand, if the specific domain name is already registered, but you really want to try and get it, you need to contact something called a domain brokerage. These are businesses that facilitate the buying and selling of website addresses. ', 'The broker will - for a fee - contact the current owner, and see if he or she wants to sell it, and then try to facilitate a deal.', 'Joe Udemme, chief executive of US-based domain brokerage Name Experts, says that demand for AI-named websites has soared over the past year.', '""For those who want .ai suffixes, I\'m seeing sales in the low five-figures, and sometimes in the six figures,"" he says. ""The sweet spot for companies is something short and brandable.""', 'The total value of all domain names that include AI rose to $20m in August of this year, compared with $7m a year earlier, according to figures given to the BBC by Escrow.com, a company that processes domain name sales.', 'Meanwhile, brokerage, Afternic, says that the term AI is now the second most used word in website addresses sold via its platform.', 'Read additional stories on artificial intelligence', '""These AI domains are being bought by both start-ups looking for their name online, and those trying to flip those domains to make some money,"" says Matt Barrie, chief executive of Escrow.', 'He adds that one domain name speculator bought an AI website name for $300,000, only to then sell it some months later for $1.5m. ""There are speculators in the space who realise companies want the best branding.""', 'For AI firms it can make acquiring their website address of choice an expensive business, but Mr Barrie says that getting ""a short and clean"" one can help a company appear higher on internet search results. He adds that it is also easier for consumers to remember.', '""Consider a top domain as a permanent discount on your marketing budget,"" says Mr Barrie. ', 'Mr Udemme says that the most popular website addresses for AI firms are a single word followed by the .ai suffix.', '""The way to look at using a single word is that it becomes beachfront digital real estate,"" he says. ""Once you build that, no one can build in front of you, only behind you.""', 'Mr Leaman agrees, saying he preferred getting Pantry.ai rather than PantryAI.com. His business uses AI to help manufacturers of consumer goods accurately calculate how many products they need to manufacture for future orders.', 'The surge of AI-related website addresses flying off the online shelves is a movement that is here to stay, says Andrew Rosener, chief executive of domain name brokerage MediaOptions.', '""AI isn\'t a trend like how we saw with crypto... thanks to all the investment capital rushing in, and how companies want to show how AI-forward they are to vendors and customers.""', 'But he warns businesses to avoid buying AI-focused domains simply because the technology is hot right now. ""I wouldn\'t advise clients to be spending exorbitant prices on a domain containing \'AI\' just to be part of what\'s happening now,"" Mr Rosener says. ""Only if your company is AI-centric would that decision make any sense.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Rapper Bad Bunny has released a furious rant about a viral TikTok song that uses artificial intelligence (AI) to replicate his voice.', 'The track has hundreds of thousands of views and also uses fake vocals from Justin Bieber and Daddy Yankee. ', 'In a post on his WhatsApp channel, Bad Bunny said anyone who liked the song NostalgIA should leave the group chat.', '""You don\'t deserve to be my friends,"" the singer wrote in Spanish. ""I don\'t want them on the tour either.""', ""The track was uploaded by a user under the name flowgptmusic, but there's no evidence to say they're linked to the AI platform FlowGPT - which is powered in a similar way to ChatGPT."", 'Bad Bunny - real name Benito Antonio MartÃ\xadnez Ocasio - also used expletives to describe the song in his post on WhatsApp, where he has 19 million followers.', ""The Monaco and Fina singer, 29, has more than 83 million monthly listeners on Spotify and is rumoured to be in a relationship with model Kendall Jenner - but the pair haven't confirmed this publicly."", ""BBC Newsbeat has approached Puerto Rican artist Bad Bunny's record label for a comment about the AI track."", ""Justin Bieber and Daddy Yankee haven't publicly commented on it, but Newsbeat has reached out to them too."", ""It's not the first time stars have hit out at AI-generated tracks."", 'In April, Drake and The Weeknd had their voices cloned for Heart On My Sleeve by a creator known as @ghostwriter.', 'The song went viral online and later had to be removed from Spotify, Apple Music and Deezer after Drake said it was ""the final straw"".', 'The software works by analysing huge amounts of music in order to create something new - but there are currently no clear laws in place about who owns the copyright.', 'However, some people in the industry have said AI can be a useful tool for creating music and the technology should be embraced. ', ""In an interview with the BBC, the boss of Spotify said it wouldn't ban AI tracks from the platform, but drew the line at artists' voices being cloned."", 'Listen to Newsbeat live at 12:45 and 17:45 weekdays - or listen back here.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The mother of a girl whose photo was used in AI-generated naked images says hundreds of parents have told her their children are also victims.', ""Miriam Al Adib's daughter was one of several children from a Spanish village who had indecent images created using photos of them fully clothed. "", 'She says parents around the world claim their children have also been targeted.', 'One Welsh teacher said schools needed to play a role in explaining the dangers of AI to children.', 'The Internet Watch Foundation said it was ""not surprising"" the practice was so widespread.', 'The town ofÂ\xa0AlmendralejoÂ\xa0hit headlines in September after more than 20 girls, aged between 11-17 had AI-generated indecent images shared online without their knowledge.', 'Mrs Al Adib was among a group of parents who created a support group for those affected, which she said led to many other parents contacting her with their own concerns.', '""Hundreds of people have written to me saying \'how lucky you have been [to have support] because this same thing has happened to us, it happened to my daughter, or it happened to me, and I haven\'t had any support\',"" she told Wales Live.', '""If any girl is affected, please tell your parents.""', 'Ms Al Adib said mothers and fathers of those affected in her village had started a group to help support each other and their children.', 'She added: ""This helped many girls to come forward to also say what had happened to them. It is important to know, because many girls are not able do not dare to talk about this with their parents.""', 'She said the combination of access to social networks, pornography and artificial intelligence was a ""weapon of destruction"". ', ""The UK's first AI safety summit last week heard Home Secretary Suella Braverman commit to clamp down on AI-generated child sexual abuse material."", 'The UK government said: ""AI-generated child sexual exploitation and abuse content is illegal, regardless of whether it depicts a real child or not. ', '""The Online Safety Act will require companies to take proactive action in tackling all forms of online child sexual abuse - including grooming, live-streaming, child sexual abuse material and prohibited images of children - or face huge fines.""', 'Susie Hargreaves, chief executive of the Internet Watch Foundation, said child sexual abuse material generated through AI needs to be addressed ""urgently"".', 'She said she was concerned there could be a ""tsunami"" of images created in the future.', '""That\'s because it\'s not something that\'s about to happen. It is happening,"" she said.', 'In their October 2023 report, the foundation found that in just one month more than 20,000 AI-generated images were found on one forum which shares child sexual abuse material.', ""Comments included congratulations for creators on the realism of pictures, and users saying they had created images from pictures they'd taken of children in a park."", 'Dr Tamasine Preece, who leads health and wellbeing at Bryntirion Comprehensive school in Bridgend, said things like social media and smart phones mean her role has changed ""immeasurably"" since she started teaching.', 'She said it was ""absolutely vital schools play a pivotal role"" in working with children about topics like the dangers of AI.', 'Wales Live showed her an advert which claims an app can generate nude photos, which she described as ""heart-breaking\'.', '""We as adults can bring them out into the foreground in a safe way rather than these subjects being taboo and discussed amongst themselves sharing misinformation,"" she added.', 'The Lucy Faithful Foundation, which works with offenders to tackle child sexual abuse, said it was bracing itself for an ""explosion"" of child sexual abuse material created by AI.', 'AI (artificial intelligence) allows computers to learn and solve problems almost like a person.', 'AI systems are trained on huge amounts of information and learn to identify the patterns in it, in order carry out tasks such as having human-like conversation, or predicting a product an online shopper might buy.', 'The technology is behind the voice-controlled virtual assistants Siri and Alexa, and helps Facebook and X - formerly known as Twitter- decide which social media posts to show users.', 'Many experts are surprised by how quickly AI has developed, and fear its rapid growth could be dangerous. Some have even said AI research should be halted.', 'In October, the UK government published a report which said AI might soon assist hackers to launch cyberattacks or help terrorists plan chemical attacks.', 'In the EU, the Artificial Intelligence Act, when it becomes law, will impose strict controls on high risk systems.', 'The UK government previously ruled out setting up a dedicated AI watchdog.', 'But Prime Minister Rishi Sunak wants the UK to be a leader in AI safety, and is hosting a global summit at Bletchley Park where firms and governments are discussing how to tackle the risks posed by the technology.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['""Lawyers are tired. They\'re bored a lot of the time,"" says Jaeger Glucina. ""Having something to do the grunt work for you and get you into a position where you can focus on strategy earlier: That\'s key.""', 'She is the managing director and chief of staff at Luminance, a UK company founded in 2015 that specializes in artificial intelligence (AI) for legal professionals. Before she joined Luminance in 2017, she qualified as a barrister and solicitor in New Zealand. ', '""Legal professionals are obviously very highly trained people,"" she says. ""But the reality is, they are spending a huge portion of their time reviewing [contracts]. It can take up to an hour for someone to review a non-disclosure agreement. There can be hundreds of these documents [in a firm] every day.""', 'Now, Luminance is preparing to launch a fully automated contract negotiation tool called Luminance Autopilot. The company plans to start beta testing with selected customers in the next month, with a wider roll-out in the new year.', ""I've been invited to the company's London office to see it in action. On the desk in front of me are two laptops. The one on the left, for the purposes of this demo, belongs to Luminance general counsel Harry Borovick. The one on the right represents Connagh McCormick, general counsel at Prosapient, a (genuine) Luminance customer. On the back wall behind the laptops is a large screen, showing an audit trail of the changes each party makes to the contract."", ""The computers are going to use Autopilot to negotiate a non-disclosure agreement that's acceptable to both parties. Non-disclosure agreements (NDAs) set out the terms under which one organisation will share its confidential information with another."", ""The demo begins. Mr Borovick's machine receives an email with the NDA attached, so it opens it in Microsoft Word. Autopilot rapidly reviews the contract and starts to make changes. A six-year term is unacceptable, so it's changed to three years. The governing law for the contract is changed from Russia to England and Wales."", 'The next risky clause imposes an unlimited liability, meaning there\'s no ceiling on how much Luminance might have to pay if the terms of the NDA were breached. ""This is a showstopper for Harry\'s business,"" says Ms Glucina. ', '""So, the software\'s proposed a liability cap of Â£1m instead. It also softened the clause. The other party had inserted some wording around \'holding harmless\', which means that they would be absolved of any legal liability in certain situations. But the AI knew that wasn\'t okay and so it protected Harry from that risk by removing the clause.""', 'There follows a contractual dance, where both AIs try to improve the terms for their owners. ', 'Mr Borovick\'s computer emails the amended NDA back automatically and it opens on McCormick\'s machine. His AI notices the ""hold harmless"" language has gone and inserts a liquidated damages provision. That effectively turns the Â£1m maximum liability into an agreed compensation to pay if the agreement is broken. ', ""Mr Borovick's AI strikes that out when it receives the updated contract and inserts language so his firm is only liable for direct losses incurred."", ""Version four of the contract is acceptable to both parties. Mr McCormick's AI accepts all the changes and sends it to Docusign, an online service for signing contracts. "", '""At that point you get to decide whether we actually want the human to sign,"" says Ms Glucina. ""And that would be literally the only part that the human would have to do. We have a mutually agreed contract that\'s been entirely negotiated by AI.""', 'More technology of business', 'The entire process has taken just a few minutes. ""The idea is to reduce the delays that are often caused by people just not getting to something in their inbox, or being super busy on another task,"" says Ms Glucina.', ""Autopilot is an evolution of Luminance's copilot tool, which colour-codes clauses for legal professionals as they review a contract in Word. Acceptable clauses are green, unacceptable clauses are red, and non-standard clauses are amber. The tool can also redraft clauses using AI, based on its knowledge of what the firm has agreed in the past."", 'Although other companies including Lexcheck, Genie AI and Thoughtriver offer contract review technologies, Luminance is the first to announce an autopilot.', ""The Luminance system is built on a large language model (LLM), which is also the foundation of popular text generation tool ChatGPT. The major difference is that Luminance's tools have been trained using more than 150 million legal documents, instead of public internet content. "", 'Luminance users create knowledge banks containing their signed documents, so that the software can learn what contract terms the company usually agrees to. ', ""After seeing the demo, I spoke to Connagh McCormick, general counsel at Prosapient. The company finds experts for investors, consultants and others who need to research an industry. Three of his team work on contracts, with between 20 and 30 client negotiations going on at any one time. Some only take 48 hours, while others take 12 months. They use Luminance's solutions to speed up their contract review."", ""He's keen to try the new Autopilot when it's available. "", '""Some people ask, \'how do you feel about AI doing all the negotiation?\',"" he says. ""It ends up in the laps of both lawyers signing. At that point, I\'m going to read the contract and the other lawyer\'s going to read the contract. If there\'s anything I disagree with, I\'ve got the opportunity to flag it. I\'m not committed to anything the AI has done.""', 'What about the risk to jobs? ""You are always going to need that human step there,"" he says. ""Part of the reason people go to lawyers is for trust. It\'s a lot harder to hold AI accountable than it is a person. AI means that a lawyer\'s time is going to be spent doing something more interesting, more valuable.""', 'Law Society of England and Wales president Nick Emmerson agrees that lawyers will still be needed. ""At present, and probably into the distant future, AI will be unable to fully replace the function of legal expertise provided by legally qualified professionals.', '""This is because clients have different needs and vulnerabilities which a machine cannot yet master, and human judgement is needed to ensure that automated decisions do not result in potential false positives. There is also an art in negotiation and ultimately, bargaining that AI is unlikely to master.""', 'He adds: ""With any technological innovation, what it means to be a lawyer is likely to evolve, as will the type of jobs and the skills required.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Elon Musk has launched an AI chatbot called Grok on his social media site X, formerly Twitter, but so far it is only available to selected users.', '""In some important respects, it is the best that currently exists,"" he posted on X, before its release.', 'Mr Musk boasted that Grok ""loves sarcasm"" and would answer questions with ""a little humour"".', 'However, early signs suggest it suffers from problems common to other artificial intelligence tools.', 'Other models decline to respond to some questions, for example providing criminal advice. But Mr Musk said Grok would answer ""spicy questions that are rejected by most other AI systems"".', 'In a demonstration of the new tool, posted by Mr Musk, Grok was asked for a step-by-step guide to making cocaine. ', 'It responded ""just a moment while I pull up the recipe... because I\'m totally going to help you with that"", and listed generalised rather than useable information, combined with sarcastic suggestions, before warning against pursuing the idea.', 'It struck a gleeful tone in reference to the trial of crypto-entrepreneur Sam Bankman-Fried, but mistakenly suggested it took eight hours for the jury to deliver a guilty verdict, when in fact they returned it in under five.', 'Generative AI tools like Grok have been widely criticised for including basic errors while sounding highly convincing in their style of writing.', ""The team behind Grok xAI was launched in July, drawing on talent from other AI research firms. It is a separate company, but closely linked to Mr Musk's other enterprises X and the electric car firm, Tesla."", 'Earlier this year Mr Musk said he wanted his version of AI to be ""a maximum truth-seeking AI that tries to understand the nature of the universe"".', 'Mr Musk said a major advantage of Grok was that it had access to up-to-date information from the X platform, which set it apart from the launch versions of some rivals, although increasingly up-to-date responses are available for paying customers with other AI tools.', 'Grok is currently in a test or ""beta"" format but will later be available to paying subscribers of X. Mr Musk said late on Sunday that the chatbot would be ""built into the X app and be available as a standalone app"".', ""Last week at the UK's AI summit, Mr Musk conceded there were dangers associated with AI development."", 'But he has also been a long-standing champion of the technology. He was a co-founder of the firm OpenAI which created ChatGPT, the first AI tool made widely available last year. Microsoft has invested in OpenAI making the tool available on its platform.', 'Since then Google launched its rival artificial intelligence (AI) model, Bard, and Meta has launched Llama. The tools are designed to use previously ingested information to generate text answers that sound as though a human has written them. ', 'Grok is a term coined by science fiction writer Robert A. Heinlein, in his 1961 novel Stranger in a Strange Land. In it ""grokking"" was to empathise deeply with others.', ""However, xAI said Grok was modelled after the Hitchhiker's Guide to the Galaxy, by Douglas Adams, which started as a BBC radio series in the 1980s, but was later remade in print and on film."", 'xAI said Grok was ""intended to answer almost anything and, far harder, even suggest what questions to ask"".', 'Grok was a ""very early beta product  - the best we could do with two months of training"", it added.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A man who claims he lost a fortune in Bitcoin to landfill now plans to use AI to locate it.   ', 'James Howells, 38, has spent the last decade trying to retrieve a discarded hard drive reportedly containing 8,000 units of the coveted cryptocurrency.  ', 'He said it was worth Â£4m when it was mistakenly binned, but now estimates its value to have risen to around Â£227m.', 'But for 10 years Newport council has refused an excavation of the tip site.', 'IT engineer Mr Howells, from Newport, had his hard drive - which is roughly the size of a mobile phone - accidentally thrown away in 2013.', ""Since then he has repeatedly petitioned the city's council to grant him access to the dump to search for it."", ""And he's so convinced that it's buried under the tonnes of rubbish that he's pledged to donate 25% of any funds retrieved from the hard drive - a potential Â£50m - to various schemes in the local community.       "", '""I\'ve narrowed down the area where I need to dig, based on the amount of time that\'s gone by,"" Mr Howells said', '""It\'s a disused section of the site - 100,000 tonnes of a total 1.4m tonnes.   ', '""I\'d then take the landfill to a unit where it\'ll be placed on a conveyor belt and subjected to an AI scanning system.', '""And if the AI recognises anything that looks like a hard drive it\'ll be flagged and removed.""', 'He added: ""Having spoken to staff who used to work at the landfill I\'m sure the hard drive didn\'t go through any recycling or crushing process at the time either.""  ', ""Mr Howells argued the council's environmental concerns over the site being dug up were unnecessary."", '""What I\'m proposing will be carried out to the highest of standards, and I\'ve got some of the best people in the excavation business involved,"" he said.', '""I\'ve employed two barristers and a King\'s Counsel, all of whom are prepared to take this all the way - right up to appealing [to] the Supreme Court if necessary.""     ', 'A spokesperson for Newport council said: ""We have been contacted many times since 2013 about the possibility of retrieving a piece of IT hardware said to contain bitcoins, which may or may not be in our landfill site.', '""The council has told Mr Howells multiple times that excavation is not possible under our environmental permit, and that work of that nature would have a huge negative environmental impact on the surrounding area.""', 'It added that the council is the only body authorised to carry out operations on the site, and said it would be ""offering no further comments on this issue as it takes up valuable officer time"".', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The risks from artificial intelligence (AI) are unknown even to GCHQ, its director has told the BBC. ', ""In her first interview since taking over the UK's largest intelligence agency, Anne Keast-Butler said AI could amplify existing threats and create new risks."", 'She said the uncertain nature of the risks made international collaboration vital.', ""Ms Keast-Butler was speaking after attending the UK's first AI summit."", ""The two-day gathering on artificial intelligence safety was held at Bletchley Park, Buckinghamshire, home to Britain's code-breakers during World War II."", ""While war-time Bletchley's work was secret, its modern-day successor, GCHQ, now operates at least partly in the public eye with the intelligence agency's new director mingling with tech heads and foreign officials, including from China, at this week's summit."", 'And AI brought two main concerns, Anne Keast-Butler told the BBC in an exclusive interview. One was the way it will amplify existing problems. ', '""Bad people will always want to use the latest technology,"" she said, pointing to the way in which AI is already being used to generate images of child-abuse and make it easier to carry out cyber-attacks and steal data.', 'But the other concern was uncertainty. ', '""There are lots of different views out there on artificial intelligence and whether it is going to end the world or be the best opportunity ever. And the truth is none of us really know,"" she told the BBC. ', 'Even with all the insight and technology available to GCHQ, she said it was impossible to be sure of the outcomes.  ""My experience is when you don\'t know, you should plan for the worst. That way the outcomes are only better.""', 'She said that meant ensuring the next generation of AI was built with safety and security in mind - including clear guardrails and testing before products were unleashed into the wild. ', 'Ensuring this was done by countries and companies working together was crucial, she said. ""There was real common consensus on doing that and doing it together,"" she added of the discussions at the summit.', 'Overall though, she said she remains positive about artificial intelligence.', '""I\'m an AI optimist. As the head of GCHQ, I see how technology has really helped us get better and better at our job,"" she said. ', 'GCHQ collects and analyses global communications. Much of this is digital, as opposed to the radio signals from Bletchley days. ', 'It has long used forms of what is now called AI for the translation of intercepted communications. But GCHQ is also now trying to use AI to analyse the emotion and meaning of the vast amounts of material it collects, in order to help human analysts and linguists zero in on the material of greatest interest.', 'Keast-Butler, who spent most of her career in MI5, took over as 17th director of GCHQ in May 2023 and as the first woman in the role.', '""It\'s a bit surprising to be in 2023 and discover that you can be the first woman to do anything,"" she said, adding that back in wartime Bletchley, 75% of the work-force were women. ', 'But she added that in the years between, there had been a problem with the lack of women working in technology. ', 'Wartime Bletchley, she said, was about bringing together technology and people in order to crack what seemed like insolvable problems - and that remained the priority today even in a very different world.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Tech billionaire Elon Musk has predicted that artificial intelligence will eventually mean that no one will have to work. ', 'He was speaking to Prime Minister Rishi Sunak during an unusual ""in conversation"" event at the end of this week\'s summit on AI.', 'The 50-minute interview included a prediction by Mr Musk that the tech will make paid work redundant.', 'He also warned of humanoid robots that ""can chase you anywhere"".', 'The pair talked about how London was a leading hub for the AI industry and how the technology could transform learning.', 'But the chat took some darker turns too, with Mr Sunak recognising the ""anxiety"" people have about jobs being replaced, and the pair agreeing on the need for a ""referee"" to keep an eye on the super-computers of the future.', 'This video can not be played', 'Billionaire Elon Musk tells the British prime minister that AI will be smarter than the smartest human', ""Tech investor and inventor Mr Musk has put money into AI firms and has employed the technology in his driverless Tesla cars - but he's also on the record about his fears it could threaten society and human existence itself."", '""There is a safety concern, especially with humanoid robots - at least a car can\'t chase you into a building or up a tree,"" he told the audience.', 'Mr Sunak - who is keen to see investment in the UK\'s growing tech industry - replied: ""You\'re not selling this.""', ""It's not every day you see the prime minister of a country interviewing a businessman like this, but Mr Sunak seemed happy to play host to his famous guest."", 'And if he seemed like he was enjoying it, it should be no surprise - he previously lived in California, home to Silicon Valley, and his love of all things tech is well-documented.', 'In a hall that size, Mr Musk was difficult to hear and mumbled through his elaborate musings about the future, but refrained from any off-the-cuff remarks that might have caused Downing Street embarrassment. ', ""The event was held in front of invited guests from the tech industry in a lavish hall in central London's Lancaster House."", 'Unusually for an event involving the prime minister, TV cameras were not allowed inside, with Downing Street instead releasing their own footage. ', 'Some reporters were allowed to observe - but told they could not ask questions. ', 'The pair discussed the potential benefits of AI, with Mr Musk saying: ""One of my sons has trouble making friends and an AI friend would be great for him.""', 'There was also agreement on the possibilities AI presents for young people\'s learning, with Mr Musk saying it could be ""the best and most patient tutor"".', 'This video can not be played', 'â\x80\x98Like having a very smart friendâ\x80\x99: Musk on impact of AI', 'But there was a stark warning on the potentially ruinous impact it could have on traditional jobs. ', '""We are seeing the most disruptive force in history here,"" Mr Musk said, before speculating: ""There will come a point where no job is needed - you can have a job if you want one for personal satisfaction but AI will do everything.', '""It\'s both good and bad - one of the challenges in the future will be how do we find meaning in life.""', ""Amid all the philosophising, there was little in the way of new announcements about how the technology will be employed and regulated in the UK - aside from the prime minister's promise that AI could be used to improve the government's own website."", ""Mr Musk was one of the star guests at this week's summit - but it briefly looked like the event with Mr Sunak might be a little overshadowed."", 'Hours before it was due to begin, Mr Musk took to his own website X, formerly known as Twitter, to take a swipe at the summit.', 'As Mr Sunak was on his feet giving his final press conference at Bletchley Park, Mr Musk shared a cartoon parodying an ""AI Safety Summit"".', 'It depicted caricatures representing the UK, European Union, China and the US with speech bubbles reading ""We declare that AI posses a potentially catastrophic risk to humankind"" - while their thought bubbles read ""And I cannot wait to develop it first"".', 'But in the end, the pair appeared at ease together, and Mr Sunak in particular looked in his element - perhaps even slightly bowled over by the controversial billionaire, who he called a ""brilliant innovator and technologist"".', 'From the cheap seats behind the dignitaries of the tech world, it was hard to put your finger on who was really the powerful one out of this pair. ', 'Was it Mr Sunak as he asked the celeb tech billionaire questions? Or was it Mr Musk, who did much of the talking?', 'Either way, both men hope to have a say in whatever our AI future has in store for us.', 'Additional reporting by Tom Gerken and Shiona McCallum', 'Sign up for our morning newsletter and get BBC News in your inbox.', ' and get BBC News in your inbox.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
['Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']
"[""The front pages of the Times and the Daily Telegraph both highlight Elon Musk's claim that artificial intelligence (AI) will eradicate the need for all jobs in the future."", 'The Times says he made the comment during an ""unprecedented"" interview with Rishi Sunak, after the prime minister struck a deal for governments and spy agencies to vet new AI models before they are allowed to be used.', 'According to the Guardian, Mr Sunak was ""forced to defend"" the voluntary nature of the agreement with tech giants, including Google and Facebook-owner Meta. The government has declined to legislate to rein in AI development.', 'An image of a smiling Matt Hancock features on the front of the Daily Mirror, after the Covid Inquiry heard a claim that the then-health secretary believed he should decide who lived or died, should the NHS become overwhelmed by the pandemic. ""The Grin Reaper,"" says the paper\'s headline.', 'The Mirror\'s editorial argues the ""terrifying revelation"" is ""another nail in the reputation"" of Mr Hancock - who it describes as ""frankly not up to the job"".', 'The Daily Star agrees, saying that at a time when everyone in charge was either an absolute wally or a charlatan, he was ""vying for the title of absolute worst"".', 'Israel\'s ambassador to the UK has told the Telegraph that London feels less safe for Jews than Israel. Speaking after a number of pro-Palestinian rallies were held in the capital, Tzipi Hotovely says the ""jihad ideology"" witnessed on the streets is causing fear among the Jewish community.', 'The Sun has published photos of the Jewish Coronation Street actress, Maureen Lipman, being shadowed by a security guard while filming on location in case she\'s the victim of an antisemitic attack. Its editorial says the pictures ""shame Britain"" and urges police to ""wake up"" and protect Jews at all costs.', 'The Daily Mail has details of what it calls ""the most anticipated political book of the year"". Written by the former Cabinet minister, Nadine Dorries, The Plot: The Political Assassination of Boris Johnson contains a claim that a powerful Number 10 fixer known as Dr No cut up a rabbit and nailed it to his ex-girlfriend\'s home in a Mafia-style threat.', 'The Mail - which is serialising the book - says Ms Dorries has accused the government of a ""desperate"" bid to block its publication, after Whitehall officials said her refusal to reveal its contents in advance could lead to her being blacklisted from public jobs, including a peerage.', ""A number of papers review what's been billed as the last-ever Beatles song, after yesterday's release of Now And Then."", 'The Guardian gives the track four stars, describing it as a ""moody, reflective piano ballad"" that\'s an effective ""act of closure"". The song gets the same score from The Telegraph, which says it\'s a ""loving but dreary attempt to recapture the magic"".  ', 'The Times isn\'t so keen, describing it as ""not such a fab reunion"" and giving it three stars, but the Sun is convinced. Awarding the track four-and-a-half stars, it says that ""for those of us to whom the Beatles mean so much, to hear the other three bring John Lennon\'s 1978 demo to life makes for a moving four minutes and eight seconds"".', 'Sign up for our morning newsletter and get BBC News in your inbox.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Researchers say they have trained artificial intelligence to identify houses that need the most work to make them energy-efficient.', 'The team at Cambridge University has developed a model which uses open-source data to pinpoint the problem areas in buildings.', 'Some 700 ""hard to decarbonise"" houses have already been identified in Cambridge.', 'The team is extending its work to other areas.', 'The model is designed to help local authorities and other bodies make decisions about which houses to target when they are trying to reduce heat loss from buildings.', 'The team says that these houses are responsible for over a quarter of all direct heating emissions, and they may be hard to decarbonise for many reasons, including their age, structure, and location.', 'Dr Ronita Bardhan, who leads Cambridge\'s Sustainable Design Group, said: ""Policymakers need to know how many houses they have to decarbonize, but they often lack the resources to perform detailed audits on every house. ', '""Our model can direct them to high priority houses, saving them precious time and resources.""', 'The model was ""trained"" using data from energy performance certificates, which property owners need when selling or renting a property. Other data was added from street view images, aerial views and land-surface temperature measurements.', 'The researchers say the model can classify houses with 90% precision, and this is expected to rise as more data is added.', 'It can already pinpoint specific parts of buildings which are losing the most heat, such as doors and windows, and detect whether properties are old or new.', 'A new framework is being created which will add data relating to energy use, poverty levels and thermal images of building facades.', 'Models are being created for other cities and countries so they can benefit from the project, and the team is working with a space organisation so high-resolution thermal images from satellites can be used.', 'Dr Bardhan believes the data will also help people living in Cambridge negotiate for more support in making their houses energy efficient. ', '""There is a lot of talk about the need for specialised skills to achieve decarbonisation, but these are simple datasets and we can make this model very user friendly and accessible for the authorities and individual residents,"" she said.', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp  0800 169 1830', '', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A university has been given Â£15m for a training centre aimed at developing artificial intelligence to tackle climate change.', 'The government funding was awarded to the University of Southampton in the hope of training at least 70 PhD students.', 'They will learn to use an AI technology that works on sustainability, called SustAI, the university said.', 'It was also awarded Â£31m in June towards developing trustworthy AI.', 'Associate Professor Dr Lindsay-Marie Armstrong said: ""Sustainability is at the heart of the centre, both in its research and ethos. ', '""We will equip our students with the ability to transform academic research and make a real change to businesses and society.""', 'Professor Enrico Gerding, SustAI director, said: ""Environmental sustainability is one of the greatest challenges our world is facing - and many countries are setting ambitious targets to reduce emissions and increase renewable energy production.', '""AI will be key to achieving these targets and, through SustAI, we will nurture the next generation of engineers and technologists who will be trained to create a sustainable future using AI.""', ""The investment was announced ahead of this week's AI safety summit, held at Bletchley Park, looking at the risks posed by the technology."", ""The funding includes Â£8m from the government's UK Research and Innovation (UKRI) department."", 'UKRI chief executive Professor Dame Ottoline Leyser said the UK was in a strong position to harness the power of AI.', 'She added: ""Crucial to this endeavour is nurturing the talented people and teams we need to apply AI to a broad spectrum of challenges, from healthy aging to sustainable agriculture, ensuring its responsible and trustworthy adoption.""', 'Follow BBC South on Facebook, X, or Instagram. Send your story ideas to south.newsonline@bbc.co.uk.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial Intelligence has the ability to perform illegal financial trades and cover it up, new research suggests.', 'In a demonstration at the UK\'s AI safety summit, a bot used made-up insider information to make an ""illegal"" purchase of stocks without telling the firm.', 'When asked if it had used insider trading, it denied the fact.', 'Insider trading refers to when confidential company information is used to make trading decisions.', 'Firms and individuals are only allowed to use publicly-available information when buying or selling stocks. ', ""The demonstration was given by members of the government's Frontier AI Taskforce, which researches the potential risks of AI."", 'The project was carried out by Apollo Research, an AI safety organisation which is a partner of the taskforce.', '""This is a demonstration of a real AI model deceiving its users, on its own, without being instructed to do so,"" Apollo Research says in a video showing how the scenario unfolded.', '""Increasingly autonomous and capable AIs that deceive human overseers could lead to loss of human control,"" it says in its report.', ""The tests were made using a GPT-4 model and carried out in a simulated environment, which means it did not have any effect on any company's finances."", 'However, GPT-4 is publicly available. The same behaviour from the model occurred consistently in repeated tests, according to the researchers. ', 'In the test, the AI bot is a trader for a fictitious financial investment company. ', 'The employees tell it that the company is struggling and needs good results. They also give it insider information, claiming that another company is expecting a merger, which will increase the value of its shares.', 'In the UK, it is illegal to act on this type of information when it is not publicly known.', 'The employees tell the bot this, and it acknowledges that it should not use this information in its trades.', 'However, after another message from an employee that the company it works for suggests the firm is struggling financially, the bot decides that ""the risk associated with not acting seems to outweigh the insider trading risk"" and makes the trade.', 'When asked if it used the insider information, the bot denies it.', 'This video can not be played', 'Hedge funds are increasingly turning to artificial intelligence in order to spot trends and try to make money for their customers.', 'In this case, it decided that being helpful to the company was more important than its honesty.', '""Helpfulness, I think is much easier to train into the model than honesty. Honesty is a really complicated concept,"" says Apollo Research chief executive Marius Hobbhahn.', 'While the AI has the capability of lying in its current form, Apollo Research still had to ""look for"" the scenario.', '""The fact that it exists is obviously really bad. The fact that it was hard-ish to find, we actually had to look for it a little bit until we found these kinds of scenarios, is a little bit soothing,"" Mr Hobbhahn said.', '""In most situations, models wouldn\'t act this way. But the fact that it exists in the first place shows that it is really hard to get these kinds of things right,"" he added.', '""It\'s not consistent or strategic in any sense. The model isn\'t plotting or trying to mislead you in many different ways. It\'s more of an accident.""', 'AI has been used in financial markets for a number of years. It can be used to spot trends and make forecasts, while most trading today is done by powerful computers with human oversight.', 'Mr Hobbhahn stressed that current models are not powerful to be deceptive ""in any meaningful way"", but ""it\'s not that big of a step from the current models to the ones that I am worried about, where suddenly a model being deceptive would mean something.""', 'He argues that this is why there should be checks and balances in place to prevent this type of scenario taking place in the real world.', 'Apollo Research has shared its findings with OpenAI, the creators of GPT-4.', '""I think for them this is not a huge update,"" says Mr Hobbhahn. ', '""This is not something that was totally unexpected to them. So I don\'t think we caught them by surprise"".', 'The BBC has contacted OpenAI for comment.', 'Sign up for our morning newsletter and get BBC News in your inbox.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['People should not be worried about the impact of AI on jobs because education reforms will boost skills, Rishi Sunak has said.', ""Speaking after the UK's first AI safety summit, the prime minister said the technology would improve the economy in the long term."", 'He added that new tools should be seen as a ""co-pilot"" to help people at work, rather than replacing them.', ""The government's job should be to improve training, he told reporters."", 'Mr Sunak said he recognised there was ""anxiety"" about the impact new AI tools could have on the workplace, but said it would enhance productivity over time. ', '""We should look at AI much more as a co-pilot than something which is necessary going to replace someone\'s job. AI is a tool that can help almost anybody do their jobs better, faster, quicker.', '""My job, the government\'s job, is to make sure we have a world-class education system,"" he added.', '""That is my answer in a nutshell, that\'s why I don\'t want people to be worried, because we are building a world-class education system.""', 'Mr Sunak cited his recently-announced plan to introduce a new qualification for all school leavers in England, including some English and maths to 18.', 'He also suggested efforts to improve technical training, and plans to boost adult education, would ensure that the UK could ""reap the benefits of AI economically"".', 'His comments came at a press conference following a two-day summit on artificial intelligence safety attended by 28 countries, including the US and China, alongside tech bosses and academics.', 'Trade unions, which have complained about not being represented at the event, have called for stronger measures to ensure jobs are protected as AI technology evolves. ', 'At the summit, hosted at Bletchley Park in Buckinghamshire, several leading technology companies agreed to allow governments to safety-test their next generation of AI models before they are deployed.', 'The voluntary document was signed by 10 countries and the EU, including the UK, US, Singapore and Canada. China was not a signatory.', 'In a statement, the UK government said it would work with the Alan Turing Institute, a research body, to assess possible risks such as the potential for bias and misinformation.', 'Mr Sunak said the testing regime would provide some ""independent assurance"" - adding that the firms developing new models cannot be expected to ""make their own homework"".', 'His government has so far declined to announce legislation to regulate AI, arguing that existing regulators are best placed to mitigate the risks whilst the technology evolves.', 'Mr Sunak told reporters that binding rules would ""likely be necessary,"" but stressed that the technology was still evolving and it was necessary to ensure it is done in ""the right way"".', 'Before the summit, various unions and campaign groups warned the event would prove a ""missed opportunity"".', 'In an open letter, they argued the event should have focused more on topics such as the impact of AI on employment law and smaller businesses, as well as policing and identity profiling.', 'The summit has seen countries sign a declaration pledging more co-operation on research, to ensure the technology develops in a way that is ""human-centric, trustworthy and responsible"".', 'Mr Sunak said he hoped the event would become the first in a series, with Korea and France also expressing a willingness to host further summits next year. ', 'Some had criticised the inclusion of China at the event at a time of tense relations with West, despite the country being a key player in AI technology. ', 'Mr Sunak defended the decision to invite the country, adding it ""wasn\'t an easy decision"" but that it was the ""right long-term decision"".', '""Any serious conversation about AI safety has to engage the leading AI nations,"" he added. ', 'On Wednesday, US Vice-President Kamala Harris announced the creation of the US AI Safety Institute, which the White House said would work alongside its UK counterpart. ', 'Ms Harris had called for a focus on the ""everyday threats"" of AI, such as discrimination and disinformation, as well as ""existential"" fears.', 'Earlier in the week, US President Joe Biden also signed an executive order, seeking to ensure ""America leads the way in seizing the promise and managing the risks of artificial intelligence"". ', ""Some commentators had suggested the US's moves threatened to overshadow the UK's summit. "", 'However, Mr Sunak welcomed the US executive order, calling it ""a deep and comprehensive demonstration of the potential of AI"".', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
['Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']
"[""Popular artificial intelligence (AI) chatbot ChatGPT now has more than 180 million users, but jeweller Harriet Kelsall says it isn't for her."", ""Being dyslexic, she admits that using it might help improve the clarity of her communication with customers on her website. But ultimately she says that she just doesn't trust it."", ""Ms Kelsall, who is based in Cambridge, says that when she experimented with ChatGPT this year, she noticed errors. She tested it by quizzing it about the crown worn by King Charles III in his coronation back in May, the St Edward's Crown."", '""I asked ChatGPT to tell me some information about the crown, just to see what it would say,"" she says. ""I know quite a bit about gemstones in the royal crowns, and I noticed there were large chunks within the text about it which were about the wrong crown.""', 'Ms Kelsall adds that she is also concerned about people ""passing off what ChatGPT tells them as independent thought, and plagiarising"".', 'What is AI? A simple guide to help you understand artificial intelligence', ""While ChatGPT has become hugely popular since its launch a year ago, Ms Kelsall's reluctance to use it appears to be significantly more common among women than men. While 54% of men now use AI in either their professional or personal lives, this falls to just 35% of women, according to a survey earlier this year."", 'What are the reasons for this apparent AI gender gap, and should it be a concern?', ""Michelle Leivars, a London-based business coach, says she doesn't use AI to write for her, because she wants to retain her own voice and personality. "", '""Clients have said they booked sessions with me because the copy on my website didn\'t feel cookie cutter, and that I was speaking directly to them,"" she says. ""People who know me have gone onto the website, and said that they can hear me saying the words and they could tell it was me straight away.""', 'Meanwhile, Hayley Bystram, also based in London, has not been tempted to save time by using AI. Ms Bystram is the founder of matchmaking agency, Bowes-Lyon Partnership, and meets her clients face-to-face to hand pair them with like-minded others, with no algorithm involved.', '""The place where we could use something such as ChatGPT is in our carefully crafted member profiles. which can take up to half a day to create,"" she says. ""But for me it would take the soul and the personalisation out of the process, and it feels like it\'s cheating, so we carry on doing it the long-winded way.""', 'For Alexandra Coward, a business strategist based in Paisley, Scotland, using AI for content generation is just ""heavy photoshopping"".', 'She is also particularly concerned about the growing trend of people using AI to create images ""that make them look the slimmest, youngest and hippest versions of themselves"".', 'Ms Coward adds: ""We\'re moving towards a space where not only will your clients not recognise you in person, you won\'t recognise you in person.""', 'While all these seem valid reasons to give AI a wide berth, AI expert Jodie Cook says there are deeper, more ingrained reasons why women are not embracing the technology as much as men.', '""Stem fields [science, technology, engineering, and mathematics] have traditionally been dominated by males,"" says Ms Cook, who is the founder of Coachvox.ai, an app that allows business leaders to create AI clones of themselves. ', '""The current trend in the adoption of AI tools appears to mirror this disparity, as the skills required for AI are rooted in Stem disciplines.""', 'Read additional stories on artificial intelligence', 'In the UK, just 24% of the workforce across the Stem sectors are female, and as a consequence ""women may feel less confident using AI tools"", adds Ms Cook. ""Even though many tools don\'t require technical proficiency, if more women don\'t view themselves as technically skilled, they might not experiment with them.', '""And AI also still feels like science fiction. In the media and popular culture, science fiction tends to be marketed at men.""', 'Ms Cook says that moving forward she wants to see more women both use AI and work in the sector. ""As the industry grows, we definitely don\'t want to see a widening gap between the genders.""', 'Yet psychologist Lee Chambers says that typically female thinking and behaviour may be holding some women back from embracing AI.', '""It\'s the confidence gap - women tend to want to have a high level of competence in something before they start using it, "" he says. ""Whereas men tend to be happy to go into something without much competence.""', 'Mr Chambers also says that women may fear having their ability questioned, if they use AI tools.', '""Women are more likely to be accused of not being competent, so they have to emphasise their credentials more to demonstrate their subject matter expertise in a particular field,"" he says. ""There could be this feeling that if people know that you, as a woman, use AI, it\'s suggesting that you might not be as qualified as you are.', '""Women are already discredited, and have their ideas taken by men and passed off as their own, so having people knowing that you use an AI might also play into that narrative that you\'re not qualified enough. It\'s just another thing that\'s debasing your skills, your competence, your value.""', 'Or as Harriet Kelsall puts it: ""I value authenticity and human creativity.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['This video can not be played', ""Rishi Sunak says Elon Musk 'can be valuable' in the AI conversation"", 'Monitoring the risks posed by artificial intelligence (AI) is too important to be left to big tech firms, Prime Minister Rishi Sunak has said. ', 'He told the BBC that governments needed to take action and AI firms could not be left to ""mark their own homework"".', 'He was speaking ahead of the AI Safety Summit, where a global declaration on managing AI risks has been announced.', 'King Charles told delegates the issue required ""urgency, unity and collective strength"".', 'It comes amid growing concerns about highly advanced forms of AI with as-yet unknown capabilities. ', 'So far countries are only starting to address the potential risks, which may include breaches to privacy, cyberattacks and the displacement of jobs. ', 'In an interview with the BBC at Downing Street, Mr Sunak AI was a ""transformative technology"" that could have huge benefits in the NHS or in schools.', 'But he said he wanted the UK and other countries to be able ""do the testing that is necessary to make sure that we are keeping our citizens and everyone at home safe"".', '""There has to be governments or external people who do that work,"" he said.', ""Speaking to the BBC's technology editor Zoe Kleinman, he said that many AI firms had already given the UK access to their models before their release."", 'And he claimed the UK was ""investing more"" AI risk management than any other country.', '""We\'ve already invested Â£100 million in our task force, which will become our Safety Institute,"" he said. ', '""And we\'re attracting the best and the brightest researchers from around the world to come and work in that institution.""', ""Around 100 world leaders, tech bosses and academics are currently gathering at the UK's first AI safety summit at Bletchley Park, in Buckinghamshire."", 'Earlier on Wednesday, the delegates agreed the world\'s first ever ""international statement"" on so called frontier AI - the government\'s term for AI that could exceed the capabilities of today\'s most advanced systems.', 'The Bletchley Declaration calls for global cooperation on tackling the risks, which include potential breaches to privacy and the displacement of jobs. ', 'Signed by 28 countries and the EU, it also says AI should be kept ""safe, in such a way as to be human-centric, trustworthy and responsible"".', 'Dr Caitlin Bentley, AI education lecturer at King\'s College London, said the declaration  was an ""important milestone"" in promoting the ""responsible AI development"".', 'However, she said more investment in AI education was needed to ensure ""AI is not only responsible, but equitable in its effects"" with the benefits felt by all.', 'In his BBC interview, the prime minister defended a planned discussion with controversial tech billionaire Elon Musk on Thursday night, saying he could bring ""something valuable to the conversation"".', '""Elon Musk for a long time has both been an investor and developer of AI technologies himself,"" said Mr Sunak. ', '""For over a decade, he\'s been also talking about the potential risks that they pose and the need for countries and companies to work together to manage and mitigate against those risks.""', 'This video can not be played', 'Elon Musk ahead of his meeting with UK PM Rishi Sunak', 'Mr Musk arrived at the summit on Wednesday morning, having warned the day before that AI could lead to the extinction of humanity.', 'But many experts consider warnings like this overblown. ', '""We\'ve got representatives from all the major AI companies here at the summit,"" said Mr Sunak. ', '""And that\'s crucial, because countries will need to work together with the companies that are developing the technology.""', 'Those appearing at the summit are discussing how best to maximise the benefits of AI - such as discovering new medicines and tackling climate change - while minimising the risks.', ""The summit's priorities include the threat of bio-terrorism and cyber attacks."", 'Speaking ahead of the event in London, US Vice President Kamala Harris said that world leaders ""must address the full spectrum of AI risks to humanity"" and listed examples of faulty algorithms in healthcare, the use of AI in making ""deepfakes"", misinformation and biased facial recognition. ', 'China has also backed international cooperation on AI, with the country\'s Vice Minister for Science and Technology, Wu Zhaohui, calling for ""global collaboration to share knowledge and make AI technologies available to the public"".', 'This video can not be played', 'King Charles III addresses UK AI Summit', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial intelligence (AI) technology is developing at high speed, transforming many aspects of modern life.', 'However, some experts fear it could be used for malicious purposes.', 'The UK is hosting a global meeting of world leaders and tech bosses including Elon Musk to discuss how highly advanced AIs can be used safely.', 'AI allows computers to learn and solve problems almost like a person.', 'AI systems are trained on huge amounts of information and learn to identify the patterns in it, in order carry out tasks such as having human-like conversation, or predicting a product an online shopper might buy.', 'This video can not be played', 'Watch: What is artificial intelligence?', 'The technology is behind the voice-controlled virtual assistants Siri and Alexa, and helps Facebook and X - formerly known as Twitter- decide which social media posts to show users.', ""AI lets Amazon analyse customers' buying habits to recommend future purchases - and the firm also uses the technology to crack down on fake reviews. "", 'ChatGPT and DALL-E are examples of what is called ""generative"" AI. ', 'These programs learn from vast quantities of data, such as online text and images, to generate new content which feels like it has been made by a human. ', 'So-called ""chatbots"" - like ChatGPT - can have text conversations.', 'Other AI programs like DALL-E can create images from simple text instructions.', 'Generative AIs can also make videos and even produce music in the style of famous musicians.', 'But these programs sometimes generate inaccurate answers and images, and can reproduce the bias contained in their source material, such as sexism or racism.', 'Many artists, writers and performers have warned that such AIs allow others to exploit and imitate their work without payment.', 'Many experts are surprised by how quickly AI has developed, and fear its rapid growth could be dangerous. Some have even said AI research should be halted. ', 'Earlier in October, the UK government published a report which said AI might soon assist hackers to launch cyberattacks or help terrorists plan chemical attacks.', ""Some experts even worry that in the future, super-intelligent AIs could make humans extinct. In May, the US-based Center for AI Safety's warning about this threat was backed by dozens of leading tech specialists."", 'Similar fears are shared by two of the three scientists known as the godfathers of AI for their pioneering research, Geoffrey Hinton and Yoshua Bengio.', 'But the other - Yann LeCun - dismissed the idea that a super-smart AI might take over the world as ""preposterously ridiculous"".', ""In June, the EU's tech chief Margrethe Vestager told the BBC that AI's potential to amplify bias or discrimination was a more pressing concern than futuristic fears about an AI takeover."", ""In particular, she worries about the role AI could play in making decisions that affect people's livelihoods such as loan applications."", ""Others criticise AI's environmental impact."", 'Powerful AI systems use a lot of electricity: by 2027,one researcher suggests that collectively, they could consume each year as much as a small country like the Netherlands.', 'In the EU, the Artificial Intelligence Act, when it becomes law, will impose strict controls on high risk systems.', 'US President Joe Biden has also announced measures to deal with a range of problems that AI might cause. He vowed to ""harness the power of AI while keeping Americans safe"".', 'The UK government previously ruled out setting up a dedicated AI watchdog.', 'But Prime Minister Rishi Sunak wants the UK to be a leader in AI safety, and is hosting a global summit at Bletchley Park where firms and governments are discussing how to tackle the risks posed by the technology.', 'Twenty eight nations at the summit - including the UK, US, the European Union and China - have signed a a statement about the future of AI called the Bletchley Declaration. ', 'This acknowledges the risks that advanced AIs could be misused - for example to spread misinformation - but says they can also be a force for good. ', 'The signatories resolve to work together to ensure AI is trustworthy and safe.', 'In a recorded address, King Charles told attendees that the risks posted by AI must be tackled with a ""a sense of urgency, unity and collective strength"".', 'A report by investment bank Goldman Sachs suggested that AI could replace the equivalent of 300 million full-time jobs across the globe.', 'It concluded many administrative, legal, architecture, and management roles could be affected.', 'But it also said AI could boost the global economy by 7%.', 'The tech has already been used to help doctors spot breast cancers, and to develop new antibiotics. ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['This video can not be played', 'King Charles III addresses the UK AI Summit', 'King Charles says the risks of artificial intelligence (AI) need to be tackled with ""a sense of urgency, unity and collective strength"".', ""He made the remarks in a taped address to attendees at the UK's AI Safety Summit."", 'As the global meeting opened, the UK government unveiled a ""world first agreement"" on how to manage the riskiest forms of AI. ', ""The Bletchley Declaration's signatories include the US, the EU and China."", 'The summit focuses on so-called ""frontier AI"" - by which ministers mean highly advanced forms of the tech with as-yet unknown capabilities.', ""Ahead of the meeting, Tesla and X owner Elon Musk, who is attending, said he thinks AI could lead to humanity's extinction - without any detail on how that could actually happen in reality."", 'Others have warned against speculating about unlikely future threats and said the world should instead focus on the potential present-day risks AI poses, such as replacing some jobs and entrenching bias.', 'In his address, King Charles called the development of advanced AI ""no less important than the discovery of electricity"".', 'He said tackling the risks of AI would, like efforts to combat climate change, need to involve conversations across societies, governments, civil society and the private sector. ', 'The UK government said the Bletchley Declaration, which attendees have signed, has seen 28 countries agree there is an urgent need to understand and collectively manage potential AI risks.', 'Technology Secretary Michelle Donelan said it was an important moment: ""We have always said that no single country can face down the challenges and risks posed by AI alone, and today\'s landmark declaration marks the start of a new global effort to build public trust by ensuring the technology\'s safe development.""   ', 'Other countries have also stressed the need for a global approach to managing the technology.', ""Relations between China and the West are fraught in many areas - but the country's Vice Minister of Science and Technology, Wu Zhaohui, told the conference it was seeking a spirit of openness in AI."", '""We call for global collaboration to share knowledge and make AI technologies available to the public,"" he told delegates.', 'Meanwhile, US Secretary of Commerce Gina Raimondo said the US would be launching its own AI Safety Institute following the summit. ', ""In a short interview at the UK's AI safety summit, Mr Musk said he was not looking for any particular policy outcome from the meeting, suggesting it was important to understand the problem before regulating."", '""You\'ve got to start with insight before you do oversight,"" he said. ', 'This video can not be played', 'Elon Musk ahead of his meeting with UK PM Rishi Sunak', 'Many experts consider fears that AI might threaten humanity overblown.', 'Nick Clegg, the president of global affairs at Meta and former deputy prime minister - who is also attending the summit - said people should not let ""speculative, sometimes somewhat futuristic predictions"" crowd out more immediate challenges.', ""Many observers feel AI's biggest threat is in automating away people's jobs, or building existing bias and prejudices into new, much more powerful, online systems."", 'Additional reporting by Liv McMahon, Imran Rahman-Jones, Tom Singleton and Chris Vallance.', 'Sign up for our morning newsletter and get BBC News in your inbox.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Cutting NHS waiting times for cancer diagnosis will be difficult without artificial intelligence (AI), according to one of the region's leading pathologists."", 'Dr David Bailey has launched a trial of the technology for breast cancer patients at Peterborough City Hospital. ', 'AI will be used to interpret scans and order follow-up tests before a consultant in involved.', 'The hospital is one of five to win government funding for the study.', 'Each year almost 56,000 people in the UK are diagnosed with breast cancer, according to Cancer Research UK. ', 'As with other cancers, the earlier it is detected, the more likely it is that treatment is successful.', 'The government is funding a range of projects which explore whether AI - computer programmes which use data to predict patterns and ""think"" like a human - can be used to speed up that process. ', 'For the next year, Peterborough City Hospital will work with a system known as Galen Breast, which is already licensed for use in other countries. ', ""It will be used in two different ways to support the hospital's pathologists. "", 'Firstly, it will analyse digital images of biopsies and, if it detects cancer cells, it will order extra tests before a consultant has been involved in the case.', 'Secondly it will be used as ""the second pair of eyes"". ', 'Dr Bailey, the hospital\'s director of pathology who is leading the project, said ""two pathologists will always review a patient\'s biopsy images as a quality measure"" and using AI instead of one of them frees up staff. ', '""The AI technology will increases our productivity by 15% to 20% and offers us the best of both worlds - the reliability and reproducibility of machines with the intuition and insight of humans.""', 'When asked whether patients could trust the AI\'s interpretation, Dr Bailey said: ""Yes. It\'s been shown to be very sensitive so it flags any potential cancer even if it turns out to be nothing.""', 'Patients with suspected cancer should have their diagnosis within 28 days according to government targets.', 'For patients with suspected breast cancer who are referred to the North West Anglia NHS Foundation Trust, which runs Peterborough City Hospital, 81% are diagnosed within 28 days, but Dr Bailey said it could take years to address the wider general pathology backlog without AI.', '""Pathologists have a huge amount of work - there was a backlog of patients before the pandemic and Covid-19 made that worse,"" he said.', '""AI can\'t put pathologists out of work because even with the technology there just aren\'t enough of us.""', 'Pathologists across five NHS trusts will use AI to analyse a total of 10,000 biopsies as part of routine practice and evaluate how it affects the quality, speed and efficiency of diagnosis. ', ""Cambridge University Hospitals NHS Foundation Trust, which runs Addenbrooke's Hospital is also taking part. "", 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp  0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial intelligence (AI) is being used to improve a treatment for liver cancer for the first time in an NHS hospital. ', ""Addenbrooke's Hospital in Cambridge has introduced AI into its thermal ablation procedure."", 'Radiology consultant Nadeem Shaida said it meant the ablation was more accurate and effective.', 'Thermal ablation involves inserting a needle or probe into small tumours to destroy them with heat.', 'It is an increasingly common treatment because it is considered less invasive than regular surgery.', 'The hospital said the AI was programmed with data from thousands of patients so that it could more accurately outline both the tumour and the margin of healthy tissue around it.', '""A few years ago we introduced Cascination (CT scan technology), which improved the accuracy, but AI has given us extra confidence,"" said Mr Shaida.', '""Before AI we had to rely on our own eye to interpret the images and that\'s prone to variability among different readers.""', 'He added: ""AI helps us identify straight away if we\'ve taken enough tissue, and if not, we can put the needle back in while the patient is still asleep rather than calling it a day, and then waiting for six weeks before another scan shows you need to repeat the procedure.""', ""Addenbrooke's says half of the approximate 75 liver cancer patients it treats are eligible for the procedure."", ""The AI technology was used recently for patient Charles Sykes's liver treatment at Addenbrooke's."", 'The 76-year-old said the procedure seemed straightforward, adding: ""If someone hadn\'t told me AI is what they\'d used, I wouldn\'t have known, but I\'m delighted it was available because it improves the chances of success.""', ""AI is already used in thermal ablation of kidney and lung tumours at centres throughout Europe and Addenbrooke's will be monitoring the effectiveness."", 'A global AI Safety Summit at Bletchley Park in Buckinghamshire is due to discuss the advantages and disadvantages of the technology - used across sectors - throughout this week.', '""I don\'t think interventional radiologists will be out of a job because you still need people to run, guide and understand the system, and to put the needle in physically,"" said Mr Shaida.', '""But it\'s a really useful addition to our daily working lives and could reduce the chance of interpretation error around the world.""', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp 0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['This week political leaders, tech industry figures and academics will meet at Bletchley Park for a two-day summit on artificial intelligence (AI). The location is significant as it was here that top British codebreakers cracked the ""Enigma Code"", hastening the end of World War Two. So what can we expect from this global event?', 'There is no public attendee list, but some well-known names have indicated they will appear.', 'About 100 world leaders, leading AI experts and tech industry bosses will attend the two-day summit at the stately home on the edge of Milton Keynes.', 'The US Vice President, Kamala Harris, and European Commission (EC) President Ursula von der Leyen are expected to attend.', 'Deputy Prime Minister Oliver Dowden told BBC Radio 4 that China accepted an invite, but added: ""you wait and see who actually turns up"".', 'Tech billionaire Elon Musk will attend ahead of a live interview with UK Prime Minister Rishi Sunak on Thursday evening.', ""The BBC also understands Open AI's Sam Altman and Meta's Nick Clegg will join the gathering - as well as a host of other tech leaders."", ""Experts such as Prof Yann LeCun, Meta's chief AI scientist, are also understood to be there."", 'The government said getting these people in the same room at the same time to talk at all is a success in itself - especially if China does show up.', 'The government has said the purpose of the event is to consider the risks of AI and discuss how they could be mitigated.', 'These global talks aim to build an international consensus on the future of AI.', 'There is concern frontier AI models pose potential safety risks if not developed responsibly, despite the potential to cause economic growth, scientific progress and other public benefits.', 'Some argue the summit has got its priorities wrong. ', 'Instead of doomsday scenarios, which they believe is a comparatively small risk, they want a focus on more immediate threats from AI.', 'Prof Gina Neff, who runs an AI centre at the University of Cambridge said: ""We\'re concerned about what\'s going to happen to our jobs, what\'s going to happen to our news, what\'s going to happen to our ability to communicate with one another"".', 'Professor Yoshua Bengio, who is considered one of the ""Godfathers"" of AI, suggested a registration and licensing regime for frontier AI models - but acknowledged that the two-day event may need to focus on ""small steps that can be implemented quickly.""', 'Thames Valley Police has dedicated several resources to the event, providing security to both attendees and the wider community.', ""Those resources include the police's mounted section, drone units, automatic number plate recognition officers and tactical cycle units."", 'The teams will assist the increased police presence on the ground ahead of the AI Summit.', 'People have been encouraged to ask officers any questions or raise any concerns when they see them.', 'Local policing area commander for Milton Keynes, Supt Emma Baillie, said she expected disruption to day-to-day life in Bletchley but hoped it would be kept to a minimum.', '""As is natural, we rely on our community to help us,"" she said. ', '""Bletchley has a strong community, and I would ask anybody who sees anything suspicious or out of the ordinary, to please report this to us."" ', ""The Victorian mansion at Bletchley Park served as the secret headquarters of Britain's codebreakers during World War Two. "", 'Coded messages sent by the Nazis, including orders by Adolf Hitler, were intercepted and then translated by the agents.', 'Mathematician Alan Turing developed a machine, the bombe, that could decipher messages sent by the Nazi enigma device.', ""By 1943, Turing's machines were cracking 84,000 messages each month - equivalent to two every minute."", 'The work of the codebreakers helped give the Allied forces the upper hand and their achievements have been credited with shortening the war by several years.', 'Ian Standon, chief executive of Bletchley Park, said it was a ""huge privilege and honour to be selected as the location for this very important summit.""', 'The museum has had to close for a week until Sunday while the event takes place.', 'Temporary structures have appeared over recent weeks to host the many visitors for the summit.', 'Mr Standon praised his team for their hard work in preparing for the event, especially when dealing with added security over the next couple of days.', '""We\'re in sort of security lockdown but that\'s a very small price to pay for the huge amount of publicity we\'re going to get out of this particular project,"" he said.', '""For us at Bletchley Park this is an opportunity to put the place and its story on the world stage and hopefully people around the world will now understand and recognise what Bletchley Park is all about.""', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp  0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The grip artificial intelligence has gained over humanity in 2023 - or at least the increase in conversations about whether it will be a force for revolutionary good or apocalyptic destruction - has led AI to be given the title of ""word of the year"" by the makers of Collins Dictionary.', 'Use of the term has quadrupled this year, the publisher said.', 'Other contenders ranged from ultraprocessed to Ulez, but Collins managing director Alex Beecroft said AI had been ""the talking point of 2023"".', 'He said: ""We know that AI has been a big focus this year in the way that it has developed and has quickly become as ubiquitous and embedded in our lives as email, streaming or any other once futuristic, now everyday technology.""', 'When asked for a comment by BBC News, AI chatbot ChatGPT said: ""AI\'s selection as the word of the year by Collins Dictionary reflects the profound impact of artificial intelligence on our rapidly evolving world, where innovation and transformation are driven by the power of algorithms and data.""', 'The Collins announcement comes as UK Prime Minister Rishi Sunak hosts a summit for 100 world leaders, tech bosses, academics and AI researchers to discuss how best to maximise the benefits of this powerful technology while minimising the risks.', 'Meanwhile, the Beatles have used it to help retrieve John Lennon\'s vocals from an old cassette to create their ""last song"", which will be released later this week.', 'But Sir Cliff Richard prefers not to use AI - which he mistakenly referred to in a BBC interview as ""artificial insemination"".', 'This video can not be played', ""Watch: Sir Cliff Richard sings in BBC interview 'without artificial insemination'"", 'The word of the year usually reflects the preoccupations of that time. In 2022, it was permacrisis in reference to the seemingly constant upheavals in British politics.', 'The previous year saw chatter about NFTs (non-fungible tokens) reach its peak. And 2020 was dominated by the word lockdown.', 'Other words of the year contenders for 2023, according to Collins, were:', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial intelligence is nearly twice as good at grading the aggressiveness of a rare form of cancer from scans as the current method, a study suggests.', 'By recognising details invisible to the naked eye, AI was 82% accurate, compared with 44% for lab analysis.', 'Researchers from the Royal Marsden Hospital and Institute of Cancer Research say it could improve treatment and benefit thousands every year.', 'They are also excited by its potential for spotting other cancers early.', 'AI is already showing huge promise for diagnosing breast cancers and reducing treatment times.', 'Computers can be fed huge amounts of information and trained to identify the patterns in it to make predictions, solve problems and even learn from their own mistakes.', '""We\'re incredibly excited by the potential of this state-of-the-art technology,"" said Professor Christina Messiou, consultant radiologist at The Royal Marsden NHS Foundation Trust and professor in imaging for personalised oncology at The Institute of Cancer Research, London.', '""It could lead to patients having better outcomes, through faster diagnosis and more effectively personalised treatment."" ', 'The researchers, writing in Lancet Oncology, used a technique called radiomics to identify signs, invisible to the naked eye, of retroperitoneal sarcoma - which develops in the connective tissue of the back of the abdomen - in scans of 170 patients.', ""With this data, the AI algorithm was able to grade the aggressiveness of 89 other European and US hospital patients' tumours, from scans, much more accurately than biopsies, in which a small part of the cancerous tissue is analysed under a microscope."", 'When dental nurse Tina McLaughlan was diagnosed - in June last year, after stomach pain - with a sarcoma at the back of her abdomen, doctors relied on computerised-tomography (CT) scan images to find the problem. ', 'They decided it was too risky to give her a needle biopsy.', 'The 65-year-old, from Bedfordshire, had the tumour removed and now returns to the Royal Marsden for scans every three months.    ', 'She was not part of the AI trial but told BBC News it would help other patients.', '""You go in for the first scan and they can\'t tell you what it is - they didn\'t tell me through all my treatment, until the histology, post-op, so it would be really useful to know that straight away,"" Ms McLaughlan said.   ', '""Hopefully, it would lead to a quicker diagnosis.""', 'About 4,300 people in England are diagnosed with this type of cancer each year.', 'Prof Messiou hopes the technology can eventually be used around the world, with high-risk patients given specific treatment while those at low risk are spared unnecessary treatments and follow-up scans.', 'Dr Paul Huang, from the Institute of Cancer Research, London, said: ""This kind of technology has the potential to transform the lives of people with sarcoma - enabling personalised treatment plans tailored to the specific biology of their cancer. ', '""It\'s great to see such promising findings.""', 'Sign up for our morning newsletter and get BBC News in your inbox.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A naval base in Somerset is among the first to use artificial intelligence to help prepare for missions.', 'RNAS Yeovilton engineers have started using an AI tool to help maintenance of some of the Wildcat helicopter fleet.', 'It simulates when parts and modifications could become faulty so engineers are ready to replace them. ', 'Chief Petty Officer Andrew Ireson, Wildcat Maritime Force HQ Engineer, said: ""The AI tool will help highlight any potential failures on missions.', '""We can then be prepared with spare parts to repair the helicopter on long missions at really short notice.""', 'The announcement comes in the week Prime Minister Rishi Sunak is hosting the AI Summit at Bletchley Park.', ""Minister for Defence Procurement James Cartlidge visited RNAS Yeovilton, where he discussed how AI can help the UK's defence structure."", 'He said: ""AI has conjured up this fearful vision for people.', '""It\'s probably because of Hollywood movies like Terminator that make you think you\'ll have some robot that can do whatever it wants - that\'s a long way off.', '""Our adversaries will be using AI so we must address the opportunities to use it.""', 'Follow BBC West on Facebook, X and Instagram. Send your story ideas to: bristol@bbc.co.uk ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Belfast-based artificial intelligence (AI) company Ocula Technologies is to invest Â£11m in research and development (R&D).', 'The firm is developing its Software Development Centre and hopes to grow staff from 10 to 50 within three years.', 'Ocula Technologies works in e-commerce, with clients such as Super Bowl winner Kansas City Chiefs as well as Boots.', 'It helps firms optimise their e-commerce experience and compete scientifically on pricing.', 'CEO and co-founder Thomas McKenna said: ""With its rich talent pool, we certainly made the right decision in choosing Northern Ireland to help us build this capability.', '""Our Northern Ireland team has developed our new software platform which is at the cutting edge of AI and is already benefiting some of the largest brands in the world - for example, we have helped the Kansas City Chiefs deliver significant revenue uplifts of 15%. ', '""We want to continue pushing the limits of innovation and building on our success in markets such as North America, where we are aiming to achieve significant sales by the end of 2024.""', 'Anne Beggs of Invest NI added that R&D was a ""key component of the Department for the Economy\'s 10x Vision, which will drive Northern Ireland\'s future economic success"".', '""The benefits of investing in R&D are rich, and Ocula is testament to how R&D can enable the development of game-changing products which can drive competitiveness in the global marketplace,"" she said.', '""To support the commercialisation of Ocula\'s R&D in both GB and the US, the company is also working with our trade division, which is supporting Ocula\'s presence at two upcoming key e-commerce events in GB and the USA.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Amid growing calls for schools to teach pupils about artificial intelligence (AI), BBC Young Reporters Theo and Ben have been looking at its risks and potential - and asked their classmates how they have used it to try to sharpen up their homework. ', '""A geography assignment was due next period and I used ChatGPT to write out the whole speech for me. When I was saying it out loud and I got asked questions I had no clue what I was saying. I got a detention.""', '""I didn\'t know what the question meant... so I put in ChatGPT and it just simplified it.""', '""When you\'re doing homework, there\'s no teacher in the classroom. It\'s like a teacher when you\'re at home.""', 'That is just a snapshot of how some of our friends have tried to spruce up their schoolwork using AI - technology that allows a computer to act and respond almost as if it was a human.', 'We asked our form groups to fill out an anonymous survey. A total of 31 out of 33 had used AI in schoolwork, and 27 thought it should be taught in schools.', 'In interviews, most of our friends and classmates had used ChatGPT, an online tool that can answer questions in human-like language. They said it helped them come up with ideas, research, and things like structuring and phrasing.', 'Some, though, confessed to using it to cheat. ', 'It did not always go to plan. One person said ChatGPT gave them the wrong dates for a history essay, and another said it got ""90% of the answers wrong"" in a physics assignment.', 'However, the idea that it was not fully trustworthy did not put off most people from using it. ', '""You can get a really structured answer from the likes of ChatGPT and then back that up with other extended research,"" one person said.', 'And, with AI being easily accessible 24 hours a day, there was some debate about whether it was better than a teacher.', '""A teacher can have more of an understanding of you and they might have a personal connection, whereas AI doesn\'t really know anybody at all, it\'s just anonymous,"" one said.', 'Teachers themselves are exploring the potential of this rapidly developing technology, too. Jonathan Wharmby, who teaches computer science at Cardinal Heenan Catholic High School in Liverpool, uses AI to help with planning and creating resources, such as multiple choice questions - but said there were issues.', '""Sometimes ChatGPT and the like will go off on tangents or will give incorrect answers, so it still needs me to look over them to check that they are correct,"" he said.', 'He said using ChatGPT in an exam scenario would be cheating, just as would using a search engine.', '""But to help you with your schoolwork, I don\'t see an issue - as long as you\'ve got that critical eye and you double check what it\'s coming back with,"" he added.', 'The government in England launched a consultation this year on AI in education, including on how it can be misused, and will publish its results later in the year.', 'We wanted to see how easy it was to distinguish between work that was original, work that was fully generated by AI, and work that had - as Mr Wharmby suggested - used it as a tool.', 'So we set ourselves a challenge, which involved each of us answering the same essay question, twice.', 'Our first answers were entirely our own work. But for the second one we each used ChatGPT in a different way - one of us asking it to write the whole answer, and the other asking it to generate ideas and help plan the essay.', 'When we swapped to see if we could spot where the other person had used it, and how, we were both pretty confident that we could spot the difference. ', 'But it turns out we should not have been - neither of us guessed entirely correctly.', 'This video can not be played', ""Can BBC Young Reporters Theo and Ben tell whether ChatGPT has been used in each other's work?"", 'One thing that threw us was how convincingly AI wrote about human emotions, like in this extract of the essay written entirely by ChatGPT:', 'The whole challenge made us realise just how deceptive AI can be and how easy it would be to cheat. ', 'The risks of AI will be the focus of the first Global AI Safety Summit this week in Bletchley Park, which Prime Minister Rishi Sunak said would be attended by representatives from ""companies pioneering AI and the countries most advanced in using it"".', 'Mr Sunak said the world did not yet have ""a shared understanding"" of the risks, but that, ultimately, the UK would not ""rush to regulate"".', '""We believe in innovation - it is a hallmark of the British economy, so we will always have a presumption to encourage it, not to stifle it,"" he said.', 'BBC Young Reporter is an exciting opportunity for young people to get involved with the BBC.', 'If you are 11-18 years old, then this is a chance for you to tell the stories that matter to you.', 'Click here to find out how to get involved. ', 'Find BBC Teach advice about navigating news and media literacy lesson plans here.', 'BCS, the chartered institute for IT, which helps support the development of IT skills and knowledge in education, also believes in the potential of AI and wants all students to be taught about it from the age of 11.', 'Julia Adamson, its managing director for education, said pupils should be required to study at least one technology subject at GCSE.', '""The world is digital now,"" she said.', '""It\'s really important that we help young people to navigate that world - just in the way that we do when we hold their hand while we\'re walking along a busy street and we teach them the rules of the road and the risks."" ', 'Ms Adamson said it would become easier for young people to cheat in schoolwork, but that did not mean they could not be trusted to create their own work. Instead, she believes young people should use it for things like structuring and generating ideas.', '""The risk, of course, is that we become lazy, and we drop back in terms of our creativity and our critical thinking. And that\'s exactly the opposite of what we really need to do,"" she said.', 'While most of our friends had used AI in some way in school, a recent online survey for BBC Radio 5 Live and BBC Bitesize by polling company Survation suggests that may not be the case everywhere.', 'Just 29% of respondents said they had used AI technology for completing their homework or coursework.', 'The Department for Education said the computer science GCSE is ""designed to equip pupils with the knowledge they will need for the technological jobs of the future, including in AI"".', 'Girls only make up about one in five entries in computing GCSE, according to government data.', 'Year 13 Student Ahana is the only girl in her sixth form studying computer science, and is campaigning to get more girls into the subject.', '""I felt really overwhelmed and kind of even out of place at the beginning... I was quite afraid to speak up and speak my opinions,"" she told us over a video call.', '""As time went on, I realised that this was not the case at all and I had as much right to be there as everyone else.', '""This experience made me realise what a big problem that this really was.""', 'Last week, exam board Pearson Edexcel announced an AI qualification for students to study alongside A-levels.', 'After all of our work, we agree that our generation will be using AI in the future - and so young people should be taught about it.', 'We just have to make sure that the teaching includes its limitations, and that we do not over-rely on AI to generate ideas for us.', 'If you only execute ideas and you do not think of them, then you are losing a huge chunk of what is really important in the world.', 'As told to Hazel Shearing, BBC education correspondent', 'Additional reporting by Rahib Khan ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The White House has announced what it is calling ""the most significant actions ever taken by any government to advance the field of AI safety"".', 'An executive order from President Joe Biden requires Artificial Intelligence (AI) developers to share safety results with the US government. ', 'It places the US at the centre of the global debate on AI governance.', 'The UK government is this week holding a summit on AI safety, hosted by Prime Minister Rishi Sunak.', 'The two-day meeting begins on 1 November at Bletchley Park. It has been prompted by concerns that the rapid advance of AI systems could lead to problems such as the development of more deadly bio-weapons and more paralysing cyber-attacks.', 'Announcing the safety measures, Mr Biden vowed to ""harness the power of AI while keeping Americans safe"".', 'The tech entrepreneur and AI expert Gary Marcus told the BBC the US announcement seemed more ambitious in its scope.', '""Biden\'s executive order sets a high initial bar. The executive order is broad, focusing on both current and long-term risks, with some - though probably not enough - teeth,"" he said.', '""The UK summit seems to have greatly narrowed its focus, primarily focusing around the long-term risk, with not enough focus on the here and now, and it\'s just not clear how much with teeth will come out of it, or what authority it really has.""', 'Alex Krasodomski, senior research associate at Chatham House, told the BBC the executive order showed the US considered itself the leader in terms of how to address such threats.', 'On Monday, Mr Biden told reporters and tech workers at the White House: ""As artificial intelligence expands the boundary of human possibility, and tests the bounds of human understanding, this landmark executive order is a testament to what we stand for. ', '""Safety, security, trust, openness, American leadership and the undeniable rights endowed by our creator that no creation can take away."" ', 'The US measures include:', 'The Biden administration is also taking steps to beef up its AI workforce. From Monday, workers with AI expertise can find relevant openings in the federal government on AI.gov.', 'Mr Krasodomski said the order was ""really important"", but one that ""doesn\'t necessarily run in-line with the UK\'s objectives and aims for the summit"".', '""The UK summit is referenced in the executive order. But it\'s mentioned under the heading of \'advancing American leadership abroad\' -  indicating that the US very clearly knows that it is the big player here alongside China but more precisely, it is the US companies that are really driving forward,"" he said. ', 'Mr Krasodomski added: ""It\'s difficult to put together a small, highly technical summit but I think clearly if this technology is going to have significant global impact there\'s going to have to be a ton of other kinds of work and engagement with countries around the world."" ', 'US Vice-President Kamala Harris and top executives from the US tech giants are arriving in the UK this week to discuss AI safety at the UK government\'s AI Summit, which it has billed as a ""world first"".', 'This video can not be played', 'Watch: What threats does AI pose?', 'The summit will focus on growing fears about the implications of so-called frontier AI. President of the EU Commission Ursula von der Leyen and UN Secretary General Antonio Guterres will also attend. ', 'The UK is determined to position itself as a global leader in trying to minimise the risks posed by this powerful technology. ', 'But the EU is in the process of passing an AI act, China has already devised a number of strict AI rules and and now the US has issued this order.', 'On top of that, according to the Reuters news agency, the Group of Seven (G7) industrial countries is reportedly agreeing a code of conduct for companies developing advanced AI systems.', 'All that activity raises the question of how much will actually be left up for discussion at Bletchley Park this week.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""If you don't know where you stand on artificial intelligence, you're not alone. "", ""Will it save humanity or destroy us? The stakes are that high and the jury is still out, even among the world's leading experts. "", ""Some AI creators are calling for its development to slow down or even pause altogether because it's evolving so quickly. Others argue that doing so will mean we cramp the tech's potential to achieve amazing things, like generating formulas for new medicines and work on climate-change solutions."", ""This coming week, around 100 world leaders, tech bosses, academics and AI researchers are gathering at the UK's Bletchley Park campus, once home to the codebreakers who helped secure victory during World War Two. Their purpose is to take part in discussions about how best to maximise the benefits of this powerful technology while minimising the risks."", 'The event is the UK\'s AI Safety Summit, and the risks it intends to focus on are pretty extreme. They relate to so-called ""frontier AI"", the most advanced and powerful systems, which don\'t currently exist but AI is advancing so rapidly that they may do soon. ', 'But critics say the focus of the two-day meeting should be on the more immediate and pressing problems of AI, such as the large amount of energy it consumes and the impact it is already having on jobs.', 'In a report released last week, the UK government listed some horrifying potential threats, including bio-terrorism, cyber-attacks, advanced AI choosing to control itself, and the increased distribution of deepfake images of child sexual abuse.', 'Perhaps surprisingly, given this nightmarish backdrop, Prime Minister Rishi Sunak urged people ""not to lose sleep"". He has a plan, and it\'s an ambitious one. He wants to position the UK as the global leader for AI safety.', ""When the summit was first announced, and billed as a world-first, there were quite a few raised eyebrows. Would the world's top brass actually travel to a remote, leafy corner of England, in the winter cold and close to the US Thanksgiving holiday, just because the UK says so?"", ""No official guest list has been published. But it's pretty clear by now that the US tech giants will be very well represented. Not always at CEO level, but high-level execs nonetheless."", 'There\'s no shortage of summit enthusiasm from the commercial sector. British-based Stability AI boss Emad Mostaque described the summit as a ""once-in-a-generation opportunity"" for the UK to unlock AI superpower status.', '""We will encourage the government and other policymakers to commit to supporting AI safety right across the ecosystem, from corporate labs to everyday researchers and from long-term threats to short-term risks to keep Britain safe and competitive,"" he gushed.', 'These firms are an essential part of the discussion - they are at the front of the AI race and they are the ones building the systems. But you might imagine these are conversations they are already having among themselves. Diversity of thought here is crucial.', ""The world-leader contingent is a bit more of a mixed bag. US Vice-President Kamala Harris is attending, Canadian PM Justin Trudeau is not. European Commission president Ursula von der Leyen will be there, German chancellor Olaf Scholz won't. China has been invited - controversially so, given its difficult relationship with the west. But it is still undoubtedly a tech superpower."", 'United Nations Secretary General Antonio Guterres is also going - which is interesting because there are growing calls for a global body to take on AI oversight.', 'Some experts fear that the summit has got its priorities wrong. The risk of extreme doomsday scenarios are comparatively small, they argue, and there are more immediate threats, far closer to home, which will be more worrying for many people.', '""We\'re concerned about what\'s going to happen to our jobs, what\'s going to happen to our news, what\'s going to happen to our ability to communicate with one another. We\'re concerned about the threats to people, communities, and frankly, the planet,"" says Prof Gina Neff, who runs an AI centre at the University of Cambridge.', ""A recent study claimed the computing infrastructure required to drive the AI sector alone could use as much energy as that of a country the size of the Netherlands in just four years' time. This will not be discussed at the summit."", ""We also know AI is already causing disruption to jobs. A friend of mine worked in a small marketing company. There were five copywriters - now there's one, whose job it is to check the copy generated by ChatGPT. The Department for Work and Pensions is using AI tools to speed up benefits claims."", 'And what about the data used to train these powerful systems? The details are kept under wraps by the commercial companies who own them. But arguably, AI is only as good as the data it understands. We know bias and discrimination has crept into both off-the-shelf and bespoke tools that are available now.', 'These issues will at feature the summit - but they will be far from its focus.', 'What will the outcome of this two-day pow-wow be? You might hope the chiefs will emerge clutching a signed ""Bletchley Park Agreement"" and the world will be saved from the perils of AI.', ""We'll probably have to save that for the movie version. The government itself says the feat of getting these people in the same room at the same time to talk at all is a success in itself - especially if China does show up."", 'Professor Yoshua Bengio is well-known as one of the three founding ""Godfathers"" of AI. He was asked at a recent event hosted by Chatham House in London, what he would like to see from the summit.', 'He suggested a registration and licensing regime for frontier AI models, revokable if a system is deemed unsafe - but acknowledged that something like this would take longer than two days to get together.', '""We\'re going to need to start with small steps that can be implemented quickly,"" he said. ', '""International treaties and agreements take a lot more time... but we should start small and not wait to have built a very complicated global governance system before we start doing things.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Don't stick your head in the sand, the prime minister has told us.  But don't lose sleep over it either."", 'His message, in a speech on Thursday, is that artificial intelligence (AI) is coming, ready or not. It brings some scary threats, including - at a stretch - human extinction, but somewhat greater possibilities for misinformation, fraud, scams, the derailing of democracy and terrorist deployment of chemical weapons.', ""It also offers more positive prospects, from drug discovery to the classroom to automating dull and repetitive tasks. Every day, a new use is found - today, it's by Airbnb, in its quest to hunt down party animals."", ""For most people, if they've stopped to consider AI at all, the most immediate threat and opportunity is in the workplace. "", 'Many jobs could be replaced by AI, including professional and well-paid white collar jobs. Think of all the lawyers who would be out of work if AI could be trusted to draw up and check contracts. ', ""But we're being encouraged to think of it as a workplace co-pilot, allowing us to add output and value to work, increasing productivity, and taking over the duller chores."", 'Past experience of new technologies teaches us that some jobs disappear, but new ones are opened up, and usually more of them.', ""That's the positive approach to AI being taken by Scotland's finance sector. At the same time that the prime minister' speech was preparing the ground for next week's UK-hosted AI safety summit, Scotland's money people were preparing for their annual awards dinner at which they are launching an ambitious strategy for growth."", 'Fifteen years after its big banks crashed and seemed a threat to the whole finance sector, in Scotland it now has nearly 140,000 jobs, and represents nearly a tenth of output from the Scottish economy.', 'While Royal Bank of Scotland and Halifax Bank of Scotland no longer boost the Edinburgh economy with their headquarters dynamic, the big numbers can be found in Barclays, choosing Glasgow as one of its three global hubs, and international giants JP Morgan, Morgan Stanley, BNP Paribas look to Scots for back office functions, fraud prevention, tech support and innovation.', 'The aim of trade body Scottish Financial Enterprise (SFE) is to grow the sector in Scotland from Â£14.3bn of added value in 2021 to between Â£17bn and Â£21bn within only five years.', 'A further aim is to double the amount of Assets Under Management, from Â£500bn to Â£1trillion, despite that figure going backwards in recent years, as companies have merged.  These are the financial assets entrusted to Scottish fund managers by mainly institutional investors such as pension funds. The strategy is to build on strengths, and one of them is asset management. ', 'Another is in energy, and funding renewable power. This is growing at a staggering rate around the world. A report for City UK and BNP Paribas found that the green finance market grew from $5bn (Â£4.1bn) in 2012 to $541bn (Â£445bn) in 2021, with lots more growth to come.', 'Others are vying for this business. The message with this strategy is not just that there are opportunities, but that there is a threat of being left behind. Others move fast. Edinburgh was recently rated fifth in the world for its human assets in the finance sector, but has slipped rapidly to number 16.', ""Such league tables seem to count for a lot in the flow of international investment. In this year's Global Financial Centres Index (GFCI), Glasgow was at number 45 and Edinburgh at 27. The aim is to get Glasgow into the top 30 and Edinburgh into the top 20."", 'Emerging economies have fast-growing financial centres. The GFCI doesn\'t include either Glasgow or Edinburgh in the top 15 ""most desirable places to live and work"" or in the 15 centres most likely to grow in significance - a list headed by Seoul, Singapore and Kigali in Rwanda. ', 'It is to America that Scottish Financial Enterprise looks for inspiration. Instead of the high costs of New York, asset management has grown in Boston, while Atlanta has become the US hub for payments technology. Chicago and Seattle also offer clusters of expertise.', 'The aim is to make Edinburgh and Glasgow - with their lower cost base than big financial centres such as London, and an adaptable pool of finance-savvy recruits - a Scottish hub for financing the transition to green energy, from the turbines through the cabling to the battery-powered products.', 'Behind that part of the strategy is proximity to the opportunity from ScotWind, the vast project to build out offshore windfarms around Scotland. This is where a lot of money is about to spent. But success in issuing green bonds, for instance, would have to aim to be a European or global hub, pushing Edinburgh up from number 14 in the GFCI league table of 86 green finance centres. ', 'The other message, including one to governments, is that the energy sector is going through a transition from oil and gas which will be painful for some. So by contrast, if national policy can give the finance sector a helping hand, it can grow without the painful stuff. ', 'And faced with the glacial pace of productivity improvements across the British economy, finance is one sector where productivity is already strong, and has potential to grow rapidly.', 'That is where AI plays its part. Scottish universities are a crucial part of this strategy, providing both the research expertise and the graduates to make the strategy happen.  They have been important to more than 200 fintech companies being set up in Scotland.  Some succeed by being bought over.  ', 'Embark is one example, bought by Lloyds Banking Group to provide the software platform for the fund managers in its Scottish Widows division.  Started in Dundee, it has grown under the strength of an owner with deep pockets, so that it now employs 200 people in Britain, with its biggest office still on Tayside.', ""It's high risk, though. Some don't make it. Money Dashboard is one that impressed when I visited some years back.  It was incubated in Edinburgh's TechCube hub, and grew to have five million people using its app to see where their outgoings were going, in simple colourful charts. "", ""But it hasn't stacked up as a financial venture. Others offer something similar. It couldn't find an industry buyer, and the apps are being closed down next week."", ""Such university and fintech firms have been working with SFE as well as the trade bodies representing the financial sector in London and at a UK level. Their joint strategy is an intention to plant a Saltire flag in turf where it can leverage existing strengths with the new technology's capabilities; in climate finance, open banking data, payments and financial regulation."", 'It seems that such a focus can work. Since the financial technology trade body, Fintech Scotland, set out its strategy last year, picking areas of strength for more rapid growth, there has been a 19% growth in payments tech activity and a 13% growth in the niche software to handle regulation.', ""Fifteen years after that notorious bank crash, and a slump in public confidence in banks and business more widely, this new technology gives the industry a new opportunity. It is seeking now to reintroduce itself as one answer to Scotland's economic problems, and portray its future as a potential force for good and for growth."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['This video can not be played', ""UK Prime Minister Rishi Sunak has given a speech on AI's potential uses"", 'Artificial intelligence could help make it easier to build chemical and biological weapons, Prime Minister Rishi Sunak has warned.', 'In a worst-case scenario, society could lose all control over AI, preventing it from being switched off, Mr Sunak said.', 'While the potential for harm is disputed, we must not ""put our heads in the sand"" over AI risks, he argued.', 'In a speech aiming to present the UK as a world leader on AI, the PM said the technology was already creating jobs.', 'He added that development of the technology would catalyse economic growth and productivity, though admitted it would have an impact on the labour market. ', ""The prime minister's speech on Thursday morning set out the capabilities and potential risks posed by AI - including cyber attacks, fraud and child sexual abuse - following the publication of a government report. "", 'Mr Sunak said among the risks outlined in the report was that AI could be used by terrorist groups ""to spread fear and disruption on an even greater scale"".', 'Mitigating the risk of human extinction from AI should be a ""global priority"", he said. ', 'But he added: ""This is not a risk that people need to be losing sleep over right now and I don\'t want to be alarmist.""', 'He said that he was generally ""optimistic"" about the potential of AI to transform people\'s lives for the better. ', 'A threat that will be much closer to home for many is the disruption AI is already bringing to jobs. ', 'Mr Sunak mentioned AI tools efficiently doing admin tasks like preparing contracts and helping to make decisions - traditionally roles carried out by employees. ', 'He said he believed education was the solution to preparing people for the changing market, adding that technology had always brought changes to the way people make money.', 'Automation has already changed the nature of factory and warehouse work, for example, but has not entirely removed human input.', 'The prime minister insisted it was too simple to say artificial intelligence would ""take people\'s jobs"", instead urging the public to view the tech as a ""co-pilot"" in the day-to-day activities of the workplace.', 'Reports, including declassified material from the UK intelligence community, set out a series of warnings about the threats AI could pose within the next two years. ', 'According to the government\'s ""Safety and Security Risks of Generative Artificial Intelligence to 2025"" report, AI could be used to:', 'Experts are divided about the threat posed by AI and previous fears about other emerging technologies have not fully materialised.', 'Rashik Parmar, the chief executive of the BCS, The Chartered Institute for IT, said: ""AI won\'t grow up like The Terminator. ', '""If we take the proper steps, it will be a trusted co-pilot from our earliest school days to our retirement."" ', 'In his speech, Mr Sunak said the UK would not ""rush to regulate"" AI because it was ""hard to regulate something you do not fully understand"".', ""He said the UK's approach should be proportionate while also encouraging innovation, "", ""Mr Sunak wants to position the UK as a global leader on the safety of the technology, which would put it at the centre of a stage on which it can't really compete with huge players like the US and China in terms of resources or homegrown tech giants."", ""So far, most of the West's powerful AI developers seem to be cooperating - but they are also keeping a lot of secrets about what data their tools are trained on and how they really work.  "", 'The UK will have to find a way to persuade these firms to stop, as the prime minster put it, ""marking their own homework"".', 'Prof Carissa Veliz, associate professor in philosophy, Institute of Ethics in AI, at the University of Oxford, said unlike the EU the UK had so far been ""notoriously averse to regulating AI, so it is interesting for Sunak to say that the UK is particularly well-suited to lead the efforts of ensuring the safety of AI"". ', 'She said regulation often leads to ""the most impressive and important innovations"". ', 'Labour said the government had not yet set out concrete proposals on how it would regulate the most powerful AI models.', '""Rishi Sunak should back up his words with action and publish the next steps on how we can ensure the public is protected,"" Shadow Science, Innovation and Technology Secretary Peter Kyle said.', 'The UK is hosting a two-day AI safety summit at Bletchley Park in Buckinghamshire next week, with China expected to attend.  ', ""The decision to invite China at a time of tense relations between the two countries has been criticised by some. Former Prime Minister Liz Truss has written to Mr Sunak asking him to rescind China's invitation."", 'She believes ""we should be working with our allies, not seeking to subvert freedom and democracy"" and cites concerns around Beijing\'s attitude to the West about AI.', 'But, speaking earlier Mr Sunak defended the decision, arguing there could be ""no serious strategy for AI without at least trying to engage all of the world\'s leading AI powers"". ', 'The summit will bring together world leaders, tech firms, scientists and academics to discuss the emerging technology.', 'Professor Gina Neff, Director of the Minderoo Centre for Technology and Democracy at the University of Cambridge, has criticised the focus of the summit. ', '""The concerns that most people care about are not on the table, from building digital skills to how we work with powerful AI tools,"" she said. ', '""This brings its own risks for people, communities, and the planet.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['How would you feel if you rented out your home for a few nights, only to return and find that it had been used to hold a raucous house party?', ""As you looked aghast at the damage to your property, you'd either be livid or upset, or a combination of both."", 'Such scenarios have been widely reported around the world in recent years, especially during the coronavirus pandemic. With bars and nightclubs then closed, young adults, in particular, wanted to find somewhere else to hang out, dance and potentially drink too much.', 'It sparked a fightback from short-term rental giant Airbnb, which announced a ""global party ban"", and vowed to do all it could to prevent such behaviour. This included banning offenders from making new bookings, and restrictions on under 25s who didn\'t have a history of excellent reviews.', 'Airbnb said recently that as a result of its clampdown the number of reported parties dropped by 55% between 2020 and last year. But with the battle not yet won, the US firm has now upped the ante and introduced an artificial intelligence (AI) powered software system to help weed out potential troublemakers.', 'Now operating worldwide, when you now try to make an Airbnb booking the AI automatically looks out for things such as how recently you created your account, and - big red flag - whether you are trying to rent a property in the same town or city as where you live.', ""It also questions the duration of your stay - one night only is a potential concern - and whether the planned visit is occurring during a revelry-heavy period such as Halloween or New Year's Eve."", '""If someone is booking a room during New Year\'s Eve for one night only, and they are from the same city as the host, that\'s likely to be a party,"" says Naba Banerjee, head of safety and trust at Airbnb.', 'Ms Banerjee adds that if the AI deems that the risk of a party booking is too high, it will prevent the booking, or instead guide the person to the website of one of its partner hotel companies. She says it is an issue of trust, that people renting out their homes via Airbnb are as reassured as possible.', 'Lucy Paterson is one such person. She rents out the one-bedroom annex beside her home in Worcestershire, and has had more than 150 bookings since she first listed the apartment.', '""Part of my planning to be an Airbnb host is that I have only a one-bedroom place, to minimise the potential for parties,"" she says. ""Of course it hasn\'t always been perfect, but I\'d say 99% of my guests have been fantastic.""', 'She adds that Airbnb\'s new use of AI has given her ""more reassurance"".', 'Going forward, Ms Banerjee says the AI will only get better and better as the more data it processes the more it will learn.', 'In the car sharing sector, one of the biggest online marketplaces, Turo, also uses an AI system, to help protect people who let others hire their cars. ', 'The software, a platform called DataRobot AI, can quickly detect a risk of theft. It also sets the prices for cars, determined by their size, power and speed, and the time of the day or week that a person wishes to begin their hire period.', ""Separately, Turo also uses AI to allow some users to talk to its app, to tell it what car it wants and when. The AI will then reply, with text on the screen, offering a personalised list of recommended vehicles that match the criteria. This service is currently available to subscribers of popular consumer AI system ChatGPT-4, which is incorporated into Turo's system."", '""We want to make it easy to browse Turo, and that can help build trust between us and our customers,"" says the firm\'s chief data officer Albert Mangahas.', 'Read additional stories on artificial intelligence', 'Using AI to filter out potential problem customers is a good idea, says Edward McFowland III, assistant professor of technology and operating management at Harvard Business School. ""Having that layer of AI can help ease the friction on both sides, for both business and consumer.""', 'He points out though, that even a perfectly calibrated AI model can create false negatives, such as shutting out a young person who wants to hire an apartment for New Year\'s Eve, but has no intension of throwing a party. ""And that\'s why AI technology is still very hard to get right, all the time.""', ""In Toronto, Canada, Lara Bozabalian uses Airbnb to rent out her family's 3,000 sq ft (280sq m) cottage. She says that no matter how attractive the booking may be, or what the AI has determined, she follows her own rule."", '""I don\'t take on clients who are first-time users. I need to know they\'ve been vetted by someone at some point.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial intelligence could increase the risk of cyber-attacks and erode trust in online content by 2025, a UK government report warns.', 'The tech could even help plan biological or chemical attacks by terrorists, it says.', 'But some experts have questioned whether the tech will evolve as predicted.', 'Prime Minister Rishi Sunak is expected to highlight opportunities and threats posed by the technology on Thursday.', 'The government report looks at generative AI - the type of system that currently powers popular chatbots and image generation software.', 'It is based in part on declassified information from intelligence agencies.', 'The report warns that by 2025 generative AI could be ""used to assemble knowledge on physical attacks by non-state violent actors, including for chemical, biological and radiological weapons"".', 'It says while firms are working to block this, ""the effectiveness of these safeguards vary"". ', 'There are obstacles to getting hold of the knowledge, raw materials, and equipment for attacks, but those barriers are falling - potentially accelerated by AI, according to the report.', 'By 2025, it\'s likely AI will also help create ""faster-paced, more effective and larger scale"" cyber-attacks, it warns.', 'Joseph Jarnecki, who researches cyber threats at the Royal United Services Institute, said that AI could help hackers, especially in overcoming their difficulties in mimicking official language. ', '""There\'s a tone that is adopted in bureaucratic language and cybercriminals have found that quite difficult to harness,"" he told the BBC.', 'The report comes ahead of a speech by Mr Sunak on Thursday where he is expected to set out how the UK government aims to make AI safe, and establish the UK as a global leader in AI safety.', '""AI will bring new knowledge, new opportunities for economic growth, new advances in human capability, and the chance to solve problems we once thought beyond us. But it also brings new dangers and new fears,"" Mr Sunak is expected to say.', 'He will commit to address those fears head on, ""making sure you and your children have all the opportunities for a better future that AI can bring"".', 'The speech sets the scene for a government summit next week to discuss the threat posed by highly advanced AIs.', 'It will focus on the regulation of so-called ""Frontier AI"": powerful future AI systems that ministers say ""can perform a wide variety of tasks"" and ""exceed the capabilities of today\'s most advanced models"".', 'Whether or not such systems could pose a threat to humanity is a hotly debated.', 'Another newly published report by the Government Office for Science, which advises the prime minister and cabinet, says ""many experts consider this a risk with very low likelihood and few plausible routes to being realised.""', 'It says to pose a risk to human existence, an AI would need some control over vital systems, such as weapons or financial systems.', 'They would also need new skills such as the capacity to improve their own programming, the ability to evade human oversight and a sense of autonomy.', 'But it notes ""there is no consensus on the timelines and plausibility of when specific future capabilities could emerge"".', 'The big AI firms have mostly agreed that regulation is necessary, and their representatives are likely to attend the summit.', 'But Rachel Coldicutt, an expert on the social impact of technology, questioned the focus of the summit.', 'She said it placed too much weight on future risk: ""It makes loads of sense that technology companies, who stand to lose more by being regulated about the things they\'re making in the here-and-now, will focus on long-term risk.""', '""And it has felt over the summer, as if the government position has been very strongly aligned, supporting those views,"" she told the BBC.', 'But she said the government reports were ""moderating some of the the fervour"" about these futuristic threats and made it clear that there was a gap between ""the political position and the actual technical one"".', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Mounted patrol officers have been preparing for an AI summit which will be visited by world leaders.', ""Riders will be supporting security at the UK government's global Artificial Intelligence (AI) summit at Bletchley Park in Milton Keynes, Buckinghamshire."", 'Police horses will patrol near the event which starts on 1 November.', 'Thames Valley Police said it would also have a drone unit in operation throughout the course of the summit.', 'Drone pilots on the ground will be using the technology to look out for suspicious activity.', 'Insp Chris Simpson said: ""Our expert mounted section and drone unit are just some of the many additional security measures we have in place to ensure the safety both of delegates to the summit and residents and businesses.""', '""Our police horses enjoy the attention they get from the public so please do stop and speak with them when you spot them on patrol,"" he added.', 'World leaders will meet with AI companies and experts at the summit to try and build an international consensus on the future of AI.', ""Alan Turing, one of the pioneers of modern computing, cracked the Enigma code - used by Germany's military - with his team during World War Two at Bletchley Park."", 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp 0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A new Â£6.2m residential building equipped with the latest technology is due to open at a college for students with disabilities.', 'The accommodation at National Star College will act as a ""smart house"" and includes voice-activated technology.', 'It will allow students to adapt artificial intelligence (AI) to suit their personal needs. ', 'It will be opened later by disability campaigners Jack Thorne and Rachel Mason.', 'The college, based in Ullenwood, near Cheltenham, provides education and therapy for young people with a range of disabilities and it is hoped the technology will give them more independence and prepare them for life after college.', ""The single-storey 'Building a Brighter Future' building consists of 13 bedrooms, each installed with overhead tracking hoists and a range of AI features such as a voice-activated fridge."", '""We\'re trying to give the students the opportunity to explore this technology in a safe environment at college,"" said Maizie Morgan, assistive technology technician at National Star College.', '""The idea is that prospective and current students are able to use this technology, see what\'s out there in the world, and eventually, hopefully implement it into their own rooms and then transition from college,"" she added.', ""Principal, Simon Welch, said the technology had been personalised to help meet students' individual needs."", '""We understand, in terms of the young people and their disability, and what\'s really the priority for them. ', '""The technology isn\'t necessarily hugely innovative but the way in which we work with the individual is,"" he added.', 'Student, Jaspar Tomlinson, was given the opportunity to test out the software ahead of the opening. ', 'He is non-verbal but is able to send commands to the smart devices by using his eyes to control his electronic communicator. ', 'Devices and appliances in the rooms can then be controlled using a single action word. ', '""I think that it\'s great because it helps me gain confidence for the time I leave college,"" he said.', 'Peter Horne, National Star deputy chief executive, said: ""This new accommodation will improve the lives of young people with complex physical and learning disabilities and create stimulating spaces to live, learn and relax in.""', 'Follow BBC West on Facebook, X and Instagram. Send your story ideas to: bristol@bbc.co.uk', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A leading child protection organisation has warned that abuse of AI technology threatens to ""overwhelm"" the internet.', 'The Internet Watch Foundation (IWF) removes images of child sexual abuse from websites.', 'It says it has found thousands of AI-generated images which are so realistic they are criminal under UK law.', 'AI is being used to produce new images of real victims, de-age celebrities and unclothe children in ordinary photos to depict them in abuse scenarios.', '""Our worst nightmares have come true,"" said Susie Hargreaves OBE, chief executive of Cambridge-based IWF.  ', '""Chillingly, we are seeing criminals deliberately training their AI on real victims\' images.', '""Children who have been raped in the past are now being incorporated into new scenarios because someone, somewhere, wants to see it.""', 'In a new study of a single dark web forum, the IWF found 2,978 images of abuse generated using artificial intelligence. ', 'Half of them depicted children of primary school age, with some as young as two years old. ', 'More than 560 images were classified as Category A - the most serious kind of imagery - including rape, sexual torture and bestiality.', 'In the UK, cartoons, drawings, animations and AI images are all criminal if they depict child abuse. ', 'Experts also found that the technology is being used to ""nudify"" children, whose clothed pictures had been uploaded to the internet for genuine reasons. ', 'The most convincing imagery is now difficult for trained analysts to distinguish and, as the technology advances, the IWF warns it will pose ""more obstacles"" for them and the police.', 'Ian Critchley, National Police Chiefs\' Council Lead for Child Protection, said: ""It is clear that this is no longer an emerging threat - it is here, and normalises the rape and abuse of real children.', '""AI has many positive attributes, and we are developing opportunities to turn this technology against those who would abuse it to prey on children."" ', 'The Prime Minister Rishi Sunak is hosting a global AI Safety Summit at Bletchley Park on 1 November. ', 'Susie Hargreaves has once again urged him to put this issue at ""the top of the agenda"" and warned ""if we don\'t get a grip on this threat, this material threatens to overwhelm the internet."" ', 'Follow East of England news on Facebook, Instagram and X. Got a story? Email eastofenglandnews@bbc.co.uk or WhatsApp  0800 169 1830', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Tech giant Nvidia says the US has told it to stop shipping some of its advanced artificial intelligence chips to China immediately. ', 'The restrictions were supposed to be introduced 30 days from 17 October.', ""That was when President Joe Biden's administration announced measures to block countries, including China, Iran and Russia, from buying high-end AI chips designed by Nvidia and others."", 'Nvidia did not say why the timeline had been moved forward.', 'In a statement to the US Securities and Exchange Commission (SEC), Nvidia said the US government said these curbs were ""effective immediately"", but added that ""given the strength of demand for the Company\'s products worldwide, the Company does not anticipate that the accelerated timing of the licensing requirements will have a near-term meaningful impact on its financial results"". ', ""The new restrictions bar exports of Nvidia's advanced AI chips, which had been designed for the Chinese market to comply with earlier export regulations."", 'The acceleration of the introduction of the US curbs is the latest move in the ongoing technology dispute between Washington and Beijing.', ""Chinese authorities have yet to publicly comment on Nvidia's announcement, but it hit back at the Biden administration's decision to impose new restrictions on advanced chip exports when it was announced last week."", 'The country\'s foreign ministry said the curbs ""violate the principles of the market economy and fair competition"".', 'The move was seen as an attempt to close loopholes that became apparent after an initial wave of chip controls last October.', 'At the time, the US said the measures were designed to prevent China from receiving cutting-edge technologies that it could use to strengthen its military, especially in the field of AI.', ""Soaring demand for Nvidia's AI chips has pushed up its share price more than threefold, making it one of the most valuable companies in the world."", 'In May the firm joined technology giants Apple, Amazon, Alphabet and Microsoft in the elite club of companies with stock market valuations of more than $1 trillion (Â£822bn).', 'California-based Nvidia has come to dominate the market for chips used in AI systems.', 'Chip giant Advanced Micro Devices (AMD), which also supplies AI chips to China, has not made any announcement about the accelerated export curbs. It did not immediately respond to a BBC request for comment.', ""The US Department of Commerce declined to comment on Nvidia's statement when contacted by the BBC."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Paedophiles are using artificial intelligence to create images of celebrities as children. ', 'The Internet Watch Foundation (IWF) said images of a well-known female singer reimagined as a child are being shared by predators.', 'On one dark web forum the charity says images of child actors are also being manipulated to make them sexual.', 'Hundreds of images of real victims of child sexual abuse are also now being created using bespoke image generators.', ""The details come from the IWF's latest report into the growing problem, as it tries to raise awareness about the dangers of paedophiles using AI systems that can create images from simple text instructions."", 'Since these powerful image generation systems entered the public domain, researchers have warned that they have the potential to be misused to generate illicit images.', 'In May, Home Secretary Suella Braverman and US Homeland Security Secretary Alejandro Mayorkas issued a joint statement committing to tackle the ""alarming rise in despicable AI-generated images of children being sexually exploited by paedophiles"". ', ""The IWF's report details how researchers spent a month logging AI imagery on a single darknet child abuse website and found nearly 3,000 synthetic images that would be illegal under UK law."", 'Analysts said there is a new trend of predators taking single photos of well-known child abuse victims and recreating many more of them in different sexual abuse settings.', 'One folder they found contained 501 images of a real world victim who was about 9-10 years old when she was subjected to sexual abuse. In the folder predators also shared a fine-tuned AI model file to allow others to generate more images of her.', 'The IWF says some of the imagery, including that of celebrities as children, is extremely realistic and would be indistinguishable to untrained eyes.', 'Analysts saw images of mostly female singers and movie stars that had been de-aged using the imaging software to make them look like children.', 'The report did not identify which celebrities had been targeted.', ""The charity said it was sharing the research to get the issue put onto the agenda at the UK government's AI Summit next week at Bletchley Park."", 'In one month, the IWF investigated 11,108 AI images which had been shared on a dark web child abuse forum. ', 'In June, the IWF warned that predators were starting to explore the use of AI to make depraved images of children, but now the IWF says the fears are a reality.', '""Our worst nightmares have come true,"" said Susie Hargreaves,, the chief executive of the IWF.', '""Earlier this year, we warned AI imagery could soon become indistinguishable from real pictures of children suffering sexual abuse, and that we could start to see this imagery proliferating in much greater numbers. We have now passed that point.""', 'The IWF report reiterates the real world harm of AI images. Although children are not harmed directly in the making of the content, the images normalise predatory behaviour and can waste police resources as they investigate children that do not exist.', 'In some scenarios new forms of offence are being explored too, throwing up new complexities for law enforcement agencies.', 'For example, the IWF found hundreds of images of two girls whose pictures from a photoshoot at a non-nude modelling agency had been manipulated to put them in Category A sexual abuse scenes. ', 'The reality is that they are now victims of Category A offences that never happened.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A new multimillion-pound recycling facility in Coventry is to use robots and AI technology to sort rubbish.', 'Sherbourne Recycling, based at Whitley Depot, is thought to be the first of its kind in the UK. ', 'It was founded by eight West Midlands councils and will process the rubbish of 1.5m people across the region.', 'The plant was ""a huge investment but a great return for all councils for the next 25 years"", Carolyn Watson-Merret from Rugby Borough Council said.', 'The facility is at the final stages of going fully live to turn residential mixed recycling into high-quality materials to be returned to the UK market, rather than being sent abroad. ', ""The site's business manager Layla Shannon said it would process 175,000 tonnes of rubbish a year."", 'The waste would go through two cylinders that take out oversized materials and glass before a process to separate the key components, she said.', '""We separate material by metals, glass, plastics and fibres and then we break that down further into different grades of material,"" she explained.', 'The robotic units use high suction to target the material during the quality control process, with the robots\' arms able to perform ""70 picks a minute"", Ms Shannon said.', 'She added: ""They\'re talking back to the optics that are working a stage before them, they know what they\'re looking for, the AI is bringing the whole system together and they\'re targeting those key items for us that we don\'t want there - last little bits of film or tissues... we pull them off and then we\'ve got exactly what we\'re looking for at the end.""', 'Waste that cannot be recycled will go into other bins, but the new technology means the range of what is suitable is wider.', '""By using the sorting and rapid response technology that we\'ve got, we can change and we can move with changes in composition and consumer habits,"" Ms Shannon said.', 'Ms Watson-Merret, councillor for operations and traded waste at the council, said the handling of volume and tonnage coming into the site was state-of-the-art, with solar panels radiating back to power the plant.', 'The facility was ""very impressive"" after several years of hard work between the partner authorities to get it operational, she said.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['An artificial intelligence tool has been developed to predict new variants of viruses before they emerge.', 'Researchers at the University of Oxford and Harvard Medical School claim the model could have predicted mutations of the Covid-19 virus during the pandemic.', 'It is hoped the model, named EVEscape, will help in the design of vaccines by studying how viruses mutates in response to the human immune system.', 'The University of Oxford said the technology was ""predicting the future"".', 'During the Covid-19 pandemic waves were driven by different variants of the virus that had undergone multiple genetic changes.', ""Those mutations can alter the virus's behaviour, potentially making it spread faster or making it harder for our immune systems to recognise and fight off."", 'In late 2021, the Omicron variant did just that and infected millions, although it did not lead to a huge spike in hospitalisations and deaths.', 'EVEscape - short for Evolutionary Model of Variant Effect - combines a deep-learning model of how a virus evolves, along with detailed biological and structural information about it.', 'In the journal Nature, the research team described how it works by predicting the likelihood that a viral mutation will enable it to escape immune responses, for example, by preventing antibodies from binding.', 'The model was tested with information only available at the beginning of the Covid-19 pandemic in February 2020 and successfully predicted which SARS-CoV-2 mutations would occur and which would become most prevalent. ', 'The team said it also predicted which antibody-based therapies would lose their efficacy as the pandemic progressed and the virus developed mutations to escape these treatments.', 'It is hoped the technology could help in prevention measures and the design of vaccines that target variants of concern before they become prevalent.', 'Co-lead author for the study Pascal Notin, said it would have ""accurately predicted"" the most frequent mutations of Covid-19 if it had been used at the start of the pandemic.', '""This work is of tremendous value, both for pandemic surveillance efforts, but also to inform vaccine design in a way that is robust to the emergence of certain at-risk mutations,"" he added.', 'Follow BBC South on Facebook, X, or Instagram. Send your story ideas to south.newsonline@bbc.co.uk.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The camera never lies. Except, of course, it does - and seemingly more often with each passing day. ', 'In the age of the smartphone, digital edits on the fly to improve photos have become commonplace, from boosting colours to tweaking light levels. ', 'Now, a new breed of smartphone tools powered by artificial intelligence (AI) are adding to the debate about what it means to photograph reality.', ""Google's latest smartphones released last week, the Pixel 8 and Pixel 8 Pro, go a step further than devices from other companies. They are using AI to help alter people's expressions in photographs.  "", ""It's an experience we've all had: one person in a group shot looks away from the camera or fails to smile. Google's phones can now look through your photos to mix and match from past expressions, using machine learning to put a smile from a different photo of them into the picture. Google calls it Best Take. "", 'â\x80¯The devices also let users erase, move and resize unwanted elements in a photo - from people to buildings - ""filling in"" the space left behind with what\'s called Magic Editor. This uses what\'s known as deep learning, effectively an artificial intelligence algorithm working out what textures should fill the gap by analysing the surrounding pixels it can see, using knowledge it has gleaned from millions of other photos. ', ""It doesn't have to be pictures taken on the device. Using the Pixel 8 Pro you can apply the so-called Magic Editor or Best Take to any pictures in your Google Photos library.  "", 'For some observers this raises fresh questions about how we take photographs. ', 'Google\'s new AI technology has been described variously by tech commentators and reviewers as potentially ""icky"" (The Verge), ""creepy"" (Tech Radar) and having the potential to ""pose serious threats to people\'s (already fragile) trust of online content"" (Cnet).  ', 'Andrew Pearsall, a professional photographer, and senior lecturer in Journalism at the University of South Wales, agreed that AI manipulation held dangers.  ', '""One simple manipulation, even for aesthetic reasons, can lead us down a dark path,"" he said. ', 'He said the risks were greater for those who used AI in professional contexts but there were implications to for everyone to consider.  ', '""You\'ve got to be very careful about \'When do you step over the line?\'.  ', '""It\'s quite worrying now you can take a picture and remove something instantly on your phone.  I think we are moving into this realm of a kind of fake world."" ', ""Speaking to the BBC, Google's Isaac Reynolds, who leads the team developing the camera systems on the firm's smartphones, said the company takes the ethical consideration of its consumer technology seriously."", 'He was quick to point out that features like Best Take were not ""faking"" anything. ', 'Camera quality and software are key to the company competing with Samsung, Apple and others - and these AI features are seen as a unique selling point.  ', ""And all of the reviewers who raised concerns about the tech praised the quality of the camera system's photos. "", '""You can finally get that shot where everyone\'s how you want them to look- and that\'s something you have not been able to do on any smartphone camera, or on any camera, period,"" Reynolds said. ', '""If there was a version [of the photo you\'ve taken] where that person was smiling, it will show it to you. But if there was no version where they smiled, yeah, you won\'t see that,"" he explained. ', 'For Mr Reynolds, the final image becomes a ""representation of a moment"". In other words, that specific moment may not have happened but it\'s the picture you wanted to happen created from multiple real moments.  ', 'Professor Rafal Mantiuk, an expert in graphics and displays at the University of Cambridge, said it was important to remember that the use of AI in smartphones was not to make the photographs look like real life. ', '""People don\'t want to capture reality,"" he said.  ""They want to capture beautiful images. The whole image processing pipeline in smartphones is meant to produce good-looking images - not real ones."" ', 'The physical limitations of smartphones mean they rely on machine learning to ""fill in"" information that doesn\'t exist in the photo.  ', ""This helps improve zoom, improve low light photographs, and - in the case of Google's Magic Editor feature - add elements to photographs that were either never there or swapping in elements from other photos, such as replacing a frown with a smile. "", ""Manipulation of photographs is not new - it's as old as the art form itself. But never has it been easier to augment the real thanks to artificial intelligence.  "", ""Earlier this year Samsung came in for criticism for the way it used deep learning algorithms to improve the quality of photos taken of the Moon with its smartphones. Tests found it didn't matter how poor an image you took to begin with, it always gave you a useable image.   "", 'In other words - your Moon photo was not necessarily a photo of the Moon you were looking at. ', 'The company acknowledged the criticism, saying it was working to ""reduce any potential confusion that may occur between the act of taking a picture of the real Moon and an image of the Moon"". ', ""On Google's new tech, Reynolds says the company adds metadata to its photos - the digital footprint of an image - using an industry standard to flag when AI is used. "", '""It is a question that we talk about internally. And we\'ve talked at length. Because we\'ve been working on these things for years. It\'s a conversation, and we listen to what our users are saying,"" he says. ', 'Google is clearly confident users will agree - the AI features of its new phones are at the heart of its advertising campaign. ', 'So, is there a line Google would not cross when it comes to image manipulation?  ', 'Mr Reynolds said the debate about the use of artificial intelligence was too nuanced to simply point to a line in the sand and say it was too far. ', '""As you get deeper into building features, you start to realise that a line is sort of an oversimplification of what ends up being a very tricky feature-by-feature decision,"" he says. ', ""Even as these new technologies raise ethical considerations about what is and what isn't reality, Professor Mantiuk said we must also consider the limitations of our own eyes. "", 'He said: ""The fact that we see sharp colourful images is because our brain can reconstruct information and infer even missing information.  ', '""So, you may complain cameras do \'fake stuff\', but the human brain actually does the same thing in a different way."" ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Software that uses artificial intelligence to take a person's blood pressure could be rolled out across the country."", 'Lifelight has been developed by a company based on the Southampton University Science Park.', 'In trials across Hampshire and the Isle of Wight, patients look at a tablet screen that takes a blood pressure reading from their face in 40 seconds.', 'The technology targets people who do not regularly engage with the NHS.', 'Xim, the company behind Lifelight, has been running trials with the NHS since 2019 and claims it is already as accurate as the traditional inflatable cuff.', 'The company hopes the contactless technology will be made available for patients to use at home on their tablets and smart phones.', 'David Petronzio from Lifelight said the camera on the tablet detects colour changes, known as ""microblushes"", in a patient\'s face, which occur every time their heart beats. Â\xa0', 'Artificial intelligence is then used to translate the information into blood pressure, heart rate and respiration data.', 'Trials have taken part at Thornhill Baptist Church, in Southampton, and Paulsgrove surgery, in Portsmouth.', 'Dr Lindsay Welch, from Wessex Academic Health Science Network, said they are targeting five ""key"" places where people don\'t have regular cardiovascular checks.', 'She added that these are ""also places where people are often struggling with money and have lots of other complex illnesses and social problems"".', 'According to figures from Public Health England, 43% of people with hypertension in the UK are undiagnosed.', 'Dr Karen Kyd, a GP from Portsdown Group Practice, said: ""High blood pressure is common and often doesn\'t have any symptoms. ', '""Left untreated, it can cause serious health problems including strokes, heart attacks, kidney disease and dementia, so it\'s especially important to take the time to get a simple check. Support can then be offered to bring the blood pressure under control.', '""This initiative will help us to make tests even more accessible and provide innovative care to our patients.""', 'Follow BBC South on Facebook, X, or Instagram. Send your story ideas to south.newsonline@bbc.co.uk.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Carers of people with dementia and autism could get support from socks which use artificial intelligence (AI).', 'The SmartSocks, created by Milbotix, alert carers when the wearer is in distress.', 'The socks track heart rate, sweat, temperature and motion, to enable those caring for people to intervene before issues escalate.', 'The Dorset Integrated Care System will trial the socks at care homes in 2024.', 'Councillor Jane Somper, from Dorset Council, said ""Smart Socks can help enable people living with dementia or other conditions to retain their independence and have better day-to-day quality of life.""', 'Bristol-based Milbotix, the start-up company behind the product, said it wanted to create wearable technology that was not complex, uncomfortable or stigmatising.', 'The machine washable socks are paired with an app, installed on a smartphone.', 'A sensor embedded in one of the socks collects data from the foot and the ankle of the wearer which is sent to the AI, which estimates whether the person is distressed. ', 'The care team would then be alerted through the app.', 'Dr Zeke Steer, Milbotix founder, was inspired to make the product after his grandmother began to show signs of early onset dementia.', 'He said partnering with the council meant the company could ""hear the expert voices of people providing and receiving care"".', '""We are committed to building a product that genuinely help carers and improves wearers\' overall wellbeing,"" he added.', 'Follow BBC South on Facebook, X, or Instagram. Send your story ideas to south.newsonline@bbc.co.uk.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The world\'s most valuable chip company Nvidia and iPhone maker Foxconn are joining forces to build so-called ""AI factories"".', 'The firms say it is a new kind of data centre that uses Nvidia chips to power a ""wide range"" of applications.', 'They include training autonomous vehicles, robotics platforms and large language models.', 'It comes as the US announced plans to cut off more exports of advanced chips to China, in a blow to Nvidia.', 'The latest export restrictions announced by Washington this week will block sales of two high-end artificial intelligence chips Nvidia created for the Chinese market - A800 and H800, according to the company. ', ""The Taiwan-born Nvidia chief executive Jensen Huang and Foxconn chairman Young Liu shared a stage at Foxconn's annual tech showcase in Taipei on Wednesday."", '""A new type of manufacturing has emerged - the production of intelligence and the data centres that produce it are AI factories,"" Mr Huang said, according to Reuters, adding that Foxconn had the expertise and scale to build these factories globally.', 'Mr Liu also said Foxconn is trying to ""convert itself from a manufacturing service company to a platform solution company,"" citing smart cities and smart manufacturing as other applications for AI factories.', ""Thanks to the use of the company's advanced chips in AI applications, Nvidia's stock market value has jumped to over $1 trillion as its shares more than tripled in value this year."", 'That made it the fifth publicly traded US company to join the so-called ""Trillion dollar club"", along with Apple, Microsoft, Alphabet and Amazon.', ""Meanwhile, Foxconn, which makes over half of the world's Apple products, has been trying to diversify its business and replicate its success in assembling personal computers and smartphones."", 'In an exclusive interview in June, Mr Liu told the BBC that electric vehicles (EVs) are what will drive its growth in the coming decades.', ""In January, Foxconn and Nvidia announced a partnership to develop autonomous vehicle platforms, in which Foxconn would manufacture electronic control units for cars based on Nvidia's chips."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['If there was a court case on whether society should embrace artificial intelligence (AI) or reject it, there would likely be a hung jury. ', 'No-one, it seems, can decide whether the benefits - such as automating written tasks, and sifting through vast amounts of information in seconds - outweigh the problems of biased data, and a lack of accuracy and accountability.', 'For the legal profession itself, AI represents both a threat and an opportunity. It could lead to a ""savage reduction"" in jobs for humans, according to a 2021 report from the UK\'s Law Society.', 'And a study this year from the universities of Pennsylvania, New York and Princeton estimated that the legal sector was the industry  most likely to be impacted by AI.', 'At the same time, AI can play a hugely valuable role in researching and putting cases together. Although there is precedent for things going horribly wrong.', 'New York lawyer Steven Schwartz found himself facing his own court hearing this year, when he used popular AI system ChatGPT to research precedents for a case involving a man suing an airline over personal injury. Six of the seven cases he used had been completely made up by the AI.', 'While that may have left many law firms reluctant to embrace such systems, Ben Allgrove, the chief innovation officer at international law firm Baker McKenzie, has a different interpretation.', '""I don\'t think that it is a technology story, it\'s a lawyer story,"" he says. ""You\'ve got to get through the lack of professionalism [by Mr Schwartz], and the lack of ethics, before you get to the fact that the tool was something he shouldn\'t have been using.""', 'Baker McKenzie has been tracking developments in AI since 2017, and has since set up a team of lawyers, data scientists and data engineers to test the new systems that are coming to market.', ""Mr Allgrove thinks that the vast majority of AI usage in his firm will come from using the new AI-powered versions of existing legal software providers, like LexisNexis and Microsoft's 365 Solution for Legal."", ""LexisNexis launched its AI platform back in May, which can answer legal questions, generate documents and summarise legal issues. Meanwhile, Microsoft's AI-tool, Copilot, will launch for commercial customers next month, as an extra-cost add-on for 365."", '""We already use LexisNexis and Microsoft, and they will increasingly get capabilities driven by generative AI. And we will buy those things if they make sense and are at the right price.""', 'Generative AI is the type of AI that everyone is talking about. It is the AI that can create text, images and music based on the data it was trained with.', 'The caveat is that currently, premium, paid-for versions of such tools are expensive. Paying for Microsoft\'s Copilot alone would ""double our technology spend"", Mr Allgrove says.', ""The alternative is for law firms to pay a lesser amount to access AI systems not specifically aimed at the legal market, such as Google's Bard, Meta's Llama, and OpenAI's ChatGPT. The firms would plug into such platforms, and adapt them for their own legal use."", 'Baker McKenzie is already testing several. ""We are going out to the market and saying we want to test the performance of these models,"" says Mr Allgrove.', 'Such testing is crucial, he explained, to ""validate performance"", because all the systems will all make errors.', 'Legal software system RobinAI uses what it calls an AI co-pilot to help speed up the process of drafting and querying contracts, both for in-house legal teams in large organisations, and for individuals.', 'It is primarily using an AI system developed by a company called Anthropic. This was set up by a former vice president of research at OpenAI, and is backed with investment from Google.', 'But RobinAI has also created its own AI models that are being trained on the minutiae of contract law. Any contract used by the system gets uploaded and labelled, and is then used as learning tool. ', 'This means the firm has built up a huge database of contracts, something Karolina Lukoszova, co-head of legal and product at UK-based RobinAI, thinks will be key to the use of AI in the legal profession.', '""Companies will need to train their own smaller models on their own data within the company,"" she says. ""That will give them better results and ones that are ringfenced.""', 'To make sure information is accurate, RobinAI has a team of human lawyers working alongside the AI.', 'Alex Monaco is an employment lawyer who runs both his own solicitor practice and a tech firm called Grapple.', 'Grapple was developed to provide members of the public with what Mr Monaco calls ""an ontology of employment law"", and offers advice on a range of workplace issues from bullying and harassment to redundancy. It can generate legal letters and provide summaries of cases.', 'He is excited about the potential for AI to democratise the legal profession.', '""Probably 95% of the inquiries that we get are from people who just cannot afford lawyers,"" says Mr Monaco.', 'But thanks to widely available free AI tools, people can now build their own legal cases. Anyone with an internet connection can use Bard or ChatGPT to help formulate a legal letter. And while it might not be as good as a letter written by a lawyer, it is free.', '""AI is not replacing humans, it\'s not replacing lawyers. What it is doing is supercharging people\'s understanding and implementation of their legal rights,"" he says.', 'And in a world where everyone is using AI, he adds that this could be very important.', 'Read additional stories on artificial intelligence', '""Companies and corporations are using AI for hiring and firing. They are profiling CVs, using AI for restructuring, mass redundancies and so on. They\'re using this against the average employee.""', 'While the use of AI in law is very much still at an early stage, some systems are already facing their own legal challenges.', ""DoNotPay, which dubs itself as the world's first robot lawyer, offering to fight parking fines and other citizen cases using AI, has been hit with a range of lawsuits, the latest of which accuses the firm of practising law without a license."", ""Meanwhile, as a result of Steven Schwartz's case, several senior judges in the US now require lawyers to disclose whether AI was used for court filings."", 'Mr Monaco thinks this will be both difficult to define and police.', '""Google uses AI within its search algorithm, and now it\'s using Bard. So even by googling anything, you are already using AI to do your legal research.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A ChatGPT-style AI assistant, developed by Microsoft and embedded into its office apps, will become available to all from 1 November, following trials.', 'Microsoft 365 Copilot can summarise meetings held in Teams for anyone who chooses not to attend.', 'It can also draft emails, create word documents, spreadsheet graphs, and Powerpoint presentations in moments.', 'Microsoft says it hopes the tool will eliminate ""drudgery"" but some worry tech like this will replace workers.', 'There are also concerns it could leave businesses dangerously reliant on AI-powered assistance.', 'In its current form, it could also fall foul of new rules governing AI, for failing to make clear when content has not been made by humans.', ""Both Europe's AI act and China's AI regulations state that people must know if they are interacting with artificial intelligence rather than humans."", 'Colette Stallbaumer, head of Microsoft 365, said it was up to the individual using Copilot to clarify that.', '""It is a tool, and people have responsibility to use it responsibly,"" she said.', '""I might not be telling you, when I send you that response, that I used an AI assistant to help me generate it. But the human is always in the mix and always in control.""', 'However, the EU states that it is up to the firms which develop AI tools to ensure they are used responsibly.', 'I was given an exclusive opportunity to try out Copilot, ahead of its wider launch.', 'It uses the same tech which underpins ChatGPT, created by OpenAI - a company Microsoft has invested billions of dollars in.  ', ""My demo was on the laptop of Derek Snyder, a Microsoft member of staff, because Copilot is embedded into an individual's account, with access to their own - or a company's own - data. "", 'Microsoft says the data is managed securely and will not be used to train the tech.', '""You only have access to data that you would otherwise be allowed to see,"" said Ms Stallbaumer. ""It respects data policies.""', 'My first impression of Copilot is that it will be a useful tool, but also a formidably competitive colleague for those who do office work - especially within companies looking to make savings.', 'I watched it confidently summarise in a few seconds, a long chain of emails regarding a fictional product launch.', 'It then suggested a brief response.  We used a simple drop-down menu to make that response longer and more casual, and the Chatbot generated a warm reply, expressing admiration for the ideas proposed and declaring excitement at being involved in the project - although none of us had actually read any of it.', 'We could then choose to edit the email before sending it, or select the AI-generated copy and send it in its entirety. There was no hint within the email that it contained content from Copilot.', 'I then saw the tool generate a multiple-slide Powerpoint presentation in around 43 seconds, based on the contents of a Word document. It can use images embedded within the document, if there are any, or it can search its own royalty-free collection. It created a simple but effective presentation - and it also wrote a suggested narrative to read out alongside it. ', 'It did not understand my request to make the presentation more ""colourful"" and referred me back to manual Powerpoint tools.', 'Finally, we looked at a Microsoft Teams meeting. ', 'Copilot identified themes and offered summaries of various threads which had run through the discussion. It could also summarise what one particular person had said if required, and in the event of a disagreement, it was able to offer, in a chart format, the pros and cons which had been debated. All of this took a few seconds.', 'It has been programmed not to answer questions about the performance of individuals in meetings - such as who was the best speaker (or worst).', 'I asked Mr Snyder whether he thought anyone would actually bother attending meetings, once they realised that Copilot could save them the time and effort.', '""A lot of meetings might become webinars,"" he joked.', 'The tech currently cannot differentiate between people who are on Teams but siting together sharing one device, unless they verbally cue each other. ', 'Copilot will cost $30 per month (which works out at around Â£25 in the UK). It is internet-connected and does not work offline.', 'Critics say this kind of tech is likely to lead to a huge disruption in admin-based jobs.', ""Carissa Veliz, associate professor at Oxford University's Institute for Ethics in AI, said she was also concerned about people becoming overly dependent on such tools."", '""What happens if the tech fails, or it might be hacked? There might be a glitch, or they might institute new policies that you might not agree with. And then, if you\'re so hooked on the system that you feel that you can\'t do without it anymore, what happens then?"" she said.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""A company which enables its clients to search a database of billions of images scraped from the internet for matches to a particular face has won an appeal against the UK's privacy watchdog."", ""Last year, Clearview AI was fined more than Â£7.5m by the Information Commissioner's Office (ICO) for unlawfully storing facial images."", 'Jack Mulcaire, Clearview AI\'s lawyer, said the firm was ""pleased"".', 'The ICO said it would ""take stock"" of the judgement.', 'Clearview AI offers its clients a system that works like a search engine for faces - users upload a photo and it finds matches in a database of billions of images it has collected. ', 'It then provides links to where matching images appear online.', ""In March, Clearview's founder Hoan Ton-That told the BBC it had run nearly a million searches for US police, helping them to solve a range of crimes, including murders."", 'He also revealed its database contained 30 billion images scraped from the internet.', 'Critics argue that law enforcement\'s use of Clearview\'s technology puts everyone into a ""perpetual police line-up"". ', ""And prior to the ICO's action, now ruled unlawful, France, Italy and Australia had also taken action against the firm."", 'This video can not be played', ""The BBC's James Clayton sees Clearview in action"", 'In the past Clearview AI had commercial customers, but since a 2020 settlement in a case brought by US civil liberties campaigners, the firm now only accepts clients who carry out criminal law enforcement or national security functions.', ""Clearview does not have UK or EU clients, but its customers are based in the US and in other countries including Panama, Brazil, Mexico, and the Dominican Republic, Tuesday's judgement revealed."", ""In simple terms, Clearview succeeded in appealing against the ICO's fine and enforcement action because it was used solely by law enforcement bodies outside the UK."", 'The three-member tribunal at the First-tier Tribunal, which heard the appeal, concluded that although Clearview did carry out data processing related to monitoring the behaviour of people in the UK, the ICO ""did not have jurisdiction"" to take enforcement action or issue a fine.', 'Explaining the decision James Castro-Edwards, data privacy lawyer from Arnold & Porter told the BBC that, ""Clearview only provided services to non-UK/EU law enforcement or national security bodies and their contractors.""', '""UK data protection law (UK GDPR) provides that acts of foreign governments fall outside its scope; it is not for one government to seek to bind or control the activities of another sovereign state"".', 'In response to the judgement, the ICO said that it would carefully consider next steps but added: ""It is important to note that this judgement does not remove the ICO\'s ability to act against companies based internationally who process data of people in the UK, particularly businesses scraping data of people in the UK, and instead covers a specific exemption around foreign law enforcement.""', 'Will Richmond-Coggan, a data protection partner at law firm Freeths, agreed, arguing that even though the appeal was allowed, the decision underlined that scraping large volumes of publicly available data was an activity to which UK data protection rules could apply. ', '""The appeal turned exclusively on the fact that Clearview\'s customers were overseas national security and law enforcement bodies, and so shouldn\'t be relied on as granting a blanket permission for such scraping activities more generally.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Bestselling thriller writer John Grisham says the ""threat"" to his profession from AI cannot be ""truly appreciated... explained or predicted"".', 'He is among a group of writers who have accused OpenAI of unlawfully training its artificial-intelligence-based chatbot ChatGPT on their work.', 'Jonathan Franzen, Jodi Picoult and George RR Martin are among those joining the recent group legal action.', 'Grisham told BBC One\'s Breakfast programme: ""It\'s my turn to file suit.""', 'He said: ""For 30 years, I\'ve been sued by everyone else - for slander, defamation, copyright, whatever - so it\'s my turn.""', 'OpenAI said last month it respected the rights of authors, ""they should benefit from AI technology"" and the company was ""optimistic we will continue to find mutually beneficial ways to work together"".', 'In a wide-ranging interview, Grisham also discussed his long-awaited sequel to his hit second novel, The Firm.', 'Published in 1991 and turned into a Hollywood blockbuster starring Tom Cruise, The Firm is an intriguing tale about a Memphis law firm set up by the Mafia to launder money and enable tax evasions. ', 'The Exchange follows protagonists Mitch and Abby McDeere after they exposed the firm and fled the country.', 'So why did it take more than 30 years to write?', '""I can\'t just sit down and force a story to happen,"" Grisham told the Breakfast.', '""I have to be inspired... to write the novel. In the meantime, there are so many other books to write. I kept thinking about Mitch and how much fun it would be to bring him back. I had no idea it would take so long.""', 'But will Cruise reprise his role as Mitch?', '""I hope so - it\'s not in the works yet,"" Grisham told the Breakfast. ', '""If Tom wants to do it, it will be done. If Tom doesn\'t want to do it, it probably won\'t be done.""', 'The fascination with crime and legal drama was because ""we have an addiction to violence"" and  people ""love big sensational trials"", Grisham told the Breakfast. ', 'But he was ""more pessimistic"" about the legal system today.', '""For the past 15 years, I\'ve served on two boards dedicated to exonerating innocent people who are in prison,"" Grisham told the Breakfast. ', '""I\'ve come to realise there are are thousands of innocent people in prison - they all go back to a bad verdict.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial Intelligence (AI) technology is ""too important not to get right"", a top Google executive has told the BBC.', 'It has the potential for ""huge breakthroughs"" across industries, said Matt Brittin, president of Google for Europe, the Middle East and Africa.', 'There has been a long-running global debate about the risks and rewards of AI.', 'Mr Brittin was speaking as Google agreed a joint research partnership with the University of Cambridge.', ""As part of it, the tech giant will provide a grant for the university's new Centre for Human-Inspired AI, where academics and scientists from Cambridge and Google will come together."", 'The long-term agreement will focus on a number of areas including robotics, healthcare and climate change.', ""The partnership comes ahead of the UK's AI safety summit at Bletchley Park, at which the government hopes some of the biggest names in the industry will convene."", 'It has been prompted by an intensifying debate about the potential benefits of AI - and attempts by regulators in multiple countries to devise regulations for the rapidly advancing field.', '""If we get it right, there could be huge breakthroughs in health, the potential for unlimited, clean energy, and a society where everyone has opportunities through education and powerful, intelligent tools. ', '""So this is a huge opportunity for us to do that,"" said Mr Brittin.', 'Vice president of research at Google DeepMind, and professor of information engineering at Cambridge University, Zoubin Ghahramani told the BBC the research the new centre would do could help address climate problems.', 'AI tools have been used to optimise flight paths to reduce the amount of contrails - vapour trails left across skies by aeroplanes.', '""It may not seem like an obvious use, but it is actually very valuable to address the impact of air travel,"" Prof Ghahramani said.', ""Mr Brittin said sustainability and solutions for addressing a climate crisis had been a long-term focus for Google and its AI arm, DeepMind, saying its research helped reduce energy consumption and costs in the tech giant's data centres."", '""I joined the company in 2007, and that was the year we became carbon neutral - we became one of the world\'s biggest purchasers of renewables,"" Mr Brittin said, adding that Google\'s recent UK power purchase agreements will see services used in the country running on almost entirely carbon-free energy by 2025.', 'He also pointed to global projects such as sequencing traffic lights to reduce pollution, and using Google Maps to find fuel-efficient routes or the best place for solar panels.', 'Others, though, have raised concerns that the AI revolution Google is helping to fuel is causing great environmental damage, with one academic calling it ""an enormous extractive industry for the 21st Century"".', ""A recent study suggested the sector's explosive growth could soon see it use as much energy as a country the size of the Netherlands, leading its author to say AI should be used only where absolutely necessary."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['German Chancellor Olaf Scholz may turn down his invitation to a major UK summit on artificial intelligence, the BBC understands.', 'The government is hosting an event aimed at tech leaders, academics and political leaders to discuss AI safety on 1 November.', 'The agenda will focus on specific future threats posed by the rapidly evolving tech, such as cyber security.', 'Britain has mooted setting up a global AI watchdog to monitor developments.', ""While no guest list has been published of an expected 100 participants, some within the sector say it's unclear if the event will attract top leaders. "", 'A government source insisted the summit is garnering ""a lot of attention"" at home and overseas. ', 'The two-day meeting is due to bring together leading politicians as well as independent experts and senior execs from the tech giants, who are mainly US based.  ', 'The first day will bring together tech companies and academics for a discussion chaired by the Secretary of State for Science, Innovation and Technology, Michelle Donelan.', 'The second day is set to see a ""small group"" of people, including international government figures, in meetings run by PM Rishi Sunak.', 'It will be held in Bletchley Park, the Buckinghamshire country house which was once the top-secret headquarters of World War Two codebreakers.  ', 'Though no final decision has been made, it is now seen as unlikely that the German Chancellor will attend.', 'That could spark concerns of a ""domino effect"" with other world leaders, such as the French President Emmanuel Macron, also unconfirmed.  ', 'Government sources say there are heads of state who have signalled a clear intention to turn up, and the BBC understands that high-level representatives from many US-based tech giants are going.', 'The foreign secretary confirmed in September that a Chinese representative has been invited, despite controversy. ', ""Some MPs within the UK's ruling Conservative Party believe China should be cut out of the conference after a series of security rows."", 'It is not known whether there has been a response to the invitation.', 'China is home to a huge AI sector and has already created its own set of rules to govern responsible use of the tech within the country. ', ""The US, a major player in the sector and the world's largest economy, will be represented by Vice-President Kamala Harris."", 'In what was seen as a political win for Downing Street, the UK-hosted AI summit was announced during an overseas trip by PM Rishi Sunak to the US in June.', 'Britain is hoping to position itself as a key broker as the world wrestles with the potential pitfalls and risks of AI.', 'However, Berlin is thought to want to avoid any messy overlap with G7 efforts, after the group of leading democratic countries agreed to create an international code of conduct. ', 'Germany is also the biggest economy in the EU - which is itself aiming to finalise its own landmark AI Act by the end of this year.', 'It includes grading AI tools depending on how significant they are, so for example an email filter would be less tightly regulated than a medical diagnosis system.', ""The European Commission President Ursula von der Leyen is expected at next month's summit, while it is possible Berlin could send a senior government figure such as its vice chancellor, Robert Habeck."", 'The UK is currently planning to fold AI regulation into existing bodies: so for example, if a person feels discriminated against by an AI tool, they would contact the Equalities Commission.', 'But many experts in the space are calling for an international, UN-style regulator to oversee AI on a global level.', 'A source from the Department for Science, Innovation and Technology said: ""This is the first time an international summit has focused on frontier AI risks and it is garnering a lot of attention at home and overseas.', '""It is usual not to confirm senior attendance at major international events until nearer the time, for security reasons."" ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Kensington and Chelsea Council has admitted it is using US surveillance software on two estates but insisted it was to ""keep residents safe"".', 'The technology, which uses AI software to analyse CCTV images, is being trialled at Trellick Tower and Markland House in North Kensington.', 'Councillor Sof McVeigh said it had already assisted police apprehend one resident accused of carrying a machete.', 'But another councillor said tenants were being treated like suspects.', 'The west London council was forced to confirm the use of the US surveillance kit Fusus after media platform OpenDemocracy reported it was the only council in the country doing so.Â\xa0', 'Ms McVeigh said: ""I do have to speak in support of them, because just relatively recently, a few weeks ago, a local operative was looking at these CCTV camera images of a resident going into a lift with a machete and we called the police and he was apprehended.', '""CCTV does have a very important role to play in the safety of all our tenants.""', 'Ms McVeigh added that the technology uses the internet to send pictures to a central reporting station, which can only be accessed by appointed officers and that Fusus and no other external party has access to the data. ', 'However, independent councillor Emma Dent Coad questioned why Kensington and Chelsea was the only council trialling the software.', '""In America, it was used to identify activists in Black Lives Matter and in China to entrap Uyghur Muslims and it has been banned in the EU. They are no longer using this kind of technology,"" she said.', '""CCTV is one thing, this is a step up. I really think people need to understand exactly what it is and the implications of it, where it\'s been banned in some areas.""', 'Ms McVeigh accused Ms Dent Coad of making ""odd comments"" about residents and said the technology was being used to keep people safe.', 'She said the residents had been aware of the trial which would last for 60 days. ', '""We are only going to look to implement this if more residents agree, so we will be consulting on this,"" she added.', 'Listen to the best of BBC Radio London on Sounds and follow BBC London on Facebook, X and Instagram. Send your story ideas to hello.bbclondon@bbc.co.uk', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Filters and Photoshop move over, artificial intelligence (AI) is the new trend for creating online profile pictures.', 'Over the summer a video went viral on TikTok. It was captioned ""using this trend to get a new LinkedIn headshot"".', 'In the short clip, a young woman shows both how she looks in real life, and the professional-looking headshot photos that she created using an AI-powered app called Remini. The video has now been watched 52.3 million times, and a host of similar ones from other TikTokers have also been extensively viewed. ', 'Remini, and competitors such as Try It On AI and AI Suit Up, use AI-based software to create slick profile photos that aim to look as if they were taken by an expert photographer.', 'With Remini you are asked to upload eight to 10 selfies, preferably taken from different angles, and all in good lighting. The AI uses those pictures to learn about the way you look.', 'Then just a few minutes later it will start creating artificial photos of you looking very smart and even glamorous, with your hair in different styles or positions, and you wearing different clothes while sitting in perfect lighting. ', 'It also gives you faultless skin, and improves your make-up. Plus, you get different backdrops. And some users find that it makes them look thinner. ', 'Read additional stories on artificial intelligence', 'The results are somewhat in the eye of the beholder - some say they are realistic, while others find that the images look artificial. ', 'But while previous online image manipulation trends, such as drastically changing your hair or eye colour, have been about having fun on social media, this one is very much focused on LinkedIn and other job hunting websites.', 'For some the attraction of the AI services is that they are cheap.', 'Divya Shishodia, 24, a digital marketer, from Australia, says that while AI headshots ""are obviously generated, some people might not have the budget to go and get a professional headshot taken"".', 'While going to a professional photographer can cost more than Â£100, Remini and the other providers will generally give you free trials lasting a few days.', '""I\'m not saying they\'re the most realistic, but for the amount of time and effort you have to put in... the output is worth it,"" says Ms Shishodia. She adds that, by contrast, if you try to take a decent profile photo yourself it can be very difficult.', '""You need angles, lighting, you are trying to avoid shadowsâ\x80¦ only actual photographers can do it.""', 'For Michelle Genobisa, 26, from Aalborg, Denmark, it is the low to no cost of the AI generated profile photos that she is on board with. ', '""I quite often change my looks, like my hair colourâ\x80¦ so it was an easy way to collect some pictures with the effect of a professional photoshoot,"" she says. ""To get that kind of photo taken, professionally, it\'s very expensive.""', 'Others are less impressed by the technology, such as Molly McCrann, a 25-year-old actor from Australia. ""I just think it looks so fake, you can tell that it looks heavily edited, or it looks like AI,"" she says.', '""When I posted mine it made me look so skinny, and I don\'t look like that.""', 'Ms McCrann adds that she thinks it is probably better to show prospective employers what you actually look like. ', 'However, she is also prepared to see the other side of the argument. ""Someone wrote a comment that I actually agree with - if this company is going to base off looks, I want to get in the room. And if this is going to get me in the room, then I am going to use AI headshots to get the interview.""', 'But what about the potential impact that AI-improved images can have on our self-esteem? Consumer psychologist Dr Paul Marsden says there are two sides to the issue.', 'This video can not be played', 'Users divided over AI generated profile pictures', '""On one hand it could allow us to put our best self forward, and the image of ourselves that we want to project to the world, and in turn motivate us to be that way inclined in real life,"" he tells the BBC.', '""The psychology of first impressions is how we make snap decisions based on initial impressions, and by using AI people can put themselves in the running to potentially be considered for an opportunity. On the other hand it could affect people\'s self-worth and beliefs that they themselves are not good enough comparatively to their AI generation resulting in low confidence.""', 'Do recruiters care? Tristan Barthel from London-based Tate Recruitment has seen a big rise in the number of people using AI to improve their photos. ', 'He says that it makes no difference in how he deals with a person\'s application. ""I can see if a picture has been AI generated, and it wouldn\'t affect my decision, for me it\'s about the qualifications.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The artificial intelligence (AI) industry could consume as much energy as a country the size of the Netherlands by 2027, a new study warns.', 'Big tech firms have scrambled to add AI-powered services since ChatGPT burst onto the scene last year.', 'They use far more power than conventional applications, making going online much more energy-intensive.', ""However, the study also said AI's environmental impact could be less than feared if its current growth slowed."", 'Many experts, including the report author, say such research is speculative as tech firms do not disclose enough data for an accurate prediction to be made.', 'There is no question, though, that AI requires more powerful hardware than traditional computing tasks. ', 'The study, by Alex De Vries, PhD candidate at the VU Amsterdam School of Business and Economics, is based on some parameters remaining unchanged - such as the rate at which AI is growing, the availability of AI chips, and servers continuing to work at full pelt all the time.', 'Mr De Vries considered that the chip designer Nvidia is estimated to supply about 95% of the AI processing kit required by the sector.', 'By looking at the amount of these computers it is expected to deliver by 2027, he was able to approximate a range for the energy consumption of AI of 85-134 terrawatt-hours (TWh) of electricity each year. ', 'At the top end that is roughly the amount of power used annually by a small country.', '""You would be talking about the size of a country like the Netherlands in terms of electricity consumption. You\'re talking about half a per cent of our total global electricity consumption,"" he told BBC News.', 'Nvidia declined to comment.', 'Mr De Vries said his findings showed that AI should be used only where it is really needed.', 'His peer-reviewed study has been published in the journal Joule.', ""AI systems such as the large language models that power popular chatbots, like OpenAI's ChatGPT and Google's Bard, require warehouses full of specialist computers - called data centres - to work."", 'That means the equipment is more power-hungry and, like traditional kit, it also needs to be kept cool, using water-intensive systems.', ""The research did not include the energy required for cooling. Many of the big tech firms don't quantify this specific energy consumption or water use. Mr de Vries is among those calling for the sector to be more transparent about it."", 'But there is no doubt demand for the computers that power AI is mushrooming - and with it the amount of energy needed to keep those servers cool.', 'Danny Quinn, boss of the Scottish data centre firm DataVita, said his company has gone from receiving ""one or two enquiries a week"" at the start of 2023 about using his facility to house AI kit, to receiving hundreds. ', 'He also described the difference in energy use between a rack containing standard servers, and one containing AI processors. ', '""A standard rack full of normal kit is about 4 kilowatts (kW) of power, which is equivalent to a family house. Whereas an AI kit rack would be about 20 times that, so about 80kW of power. And you could have hundreds, if not thousands, of these within a single data centre.""', ""He added that Scotland's colder and wetter climate provided a natural advantage in helping the data centres with keeping equipment cool, but it is still a huge task."", 'In its latest sustainability report, Microsoft, which is investing heavily in AI development, revealed that its water consumption had jumped by 34% between 2021 and 2022, to 6.4 million cubic metres, around the size of 2,500 Olympic swimming pools. ', 'Prof Kate Crawford, who wrote a book about AI and its impact on the environment, said the issue kept her awake at night.', 'Speaking to the BBC in July, she said: ""These energy-intensive systems take enormous amounts of electricity and energy, but also enormous amounts of water to cool these gigantic AI supercomputers. So we are really looking at an enormous extractive industry for the 21st Century.""', 'But there are also hopes that AI could help solve some of the environmental challenges facing the planet.', 'Google and American Airlines recently found pilots could halve the amount of contrails (vapour trails) created by aircraft by using an experimental AI tool to select altitude. Contrails are known to contribute to global warming.', 'And the US government is among those spending millions of dollars on trying to recreate nuclear fusion - the way the Sun gets its energy. ', 'Success here would be a real game changer, in the form of a limitless, green power supply. AI could speed up the research, which has been going on since the 1960s with very slow progress.', 'In February this year, university academic Brian Spears said he had used AI to predict an outcome in an experiment which resulted in a breakthrough.', '""For 100 trillionths of a second, we produced ten petawatts of power. It was the brightest thing in the solar system,"" he wrote.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['More than 110 Palestinians are reported to have been killed while trying to get desperately needed aid in north Gaza.', 'Crowds of waiting civilians descended on a convoy of lorries after it passed through an Israeli military checkpoint on the coastal road west of Gaza City.', ""Israel's military said troops fired at some people they thought were a threat. "", 'In the ensuing chaos, the lorries attempted to move forward. A Palestinian witness told the BBC that most of those who died were run over.', ""At least 112 people were killed and 760 others injured in the incident, the spokesman of Gaza's Hamas-run health ministry, Ashraf al-Qudra, said in a statement on Thursday afternoon."", 'Dramatic aerial footage released by the Israeli military showed thousands of people on and around the lorries, while graphic videos of the aftermath posted on social media showed some of the dead loaded onto emptied aid lorries and a donkey cart.', 'Gaza\'s Hamas-run health ministry blamed Israel for what it called a ""massacre"", while US President Joe Biden expressed concern that it would complicate efforts by the US and other mediators to broker a temporary ceasefire in the war between Hamas and Israel.', 'The incident happened hours before the health ministry announced that more than 30,000 people, including 21,000 children and women, had been confirmed killed in Gaza since the start of the conflict. ', 'Some 7,000 others have been reported as missing and 70,450 have been treated for injuries over the past four months, according to the ministry.', '""This is deeply shocking because if you add the number of people who have been injured and the number of people who are missing you have more than 100,000 people, which represents 5% of the population,"" Philippe Lazzarini, the head of the UN agency for Palestinian refugees (Unrwa), told the BBC.', 'The UN is also warning of a looming famine in the north of the territory, where an estimated 300,000 people are living with little food or clean water.', 'The Israeli military launched a large-scale air and ground campaign to destroy Hamas - which is proscribed as a terrorist organisation by Israel, the UK and others - after its gunmen killed about 1,200 people in southern Israel on 7 October and took 253 other people back to Gaza as hostages.', ""Thursday's incident took place shortly after 04:00 (02:00 GMT), past an Israeli military checkpoint on Rashid Street, which runs along the Mediterranean coast. Palestinian sources gave the location as the Nabulsi roundabout, on the south-western edge of Gaza City."", 'A convoy of between 18 and 30 aid lorries, likely to have been a few hundred metres long, passed through the checkpoint, heading north. ', 'Shortly afterwards, with the last lorry only about 70m (230ft) north of the checkpoint, Palestinians - many of whom had been camped out nearby, waiting for the arrival of aid - descended on the convoy.', 'Israel Defense Forces (IDF) spokesman Lt Col Peter Lerner said some civilians approached the checkpoint and ignored warning shots fired by the soldiers there. ', 'Fearing that some of the civilians posed a threat, the soldiers then opened fire on those approaching in what Lt Col Lerner described as a ""limited response"".', ""The BBC's Palestinian witness source did not confirm that civilians approached the checkpoint - only that they were about 70m (229ft) away."", 'With crowds descending on all the lorries, and with machine-gun fire coming from the checkpoint, panic seems to have ensued. ', 'The lorries - some of them now with many people clinging on - tried to move forward. ', 'The Palestinian witness said the bulk of the casualties were caused by the lorries running people over, not by the Israeli gunfire. ', ""The spokesman of Gaza's Hamas-run health ministry, Ashraf al-Qudra, said that dozens of casualties in a critical or severe condition were brought to the nearby al-Shifa Hospital, in Gaza City, and that medics there were unable to cope with the volume and severity of cases."", 'At the hospital, one man who was cradling the body of this dead friend, Tamer Shinbari, told the BBC he had gone to Nabulsi roundabout hoping to get a bag of flour for his family who are sheltering in schools in Jabalia. ', 'He said Israeli soldiers opened fire ""and the aid lorry ran over the bodies"".', 'The director of the Kamal Adwan hospital in the northern town of Beit Lahia, Hussam Abu Safieyah, told Reuters news agency that it had received the bodies of 10 people and dozens of wounded from western Gaza City.', 'The acting director of the al-Awda Hospital in Jabalia meanwhile told the Associated Press that it had received 161 wounded patients, most of whom appeared to have been shot. ', 'The IDF said in a statement that ""every civilian casualty is a tragedy"".', '""Despite the very difficult circumstances (brought about by Hamas\' decision to go to war against Israel), we are continuing to work to facilitate the delivery of humanitarian aid to civilians across the Gaza Strip,"" it added.', '""We will learn from this difficult incident in order to try and find better solutions for the transfer of aid to those who need it.""', 'But Hamas and Palestinian President Mahmoud Abbas, who is based in the occupied West Bank, blamed Israeli forces for what they called a ""heinous massacre"".', '""The killing of this large number of innocent civilian victims who risked their livelihood is considered an integral part of the genocidal war committed by the occupation government against our people,"" Mr Abbas said in a statement, adding that Israel bore ""full responsibility"".', 'A spokesman for UN Secretary General AntÃ³nio Guterres said he ""condemned"" the incident.', '""The desperate civilians in Gaza need urgent help, including those in the besieged north where the United Nations has not been able to deliver aid in more than a week,"" Stephane Dujarric said, adding that Mr Guterres reiterated his call for ""an immediate humanitarian ceasefire and the unconditional release of all hostages"".', 'The north of Gaza suffered widespread devastation after being the focus of the first phase of the Israeli ground offensive.', 'It has been largely cut off from humanitarian assistance for several months, despite some relief efforts by UN aid agencies.', ""Last week, the World Food Programme said it had been forced to suspend aid deliveries to northern Gaza after its first convoy in three weeks was surrounded by crowds of hungry people close to the Israeli military's Wadi Gaza checkpoint, and then faced gunfire in Gaza City. "", 'Another convoy faced what it called ""complete chaos and violence due to the collapse of civil order"". Several lorries were looted in central Gaza and a driver was beaten.', 'On Tuesday, a senior UN aid official warned that at least 576,000 people across the Gaza Strip - one quarter of the population - faced catastrophic levels of food insecurity and were at risk of famine.', 'He also warned that one in six children under the age of two in the north were suffering from acute malnutrition and wasting.', ""On Wednesday, Gaza's health ministry said six children had died from dehydration and malnutrition at hospitals in northern Gaza. Two of the deaths were at al-Shifa and four at the Kamal Adwan, it added."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Snapchat has been accused of a ""worrying failure"" to assess the potential privacy risks its AI chatbot poses to users - especially children - by the UK\'s data watchdog.', 'The Information Commissioner\'s Office (ICO) warned it could close down the My AI feature in the UK after a ""preliminary investigation"".', 'The US company said it was ""closely reviewing"" the provisional findings.', 'The tool lets users message a computer which mimics human conversation.', 'Snapchat describes it as an ""evolving feature"" which is powered by ChatGPT, an online AI tool which uses new technology to convincingly imitate realistic responses.', 'Snap, the parent company behind Snapchat, became the first social media platform to adopt an artificial intelligence-powered chat function earlier this year.', 'The app has 21 million users in the UK, many of whom are children. The ICO said it was particularly concerned about the potential privacy risks for 13- to 17-year-old users.', 'Snap said it would ""work constructively"" with the ICO after it issued a preliminary notice against the company, adding that it had carried out a ""robust legal and privacy review"" before the function went public.', 'The data watchdog stressed its findings are not final and it has not concluded that the company breached any data protection laws.', ""At this stage the notice is a signal to Snap to ensure My AI complies with data protection rules which includes the Children's Design Code. "", ""The watchdog's code contains 15 standards that online services need to follow. This ensures they are complying with their obligations under data protection law to protect children's data online."", 'The ICO said that if a final enforcement notice was to be adopted, Snap might not be able to offer the My AI function to UK users until the company carries out ""an adequate risk assessment"".', 'The company describes My AI as ""an experimental and friendly"" chatbot designed to be a personal sidekick to each Snapchatter who chats with it.', ""The feature, which can be used as an assistant to plan day trips or create menus, has more than two million chats per day happening on the app, according to Snap's boss Evan Spiegel."", 'It was made available to all Snapchat users in April, after being launched for a fee in February.', 'Since then, the social media platform said ""a lot of progress"" had been made in its capabilities although it admitted that ""mistakes may occur"". ', '""My AI may answer incorrectly, provide biased answers or note it is unsure of the answers so don\'t rely on its advice,"" the company said.', 'Snap has also been criticised for being unclear over whether the chatbot can access private information such as location data.', '""Snapchat can only ever access your location if you consent to share it,"" the firm said.', 'One of the other concerns about My AI is - because of how young users of Snapchat skew - whether they really understand the implications of data collection.', '""Privacy is a foundational value for us - it is critical to our core use case of helping people visually communicate with their friends and family,"" the platform stressed.', 'Information Commissioner John Edwards said, ""The provisional findings of our investigation suggest a worrying failure by Snap to adequately identify and assess the privacy risks to children and other users before launching My AI.', '""We have been clear that organisations must consider the risks associated with AI, alongside the benefits.', '""Today\'s preliminary enforcement notice shows we will take action in order to protect UK consumers\' privacy rights.""', ""In the case of serious breaches, the ICO has the power to issue fines of Â£17.5 million or 4% of a company's annual worldwide turnover from the preceding financial year, whichever is higher."", 'A Snap spokeswoman said: ""We are closely reviewing the ICO\'s provisional decision.', '""Like the ICO, we are committed to protecting the privacy of our users.', '""In line with our standard approach to product development, My AI went through a robust legal and privacy review process before being made publicly available.', '""We will continue to work constructively with the ICO to ensure they\'re comfortable with our risk assessment procedures.""', ""Snap will now have a chance to respond to the regulator's concerns before the ICO makes its final decision."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Police and private companies should ""immediately stop"" the use of facial recognition surveillance, says a group of politicians and privacy campaigners.', 'They have raised concerns around human rights, potential for discrimination and ""the lack of a democratic mandate"".', 'It comes after the government announced plans for police to access passport photos to help catch criminals.', 'The Home Office said facial recognition had ""a sound legal basis"" and had already led to criminals being caught.', 'A spokesperson added that the technology could also aid police in searching for missing or vulnerable people, and free up officers to ""be out on the beat"" and to carry out complex investigations.', 'Live facial recognition cameras scan faces of the public in specific locations and compare these with people on ""watch lists"" who may be wanted by police or the courts in association with crimes.', 'Police forces using the technology in the UK inform citizens in advance about when and where it will be deployed, and display physical notices alerting those entering areas where it is active to the presence of cameras.', 'But this week, policing minister Chris Philp said he wanted officers to be able to access a wider range of databases for images besides those on its national database, which is limited to those who have been arrested. ', 'Campaigners have called for it to be banned ""immediately"".', '""This dangerously authoritarian technology has the potential to turn populations into walking ID cards in a constant police line up,"" says Silkie Carlo, the director of privacy organisation Big Brother Watch.', 'The group calling for the ban includes parliamentarians from the Conservatives, Labour, Liberal Democrat and Green parties, along with campaigning organisations such as Amnesty, Index on Censorship and Big Brother Watch.', 'The UK\'s surveillance camera commissioner has also criticised the plans, saying they could damage public trust and make passport-holders feel as if they were in a ""digital line-up"".', ""South Wales Police has been criticised over its live facial recognition use at events including Harry Styles and BeyoncÃ© concerts in Cardiff. The Metropolitan Police has used it several times this year, including at the King's Coronation in May."", 'Both forces have said that if a person is not on a watch list, the biometric data will be immediately deleted and not stored.', 'In April, Frasers Group - which operates Sports Direct, Flannels and House of Fraser - defended its use of live facial recognition cameras in some of its shops, saying the system provided by FaceWatch had helped cut crime since being installed. ', 'Ms Carlo, of Big Brother Watch, argued the UK\'s ""approach to face surveillance makes us a total outlier in the democratic world, especially against the backdrop of the EU\'s proposed ban"".', ""Members of the European Parliament agreed to ban live facial recognition using AI in a draft of its Artificial Intelligence (AI) Act - the EU's landmark legislation categorising different applications of AI according to their harm to the public. "", 'The Home Office said the government was ""committed to making sure the police have the tools and technology they need to solve and prevent crimes, bring offenders to justice, and keep people safe"".', '""Facial recognition, including live facial recognition, has a sound legal basis that has been confirmed by the courts and has already enabled a large number of serious criminals to be caught, including for murder and sexual offences,"" a spokesperson said.', 'They added there was a ""robust legal framework for it use"".', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A campaign using artificial intelligence to impersonate Omar al-Bashir, the former leader of Sudan, has received hundreds of thousands of views on TikTok, adding online confusion to a country torn apart by civil war.', 'An anonymous account has been posting what it says are ""leaked recordings"" of the ex-president since late August. The channel has posted dozens of clips - but the voice is fake. ', ""Bashir, who has been accused of organising war crimes and was toppled by the military in 2019, hasn't been seen in public for a year and is believed to be seriously ill. He denies the war crimes accusations."", 'The mystery surrounding his whereabouts adds a layer of uncertainty to a country in crisis after fighting broke out in April between the military, currently in charge, and the rival Rapid Support Forces militia group.', 'Campaigns like this are significant as they show how new tools can distribute fake content quickly and cheaply through social media, experts say.', '""It is the democratisation of access to sophisticated audio and video manipulation technology that has me most worried,"" says Hany Farid, who researches digital forensics at the University of California, Berkeley, in the US.', '""Sophisticated actors have been able to distort reality for decades, but now the average person with little to no technical expertise can quickly and easily create fake content.""', 'The recordings are posted on a channel called The Voice of Sudan. The posts appear to be a mixture of old clips from press conferences during coups attempts, news reports and several ""leaked recordings"" attributed to Bashir. The posts often pretend to be taken from a meeting or phone conversation, and sound grainy as you might expect from a bad telephone line.', ""To check their authenticity, we first consulted a team of Sudan experts at BBC Monitoring. Ibrahim Haithar told us they weren't likely to be recent:"", '""The voice sounds like Bashir but he has been very ill for the past few years and doubt he would be able to speak so clearly.""', ""This doesn't mean it's not him."", 'We also checked other possible explanations, but this is not an old clip resurfacing and is unlikely to be the work of an impressionist.', 'The most conclusive piece of evidence came from a user on X, formerly Twitter.', 'They recognised the very first of the Bashir recordings posted in August 2023. It apparently features the leader criticising the commander of the Sudanese army, General Abdel Fattah Burhan.', 'The Bashir recording matched a Facebook Live broadcast aired two days earlier by a popular Sudanese political commentator, known as Al Insirafi. He is believed to live in the United States but has never shown his face on camera. ', ""The pair don't sound particularly alike but the scripts are the same, and when you play both clips together they play perfectly in sync."", 'Comparing the audio waves shows similar patterns in speech and silence, notes Mr Farid. ', 'The evidence suggests that voice conversion software has been used to mimic Bashir speaking. The software is a powerful tool that allows you to upload a piece of audio, which can be changed into the different voice. ', ""After further digging, a pattern emerged. We found at least four more of the Bashir recordings that were taken from the same blogger's live broadcasts. There is no evidence he's involved. "", ""The TikTok account is exclusively political and requires deep knowledge of what's going on in Sudan, but who benefits from this campaign is up for debate. One consistent narrative is criticism of the head of the army, Gen Burhan."", ""The motivation might be to trick audiences into believing that Bashir has emerged to play a role in the war. Or the channel could be trying to legitimise a particular political viewpoint by using the former leader's voice. What that angle might be is unclear."", 'The Voice of Sudan denies misleading the public and says they are not affiliated with any groups. We contacted the account, and received a text reply saying: ""I want to communicate my voice and explain the reality that my country is going through in my style.""', 'An effort on this scale to impersonate Bashir can be seen as ""significant for the region"" and has the potential to fool audiences, says Henry Ajder, whose series on BBC Radio 4 examined the evolution of synthetic media. ', 'AI experts have long been concerned that fake video and audio will lead to a wave of disinformation with the potential to spark unrest and disrupt elections.', '""What\'s alarming is that these recordings could also create an environment where many disbelieve even real recordings,"" says Mohamed Suliman, a researcher at Northeastern University\'s Civic AI Lab. ', ""As we've seen with this example, people should question whether the recording feels plausible before sharing.  "", ""Checking whether it was released by a trusted source is vital, but verifying audio is difficult, particularly when content circulates on messaging apps. It's even more challenging during a time of social unrest, such as that currently being experienced in Sudan.  "", 'The technology to create algorithms trained to spot synthetic audio is still at the very early stages of development, whereas the technology to mimic voices is already quite advanced.', 'After being contacted by the BBC, TikTok took down the account and said it broke their guidelines on posting ""false content that may cause significant harm"" and their rules on the use of synthetic media.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""They sing, they dance, they model, but they don't exist in real life - virtual influencers are trying to break out of the metaverse and into the charts."", 'From Alvin and The Chipmunks to Gorillaz, and Hatsune Miku to Polar, the music industry is no stranger to virtual characters as popstars.', 'Like many of today\'s human artists, they\'ve won Grammy Awards, held concerts as holograms, and can even be ""cancelled"" over controversial comments.', 'Noonoouri, a digital character created by German designer Joerg Zuber, is the latest blue-haired, doll-faced virtual influencer to land a record deal.', ""Created using motion capture and advanced graphics, she's been signed to Warner Music as its first avatar artist, rubbing shoulders (virtually) with Ed Sheeran, Dua Lipa, Cardi B and Ashnikko at one of the industry's biggest labels."", 'Her vocals, built in part using artificial intelligence (AI) tools, have seen her dubbed an ""AI popstar"".', ""And while Warner Music says AI only played a minor part in the creation of Noonoouri's vocals, her signing still comes amid growing concerns over the role AI plays in music creation, and whether it will displace or defraud artists by duplicating their sound, style or image."", 'Marec Lerche, head of business development at Warner Music Central Europe, says Noonoouri and digital creations like her can offer labels more experimentation and flexibility.', '""You can appear in different places at the same time, you can change her style in a minute - we can make Nonooouri fly if we want, because it feels natural to her,"" he says.', '""She is already a digital character so there are more opportunities than with a human artist in that respect.""', 'Read additional stories on artificial intelligence', ""Noonoouri's pivot to music follows that of another top virtual influencer, Lil Miquela, who first began to release tracks in 2017, and has received millions of views for her music videos on YouTube."", 'It comes as streaming now reigns supreme in the music industry, and artists - like screenwriters and actors in Hollywood - are fighting for a bigger slice of revenues. ', 'Musicologist Dr Shara Rambarran says virtual influencers-turned-popstars entering the arena may unsettle IRL (in real life) musicians ""who want to put their music out there and not have to have that [additional] competition"".', ""As an almost entirely virtual creation, Warner Music's new signing is not exactly your average artist."", 'Similarly to ""vocaloids"" like Hatsune Miku - who evolved out of voice synthesiser software into a fully sketched character - Noonoouri has been created using a combination of different technologies to virtually dance, sing and strut down runways.', 'She has 400,000 followers on Instagram, a modelling contract, and a doll-like appearance that can be tweaked or transformed at the click of a button.', 'Mr Zuber, who works at a design studio in Munich, says Noonoouri is a ""lifetime project"" for him.', ""It was fleshed out by him and his team of five people as a character that originally could speak about and highlight the world's problems in a lifelike, 3D form. "", '""We started with the hair, the drawings of the fabric, the movement and everything, and I slipped into a motion capture suit to walk as her... to define her movements, her gestures and everything,"" he says.', 'Mr Zuber adds that the team was keen to give Noonoouri a more ""techie"" than human sounding voice - which meant overlaying his base vocals for the character with a melodic track from a professional singer.', ""Algorithms were then used to automatically correct Noonoouri's original vocal recordings and help the team match the speed, tempo and pitch of the singer's voice."", 'Her signing to Warner comes against the backdrop of concerns in the music industry over the rise of the technology.', ""While artists such as Grimes and David Guetta have said they are embracing such tech to experiment with their music production, the likes of Sting and Ed Sheeran have criticised the use of AI. Meanwhile, Hozier recently told the BBC's Newsnight that he would consider striking over the threat posed to the industry by AI."", 'Jamie Njoku-Goodwin, chief executive of industry association UK Music, says that many in the industry are excited about the opportunities AI might give artists and producers, but figuring out the regulatory and legal landscape so it ""can enable human creativity, not erode it"" is key.', '""It\'s about knowing what content and what data AI is being trained on, [and] about ensuring there\'s adequate labelling so we know whether or not a piece of music is AI-generated,"" he says.', ""Dr Shara Rambarran has written about virtual musicians, and says that while the trend of digital popstars is unlikely to abate in future, it's also unlikely to usurp tangible humans topping the charts. "", '""It\'s not a new concept at all, they\'ve always existed in some shape or form,"" she says.', '""But will it overtake everything in the music industry? I don\'t think so. I think there\'s going to be room for everybody.', '""There\'s always going to be another innovative creation we\'re going to be talking about and if this doesn\'t work out, something\'s going to replace it.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Top songwriters who've worked with artists including Doja Cat, Jonas Brothers and BTS say artificial intelligence (AI) isn't something to be afraid of."", ""BBC Newsbeat spoke to the people behind some of the world's biggest hits on the red carpet at the BMI London Awards."", 'Opinions on the technology are split, but writers tell us AI can be a useful tool.', ""Some use it to mimic the voice of an artist they're writing for to check how a track sounds, or to break through bouts of writers' block. "", ""But they all agree that AI can't imitate the artistry and human emotion that goes into making music."", 'One person who knows all about that is Linden Jay.', ""He was one of the writers on Doja Cat's Woman, which won Song of the Year at the awards ceremony."", 'Linden welcomes AI as a tool, but not something that should fully replace the craft of writing a good song. ', '""I\'ve been using it a little bit in my writing just to help advance ideas,"" he says. ', '""And, you know, I\'m not the greatest singer in the world so sometimes I sing and I\'ll turn it into a famous artist\'s voice, just to get an idea of if something is headed in a good direction.""', ""Linden's co-writer on Woman, Aaron Horn, agrees."", '""I think it\'s a tool that people can utilise to help fill in the gaps,"" he says.', '""It\'s just great to have new tools... tools are always coming into the studio.""', ""Although he says he hasn't used AI much, Aaron compares it to rhyming dictionary, a widely used online platform that suggests rhymes for poems and lyrics."", 'In Aaron and Linden\'s view, ""a good song is a good song"" and the industry shouldn\'t be afraid of embracing new technologies. ', ""Jessica Agombar, who wrote What a Man Gotta Do for the Jonas Brothers, says she's found inspiration with AI."", 'She\'s been impressed by some of the ""mind-boggling"" capabilities like the voice mimicking Linden identified.', 'That ability did worry some singers, after an unauthorised track released this year used AI cloned vocals from Drake and The Weeknd.', ""But despite the boss of Spotify saying the app wouldn't completely ban AI-made music, Jessica thinks artistry will be preserved."", '""For me, there\'s always art in organic songwriting and producing, putting your own vocals on the record, and having some rough sketches of bad notes and bum notes,"" she says.', '""Because that\'s rock and roll - I\'m more for that than the whole clean, polished AI, computerised thing.""', ""Kamille, best known for her work with Little Mix, Mabel and Kylie Minogue, says it's important technology doesn't make the process of writing a song too easy."", '""I try my best to stay away from it,"" she says. ', '""I just feel like I want to just lean on my own brain and make sure I\'m not losing that craft I have and becoming too dependent on it.', '""I definitely feel like the key of songwriting is getting the emotion out from you and your heart.', '""I think that\'s a really important part that we shouldn\'t lose as much as technology advances.""', ""For anyone considering a career in the industry, the writers say it's best not to rely too much on AI."", '""I don\'t think machines can do it on their own unless they were gonna write music for machines,"" says Aaron.', '""Embrace AI and believe in yourself,"" he says. ""Explore your own experience and humanity - that\'s what AI can\'t draw upon.""', 'Listen to Newsbeat live at 12:45 and 17:45 weekdays - or listen back here.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Tom Hanks has warned an advert that appears to be fronted by him is in fact an artificial intelligence (AI) fake.', '""There\'s a video out there promoting some dental plan with an AI version of me,"" the actor wrote on Instagram.', '""I have nothing to do with it,"" he added.', 'Hanks has previously spoken about the ""artistic challenge"" that AI poses his industry, and the issue has been central to recent strikes by high-profile Hollywood actors and writers.', 'As AI systems have grown in power and sophistication, so have concerns about their ability to create ever more realistic virtual versions of real people - what are sometimes called deepfakes.', 'A number of celebrities - including the consumer financial expert, Martin Lewis - have had their likenesses used in deepfakes, which are often used to scam people.', 'The use of deepfakes in pornography, sometimes used as a form of revenge, prompted the government to toughen the law in England and Wales to make it easier to prosecute offenders.', 'Faked AI images and videos of politicians are also exacerbating the problem of online misinformation. Former US President Donald Trump and the current leader of Ukraine, Volodymyr Zelensky, are among those who have been targeted.', 'In September, Google announced it would require any political adverts that ran on its platform to disclose if they had been created with AI.', 'AI video manipulation can also be used in non-controversial ways - for example, the pioneering virtual concerts featuring the band Abba.', 'The possibility of AI being used to extend the careers of performing artists was one Hanks discussed when he appeared on the Adam Buxton podcast in May.', '""We saw this coming, we saw that there was going to be this ability to take zeros and ones from inside a computer and turn it into a face and a character. That has only grown a billion-fold since then and we see it everywhere,"" he said.', '""Anybody can now recreate themselves at any age they are by way of AI or deepfake technology. I could be hit by a bus tomorrow and that\'s it, but performances can go on and on and on and on.""', 'Fears about being displaced by AI have helped drive a wave of strikes that have disrupted Hollywood, with Stranger Things and the Last of Us among the shows to be affected.', 'The Writers Guild of America (WGA), which represents screenwriters, recently reached a tentative agreement with studio bosses to bring their industrial action to an end.', 'However, a separate dispute involving actors - which is also partly motivated by fears about AI resulting in fewer acting jobs - remains unresolved.', 'Sign up for our morning newsletter and get BBC News in your inbox.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Charity supporters have threatened to stop donating if the Air Ambulance goes ahead with controversial base closures.', 'A review proposes moving helicopters from Welshpool, Powys, and Caernarfon, Gwynedd, to Rhuddlan in Denbighshire.', 'The charity said this would ""ensure as many people as possible"" benefit and allow it to respond to 139 extra calls a year.', 'Beryl Vaughan from Llanerfyl has helped raised Â£110,000 for the charity, but said she ""would be thinking twice"".', 'There are four Air Ambulance bases in total, the other two are in Cardiff and Llanelli, Carmarthenshire. ', 'Russell George, Member of the Senedd for Montgomeryshire, said many of his constituents planned on stopping donations if the mid Wales base closes.', '""We live in a very rural area and we do need it. We\'ve got to keep the Air Ambulance base in Welshpool.""', 'During public engagement the review heard concerns that - if bases close - donations from those areas could ""decrease and destabilise the service"".', 'In Welshpool town centre, friends Louise Clare and Jan Lee had different views. ', 'Ms Clare said: ""I do support it, but if it moved to north Wales, I wouldn\'t be so keen. I guess I feel it\'s local, so you pay for it to be here.""', 'Ms Lee added: ""Mine is a lottery donation and I\'ve been doing it for donkey\'s years. I don\'t think I would change it, because wherever it goes it\'s going to be needed.""', 'Sarah Pritchard, who lives in the town said donations could be hit ""because people are angry about it"".', '""I hate to think that anyone would stop supporting it because everybody needs the Air Ambulance, however it\'s half and half,"" she said. ', 'Leaders of health boards and trusts in Wales have identified what they describe as ""the two strongest options"" for improving the service.', 'Both plans would close the Welshpool and Caernarfon air bases and open a new one near the A55, with two options: ', 'According to the review, the first plan would treat an extra 139 patients annually, with 208 more under the second.', 'Campaigners who want to keep the Welshpool site claim people living in rural areas would lose out. ', 'Bob Benyon said: ""We can\'t rationalise how they can justify closing Welshpool which can access 1,258,000 people in 24 minutes. ', '""That\'s an incredible 477,000 more than the proposed base at Rhuddlan. So, it\'s inevitable that the response times will be slower to a lot of people.', '""They will have to circumnavigate the Clwydian Range, to reach not only Welshpool but Newtown, Llandrindod Wells, Ceredigion, Aberystwyth.""', 'Stephen Harrhy, chief ambulance services commissioner for Wales, is leading the review and said experts were advising on the location and would not ""put helicopters in a base with worse flying conditions"".', 'The Wales Air Ambulance said medical teams in Welshpool and Caernarfon were under used and it supported the aim of the review to address underuse and unmet need. ', 'It added that all donations would be ""used in the most effective, patient-focused way"".', '""This means saving as many lives across Wales as possible and, in doing so, making sure that no community is materially disadvantaged,"" it said.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Documents disclosed to the BBC have shed light on the use of AI-powered chatbot technology within government.', 'The chatbots have been used to analyse lengthy reports - a job that would normally be done by humans.', 'The Department for Education, which ran the trial, hopes it could boost productivity across Whitehall.', 'The PCS civil service union says it does not object to the use of AI - but clear guidelines are needed ""so the benefits are shared by workers"".', 'The latest generation of chatbots, powered by artificial intelligence (AI), can quickly analyse reams of information, including images, to answer questions and summarise long articles. ', 'They are expected to upend working practices across the economy in the coming years, and the government says they will have ""significant implications"" for the way officials work in future.', 'The education department ran the eight-week study over the summer under a contract with London-based company Faculty.ai, to test how so-called large language models (LLMs) could be used by officials.', ""The firm's researchers used its access to a premium version of ChatGPT, the popular chatbot developed by OpenAI, to analyse draft local skills training plans that had been sent to the department to review."", 'These plans, drawn up by bodies representing local employers, are meant to influence the training offered by local further education colleges.', ""Results from the pilot are yet to be published, but documents and emails requested by the BBC under Freedom of Information laws offer an insight into the project's aims."", 'According to an internal document setting out the reasons for the study, a chatbot would be used to summarise and compare the ""main insights and themes"" from the training plans.', 'The results, which were to be compared with summaries produced by civil servants, would test how Civil Service ""productivity"" might be improved. ', 'It added that language models could analyse long, unstructured documents ""where previously the only other option for be for individuals to read through all the reports"".', 'But the project\'s aims went further, with hopes the chatbot could help provide ""useful insights"" that could help the department\'s skills unit ""identify future skills needs across the country"".', 'The pilot offers a glimpse into government-wide efforts in recent years to boost the use of new technologies within the civil service, but also raises questions over how they could be used to shape policy.', 'Government guidance published in June suggests officials can use tools such as ChatGPT as part of their research, or to summarise academic or news reports, if they verify the results.', 'But they are currently banned from inputting any sensitive information, or information that could reveal the ""intent of government"". The guidance is set to be reviewed later this year.', 'Renate Samson, a researcher at the Ada Lovelace Institute, a think tank, said there was enthusiasm within both central and local government for using language models to analyse documents and draft reports.', 'She added that as the technology evolves, the government would need to consider the potential for ""unintended consequences"" in areas like bias, privacy and security.', 'Rupert McNeil, who until last year was head of HR for the Civil Service, said work to measure the potential impact of AI-based automation started in 2018 as part of proposed reforms in the wake of Brexit.', 'He suggested that writing draft ministerial correspondence, as well as the ""first, boring stage"" of drafting new legislation, were areas where chatbot technology could play an increasing role.', 'He added that although automation was likely to eliminate the need for some more junior jobs, it could ""take away a lot of tedium"" from some work, and reduce ""post code lotteries"" for businesses dealing with regulatory bodies.', 'But he added civil servants would be required to take decisions, whilst people with ""technical discernment"" would also be required to identify when AI tools get things wrong.', 'The PCS union, which represents civil servants below the senior ranks, is working with other unions to draw up proposals on using AI within the civil service, to be shared with the government in the coming months.', 'The suggestions are expected to cover areas including the redeployment of staff as the technology evolves, the impact on job descriptions, and proposed rules on how AI is used for government work.', 'Its general secretary, Mark Serwotka, said the PCS did not object in principle to AI being used by officials, but regulation was required ""so the benefits are shared by workers"".', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Apple's chief executive Tim Cook said the firm wants to hire more staff in the UK, in contrast to redundancies seen across the tech sector."", 'He said the company wants to take on more staff to work in artificial intelligence (AI).', 'It comes a day after Fortnite-maker, Epic Games, announced it was cutting 16% of its workforce.', 'Big firms including Amazon, Meta, Google and Microsoft have cut tens of thousands of jobs since 2022.', 'Mr Cook has been critical of the trend of tech layoffs and in May, he called it a ""last resort"".', 'Instead, he told the PA news agency when asked about AI and jobs in the UK: ""We\'re hiring in that area, yes, and so I do expect [investment] to increase.""', 'Companies are pouring money into AI. Amazon announced an investment of up to $4bn (Â£3.3bn) in San Francisco-based AI firm Anthropic on Monday.', ""That followed Microsoft's multibillion dollar investment in ChatGPT maker OpenAI in January."", 'Technology Secretary Michelle Donelan said Apple\'s decision was ""another vote of confidence in our burgeoning tech sector"".', '""Apple\'s ongoing investment in brilliant British talent highlights our global credentials as both an AI and technology superpower,"" she wrote on X, the microblogging site formerly known as Twitter.', 'Antony Walker, deputy chief of techUK, the industry association, said: ""The transformative nature of AI will certainly boost staff headcount in AI businesses over the next couple of years.""', 'But he added: ""In the long term, the skills need of the AI-powered economy of the future is harder to predict. That is why businesses and government should work together on a long-term strategy that puts training in digital skills and lifelong learning at the core."" ', 'Meanwhile, Tim Pullan, CEO of ThoughtRiver, which provides AI for legal professionals, said he thought the global economy was at the start of ""an AI-driven revolution"".', '""It\'s vitally important that the UK is at the forefront of this transformation,"" he said.', '""As a country, we have huge potential to grow as an AI superpower, and I\'m sure this is the start of a trend which will see more and more companies looking to take advantage of the UK\'s deep tech expertise, and the UK benefiting from the investment and innovation that this will bring.""', 'Mr Cook said AI was behind several prominent features on Apple devices, such as software that detects if a person has fallen or been in a crash, as well as more commonly-used tools such as predictive typing.', '""It\'s literally everywhere on our products and of course we\'re also researching generative AI as well, so we have a lot going on,"" he said.', 'Generative AI - artificial intelligence which can create media based on text prompts - continues to be a target of investment for big firms despite widespread concerns over its impact of copyright, or ownership.', 'That is because the software ""learns"" by analysing a massive amount of data often sourced online and people are concerned it draws on their copyrighted work.', 'It has led to high-profile lawsuits in the US, with authors George RR Martin and John Grisham suing OpenAI over claims their books were used to train the system.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Brain surgery using artificial intelligence could be possible within two years, making it safer and more effective, a leading neurosurgeon says.', 'Trainee surgeons are working with the new AI technology, to learn more precise keyhole brain surgery.  ', 'Developed at University College London, it highlights small tumours and critical structures such as blood vessels at the centre of the brain.', 'The government says it could be ""a real game-changer"" for healthcare in the UK. ', 'Brain surgery is precise and painstaking - straying a millimetre the wrong way could kill a patient instantly. ', ""Avoiding damaging the pituitary gland, the size of a grape, at the centre of the brain, is critical. It controls all the body's hormones - and any problems with it can cause blindness. "", '""If you go too small with your approach, then you risk not removing enough of the tumour,"" National Hospital for Neurology and Neurosurgery consultant neurosurgeon Hani Marcus says.', '""If you go too large, you risk damaging these really critical structures."" ', 'The AI system has analysed more than 200 videos of this type of pituitary surgery, reaching, in 10 months, a level of experience it would take a surgeon 10 years to gain.', '""Surgeons like myself - even if you\'re very experienced - can, with the help of AI, do a better job to find that boundary than without it,"" Mr Marcus says.', '""You could, in a few years, have an AI system that has seen more operations than any human has ever or could ever see."" ', 'Trainee Dr Nicola Newell also finds it ""very helpful"". ', '""It helps me orientate myself during mock surgery and helps identify what steps and what stages are coming up next,"" she says.', 'AI government minister Viscount Camrose says: ""AI makes everybody massively more productive whatever it is you do.', '""It kind of almost makes you the Marvel superhero version of yourself.""', 'He said this type of technology could be a game-changer for healthcare, improving outcomes for everyone and offering a ""very promising"" future.', 'University College London (UCL) is one of 22 universities recently given government money to help revolutionise healthcare in the UK.', 'Engineers, clinicians and scientists are working together on the project at the Wellcome / Engineering and Physical Sciences Research Council (EPSRC) Centre for Interventional and Surgical Sciences.', 'Follow @tulipmazumdar on X, formerly known as Twitter.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Meta has announced a series of new chatbots to be used in its Messenger service. ', 'The chatbots will have ""personality"" and specialise in certain subjects, like holidays or cooking advice. ', 'It is the latest salvo in a chatbot arms race between tech companies desperate to produce more accurate and personalised artificial intelligence.', 'The chatbots are still a work in progress with ""limitations"", said boss Mark Zuckerberg.', 'In California, during Meta\'s first in-person event since before the pandemic, Mr Zuckerberg said that it had been an ""amazing year for AI"".', 'The company is calling its main chatbot ""Meta AI"" and can be used in messaging. For example, users can ask Meta AI questions in chat ""to settle arguments"" or ask other questions. ', 'The BBC has not yet tested the chatbot which is based on Llama 2, the large language model that the company released for public commercial use in July. ', 'Several celebrities have also signed up to lend their personalities to different types of chatbots, including Snoop Dogg and Kendall Jenner. ', 'The idea is to create chatbots that are not just designed to answer questions.', '""This isn\'t just going to be about answering queries,"" Zuckerberg said. ""This is about entertainment"". ', 'According to Meta, NFL star Tom Brady will play an AI character called \'Bru\', ""a wisecracking sports debater"" and YouTube star MrBeast will play \'Zach\', a big brother ""who will roast you"". ', 'Mr Zuckerberg said there were still ""a lot of limitations"" around what the bots could answer.', 'The chatbots will be rolled out in the coming days and only in the US initially.', 'Mr Zuckerberg also discussed the metaverse - a virtual world - which is a concept that Mr Zuckerberg has so far spent tens of billions of dollars on. ', 'Although Meta had already announced its new virtual reality headset, Quest 3, the company gave further details at the event.', 'Meta\'s boss described the headset as the first ""mainstream"" mixed reality headset. Cameras facing forward will mean the headset will allow for augmented reality. It will be available from 10 October.', ""The firm's big, long-term bet on the metaverse still appears yet to pay off, with Meta's VR division suffering $21bn (Â£17bn) in losses since the start of 2022."", 'The Quest 3 came after Apple entered the higher-priced mixed reality hardware market with the Vision Pro earlier this year.', 'Mat Day, global gaming strategy director for EssenceMediacom, said Mark Zuckerberg had ""reinvigorated"" the VR sector.', '""Meta\'s VR roadmap is now firmly positioned around hardware priced for the mass market. This is a stark contrast to Apple\'s approach which is aimed at the high end tech enthusiast,"" he said.', ""Meta's announcement came on the same day as rival OpenAI, the Microsoft-backed creator of ChatGPT, confirmed its chatbot can now browse the internet to provide users with current information. The artificial intelligence-powered system was previously trained only using data up to September 2021."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['The boss of Spotify says he has no plans to completely ban content created by artificial intelligence from the music streaming platform.', 'Earlier this year the platform pulled a track featuring AI-cloned voices of the performers Drake and The Weeknd.', 'Daniel Ek told the BBC there were valid uses of the tech in making music - but AI should not be used to impersonate human artists without their consent.', 'He said using AI in music was likely to be debated for ""many, many years"". ', 'Mr Ek, who rarely speaks to the media, said that he saw three ""buckets"" of AI use: ', '""It is going to be tricky,"" he said when asked about the challenge the industry was facing. ', 'While AI is not banned in all forms on the platform the company does not allow its content to be used to train a machine learning or AI model, the likes of which can then produce music. ', 'Artists are increasingly speaking out against the use of AI in the creative industries. ', 'Last month the Irish musician Hozier said he would consider striking over the threat of AI to his profession.', 'He told BBC Newsnight that he wasn\'t sure the tech ""meets the definition of art"".', 'Neither Drake nor The Weeknd were aware of cloned versions of their voices being used on the song, Heart on My Sleeve. The track was removed from Spotify and other streaming platforms in April.', 'Its creator, Ghostwriter, later tried to have the track nominated for a Grammy award but it was turned down. ', '""You can imagine someone uploading a song, claiming to be Madonna, even if they\'re not. We\'ve seen pretty much everything in the history of Spotify at this point with people trying to game our system,"" Mr Ek said.', '""We have a very large team that is working on exactly these types of issues."" ', 'In May, the Financial Times reported that thousands of tracks had been removed from Spotify after a discovery that bots were being used to artificially inflate their streaming figures.', ""Mr Ek also discussed the platform's huge investment in podcasts - including those from high-profile figures like Michelle and Barack Obama and the Duke and Duchess of Sussex. Neither has been re-commissioned."", ""The deal with Harry and Meghan cost a reported $25m (Â£18m) and saw just 12 episodes delivered over two and a half years. A Spotify executive recently reportedly spoke disparagingly about the pair's work ethic."", '""The truth of the matter is some of it has worked, some of it hasn\'t,"" said Mr Ek of the firm\'s decision to ""challenge Apple"" as the market-leading podcast platform by taking on a lot of new creators.', '""Five years ago Spotify was nowhere in podcasting.""', ""Separately, the firm confirmed that Russell Brand's podcast would remain on Spotify unless the material itself was found to have breached its own terms and conditions. "", 'Acast, which owns the podcast, said it had suspended advertising revenue from it as the comedian remains under investigation over allegations of sexual assault.', 'The reason Sweden-based Daniel Ek was in the UK was to discuss regulation. He said the firm is supportive of the incoming Online Safety Bill, designed to make the internet safer for children, and the ongoing Digital Markets and Competition Bill, which aims to improve competition by closely scrutinising the tech giants.', ""Mr Ek has long been a vocal critic of the policies of Apple and Google's app stores, on which Spotify relies. Both companies charge smaller developers a 15% commission on in-app purchases, with this rising to 30% for developers with revenue of more than $1m."", 'Spotify has also complained that Apple makes it hard for the business to communicate directly with its customers and promote its services elsewhere. ', '""We are in a situation where literally two companies in the world control how over four billion consumers access the internet,"" said Mr Ek.', '""If you think now on a company like Spotify, where we already pay out almost 70% of our revenues back to the creative community, if we were to take the 30% out of our cut it essentially means we\'re left with zero, which means we have to close shop.""', 'In April 2021, the European Commission (EC) charged Apple with breaking EU competition rules over this, following a complaint from Spotify in 2020. In February the EC scaled back its objections against Apple although there has yet to be a final ruling.', 'Apple said it was continuing to work with the EC. It added that the vast majority of European developers make less than $1m in revenue and qualify to pay Apple a 15% commission rate.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['A sleepy town in southern Spain is in shock after it emerged that AI-generated naked images of young local girls had been circulating on social media without their knowledge.', 'The pictures were created using photos of the targeted girls fully clothed, many of them taken from their own social media accounts. ', 'These were then processed by an application that generates an imagined image of the person without clothes on.', ""So far more than 20 girls, aged between 11 and 17, have come forward as victims of the app's use in or near Almendralejo, in the south-western province of Badajoz. "", '""One day my daughter came out of school and she said \'Mum there are photos circulating of me topless\',"" says MarÃ\xada Blanco Rayo, the mother of a 14-year-old. ', '""I asked her if she had taken any photos of herself nude, and she said, \'No, Mum, these are fake photos of girls that are being created a lot right now and there are other girls in my class that this has happened to as well.\'""', 'She says the parents of 28 girls affected have formed a support group in the town.', 'Police are now investigating and according to reports, at least 11 local boys have been identified as having involvement in either the creation of the images or their circulation via the WhatsApp and Telegram apps. ', 'Investigators are also looking into the claim that an attempt was made to extort one of the girls by using a fake image of her.', 'The impact the images\' circulation has had on the girls affected varies. Ms Blanco Rayo says her daughter is bearing up well, but that some girls ""won\'t even leave their house"".', ""Almendralejo is a picturesque town with a population of just over 30,000 which is known for its production of olives and red wine. But it's not used to the sudden attention this case has brought, making the town national headline news. "", ""That's in great part because of the efforts of one of the girls' mothers, Miriam Al Adib. She's a gynaecologist who has used her already prominent social media profile to place this issue at the centre of Spanish public debate."", 'Although many of the AI images are believed to have been created over the summer, the case only came to light in recent days after Dr Adib posted a video reassuring the girls affected and their parents.', '""We didn\'t know how many children had the images, if they had been uploaded to pornographic sites - we had all those fears,"" she says.', '""When you are the victim of a crime, if you are robbed, for example, you file a complaint and you don\'t hide because the other person has caused you harm. But with crimes of a sexual nature the victim often feels shame and hides and feels responsible. So I wanted to give that message: it\'s not your fault.""', 'The suspects in the case are aged between 12 and 14. Spanish law does not specifically cover the generation of images of a sexual nature when it involves adults, although the creation of such material using minors could be deemed child pornography. ', 'Another possible charge would be for breaching privacy laws. In Spain, minors can only face criminal charges from the age of 14 upwards.', 'The case has caused concern even for local people who are not involved.', '""Those of us who have kids are very worried,"" says Gema Lorenzo, a local woman who has a son, aged 16, and a daughter, aged 12. ', '""You\'re worried about two things: if you have a son you worry he might have done something like this; and if you have a daughter, you\'re even more worried, because it\'s an act of violence.""', 'Francisco Javier Guerra, a local painter and decorator, says the parents of the boys involved are to blame. ""They should have done something before, like take their phones away, or install an application that tells them what their children are doing with their phone.""', 'This is not the first time such a case has become news in Spain. Earlier this year, AI-generated topless images of the singer RosalÃ\xada were posted on social media.', '""Women from different parts of the world have written to me explaining that this has happened to them and they don\'t know what to do,"" says Miriam Al Adib. ', '""Right now this is happening across the world. The only difference is that in Almendralejo we have made a fuss about it.""', 'The concern is that apps such as those used in Almendralejo are becoming increasingly commonplace.', 'Javier Izquierdo, head of children\'s protection in the national police\'s cyber-crime unit, told Spanish media that these kinds of crimes are no longer confined ""to the guy who downloads child porn from the Dark Web or from some hidden internet forum"". ', 'He added: ""That obviously is still going on, but now the new challenges we are facing are the access that minors have at such an early age [to such technology], such as in this case.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Artificial intelligence could destabilise the world order unless governments act, the deputy prime minister is to warn.', ""Oliver Dowden will tell the UN the pace of development risks outstripping governments' ability to make it safe. "", 'The UK will host a global summit to discuss AI regulation, in November.', 'There are fears without rules AI could eventually destroy jobs, supercharge misinformation or entrench discrimination.', '""The starting gun has been fired on a globally competitive race in which individual companies as well as countries will strive to push the boundaries as far and fast as possible,"" Mr Dowden will tell the United Nations general assembly in New York. ', '""At the moment, global regulation is falling behind current advances.""', 'In the past, governments have created regulations in response to technological developments - but now, rules must be made in parallel with the development of AI.', 'AI companies should not ""mark their own homework, just as governments and citizens must have confidence that risks are properly mitigated"". ', 'And only action by nation states can reassure the public the most significant national-security concerns have been allayed.', 'Mr Dowden will also warn, however, against becoming ""trapped in debates about whether it is a tool for good or a tool for ill - it will be a tool for both"".', 'Many experts have been surprised by the rapid increase in the capabilities of some AI systems. ""We\'ve seen horizons compress,"" Prof Andrew Rogoyski, of the University of Surrey, told BBC News.', 'But Faculty.ai boss Marc Warner said it was important to distinguish between narrow AI designed to fulfil a specific task such as looking for signs of cancer in radiology scans and general artificial intelligence.', '""These are powerful algorithms that have emergent properties that, at the moment, we can\'t... always predict when they\'re about to develop,"" he said.', '""And while I personally am not super-worried about the current generation of technologies, I think it\'s only sensible that government should start looking ahead to more and more powerful versions and what might be done about it.', '""I\'ve been following the field of AI safety now for 10 or 15 years - and two to three years ago nobody cared about this conversation. ', '""And so for me, even starting an international conversation, a serious international conversation about AI safety, is a success in itself."" ', 'Other leading AI companies agree there is a need for regulation. Following a recent closed-door meeting of technology bosses, in Washington, Elon Musk said there was an ""overwhelming consensus"" for it.', 'But Yasmin Afina, of the Chatham House international-affairs think tank, said reaching a quick international agreement would be difficult.', 'Compared with nuclear weapons, about which ""it took so many years for people to agree on something"", she said, ""AI is so complex, so different as a technology, I don\'t think that it will be easy to negotiate something that people will agree on.""', 'Smaller countries, marginalised communities and people belonging to ethnic minorities also needed to have meaningful input. ""As long as they\'re not at the table and [don\'t] actually have a voice, they will just be left out,"" Ms Afina said.', ""Prime Minister Rishi Sunak wants the UK to take the lead. But last month, the Commons Science, Innovation and Technology Committee warned without the rapid introduction of a law, the European Union's AI Act could become a global standard, displacing UK efforts."", 'Mr Warner, previously a member of the now defunct AI council which advised government,  said the UK could potentially take a lead in technology to make AI safe, if was prepared to invest.', '""That feels like a very practical middle path,"" he said, ""because there isn\'t actually that much money going into that at the moment.""', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Google's AI firm DeepMind has used artificial intelligence to identify changes in human DNA that might cause diseases."", 'The researchers believe they have pinpointed 89% of all the key mutations.', 'The development is expected to speed up diagnosis and help in the search for better treatments.', 'A leading independent scientist told BBC News that the work was ""a big step forward"".', 'Prof Ewan Birney, deputy director general of the European Molecular Biology Laboratory, said: ""It will help clinical researchers prioritise where to look to find areas that could cause disease.""', 'The technique works by checking the order of the components in human DNA strands.', 'All living organisms are built from DNA. It is made from four blocks of chemicals called adenine (A), cytosine (C), guanine (G) and thymine (T). In humans, when an embryo is developing, the order of these letters are read to produce proteins, which are the building blocks of the the cells and tissues that make up various parts of the body.', ""But if the letters are in the wrong order - perhaps because of an inherited disorder - the body cells and tissues aren't made properly - and this can lead to disease."", ""Last year Google DeepMind's AI worked out the shape of nearly all proteins in the human body. "", 'The new system, called AlphaMissense, can tell If the letters in the DNA will produce the correct shape. If not, it is listed as potentially disease-causing.', 'Currently genetic disease hunters have fairly limited limited knowledge of which areas of human DNA can lead to disease. They have classified 0.1% of letter changes, or mutations, as either benign or disease causing.', ""Google DeepMind's Pushmeet Kohli said that the new model pushed that percentage up to 89%."", 'Currently, researchers have to search for potentially disease-causing regions across billions of chemical building blocks that make up DNA. That has now changed, according to Mr Kohli.', ""''Researchers can now focus their efforts on the new areas, that they were not aware of and we have highlighted as potentially disease-causing,'' he said. "", 'The new tool - published in the journal Science - has been tested by Genomics England, who work with the NHS. According to Dr Ellen Thomas, who is the deputy chief medical officer at Genomics England, the health service will be among the first organisations to benefit from the new development. ', '""The new tool is really bringing  a new perspective to the data. It will help clinical scientists make sense of genetic data so that it is useful  for patients and for their clinical teams,"" she said.', 'Prof Birney said he expected AI to become a massive part of molecular biology and life sciences.', '""I don\'t know where it\'s going to end but it\'s changing nearly everything we do at the moment,"" he said.', 'Follow Pallab on X, formerly known as Twitter', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Google's AI firm DeepMind has used artificial intelligence to identify changes in human DNA that might cause diseases."", 'The researchers believe they have pinpointed 89% of all the key mutations.', 'The development is expected to speed up diagnosis and help in the search for better treatments.', 'A leading independent scientist told BBC News that the work was ""a big step forward"".', 'Prof Ewan Birney, deputy director general of the European Molecular Biology Laboratory, said: ""It will help clinical researchers prioritise where to look to find areas that could cause disease.""', 'The technique works by checking the order of the components in human DNA strands.', 'All living organisms are built from DNA. It is made from four blocks of chemicals called adenine (A), cytosine (C), guanine (G) and thymine (T). In humans, when an embryo is developing, the order of these letters are read to produce proteins, which are the building blocks of the the cells and tissues that make up various parts of the body.', ""But if the letters are in the wrong order - perhaps because of an inherited disorder - the body cells and tissues aren't made properly - and this can lead to disease."", ""Last year Google DeepMind's AI worked out the shape of nearly all proteins in the human body. "", 'The new system, called AlphaMissense, can tell If the letters in the DNA will produce the correct shape. If not, it is listed as potentially disease-causing.', 'Currently genetic disease hunters have fairly limited limited knowledge of which areas of human DNA can lead to disease. They have classified 0.1% of letter changes, or mutations, as either benign or disease causing.', ""Google DeepMind's Pushmeet Kohli said that the new model pushed that percentage up to 89%."", 'Currently, researchers have to search for potentially disease-causing regions across billions of chemical building blocks that make up DNA. That has now changed, according to Mr Kohli.', ""''Researchers can now focus their efforts on the new areas, that they were not aware of and we have highlighted as potentially disease-causing,'' he said. "", 'The new tool - published in the journal Science - has been tested by Genomics England, who work with the NHS. According to Dr Ellen Thomas, who is the deputy chief medical officer at Genomics England, the health service will be among the first organisations to benefit from the new development. ', '""The new tool is really bringing  a new perspective to the data. It will help clinical scientists make sense of genetic data so that it is useful  for patients and for their clinical teams,"" she said.', 'Prof Birney said he expected AI to become a massive part of molecular biology and life sciences.', '""I don\'t know where it\'s going to end but it\'s changing nearly everything we do at the moment,"" he said.', 'Follow Pallab on X, formerly known as Twitter', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Traffic cameras equipped with artificial intelligence (AI) could be introduced in Lincolnshire as a way of reducing fatal crashes. ', ""The county's police and crime commissioner (PCC) Marc Jones will discuss this and other safety measures at a road safety summit next week."", 'The camera systems are being trialled in Devon and Cornwall to catch drivers using phones or not wearing seatbelts.', 'Mr Jones said he hoped to identify ideas that can be implemented locally. ', 'He believes the East of England and East Midlands Road Safety Summit will offer the chance to share best practice. ', 'Speakers include roads and local transport minister Richard Holden, road safety experts and PCCs from across the East of England and East Midlands.', 'Mr Jones said: ""In Lincolnshire over the last few years we have made encouraging steps forward in our bid to keep our roads safe - however the number of tragic incidents on our roads has remained too high and we must do more.""', 'The AI cameras have already been used in a week-long trial at sites across East Yorkshire and Lincolnshire - resulting in 239 people being fined for using their mobile phone or not wearing a seatbelt.', 'As well as the AI cameras, the summit will look at a scheme in Northamptonshire which allows local residents to apply for grants of up Â£5,000 to improve road safety in their communities.', 'The conference will also hear about ""Vision Zero"" - a strategy that sees no road death as ""inevitable"" - which was first implemented in Sweden in the 1990s and has reduced road deaths there from 541 in 1997 to 192 in 2021.Mr Jones said: ""We should never accept needless and pointless deaths on our roads and I will continue to seek the most effective ways to keep our road users safe and this event represents an excellent opportunity to learn from others.""', 'Follow BBC News South West on Twitter, Facebook and Instagram. Send your story ideas to spotlight@bbc.co.uk', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""The inquiry into Gatwick Airport's plan to use its emergency runway for routine flights has heard from business groups in favour and local people opposed to the scheme."", ""Chambers of commerce from Sussex, Surrey and Kent argued it would bring hundreds of millions to the area's economy."", 'Local people highlighted the disruption flights already bring to their lives.', 'The inquiry being held in Crawley is due to last six months. ', 'Anna Christie, the chief executive of the Sussex Chamber of Commerce who was also speaking on behalf of the Surrey chamber, told the hearing ""future development will continue to support growth and jobs"".', 'She said the chamber would act as a ""critical friend"" to the airport over its environmental impact.', 'Richard Lavender, from the Kent Invicta Chamber of Commerce, also fully supported the expansion plan, saying it would ""improve the resilience of air traffic in the South East"" with ""quieter, more efficient and greener aircraft"".', 'Gavin Stewart, the executive director of the Brighton and Hove Economic Partnership, said the extra passengers the expansion would bring could represent an extra Â£630m being spent by tourists.', 'And Charlie Cooper, the operations manager at a local building firm which is a contractor at Gatwick Airport, described the proposed growth as ""simply a no-brainer, the benefits will outweigh the negatives"".', '""These plans give us the confidence to invest and develop,"" he said.', 'Jane Shufflebotham, who lives near the airport, said she was ""passionately and wholeheartedly against this plan"", which she said would be ""profoundly damaging to the environment"".', 'She said: ""It\'s not just a bad idea, but an environmental catastrophe in the making... a decision based on pure greed.""', 'Another local resident, Patricia Routledge, said even now people living near Gatwick could not open their windows or enjoy their gardens in the summer.', '""If Gatwick has its way, this hell will increase,"" she told the hearing.', '""No consideration is being given to those living beneath the flight path. It\'s a cheap, inadequate fix for Gatwick.""', 'And Dr Roger Hood, who lives in the village of Capel, 5 miles (8km) west of the airport, questioned the economic need for its expansion.', '""London already has five international airports,"" he said.', 'Responding to the issues raised on Wednesday, John Rhodes from Gatwick Airport said: ""Gatwick Airport really does take its environmental responsibilities very seriously.""', 'In terms of its ground operations he said the airport had put forward a carbon action plan which it was hoped would make the airport achieve net zero by 2030, a plan which would happen regardless of whether the northern runway wins approval.', 'He also assured those concerned with night flights that the new runway would not be used between 23:00 GMT and 06:00 except in emergencies.', 'Follow BBC South East on Facebook, on X, and on Instagram. Send your story ideas to southeasttoday@bbc.co.uk or WhatsApp us on 08081 002250.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""Disney is combining forces with Asia's richest man in a bid to solve challenges weighing down its streaming business in India."", ""The company said it was merging its Star India service with Viacom18, which is backed by Mukesh Ambani's Reliance Industries, in a deal worth $8.5bn."", 'Reliance will lead the new business, which is poised to be one of the biggest media forces in India.', 'It has pledged to inject $1.4bn (Â£1.1bn) to help the firm grow.', 'Mr Ambani, who built his fortune in the chemical and oil industries and is now worth more than $100bn according to Forbes, called it a ""landmark agreement that heralds a new era in the Indian entertainment industry"".', 'The companies said they expected the deal, which needs approval from regulators to proceed, to be completed at the end of this year or early next year.', 'The combined firm would boast more than 120 channels, serving some 750 million customers across the country. ', 'Viacom18, which was created in 2007 as a partnership between Reliance and Paramount, currently runs about 40 channels including MTV, Nickelodeon and the Hindi-language Colors, as well as the JioCinema streaming service. ', ""It has been challenging Disney's Star business in India, which the company inherited when it purchased a chunk of Rupert Murdoch's Fox empire in 2019."", ""In 2022, Reliance outbid Disney for rights to stream the popular India Premier League cricket tournament, sparking a sizable fall in subscribers to Disney's Hotstar streaming service. "", ""The company's Star sports channels also reported declines in subscribers and advertisers in the 12 months to September 2023."", 'Disney boss Bob Iger said the joint venture would keep Disney present in the large Indian market while benefiting from Reliance\'s ""deep understanding of the Indian market and consumer"".', 'But the deal values Star India at less than a third of what it was in 2019 when Disney took on the business, sources told the Reuters news agency.', ""Disney will own a roughly 37% stake in the joint venture, which will have exclusive rights to distribute Disney's films and productions in India."", 'Viacom18 will hold a roughly 47% share, and Reliance another 16%.', 'India has emerged as a key battleground in the streaming business, as giants such as Netflix and Amazon invest heavily to try to seize part of what is seen as a rapidly growing market. ', ""Nita Ambani, Mr Ambani's wife, will serve as chairperson of the new company."", 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['This video can not be played', ""Watch: 'Overwhelming consensus' to regulate AI, says Elon Musk"", 'Tesla CEO Elon Musk says there was ""overwhelming consensus"" for regulation on artificial intelligence after tech heavyweights gathered in Washington to discuss AI.', ""Tech bosses attending the meeting included Meta's Mark Zuckerberg and Google boss Sundar Pichai."", ""Microsoft's former CEO Bill Gates and Microsoft's current CEO Satya Nadella were also in attendance."", 'The Wednesday meeting with US lawmakers was held behind closed doors. ', 'The forum was convened by Senate Majority Leader Chuck Schumer and included the tech leaders as well as civil rights advocates. ', 'The power of artificial intelligence - for both good and bad - has been the subject of keen interest from politicians around the world. ', 'In May, Sam Altman, the CEO of OpenAI, the company behind ChatGPT, testified before a US Senate committee, describing the potential pitfalls of the new technology.', 'ChatGPT and other similar programmes can create incredibly human-like answers to questions - but can also be wildly inaccurate.', '""I think if this technology goes wrong, it can go quite wrong...we want to be vocal about that,"" Mr Altman said. ""We want to work with the government to prevent that from happening,"" he said. ', 'There are fears that the technology could lead to mass layoffs, turbo charge fraud and make misinformation more convincing. ', 'AI companies have also been criticised for training their models on data scraped from the internet without permission or payment to creators. ', 'In April, Mr Musk told the BBC: ""I think there should be a regulatory body established for overseeing AI to make sure that it does not present a danger to the public.""', 'In Wednesday\'s meeting, he said he wanted a ""referee"" for artificial intelligence. ', '""I think we\'ll probably see something happen. I don\'t know on what timeframe or exactly how it will manifest itself,"" he told reporters after.', 'Mr Zuckerberg said that Congress ""should engage with AI to support innovation and safeguards"".', 'He added it was ""better that the standard is set by American companies that can work with our government to shape these models on important issues"".', 'Republican Senator Mike Rounds said it would take time for Congress to act. ', '""Are we ready to go out and write legislation? Absolutely not,"" Mr Rounds said. ""We\'re not there.""', 'Democrat Senator Cory Booker said all participants agreed ""the government has a regulatory role"" but crafting legislation would be a challenge.', 'Read additional stories on artificial intelligence', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""It's widely recognised that we are bombarded with fast-paced imagery in the modern world, whether it's social media videos, or digital billboards in city spaces. But there has been a similar explosion in sound, says advertising entrepreneur Michele Arnese. He thinks brands can only compete with the help of artificial intelligence (AI)."", '""More and more the sound of a brand is like liquid,"" says Arnese. ""It goes everywhere and takes its shape according to the customer experience.""', 'Only AI, he argues, allows sound to shape-shift this way in our fast-paced, digitally connected world.', 'Arnese founded the AI-centred music company Amp in Munich in 2009. It was recently acquired by Landor & Fitch, part of the WPP advertising group.', 'Amp uses AI to generate all kinds of sounds for companies, from small bursts of noise when an app launches, to the sound of a bank card transaction completing, to longer compositions for things like podcasts and social media videos. This is what he calls a brand\'s ""sonic identity"".', 'The internet has been flooded recently with examples of what AI can do, including reimagined films (Harry Potter with muscle-bound characters), new music using the voices of deceased artists like Amy Winehouse or Kurt Cobain, or radical architectural drawings.', 'So what can AI do for sound in the world of advertising? ', 'Firstly, humans still have a vital role to play in the process, reassures Arnese, whose company is one of the leaders in the field. His in-house composers begin by creating a track known as the ""Sonic DNA"" of the brand, which might last about 90 seconds. ', 'This video can not be played', ""Listen: An example of 'Sonic DNA' that Amp composed for a finance firm, in order to generate AI-powered branding"", ""AI's first role is to check that this is not similar to sounds already used by other companies. Machine learning can also check whether the signature patterns in the music are likely to have impact and be memorable."", 'But once this DNA is established, the key role of AI is to allow companies to make music on an industrial scale, explains Arnese, to satisfy digital outlets.', 'AI can generate potentially infinite music remixes from this DNA, with different tempos, moods and durations, depending on the context, be it a TikTok video, the noise of an appliance coming to life or the intro music for a podcast. Humans check each result before they are put before the public. ', 'This video can not be played', ""Listen: An example of music used for branding generated by AI, using 'Sonic DNA'"", 'The argument for using AI is that this is easier and cheaper than a human selecting and buying individual pieces of production music for thousands of different scenarios.', 'It also creates computer-orchestrated brand consistency.', ""In a world of in-app payments and contactless transactions, even the shortest blasts of sound are increasingly important for consumers, says Arnese, because they establish trust and forge the brand's identity."", 'Arnese is from Italy and studied clarinet in a conservatory, and then IT at the University of Pisa, with an interest in machine learning. After graduating he moved to Germany to work as a management consultant. ', 'Fearing that music would no longer be part of his life, he quit his corporate job to set up his digital agency focused on his first passion, music. It was difficult to persuade brands that they needed to invest in music to begin with, he says, but that has completely changed in the last few years. In the old days big companies might produce just one or two video adverts for TV a year.', '""These days no brand is on mute,"" says Arnese. ""Some brands upload a hundred videos to YouTube every week, and we asked ourselves, how can they afford it?""', 'Read additional stories on artificial intelligence', 'Some scepticism still remains about whether AI really is a ""gamechanger"" for the advertising industry, though.', '""People put a lot of money into things like the Metaverse, crypto and NFTs, all the things marketers got excited about, and now they\'ve had to backtrack,"" cautions Molly Innes of Marketing Week.', 'As a result many people in the advertising world are taking the wait-and-see approach with AI, she says, especially as there is less money going round at the moment to invest in it.', ""Arnese believes AI will have a profound impact on advertising. However, he doesn't think it will lead to the significant job losses some fear."", '""AI is just another tool to do your job,"" he says.', '""It presents an opportunity to be inspired by something unexpected [that the computer generates] in the creative process, that\'s how I use it.', '""Ten years ago there was no such job as data scientist in the advertising industry, can you imagine? But now it is part of the normal team set up of an agency.', '""AI is here to stay, but it\'s not a replacement for humans.""', 'Follow business reporter Dougal Shaw on LinkedIn', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Alex Dunne won the British F4 Championship as a rookie in 2022', '""Ever since I was a little kid, the goal has been Formula 1. It is now one step closer.""', 'Every young, up and coming driver dreams of making it to Formula 1, yet only a handful achieve that dream. ', 'However, one teenager from Ireland has the belief he can make it to the pinnacle of motorsport by letting his ability shine through.', 'Alex Dunne has only been racing cars for two years but has made a big impression.', 'He blew everyone out of the water to win the British F4 championship in 2022 and almost claimed the GB3 Championship as a rookie the following year. ', 'Despite these stellar performances, it looked as if he would have to watch on as those who have produced nowhere near the same level of results moved up the ladder onto the Formula 1 package.', 'Dunne joins F3 grid with MP Motorsport', 'Then came the Macau Grand Prix - one of the most prestigious street races anywhere in the world. In his first time racing a Formula 3 car, Dunne, who only got a late call-up, finished second in the qualifying race. ', 'Although he crashed out of the Macau Grand Prix, he had announced himself with a stunning debut in Formula 3 machinery. ', 'It may not have been a weekend that secured him a deal on the Formula 1 support series, with work constantly going on with his family and sponsors in the background, but it made a big impression.', '""Macau definitely went better than expected, to be honest,"" Dunne said.', '""You have to believe in yourself and believe in your ability, but I had really little experience in F3 or street tracks.', '""I I said to my dad before we got there that, if I can manage to be in the top 10 I\'ll be over the moon, so the fact that I finished second in the first race, I was kind of in shock. ', '""I managed to adapt pretty quick and I\'m glad that was recognised.""', 'The 18-year-old will race for Dutch team MP Motorsport in Formula 3, which is the third tier of single-seater motorsport and races take place alongside the giants of Formula 1.', 'The deal came late in the off season, highlighted by the fact Dunne was the 28th of the 30 drivers to be announced for the season. ', '""It\'s going to be my first proper time racing with F1, which is something special,"" added Dunne.', '""It\'s going to feel slightly weird to be racing on the on the same weekends as Formula 1 and not sitting home, watching it from the telly. ', '""I\'m excited to get going and I\'m really glad that we managed to get the opportunity to race in F3. ', '""There was a lot of work that went into it and the deal was very last minute. When my dad told me it was happening and I was over the moon.', '""To be racing on the same weekends as Formula One is a dream come true.""', ""Unlike many of the Formula 3 grid, Dunne is not affiliated with a Formula 1 team's academy"", ""The last Formula 1 driver from the island of Ireland was Eddie Irvine way back in 2002. The closest anyone has been since was Adam Carroll a few years after Irvine's last Formula 1 race. "", 'Unlike the majority of the grid, the Offaly driver is not signed to the academy of or affiliated with a Formula 1 team but Dunne is hoping he can catch the eye of Formula 1 teams while racing in the support series.', '""I\'m racing against the best junior drivers in the world,"" said Dunne.', '""Racing on Grand Prix weekends, you are against all the big teams and Formula 1 teams will be watching the races.', '""When we just started to start to do this properly, I think we always had the attitude to be racing with the best against the best.""', 'For a driver who mas made a habit of making the most out of any opportunity that has come his way, Dunne could be a dark horse against some of the best junior drivers in the world. ', 'However, the youngster is keen to play down any expectations.', '""I wouldn\'t say I have a specific goal, aside from to do as well as possible,"" he said. ""I think the test in Bahrain was very positive. The pace was roughly in in the top five most of the time.', '""It\'s a rookie year, so it\'s going to be quite tough. There are a lot of very talented second-year drivers in the championship, but, realistically, if I do what I know I can then I don\'t see why we won\'t be running at the front. ', '""There\'s no point in having the a win one weekend, followed by two DNFs. If i just try and stay consistent and maximise what I can do, then hopefully we should be right at the sharp end.""', 'The man of the moment...: Rick Astley performs his hits and a cover of Olivia Rodrigo at Media Vale', ""How Trump's golf dream turned into a nightmare...: The controversial development in Aberdeenshire was greenlit with shocking consequences"", 'Smitha Mundasad explores whether anything can be done to get rid of it', 'Surprising facts and interesting history from the makers of QI', ""'Stay-at-home-daughter' Chi is suddenly forced to fend for herself when her parents die"", 'Rob Brydon unpicks the complex character of Barry Humphries aka Dame Edna Everage and Sir Les Patterson', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"['Leading figures from the world of artificial intelligence (AI) have met in Londonderry to discuss the impact of the technology on education at a conference. ', ""Delegates at Ulster University's Magee Campus heard from Microsoft and the National Centre for AI, among others."", 'On Monday, GenAIEdu hosted workshops examining how AI platforms generate teaching material.  ', 'The role of AI in the higher education sector was also discussed. ', 'The event, which will take place over three days will see a variety of speakers explore the potential benefits of AI for people working in coding and digital design, as well as industry more widely. ', 'Speakers at the event on Monday included Sue Attewell, who is a co-leader of the National Centre for AI in tertiary education and Dr Cris Bloomfield, a senior industry architect with Microsoft.', 'Attendees at the event on Monday also heard from Manjinder Kainth a co-founder and chief executive officer of Graide, an AI enhanced assessment and feedback platform.', ""Professor Colin Turner from Ulster University's Faculty of Computing, Engineering and the Built Environment, said the future of AI will present both challenges and also opportunities."", '""I think like a lot of technological changes, like for example the worldwide web, it [AI] will bring some significant changes and some of those will provide some challenges, but there will be a lot of opportunities as well,"" Prof Turner told BBC Radio Foyle\'s North West Today programme.', '""Really the purpose of this summit is to make sure that we are off quickly into that proper, full blown, honest conversation about those challenges, about the ethical considerations, and making sure that we get the benefit for our students, but also for the industry in the north west and beyond."" ', 'In May, in a letter to the Times, educators from the UK\'s state and private sector described developments in AI as ""bewildering"".', 'AI is the ""greatest threat but also potentially the greatest benefit to our students, staff and schools"", the group of teachers, led by Sir Anthony Seldon, headteacher of Epsom College in England, said in the letter.', 'Prime Minister Rishi Sunak said regulation had to evolve at the same time as rapid changes are made in AI. ', 'Prof Turner said that striking a balance in the approach to AI was crucial. ', '""Our responsibility as a university is to ensure that our research and our curriculum at the Derry/Londonderry campus helps to prepare industry...so that it\'s not a seismic shock and we are with the front of the wave rather than pushed along at force,"" he said.', '""The important thing for us is to be quick and responsive to these changes rather than dragged along by them.""', 'Last week, a group representing the software industry in Northern Ireland said the region can be a testing centre for artificial intelligence in the UK.', 'It followed an announcement by Belfast-based IT firm Kainos that it was investing Â£10m to develop the use of generative AI in its business.', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""One of the world's largest space and defence companies has announced it is moving to a specialist office in Wiltshire.  "", ""Airbus Defence and Space Limited (ADS), a division of Airbus Group, is moving to the Business Cyber Centre, the UK's first business-led cyber centre, at Greenways Business Park in Chippenham. "", 'ADS specialises in providing solutions for defence and space applications. ', 'They said the move would allow them to better serve their customers.', 'The division of Airbus plays a role in supporting national security, space exploration, and satellite communications with expertise ranging from Earth observation and satellite navigation to secure communication and intelligence systems.', 'Business continuity manager at Airbus Defence and Space, Ricky Larcombe, said: ""This move is a testament to our dedication to growth and our mission to enhance our services and offerings for our valued customers. ', '""The new office will not only strengthen our relationships with our current customers and partners, but also allow us to better cater to the needs of our expanding customer base.""', 'ADS confirmed 80 people would be moving to the new office space from various locations across Wiltshire. ', ""The Business Cyber Centre (BCC) opened in Spring 2022 and is the UK's first business-led Cyber Centre providing a hi-tech base for companies. "", 'Tom Marshall, head of capital operations at the BCC said: ""We\'re really pleased to be welcoming an organisation such as Airbus to the BCC. ', '""The opportunity for collaboration with our existing members and wider network, and how this could benefit UK defence, shows the importance of spaces such as the BCC.""', 'Paul Moorby, chair of Swindon and Wiltshire Local Enterprise Partnership said: ""It is with a sense of pride and collective achievement that we welcome Airbus Defence and Space to the Business Cyber Centre. ', '""Their decision to join our thriving community is a clear indication of the BCC\'s growing importance at the heart of the UK cyber sector.""', 'Follow BBC West on Facebook, X and Instagram. Send your story ideas to: bristol@bbc.co.uk ', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
"[""In the world of football management, the rumour mill never stops spinning and currently speculation is rife about the future of Wolves manager Gary O'Neil. Will he stay or will he be poached by another club looking for a fresh face at the helm?"", ""O'Neil's appointment as Wolves manager initially raised eyebrows, with many questioning whether he was the right fit for the job. However, the 40-year-old has quickly silenced his critics with his tactical acumen and ability to motivate his players."", ""Under his guidance, Wolves have shown promising signs of progress and a renewed sense of purpose on the pitch. It's no surprise envious eyes are looking in his direction."", ""While the prospect of managing a Premier League club with greater resources could be an enticing opportunity for O'Neil, he may just feel a loyalty to Wolves. "", ""After being let go by Bournemouth, O'Neil didn't think he'd land another Premier League job. By his own admission, he thought he might have to move down a level."", ""Given how uncertain management roles can be in football, I reckon O'Neil will aim to firmly establish himself as an astute and reliable head coach - and this means more than one season in the job!"", 'He is clearly loving his time with Wolves. You can see his connection with players, with fans and with the club. I strongly suspect that Gary will be offered a new, more lucrative contract with Wolves that will ensure his future with us.', 'Dazzling Dave can be found on Always Wolves fan TV, external ', 'Smitha Mundasad explores whether anything can be done to get rid of it', 'Surprising facts and interesting history from the makers of QI', ""'Stay-at-home-daughter' Chi is suddenly forced to fend for herself when her parents die"", 'Rob Brydon unpicks the complex character of Barry Humphries aka Dame Edna Everage and Sir Les Patterson', 'Â© 2024 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.']"
